{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e2283361-fa09-4bcd-9dbe-a665bda2c873",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a154926a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%set_env ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
        "%set_env LANGSMITH_TRACING=true\n",
        "%set_env LANGSMITH_API_KEY=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\n",
        "%set_env TAVILY_API_KEY=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\n",
        "\n",
        "\n",
        "%set_env OPENAI_API_KEY=${OPENAI_API_KEY}\n",
        "%set_env LANGCHAIN_API_KEY=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\n",
        "\n",
        "%set_env LANGCHAIN_TRACING_V2=true\n",
        "%set_env LANGCHAIN_PROJECT=\"Multi-agent Collaboration\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2ae034e9-1c05-47e1-aae2-6fb30701166a",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\"  #우리꺼\n",
        "# llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님\n",
        "llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "\n",
        "def ChatAnthropicSKT(model=\"skt/claude-3-5-sonnet-20241022\", max_tokens=100):\n",
        "    # llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\" #우리꺼 # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님  # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "    \n",
        "    llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "    \n",
        "    # llm_api_url = \"https://43.203.77.11:443/v1\"\n",
        "\n",
        "    # model = \"anthropic/claude-3-5-sonnet-20240620\"\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        openai_api_key=llm_api_key, \n",
        "        openai_api_base=llm_api_url, \n",
        "        model=model,\n",
        "        max_tokens=max_tokens\n",
        "        )\n",
        "    return model\n",
        "\n",
        "llm_cld35 = ChatAnthropicSKT()\n",
        "\n",
        "llm_cld37 = ChatAnthropic(\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "    model=\"claude-3-7-sonnet-20250219\",\n",
        "    max_tokens=3000\n",
        ")\n",
        "\n",
        "llm_chat = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        model=\"gpt-4o\",\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        max_tokens=2000,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1aecb3f7-22b9-4612-a6ec-a54c42482dd8",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \n",
        "    # json_str = broken_json.replace(\"'\",'\"')\n",
        "    \n",
        "    # Fix unquoted values (like NI00001863)\n",
        "    json_str = re.sub(r':\\s*([a-zA-Z0-9_]+)(\\s*[,}])', r': \"\\1\"\\2', broken_json)\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/a.x-3-lg\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n",
        "\n",
        "def convert_to_custom_format(json_items):\n",
        "    custom_format = []\n",
        "    \n",
        "    for item in json_items:\n",
        "        item_name = item.get(\"item_name_in_message\", \"\")\n",
        "        item_id = item.get(\"item_id\", \"\")\n",
        "        category = item.get(\"category\", \"\")\n",
        "        \n",
        "        # Create custom format for each item\n",
        "        custom_line = f\"[Item Name] {item_name} [Item ID] {item_id} [Item Category] {category}\"\n",
        "        custom_format.append(custom_line)\n",
        "    \n",
        "    return \"\\n\".join(custom_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "859204ed-4376-4b9b-b64f-71bfa6174257",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "\n",
        "class KoreanEntityMatcher:\n",
        "    def __init__(self, min_similarity=70, ngram_size=2, min_entity_length=2, token_similarity=True):\n",
        "        self.min_similarity = min_similarity\n",
        "        self.ngram_size = ngram_size\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.token_similarity = token_similarity  # 토큰 단위 유사도 비교 옵션 추가\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "    def build_from_list(self, entities):\n",
        "        \"\"\"Build entity index from a list of entities\"\"\"\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "        for i, entity in enumerate(entities):\n",
        "            if isinstance(entity, tuple) and len(entity) == 2:\n",
        "                entity_name, data = entity\n",
        "                self.entities.append(entity_name)\n",
        "                self.entity_data[entity_name] = data\n",
        "            else:\n",
        "                self.entities.append(entity)\n",
        "                self.entity_data[entity] = {'id': i, 'entity': entity}\n",
        "                \n",
        "        # 각 엔티티의 정규화된 형태를 저장 (검색 최적화)\n",
        "        self.normalized_entities = {}\n",
        "        for entity in self.entities:\n",
        "            normalized = self._normalize_text(entity)\n",
        "            self.normalized_entities[normalized] = entity\n",
        "                \n",
        "        # Create n-gram index for faster candidate selection\n",
        "        self._build_ngram_index(n=self.ngram_size)\n",
        "    \n",
        "    def _normalize_text(self, text):\n",
        "        \"\"\"텍스트 정규화 - 소문자 변환, 공백 제거 등\"\"\"\n",
        "        # 소문자로 변환\n",
        "        text = text.lower()\n",
        "        # 연속된 공백을 하나로 통일\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"텍스트를 토큰으로 분리 (한글, 영문, 숫자 분리)\"\"\"\n",
        "        # 한글, 영문, 숫자 토큰 추출\n",
        "        tokens = re.findall(r'[가-힣]+|[a-z0-9]+', self._normalize_text(text))\n",
        "        return tokens\n",
        "    \n",
        "    def _build_ngram_index(self, n=2):\n",
        "        \"\"\"Build n-gram index optimized for Korean characters\"\"\"\n",
        "        self.ngram_index = {}\n",
        "        \n",
        "        for entity in self.entities:\n",
        "            # Skip entities shorter than min_entity_length\n",
        "            if len(entity) < self.min_entity_length:\n",
        "                continue\n",
        "                \n",
        "            # 정규화된 엔티티 사용\n",
        "            normalized_entity = self._normalize_text(entity)\n",
        "            \n",
        "            # Create n-grams for the entity\n",
        "            entity_chars = list(normalized_entity)  # Split into characters for proper Korean handling\n",
        "            ngrams = []\n",
        "            \n",
        "            # Create character-level n-grams (better for Korean)\n",
        "            for i in range(len(entity_chars) - n + 1):\n",
        "                ngram = ''.join(entity_chars[i:i+n])\n",
        "                ngrams.append(ngram)\n",
        "            \n",
        "            # Add entity to the index for each n-gram\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram_index:\n",
        "                    self.ngram_index[ngram] = set()\n",
        "                self.ngram_index[ngram].add(entity)\n",
        "                \n",
        "            # 토큰 기반 n-gram도 추가 (실험적)\n",
        "            tokens = self._tokenize(normalized_entity)\n",
        "            for token in tokens:\n",
        "                if len(token) >= n:\n",
        "                    token_key = f\"TOKEN:{token}\"\n",
        "                    if token_key not in self.ngram_index:\n",
        "                        self.ngram_index[token_key] = set()\n",
        "                    self.ngram_index[token_key].add(entity)\n",
        "    \n",
        "    def _get_candidates(self, text, n=None):\n",
        "        \"\"\"Get candidate entities based on n-gram overlap (optimized for Korean)\"\"\"\n",
        "        if n is None:\n",
        "            n = self.ngram_size\n",
        "            \n",
        "        # 텍스트 정규화\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        \n",
        "        # 정규화된 텍스트가 정확히 일치하는지 확인 (빠른 경로)\n",
        "        if normalized_text in self.normalized_entities:\n",
        "            entity = self.normalized_entities[normalized_text]\n",
        "            return [(entity, float('inf'))]  # 정확한 일치는 무한대 점수로 표시\n",
        "        \n",
        "        text_chars = list(normalized_text)  # Split into characters for proper Korean handling\n",
        "        text_ngrams = set()\n",
        "        \n",
        "        # Create character-level n-grams\n",
        "        for i in range(len(text_chars) - n + 1):\n",
        "            ngram = ''.join(text_chars[i:i+n])\n",
        "            text_ngrams.add(ngram)\n",
        "        \n",
        "        # 토큰 기반 n-gram 추가\n",
        "        tokens = self._tokenize(normalized_text)\n",
        "        for token in tokens:\n",
        "            if len(token) >= n:\n",
        "                text_ngrams.add(f\"TOKEN:{token}\")\n",
        "        \n",
        "        candidates = set()\n",
        "        for ngram in text_ngrams:\n",
        "            if ngram in self.ngram_index:\n",
        "                candidates.update(self.ngram_index[ngram])\n",
        "        \n",
        "        # Prioritize candidates with multiple n-gram matches\n",
        "        candidate_scores = {}\n",
        "        for candidate in candidates:\n",
        "            candidate_normalized = self._normalize_text(candidate)\n",
        "            candidate_chars = list(candidate_normalized)\n",
        "            candidate_ngrams = set()\n",
        "            \n",
        "            # 문자 n-gram\n",
        "            for i in range(len(candidate_chars) - n + 1):\n",
        "                ngram = ''.join(candidate_chars[i:i+n])\n",
        "                candidate_ngrams.add(ngram)\n",
        "            \n",
        "            # 토큰 기반 n-gram\n",
        "            candidate_tokens = self._tokenize(candidate_normalized)\n",
        "            for token in candidate_tokens:\n",
        "                if len(token) >= n:\n",
        "                    candidate_ngrams.add(f\"TOKEN:{token}\")\n",
        "            \n",
        "            # n-gram 교집합 크기로 초기 점수 계산\n",
        "            overlap = len(candidate_ngrams.intersection(text_ngrams))\n",
        "            \n",
        "            # 토큰 수준 유사도 보너스 점수 추가\n",
        "            token_bonus = 0\n",
        "            if self.token_similarity:\n",
        "                query_tokens = set(tokens)\n",
        "                cand_tokens = set(candidate_tokens)\n",
        "                \n",
        "                # 공통 토큰 비율 계산\n",
        "                if query_tokens and cand_tokens:\n",
        "                    common = query_tokens.intersection(cand_tokens)\n",
        "                    token_bonus = len(common) * 2  # 토큰 일치에 높은 가중치 부여\n",
        "            \n",
        "            candidate_scores[candidate] = overlap + token_bonus\n",
        "        \n",
        "        # Return candidates sorted by n-gram overlap score\n",
        "        return sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    def _calculate_similarity(self, text, entity):\n",
        "        \"\"\"다양한 유사도 측정 방법을 결합하여 더 정확한 유사도 계산\"\"\"\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        normalized_entity = self._normalize_text(entity)\n",
        "        \n",
        "        # 정확히 일치하면 100점 반환\n",
        "        if normalized_text == normalized_entity:\n",
        "            return 100\n",
        "        \n",
        "        # 기본 문자열 유사도 (fuzz.ratio)\n",
        "        ratio_score = fuzz.ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 부분 문자열 체크 (한 문자열이 다른 문자열의 부분 문자열인 경우)\n",
        "        partial_score = 0\n",
        "        if normalized_text in normalized_entity:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (text_len / entity_len) * 100 if entity_len > 0 else 0\n",
        "        elif normalized_entity in normalized_text:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (entity_len / text_len) * 100 if text_len > 0 else 0\n",
        "        \n",
        "        # 토큰 유사도 (토큰 단위로 비교)\n",
        "        token_score = 0\n",
        "        if self.token_similarity:\n",
        "            text_tokens = set(self._tokenize(normalized_text))\n",
        "            entity_tokens = set(self._tokenize(normalized_entity))\n",
        "            \n",
        "            if text_tokens and entity_tokens:\n",
        "                common_tokens = text_tokens.intersection(entity_tokens)\n",
        "                all_tokens = text_tokens.union(entity_tokens)\n",
        "                \n",
        "                if all_tokens:\n",
        "                    token_score = (len(common_tokens) / len(all_tokens)) * 100\n",
        "        \n",
        "        # 토큰 순서 무시 유사도 (fuzz.token_sort_ratio)\n",
        "        token_sort_score = fuzz.token_sort_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 토큰 집합 유사도 (fuzz.token_set_ratio)\n",
        "        token_set_score = fuzz.token_set_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 최종 유사도는 여러 점수의 가중 평균\n",
        "        # 토큰 유사도에 높은 가중치를 부여하여 \"T우주\"와 \"T 우주패스\"의 매칭 향상\n",
        "        final_score = (\n",
        "            ratio_score * 0.3 +  # 기본 유사도\n",
        "            max(partial_score, 0) * 0.1 +  # 부분 문자열 유사도\n",
        "            token_score * 0.2 +  # 토큰 유사도\n",
        "            token_sort_score * 0.2 +  # 토큰 순서 무시 유사도\n",
        "            token_set_score * 0.2  # 토큰 집합 유사도\n",
        "        )\n",
        "        \n",
        "        return final_score\n",
        "    \n",
        "    def find_entities(self, text, max_candidates_per_span=10):\n",
        "        \"\"\"Find entity matches in Korean text using fuzzy matching\"\"\"\n",
        "        # Extract spans that might contain entities\n",
        "        potential_spans = self._extract_korean_spans(text)\n",
        "        matches = []\n",
        "        \n",
        "        for span_text, start, end in potential_spans:\n",
        "            if len(span_text.strip()) < self.min_entity_length:  # Skip spans shorter than min_entity_length\n",
        "                continue\n",
        "\n",
        "            # Get candidate entities based on n-gram overlap\n",
        "            candidates = self._get_candidates(span_text)\n",
        "\n",
        "            # If no candidates found through n-gram filtering, skip\n",
        "            if not candidates:\n",
        "                continue\n",
        "            \n",
        "            # Limit the number of candidates to check\n",
        "            top_candidates = [c[0] for c in candidates[:max_candidates_per_span]]\n",
        "            \n",
        "            # 각 후보 엔티티에 대해 개선된 유사도 계산\n",
        "            scored_matches = []\n",
        "            for entity in top_candidates:\n",
        "                score = self._calculate_similarity(span_text, entity)\n",
        "                \n",
        "                if score >= self.min_similarity:\n",
        "                    scored_matches.append((entity, score, 0))  # 호환성을 위해 3번째 매개변수 추가\n",
        "\n",
        "            # 기존 process.extract 대신 개선된 유사도 계산 사용\n",
        "            best_matches = scored_matches\n",
        "\n",
        "            for entity, score, _ in best_matches:\n",
        "                matches.append({\n",
        "                    'text': span_text,\n",
        "                    'matched_entity': entity,\n",
        "                    'score': score,\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'data': self.entity_data.get(entity, {})\n",
        "                })\n",
        "\n",
        "        # Sort by position in text\n",
        "        matches.sort(key=lambda x: (x['start'], -x['score']))\n",
        "        \n",
        "        # Handle overlapping matches by keeping the best match\n",
        "        final_matches = self._resolve_overlapping_matches(matches)\n",
        "\n",
        "        return final_matches\n",
        "    \n",
        "    def _extract_korean_spans(self, text):\n",
        "        \"\"\"한국어와 영어가 혼합된 텍스트에서 엔티티일 수 있는 잠재적 텍스트 범위 추출\"\"\"\n",
        "        spans = []\n",
        "        min_len = self.min_entity_length\n",
        "        \n",
        "        # 1. 영문+한글 혼합 패턴 (붙여쓰기) 예: \"T우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 2. 영문+한글 혼합 패턴 (띄어쓰기) 예: \"T 우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+\\s+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 3. 특수한 혼합 패턴 - 영문+한글+숫자 (예: \"T우주365\", \"SK텔레콤\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:[0-9]+)?', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 4. 연속된 두 단어까지 확장 (예: \"T우주 패스\")\n",
        "        # 영문+한글 후 공백 하나를 두고 다른 한글 단어가 나오는 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "                \n",
        "        # 5. 연속된 세 단어까지 확장 (예: \"T우주 멤버십 패스\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 6. 브랜드명 + 제품명 패턴 (예: \"SK텔레콤 T우주\")\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣]+(?:\\s+[a-zA-Z가-힣]+){1,3}', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 7. 숫자와 영문 결합 패턴 (숫자 space 영문 패턴, e.g. \"0 day\")\n",
        "        for match in re.finditer(r'\\d+\\s+[a-zA-Z]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 8. 더 일반적인 영한 혼합 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣0-9]+(?:\\s+[a-zA-Z가-힣0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 9. 일반적인 구분자로 분리된 텍스트 조각도 추출\n",
        "        for span in re.split(r'[,\\.!?;:\"\\'…\\(\\)\\[\\]\\{\\}\\s_/]+', text):\n",
        "            if span and len(span) >= min_len:\n",
        "                span_pos = text.find(span)\n",
        "                if span_pos != -1:\n",
        "                    spans.append((span, span_pos, span_pos + len(span)))\n",
        "                \n",
        "        return spans\n",
        "    \n",
        "    def _remove_duplicate_entities(self, matches):\n",
        "        \"\"\"Keep only one instance of each unique entity\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Dictionary to track highest-scoring match for each entity\n",
        "        best_matches = {}\n",
        "        \n",
        "        for match in matches:\n",
        "            entity_key = match['matched_entity']\n",
        "            \n",
        "            # If we haven't seen this entity before, or if this match has a higher score\n",
        "            # than the previously saved match for this entity, save this one\n",
        "            if (entity_key not in best_matches or \n",
        "                match['score'] > best_matches[entity_key]['score']):\n",
        "                best_matches[entity_key] = match\n",
        "        \n",
        "        # Return the best matches sorted by start position\n",
        "        return sorted(best_matches.values(), key=lambda x: x['start'])\n",
        "    \n",
        "    def _resolve_overlapping_matches(self, matches, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "        \"\"\"\n",
        "        개선된 중복 매치 해결 - 점수 기반으로 매치들을 유지하거나 제거\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        matches : list\n",
        "            매치 목록\n",
        "        high_score_threshold : int, default=50\n",
        "            이 점수 이상이면 중복/포함 관계에서도 별도 매치로 고려할 임계값\n",
        "        overlap_tolerance : float, default=0.5\n",
        "            중복 허용 범위 (0~1), 값이 클수록 더 많은 중복을 허용\n",
        "        \"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # 점수 내림차순, 길이 오름차순으로 정렬 (높은 점수, 짧은 매치 우선)\n",
        "        sorted_matches = sorted(matches, key=lambda x: (-x['score'], x['end'] - x['start']))\n",
        "        \n",
        "        final_matches = []\n",
        "        \n",
        "        for current_match in sorted_matches:\n",
        "            current_score = current_match['score']\n",
        "            current_start, current_end = current_match['start'], current_match['end']\n",
        "            current_range = set(range(current_start, current_end))\n",
        "            current_len = len(current_range)\n",
        "            \n",
        "            # 높은 점수의 매치는 항상 포함\n",
        "            if current_score >= high_score_threshold:\n",
        "                # 기존 매치들과 비교하여 너무 많은 중복이 있는지 확인\n",
        "                is_too_similar = False\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    if existing_match['score'] < high_score_threshold:\n",
        "                        continue  # 낮은 점수의 기존 매치와는 비교하지 않음\n",
        "                        \n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    \n",
        "                    # 현재 매치에 대한 중복 비율\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하고, 동일한 엔티티가 아니면 추가하지 않음\n",
        "                    if (current_overlap_ratio > overlap_tolerance and \n",
        "                        current_match['matched_entity'] == existing_match['matched_entity']):\n",
        "                        is_too_similar = True\n",
        "                        break\n",
        "                \n",
        "                if not is_too_similar:\n",
        "                    final_matches.append(current_match)\n",
        "            else:\n",
        "                # 낮은 점수의 매치는 기존 로직 적용 (중복 확인)\n",
        "                should_add = True\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하면 추가하지 않음\n",
        "                    if current_overlap_ratio > (1 - overlap_tolerance):\n",
        "                        should_add = False\n",
        "                        break\n",
        "                \n",
        "                if should_add:\n",
        "                    final_matches.append(current_match)\n",
        "        \n",
        "        # 시작 위치별로 정렬\n",
        "        final_matches.sort(key=lambda x: x['start'])\n",
        "        \n",
        "        return final_matches\n",
        "\n",
        "def find_entities_in_text(text, entity_list, min_similarity=70, ngram_size=3, min_entity_length=2, \n",
        "                         token_similarity=True, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "    \"\"\"\n",
        "    Find entity matches in text using fuzzy matching.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The text to search for entities\n",
        "    entity_list : list\n",
        "        List of entities to match against\n",
        "    min_similarity : int, default=70\n",
        "        Minimum similarity score (0-100) for fuzzy matching\n",
        "    ngram_size : int, default=2\n",
        "        Size of character n-grams to use for indexing (2 or 3 recommended for Korean)\n",
        "    min_entity_length : int, default=2\n",
        "        Minimum length of entities to consider (characters)\n",
        "    token_similarity : bool, default=True\n",
        "        Whether to use token-based similarity measures\n",
        "    high_score_threshold : int, default=50\n",
        "        Score threshold above which matches are always kept regardless of overlap\n",
        "    overlap_tolerance : float, default=0.5\n",
        "        Overlap tolerance ratio (0-1), higher values allow more overlapping matches\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        List of matched entities with position and metadata\n",
        "    \"\"\"\n",
        "    matcher = KoreanEntityMatcher(\n",
        "        min_similarity=min_similarity,\n",
        "        ngram_size=ngram_size,\n",
        "        min_entity_length=min_entity_length,\n",
        "        token_similarity=token_similarity\n",
        "    )\n",
        "    matcher.build_from_list(entity_list)\n",
        "    \n",
        "    matches = matcher.find_entities(text)\n",
        "    \n",
        "    # 기존 _resolve_overlapping_matches 메서드 대신 직접 호출\n",
        "    final_matches = matcher._resolve_overlapping_matches(\n",
        "        matches, \n",
        "        high_score_threshold=high_score_threshold,\n",
        "        overlap_tolerance=overlap_tolerance\n",
        "    )\n",
        "    \n",
        "    return final_matches\n",
        "# Function to highlight entities in text\n",
        "def highlight_entities(text, matches):\n",
        "    marked_text = text\n",
        "    offset = 0\n",
        "    for match in sorted(matches, key=lambda x: x['start'], reverse=True):\n",
        "        start = match['start'] + offset\n",
        "        end = match['end'] + offset\n",
        "        entity = match['matched_entity']\n",
        "        score = match['score']\n",
        "        marked_text = marked_text[:start] + f\"[{marked_text[start:end]}→{entity} ({score:.1f}%)]\" + marked_text[end:]\n",
        "        offset += len(f\"[→{entity} ({score:.1f}%)]\") + 2\n",
        "    \n",
        "    return marked_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0655b0b0-b7ec-4627-a3bc-ad8ece7cb65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def calculate_list_similarity(list1, list2):\n",
        "    # Handle cases where input might be a dictionary or other types\n",
        "    if isinstance(list1, dict):\n",
        "        list1 = [str(item) for item in list1.values()]\n",
        "    if isinstance(list2, dict):\n",
        "        list2 = [str(item) for item in list2.values()]\n",
        "    \n",
        "    # Ensure lists contain strings\n",
        "    list1 = [str(item) for item in list1]\n",
        "    list2 = [str(item) for item in list2]\n",
        "    \n",
        "    # Convert lists to sets for comparison\n",
        "    set1 = set(sorted(set(list1)))\n",
        "    set2 = set(sorted(set(list2)))\n",
        "    \n",
        "    # Calculate Jaccard similarity\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    \n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def calculate_text_similarity(text1, text2):\n",
        "    return SequenceMatcher(None, str(text1), str(text2)).ratio()\n",
        "\n",
        "def calculate_message_info_similarity(minfo1, minfo2):\n",
        "    if not isinstance(minfo1, dict) or not isinstance(minfo2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    sender_sim = calculate_text_similarity(minfo1.get('sender',''), minfo2.get('sender',''))\n",
        "    title_sim = calculate_text_similarity(minfo1.get('title',''), minfo2.get('title',''))\n",
        "    main_theme_sim = calculate_text_similarity(minfo1.get('main_theme',''), minfo2.get('main_theme',''))\n",
        "    period_sim = calculate_text_similarity(minfo1.get('period',''), minfo2.get('period',''))\n",
        "    \n",
        "    return (sender_sim + title_sim + main_theme_sim + period_sim) / 4\n",
        "\n",
        "def calculate_metadata_similarity(minfo1, minfo2):\n",
        "    if not isinstance(minfo1, dict) or not isinstance(minfo2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    campaign_id_sim = calculate_text_similarity(minfo1.get('campaign_id',''), minfo2.get('campaign_id',''))\n",
        "    message_type_sim = calculate_text_similarity(minfo1.get('message_type',''), minfo2.get('message_type',''))\n",
        "    target_response_sim = calculate_text_similarity(minfo1.get('target_response',''), minfo2.get('target_response',''))\n",
        "    \n",
        "    success_metrics_similarity = calculate_list_similarity(\n",
        "        minfo1.get('success_metrics',[]), \n",
        "        minfo2.get('success_metrics',[])\n",
        "    )\n",
        "    \n",
        "    return (campaign_id_sim + message_type_sim + target_response_sim + success_metrics_similarity) / 4\n",
        "\n",
        "\n",
        "def calculate_product_similarity(prod1, prod2):\n",
        "    if not isinstance(prod1, dict) or not isinstance(prod2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    name_sim = calculate_text_similarity(prod1.get('name',''), prod2.get('name',''))\n",
        "    category_sim = calculate_text_similarity(prod1.get('category',''), prod2.get('category',''))\n",
        "    benefit_sim = calculate_text_similarity(prod1.get('benefit',''), prod2.get('benefit',''))\n",
        "    action_sim = calculate_text_similarity(prod1.get('action',''), prod2.get('action',''))\n",
        "    conditions_sim = calculate_text_similarity(prod1.get('conditions',''), prod2.get('conditions',''))\n",
        "    \n",
        "    return (name_sim + category_sim + benefit_sim + action_sim + conditions_sim) / 5\n",
        "\n",
        "def calculate_channel_similarity(chan1, chan2):\n",
        "    if not isinstance(chan1, dict) or not isinstance(chan2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    type_sim = calculate_text_similarity(chan1.get('type',''), chan2.get('type',''))\n",
        "    value_sim = calculate_text_similarity(chan1.get('value',''), chan2.get('value',''))\n",
        "    action_sim = calculate_text_similarity(chan1.get('action',''), chan2.get('action',''))\n",
        "    availability_sim = calculate_text_similarity(chan1.get('availability',''), chan2.get('availability',''))\n",
        "    \n",
        "    return (type_sim + value_sim + action_sim + availability_sim) / 4\n",
        "\n",
        "def calculate_target_similarity(target1, target2):\n",
        "    if not isinstance(target1, list) or not isinstance(target2, list):\n",
        "        return 0.0\n",
        "    \n",
        "    similarities = []\n",
        "    for t1 in target1:\n",
        "        for t2 in target2:\n",
        "            segment_sim = calculate_text_similarity(t1.get('segment',''), t2.get('segment',''))\n",
        "            characteristics_sim = calculate_text_similarity(t1.get('characteristics',''), t2.get('characteristics',''))\n",
        "            similarities.append((segment_sim + characteristics_sim) / 2)\n",
        "    \n",
        "    return max(similarities) if similarities else 0.0\n",
        "\n",
        "def calculate_dictionary_similarity(dict1, dict2):\n",
        "    # Calculate similarities for each component\n",
        "    message_info_similarity = calculate_message_info_similarity(\n",
        "        dict1.get('message_info',{}), \n",
        "        dict2.get('message_info',{})\n",
        "    )\n",
        "    \n",
        "    meta_similarity = calculate_metadata_similarity(\n",
        "        dict1.get('metadata',{}), \n",
        "        dict2.get('metadata',{})\n",
        "    )\n",
        "    \n",
        "    purpose_similarity = calculate_list_similarity(\n",
        "        dict1.get('purpose',[]), \n",
        "        dict2.get('purpose',[])\n",
        "    )\n",
        "    \n",
        "    target_similarity = calculate_target_similarity(\n",
        "        dict1.get('target',[]), \n",
        "        dict2.get('target',[])\n",
        "    )\n",
        "    \n",
        "    # Calculate product similarity\n",
        "    product_similarities = []\n",
        "    products1 = dict1.get('product', [])\n",
        "    products2 = dict2.get('product', [])\n",
        "    for p1, p2 in zip(products1, products2):\n",
        "        product_similarities.append(calculate_product_similarity(p1, p2))\n",
        "    product_similarity = sum(product_similarities) / len(product_similarities) if product_similarities else 0.0\n",
        "    \n",
        "    # Calculate channel similarity\n",
        "    channel_similarities = []\n",
        "    channels1 = dict1.get('channel', [])\n",
        "    channels2 = dict2.get('channel', [])\n",
        "    for c1, c2 in zip(channels1, channels2):\n",
        "        channel_similarities.append(calculate_channel_similarity(c1, c2))\n",
        "    channel_similarity = sum(channel_similarities) / len(channel_similarities) if channel_similarities else 0.0\n",
        "\n",
        "    # Calculate overall similarity\n",
        "    overall_similarity = (\n",
        "        message_info_similarity + \n",
        "        purpose_similarity + \n",
        "        target_similarity + \n",
        "        product_similarity + \n",
        "        channel_similarity +\n",
        "        meta_similarity\n",
        "    ) / 5\n",
        "    \n",
        "    return {\n",
        "        'overall_similarity': overall_similarity,\n",
        "        'message_info_similarity': message_info_similarity,\n",
        "        'purpose_similarity': purpose_similarity,\n",
        "        'target_similarity': target_similarity,\n",
        "        'product_similarity': product_similarity,\n",
        "        'channel_similarity': channel_similarity,\n",
        "        'meta_similarity': meta_similarity\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b275d1-1e26-40b4-93f4-898cc1345c22",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# mms_pdf = pd.read_excel(\"./data/mms_data_250408.xlsx\", engine=\"openpyxl\")\n",
        "mms_pdf = pd.read_csv(\"./data/mms_data_250408.csv\")\n",
        "\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "\n",
        "mms_pdf = mms_pdf.reset_index()\n",
        "\n",
        "mms_pdf = mms_pdf.astype('str')\n",
        "\n",
        "mms_pdf.head(1)\n",
        "# item_pdf = pd.read_pickle(\"/home/skinet/myfiles/tos_ace/data/item_info_250318.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "8ba8f0a7-090e-44df-bf2d-7bfd5189bdca",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# item_pdf_raw = pd.read_excel(\"./data/item_info_250401.xlsx\", engine=\"openpyxl\")\n",
        "item_pdf_raw = pd.read_csv(\"./data/item_info_250401.csv\")\n",
        "\n",
        "item_pdf = item_pdf_raw.drop_duplicates(['item_nm','item_desc']).copy()\n",
        "item_pdf['item_item'] = item_pdf['item_nm']+\"\\n\"+item_pdf['item_desc']\n",
        "\n",
        "item_pdf['item_nm_cl'] = item_pdf['item_nm'].apply(clean_text)\n",
        "item_pdf['item_desc_cl'] = item_pdf['item_desc'].fillna('').astype(str).apply(clean_text)\n",
        "item_pdf['item_item_cl'] = item_pdf['item_nm_cl']+\"\\n\"+item_pdf['item_desc_cl']\n",
        "\n",
        "entity_list = []\n",
        "for row in item_pdf.to_dict('records'):\n",
        "    entity_list.append((row['item_nm'], {'item_id':row['item_id'],'category':row['item_cate_ax'], 'description':row['item_desc'], 'create_dt':row['create_dt']}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319db51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import konlpy.tag\n",
        "\n",
        "# For Korean text preprocessing, use KoNLPy\n",
        "class KoreanTokenizer:\n",
        "    def __init__(self, tagger_type='Okt'):\n",
        "        # Choose a Korean morphological analyzer\n",
        "        # Options: Okt, Mecab, Komoran, Hannanum, Kkma\n",
        "        if tagger_type == 'Okt':\n",
        "            from konlpy.tag import Okt\n",
        "            self.tagger = Okt()\n",
        "        elif tagger_type == 'Mecab':\n",
        "            from konlpy.tag import Mecab\n",
        "            self.tagger = Mecab()\n",
        "        # Add other taggers as needed\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        # Extract nouns, adjectives, and verbs (you can customize this)\n",
        "        tokens = self.tagger.morphs(text)\n",
        "        return tokens\n",
        "\n",
        "# Initialize the tokenizer (Okt is generally good for most purposes)\n",
        "korean_tokenizer = KoreanTokenizer(tagger_type='Okt')\n",
        "\n",
        "tfidf_name = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(3,10),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "tfidf_cate = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2,4),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "tfidf_desc = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(3,5),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "# tfidf_desc = TfidfVectorizer(\n",
        "#             analyzer='word',\n",
        "#             ngram_range=(1,5),\n",
        "#             min_df=1,\n",
        "#             stop_words=None,  # Don't use stop words initially\n",
        "#             token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "#             strip_accents='unicode',  # Handle accented characters\n",
        "#         )\n",
        "\n",
        "# tfidf_name = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "# tfidf_desc = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "# tfidf_item = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "fs_model = \"gpt\"\n",
        "# few_shot_pdf = pd.read_pickle(\"./data/few_shot_data_chat_250410_rd_500.pkl\")\n",
        "few_shot_pdf = pd.read_csv(f\"./data/few_shot_data_{fs_model}_250415_rd_500.csv\")\n",
        "few_shot_pdf = few_shot_pdf.astype('str')\n",
        "\n",
        "tfidf_matrix_name = tfidf_name.fit_transform(item_pdf['item_nm_cl'])\n",
        "tfidf_matrix_cate = tfidf_cate.fit_transform(item_pdf['item_cate_ax'])\n",
        "tfidf_matrix_desc = tfidf_desc.fit_transform(few_shot_pdf[\"msg_body\"])\n",
        "# tfidf_matrix_item = tfidf_item.fit_transform(item_pdf['item_item_cl'])\n",
        "\n",
        "# import torch\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# emb_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')  # Or other appropriate model\n",
        "# emb_model = emb_model.to(device)\n",
        "# item_all = item_pdf['item_item_cl'].tolist()\n",
        "# item_embeddings = emb_model.encode(item_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e3cf3f-68fe-4abd-b6f1-7457493f88d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "아래 광고 메세지의 구조 분석해라.\n",
        "분석의 목적은 메세지에서 구조화된 정보를 추출하기 위함이다.\n",
        "다양한 구조의 다른 메세지들도 분석할 예정인데, 이를 위한 prompt에 추가할 JSON이 아래와 같은 데, 개선점을 파악해서, 새로운 JSON을 추천해라.\n",
        "===광고 메세지 예시===\n",
        "{mms_pdf['msg'][:100]}\n",
        "===PROMPT에 넣을 JSON===\n",
        "{schema_cld}\n",
        "\"\"\"\n",
        "\n",
        "cot_list = []\n",
        "for row in few_shot_pdf.to_dict('records'):\n",
        "    prompt = f\"\"\"\n",
        "    당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 매칭되는 JSON 개체를 만들기 위한 Chain-of-Thought를 만들어주세요.\n",
        "    당신이 제공한 Chain-of-Thought는 다른 LLM이 유사한 광고 메세지에서 구조화된 정보를 추출하는 데 사용됩니다.\n",
        "    다른 LLM이 이 Chain-of-Thought를 사용하여 광고 메세지에서 구조화된 정보를 추출할 수 있도록, 예시 광고 메세지의 특징을 파악해야 하지만, 그렇다고 너무 의존적이어서는 안됩니다. \n",
        "    왜나허면, 동일한 메세지 분석에 사용되는 것이 아니기 때문입니다.\n",
        "    Chain-of-Thought는 한국어로 작성하고, 다른 LLM에 prompt로 바로 사용될 수 있도록 hain-of-Thought 결과만을 제공해 주세요.\n",
        "    \n",
        "    ===광고 메세지 예시===\n",
        "    메세지 제목: {row['msg_head']}\n",
        "    메세지 내용: {row['msg_body']}\n",
        "\n",
        "    ===결과 Schema===\n",
        "    {schema_cld}\n",
        "    \"\"\"\n",
        "    row['cot_chat'] = (llm_chat.invoke(prompt).content)\n",
        "\n",
        "    cot_list.append(row)\n",
        "\n",
        "    if len(cot_list) % 10 == 0:\n",
        "        print(f\"Processed {len(cot_list)} rows\")\n",
        "\n",
        "    # print((llm_chat.invoke(prompt).content))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be43b81-d653-482e-87c3-8bbe4e5a8951",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Define your schema\n",
        "schema_cld = {'properties': {\n",
        "    'message_info': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'title': {'type': 'string', 'description': '광고 제목 - 원본 그대로 추출'},\n",
        "            'main_theme': {'type': 'string', 'description': '광고의 핵심 주제와 가치 제안을 명확하게 설명'},\n",
        "            'period': {'type': 'string', 'description': '이벤트/프로모션 기간 - 명시적으로 언급된 경우에만 추출, 없으면 \"상시\"로 설정'}\n",
        "        }\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'target': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'segment': {'type': 'string', 'description': '타겟 고객층 - 광고에서 명시적으로 언급하거나 암시한 대상 고객'},\n",
        "                'characteristics': {'type': 'string', 'description': '해당 타겟의 특성과 니즈'},\n",
        "                'priority': {'type': 'integer', 'description': '광고 내용에서의 중요도에 따른 타겟팅 우선순위 (1이 최우선)'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array',\n",
        "        'items': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "            'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름 - \"상품 후보 정보\" 목록에서 일치하는 항목 선택'},\n",
        "            'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "            'category': {'type': 'string', 'description': '제품/서비스 카테고리 - \"상품 후보 정보\"에서 확인'},\n",
        "            'benefit': {\n",
        "                'type': 'array',\n",
        "                'items': {\n",
        "                'type': 'object',\n",
        "                'properties': {\n",
        "                    'name': {'type': 'string', 'description': '제공되는 구체적인 혜택 내용 상세 설명'},\n",
        "                    'type': {'type': 'string', 'description': '혜택 유형: [할인, 쿠폰, 무료체험, 포인트, 사은품, 기타] 중에서 선택'},\n",
        "                    'target_audience': {'type': 'string', 'description': '혜택이 제공되는 특정 대상 (예: 만 13~34세, T멤버십 고객 등)'},\n",
        "                    'valid_period': {'type': 'string', 'description': '혜택 유효 기간 (있는 경우)'}\n",
        "                }\n",
        "                }\n",
        "            },\n",
        "            'conditions': {\n",
        "                'type': 'array',\n",
        "                'items': {\n",
        "                'type': 'object',\n",
        "                'properties': {\n",
        "                    'description': {'type': 'string', 'description': '혜택/구매를 받기 위한 구체적인 조건'},\n",
        "                    'type': {'type': 'string', 'description': '조건 유형: [약정기간, 요금제가입, 결합상품, 선착순, 카드결제, 기타] 중에서 선택'}\n",
        "                }\n",
        "                }\n",
        "            },\n",
        "            'related_products': {\n",
        "                'type': 'array',\n",
        "                'items': {\n",
        "                'type': 'object',\n",
        "                'properties': {\n",
        "                    'name': {'type': 'string', 'description': '주요 상품과 연계된 관련 상품/서비스 이름'},\n",
        "                    'relationship_type': {'type': 'string', 'description': '관계 유형: [결합상품, 부가서비스, 제휴서비스, 앱서비스, 기타] 중에서 선택'}\n",
        "                }\n",
        "                }\n",
        "            },\n",
        "            'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'metadata': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'message_type': {'type': 'string', 'description': '메시지 유형 - 항상 \"광고\"로 설정'},\n",
        "            'target_response': {'type': 'string', 'description': '광고주가 고객으로부터 기대하는 반응 요약'},\n",
        "            'success_metrics': {'type': 'array', 'description': '이 캠페인의 성공을 측정하기 위한 지표들 추출'}\n",
        "        }\n",
        "    }\n",
        "}, \n",
        "'required': ['purpose', 'target', 'product', 'channel', 'metadata'], \n",
        "'objectType': 'object'}\n",
        "\n",
        "# Improved extraction guidance\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 주요 포인트 ###\n",
        "1. 전체 메시지 분석: 먼저 광고 메시지를 전체적으로 읽고 주요 내용, 목적, 혜택, 타겟을 종합적으로 파악하세요.\n",
        "\n",
        "2. 상품 식별 및 관계 분석: \n",
        "   - \"상품 후보 정보\" 목록을 확인하여 광고에 언급된 제품/서비스와 일치하는 항목을 모두 선택하세요.\n",
        "   - 광고에 여러 상품이 포함된 경우 각 상품별로 별도 객체를 생성하고 related_products에 관계를 명시하세요.\n",
        "\n",
        "3. 혜택 및 조건 분석: \n",
        "   - 각 상품의 다양한 혜택을 benefit 배열에 추가하고 대상 고객층과 유효기간을 명시하세요.\n",
        "   - 각 제품별 혜택과 조건을 명확하게 구분하여 추출하고, 혜택을 받기 위한 조건들을 conditions 배열에 추가하세요.\n",
        "\n",
        "4. 채널 정보 분석: \n",
        "   - 모든 연락 채널(URL, 전화번호, 대리점 등)을 channel 배열에 빠짐없이 추가하세요.\n",
        "   - tworldfriends.co.kr URL이 있는 경우 반드시 매장 코드(D+숫자 패턴)를 추출하여 store_code에 포함하세요.\n",
        "\n",
        "5. 타겟 고객층 분석: 명시적으로 언급된 타겟 고객층을 target 배열에 우선순위대로 추가하세요.\n",
        "\n",
        "6. 정확한 정보 우선: \n",
        "   - 각 필드에 적합한 값을 선택할 때 광고 내용에 명시적으로 언급된 정보를 우선하고, 없는 경우에만 적절히 추론하세요.\n",
        "   - 결과 JSON은 완전하고 정확해야 하며, schema에 정의된 모든 필수 필드를 포함해야 합니다.\n",
        "\n",
        "7. 학습용 예시를 참고 정보로 사용하지 마세요.\n",
        "\n",
        "### 응답 형식 ###\n",
        "순수 JSON만 반환하세요. 설명이나 추가 텍스트는 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Custom parser to extract JSON from text response\n",
        "class CustomJsonOutputParser(JsonOutputParser):\n",
        "    def parse(self, text):\n",
        "        # Try to find JSON in the response using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                raise OutputParserException(f\"Failed to parse JSON: {json_str}\")\n",
        "        else:\n",
        "            raise OutputParserException(f\"Could not find JSON in response: {text}\")\n",
        "\n",
        "# Create a prompt template with improved system message - emphasize JSON-only output\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{schema}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "### 학습용 예시 ###\n",
        "{few_shot_exm_str}\n",
        "\n",
        "이제 다음 광고 메시지를 분석하여 schema에 맞는 완전한 JSON 객체를 생성해 주세요.\n",
        "중요: 응답은 설명이나 추가 텍스트 없이 순수한 JSON 형식으로만 제공하세요. 응답은 '{{'로 시작하고 '}}'로 끝나야 합니다.\"\"\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Create the custom parser\n",
        "parser = CustomJsonOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# Wrapper function to handle parsing failures\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"Extract JSON from LLM response text even if it has explanatory text\"\"\"\n",
        "    try:\n",
        "        # First try to parse as pure JSON\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If that fails, try to extract JSON using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', response_text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Failed to parse extracted JSON pattern: {json_str}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Could not find JSON pattern in response: {response_text}\")\n",
        "            return None\n",
        "\n",
        "# Run the extraction with error handling\n",
        "msg_text = \"\"\"\n",
        "### 분석할 메시지 ###\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\"\n",
        "\n",
        "# Your existing code for similarities calculation and examples...\n",
        "similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "# Assign similarities to dataframe\n",
        "few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "few_shot_temp['sim'] = similarities\n",
        "few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "# Get examples with better structure - make sure each example shows JSON only in the result\n",
        "few_shot_exm = []\n",
        "top_examples = few_shot_temp[\n",
        "    (few_shot_temp['rank'] >= 1) & \n",
        "    (few_shot_temp['rank'] <= 2)\n",
        "].sort_values(\"rank\")\n",
        "\n",
        "# fs_model = \"gpt\"\n",
        "for _, r in top_examples.iterrows():\n",
        "    # Ensure the examples show proper JSON-only outputs\n",
        "    example_json = r[f'res_{fs_model}']\n",
        "    \n",
        "    # If the example contains explanatory text, try to extract just the JSON\n",
        "    if not (example_json.startswith('{') and example_json.endswith('}')):\n",
        "        json_match = re.search(r'({[\\s\\S]*})', example_json)\n",
        "        if json_match:\n",
        "            example_json = json_match.group(1)\n",
        "    \n",
        "    few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지_{_}]\n",
        "\\t광고 제목:{r['msg_head']}\n",
        "\\t광고 내용:{r['msg_body']}\n",
        "[학습용 정답 결과_{_}]\n",
        "\\t{example_json}\"\"\")\n",
        "\n",
        "few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "\n",
        "# Enhanced entity matching\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list, \n",
        "    min_similarity=65,\n",
        "    ngram_size=3,\n",
        "    min_entity_length=2\n",
        ")\n",
        "\n",
        "product_info = \", \".join(set([match['text'] for match in matches]))\n",
        "\n",
        "\n",
        "stop_item_names = ['SK텔레콤','혜택안내','수신거부','멤버십','150','고객센터']\n",
        "mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "\n",
        "rag_context = f\"### 상품 정보 매핑 ###\\n{product_info}\"\n",
        "\n",
        "\n",
        "# Include extraction guide in prompt\n",
        "prompt_text = prompt.format(\n",
        "    text=msg_text, \n",
        "    schema=str(schema_cld), \n",
        "    rag_context=rag_context, \n",
        "    few_shot_exm_str=few_shot_exm_str,\n",
        "    extraction_guide=extraction_guide\n",
        ")\n",
        "print(\"Prompt being sent to LLM:\")\n",
        "print(prompt_text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Use the extraction chain with the custom parser\n",
        "    result_cld = extraction_chain.invoke({\n",
        "        \"text\": msg_text, \n",
        "        \"schema\": str(schema_cld), \n",
        "        \"rag_context\": rag_context, \n",
        "        \"few_shot_exm_str\": few_shot_exm_str,\n",
        "        \"extraction_guide\": extraction_guide\n",
        "    })\n",
        "    print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "except Exception as e:\n",
        "    print(f\"Error with extraction chain: {e}\")\n",
        "    \n",
        "    # Get direct response from LLM without parser\n",
        "    raw_response = llm_cld35.invoke(prompt_text)\n",
        "    \n",
        "    # Extract JSON from the raw response\n",
        "    result_cld = extract_json_from_response(raw_response)\n",
        "    if result_cld:\n",
        "        print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "    else:\n",
        "        print(\"Failed to extract valid JSON from the response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dac51f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Define your schema\n",
        "schema_prd = {\n",
        "  'properties': {\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'product': {\n",
        "      'type': 'array',\n",
        "      'items': {\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "          'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름 - \"상품 후보 정보\" 목록에서 일치하는 항목 모두 선택'},\n",
        "          'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "          'category': {'type': 'string', 'description': '제품/서비스 카테고리 - \"상품 후보 정보\"에서 확인'},\n",
        "          'benefit': {\n",
        "            'type': 'array',\n",
        "            'items': {\n",
        "              'type': 'object',\n",
        "              'properties': {\n",
        "                'name': {'type': 'string', 'description': '제공되는 구체적인 혜택 내용 상세 설명'},\n",
        "                'type': {'type': 'string', 'description': '혜택 유형: [할인, 쿠폰, 경품, 포인트, 무료체험, 사은품, 기타] 중에서 선택'},\n",
        "                'target_audience': {'type': 'string', 'description': '혜택이 제공되는 특정 대상 (예: 만 13~34세, T멤버십 고객, 장기 우수 고객 등)'},\n",
        "                'valid_period': {'type': 'string', 'description': '혜택 유효 기간이나 이벤트 기간 (있는 경우)'}\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          'conditions': {\n",
        "            'type': 'array',\n",
        "            'items': {\n",
        "              'type': 'object',\n",
        "              'properties': {\n",
        "                'description': {'type': 'string', 'description': '혜택/구매를 받기 위한 구체적인 조건'},\n",
        "                'type': {'type': 'string', 'description': '조건 유형: [약정기간, 요금제가입, 결합상품, 선착순, 카드결제, 기타] 중에서 선택'}\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          'related_products': {\n",
        "            'type': 'array',\n",
        "            'items': {\n",
        "              'type': 'object',\n",
        "              'properties': {\n",
        "                'name': {'type': 'string', 'description': '주요 상품과 연계된 관련 상품/서비스 이름'},\n",
        "                'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "                'relationship_type': {'type': 'string', 'description': '관계 유형: [결합상품, 부가서비스, 제휴서비스, 앱서비스, 기타] 중에서 선택'}\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'}\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  'objectType': 'object'\n",
        "}\n",
        "\n",
        "\n",
        "# Improved extraction guidance\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 시 고려사항 ###\n",
        "1. 하나의 광고에 여러 상품이 포함될 수 있으며, 각 상품별로 별도 객체 생성\n",
        "2. 특정 상품에 다양한 혜택이 제공될 수 있으며, 각 혜택은 별도 benefit 항목으로 추출\n",
        "3. 서로 다른 상품 간 관계가 있을 경우(결합상품, 앱서비스 등) related_products에 명시\n",
        "4. 혜택을 받기 위한 다양한 조건들을 모두 conditions 배열에 포함\n",
        "5. 대상 고객층이 특정된 경우(연령, 멤버십 등급 등) target_audience에 명시\n",
        "6. 시기별 프로모션(명절, 새학기 등) 특성을 valid_period에 포함\n",
        "\n",
        "### 분석 목표 ###\n",
        "\"상품 후보 정보\" 목록을 확인하여 광고에 언급된 제품/서비스와 일치하는 항목을 모두 선택하세요.\n",
        "\n",
        "### JSON 응답 형식 ###\n",
        "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Custom parser to extract JSON from text response\n",
        "class CustomJsonOutputParser(JsonOutputParser):\n",
        "    def parse(self, text):\n",
        "        # Try to find JSON in the response using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                raise OutputParserException(f\"Failed to parse JSON: {json_str}\")\n",
        "        else:\n",
        "            raise OutputParserException(f\"Could not find JSON in response: {text}\")\n",
        "\n",
        "# Create a prompt template with improved system message - emphasize JSON-only output\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{schema}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "=====참고 정보=====\n",
        "{rag_context}\n",
        "\n",
        "\n",
        "이제 다음 광고 메시지를 분석하여 schema에 맞는 완전한 JSON 객체를 생성해 주세요.\n",
        "중요: 응답은 설명이나 추가 텍스트 없이 순수한 JSON 형식으로만 제공하세요. 응답은 '{{'로 시작하고 '}}'로 끝나야 합니다.\"\"\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Create the custom parser\n",
        "parser = CustomJsonOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# Wrapper function to handle parsing failures\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"Extract JSON from LLM response text even if it has explanatory text\"\"\"\n",
        "    try:\n",
        "        # First try to parse as pure JSON\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If that fails, try to extract JSON using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', response_text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Failed to parse extracted JSON pattern: {json_str}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Could not find JSON pattern in response: {response_text}\")\n",
        "            return None\n",
        "\n",
        "# Run the extraction with error handling\n",
        "msg_text = \"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "\n",
        "\t광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "msg_text = \"\"\"\n",
        "'\\n  \\t광고 제목:통화 부가서비스를 패키지로 저렴하게!\\n  \\t광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ', \n",
        "\"\"\"\n",
        "\n",
        "# Your existing code for similarities calculation and examples...\n",
        "similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "# Assign similarities to dataframe\n",
        "few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "few_shot_temp['sim'] = similarities\n",
        "few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "# Get examples with better structure - make sure each example shows JSON only in the result\n",
        "few_shot_exm = []\n",
        "top_examples = few_shot_temp[\n",
        "    (few_shot_temp['rank'] >= 1) & \n",
        "    (few_shot_temp['rank'] <= 2)\n",
        "].sort_values(\"rank\")\n",
        "\n",
        "# fs_model = \"gpt\"\n",
        "for _, r in top_examples.iterrows():\n",
        "    # Ensure the examples show proper JSON-only outputs\n",
        "    example_json = r[f'res_{fs_model}']\n",
        "    \n",
        "    # If the example contains explanatory text, try to extract just the JSON\n",
        "    if not (example_json.startswith('{') and example_json.endswith('}')):\n",
        "        json_match = re.search(r'({[\\s\\S]*})', example_json)\n",
        "        if json_match:\n",
        "            example_json = json_match.group(1)\n",
        "\n",
        "    example_json = {'product': extract_json_objects(example_json)[0]['product']}\n",
        "    example_json = str(example_json)\n",
        "    \n",
        "    few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지_{_}]\n",
        "\\t광고 제목:{r['msg_head']}\n",
        "\\t광고 내용:{r['msg_body']}\n",
        "[학습용 정답 결과_{_}]\n",
        "\\t{example_json}\"\"\")\n",
        "\n",
        "few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "\n",
        "# Enhanced entity matching\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list, \n",
        "    min_similarity=50,\n",
        "    high_score_threshold=50,\n",
        "    overlap_tolerance=0.5\n",
        ")\n",
        "\n",
        "product_info = \", \".join(set([match['text'] for match in matches]))\n",
        "\n",
        "\n",
        "stop_item_names = ['SK텔레콤','혜택안내','수신거부','멤버십','150','고객센터']\n",
        "mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "\n",
        "rag_context = f\"\\t상품 후보 정보: {product_info}\"\n",
        "\n",
        "\n",
        "# Include extraction guide in prompt\n",
        "prompt_text = prompt.format(\n",
        "    text=msg_text, \n",
        "    schema=str(schema_prd), \n",
        "    rag_context=rag_context, \n",
        "    extraction_guide=extraction_guide\n",
        ")\n",
        "print(\"Prompt being sent to LLM:\")\n",
        "print(prompt_text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Use the extraction chain with the custom parser\n",
        "    result_cld = extraction_chain.invoke({\n",
        "        \"text\": msg_text, \n",
        "        \"schema\": str(schema_prd), \n",
        "        \"rag_context\": rag_context, \n",
        "        \"extraction_guide\": extraction_guide\n",
        "    })\n",
        "    print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "except Exception as e:\n",
        "    print(f\"Error with extraction chain: {e}\")\n",
        "    \n",
        "    # Get direct response from LLM without parser\n",
        "    raw_response = llm_cld35.invoke(prompt_text)\n",
        "    \n",
        "    # Extract JSON from the raw response\n",
        "    result_cld = extract_json_from_response(raw_response)\n",
        "    if result_cld:\n",
        "        print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "    else:\n",
        "        print(\"Failed to extract valid JSON from the response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e7a754",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define your schema\n",
        "schema_cld_prd_graph = {\n",
        "    \"properties\": {\n",
        "      'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "      },\n",
        "      \"product\": {\n",
        "        \"type\": \"array\",\n",
        "        \"description\": \"메시지에서 추출한 상품 간의 관계 정보입니다.\",\n",
        "        \"items\": {\n",
        "          \"type\": \"object\",\n",
        "          \"description\": \"하나의 상품과 다른 상품 간의 관계를 나타냅니다.\",\n",
        "          \"properties\": {\n",
        "            \"source_item\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"관계의 주체가 되는 상품입니다. 보통 혜택이나 서비스를 제공하는 주체입니다.\",\n",
        "              \"properties\": {\n",
        "                \"id\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 고유 식별자(ID)입니다.\"\n",
        "                },\n",
        "                \"name_in_voca\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"아래 상품 후보 정보에 정의된 상품 이름입니다.\"\n",
        "                },\n",
        "                \"name_in_message\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"메시지 텍스트 내에서 사용된 상품 이름입니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품에 대한 간략한 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"id\", \"name_in_voca\", \"name_in_message\"]\n",
        "            },\n",
        "            \"relation\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"두 상품 간의 관계를 설명하는 객체입니다.\",\n",
        "              \"properties\": {\n",
        "                \"type\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"관계의 유형입니다. '제공', '협업', '활용', '포함', '결합', '부가', '제휴' 등의 값을 가질 수 있습니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"관계에 대한 상세 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"type\"]\n",
        "            },\n",
        "            \"target_item\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"관계의 대상이 되는 상품입니다. 보통 혜택이나 서비스를 받는 대상입니다.\",\n",
        "              \"properties\": {\n",
        "                \"id\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 고유 식별자(ID)입니다.\"\n",
        "                },\n",
        "                \"name_in_voca\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"아래 상품 후보 정보에 정의된 상품 이름입니다.\"\n",
        "                },\n",
        "                \"name_in_message\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"메시지 텍스트 내에서 사용된 상품 이름입니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품에 대한 간략한 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"id\", \"name_in_voca\", \"name_in_message\"]\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"source_item\", \"relation\", \"target_item\"]\n",
        "        },\n",
        "        \"examples\": {\n",
        "          \"relation_types\": {\n",
        "            \"제공\": \"한 상품이 다른 상품에 혜택을 제공하는 관계입니다. 예: 0 day가 베어유 무료 수강 쿠폰을 제공\",\n",
        "            \"협업\": \"두 상품이 공동으로 이벤트나 프로모션을 진행하는 관계입니다. 예: 에이닷과 T 멤버십이 시크릿코드 이벤트 진행\",\n",
        "            \"활용\": \"한 상품이 다른 상품의 기능을 활용하는 관계입니다. 예: 에이닷이 T 멤버십 쿠폰함 기능을 활용\",\n",
        "            \"포함\": \"한 상품이 다른 상품을 포함하는 관계입니다. 예: T 우주 패스가 YouTube Premium을 포함\",\n",
        "            \"결합\": \"두 개 이상의 상품이 하나로 결합되어 새로운 가치를 창출하는 관계입니다. 예: 인터넷과 TV가 결합하여 할인 혜택 제공\",\n",
        "            \"부가\": \"기본 상품에 추가적인 상품이나 서비스가 부가되는 관계입니다. 예: SK텔레콤에 T 올케어플러스5가 부가서비스로 추가\",\n",
        "            \"제휴\": \"서로 다른 회사나 브랜드의 상품이 제휴 관계를 맺고 협력하는 관계입니다. 예: SK텔레콤과 Uber Taxi의 제휴\"\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"purpose\",\"product\"]\n",
        "}\n",
        "\n",
        "# Improved extraction guidance with clear focus on JSON output\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 가이드 ###\n",
        "다음과 같은 너무 일반적인 명칭의 개체명은 제외하세요: 인터넷, 이벤트, 휴대폰, 친구, 할인, 적립, 전화 등\n",
        "순수 JSON만 반환하세요. 설명이나 추가 텍스트는 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Custom parser to extract JSON from text response\n",
        "class CustomJsonOutputParser(JsonOutputParser):\n",
        "    def parse(self, text):\n",
        "        # Try to find JSON in the response using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                raise OutputParserException(f\"Failed to parse JSON: {json_str}\")\n",
        "        else:\n",
        "            raise OutputParserException(f\"Could not find JSON in response: {text}\")\n",
        "\n",
        "# Create a prompt template with improved system message - emphasize JSON-only output\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{schema}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "=====참고 정보=====\n",
        "{rag_context}\n",
        "\n",
        "이제 다음 광고 메시지를 분석하여 schema에 맞는 완전한 JSON 객체를 생성해 주세요.\n",
        "중요: 응답은 설명이나 추가 텍스트 없이 순수한 JSON 형식으로만 제공하세요. 응답은 '{{'로 시작하고 '}}'로 끝나야 합니다.\"\"\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Create the custom parser\n",
        "parser = CustomJsonOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# Wrapper function to handle parsing failures\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"Extract JSON from LLM response text even if it has explanatory text\"\"\"\n",
        "    try:\n",
        "        # First try to parse as pure JSON\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If that fails, try to extract JSON using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', response_text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Failed to parse extracted JSON pattern: {json_str}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Could not find JSON pattern in response: {response_text}\")\n",
        "            return None\n",
        "\n",
        "# Enhanced entity matching - This assumes you have the find_entities_in_text and entity_list functions defined\n",
        "entity_list_trunc = [d for d in entity_list if d[1]['create_dt']>20200101]\n",
        "\n",
        "stop_item_names = ['SK텔레콤','혜택안내','수신거부','멤버십','150','고객센터','TBD Product','SKT','인터넷','이벤트','휴대폰','친구','할인','적립','전화','product','구독상품','mms','가입']\n",
        "\n",
        "pred_results = []\n",
        "\n",
        "for m in mms_pdf.query(\"not msg.str.contains('대리점')\").sample(n=10).to_dict(orient='records'):\n",
        "    # Run the extraction with pre-processing\n",
        "    msg_text = f\"\"\"\n",
        "  \\t광고 제목:{m['msg_nm']}\n",
        "  \\t광고 내용:{m['mms_phrs']}\n",
        "    \"\"\"\n",
        "\n",
        "#     msg_text = \"\"\"\n",
        "# '\\n  \\t광고 제목:통화 부가서비스를 패키지로 저렴하게!\\n  \\t광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ', \n",
        "# \"\"\"\n",
        "\n",
        "    # Enhanced entity matching\n",
        "    matches = find_entities_in_text(\n",
        "        replace_strings(msg_text, {x:'#' for x in stop_item_names}), \n",
        "        entity_list_trunc, \n",
        "        min_similarity=50,\n",
        "        high_score_threshold=50,\n",
        "        overlap_tolerance=0.5\n",
        "    )\n",
        "\n",
        "    matches = [t for t in matches if not t['text'] in stop_item_names]\n",
        "\n",
        "    if len(matches)<1:continue\n",
        "\n",
        "    product_info = \", \".join(set([match['text'] for match in matches]))\n",
        "    \n",
        "    mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "    mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "    product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "\n",
        "    rag_context = f\"\\t상품 후보 정보: {product_info}\"\n",
        "\n",
        "\n",
        "    # Include extraction guide in prompt\n",
        "    prompt_text = prompt.format(\n",
        "        text=replace_strings(msg_text, {x:'#' for x in stop_item_names}), \n",
        "        schema=str(schema_cld_prd_graph), \n",
        "        rag_context=rag_context, \n",
        "        extraction_guide=extraction_guide\n",
        "     )\n",
        "    print(\"Prompt being sent to LLM:\")\n",
        "    print(prompt_text)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        # Use the extraction chain with the custom parser\n",
        "        result_cld = extraction_chain.invoke({\n",
        "            \"text\": msg_text, \n",
        "            \"schema\": str(schema_cld_prd_graph), \n",
        "            \"rag_context\": rag_context, \n",
        "            \"extraction_guide\": extraction_guide\n",
        "        })\n",
        "        \n",
        "        # print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "    except Exception as e:\n",
        "        print(f\"Error with extraction chain: {e}\")\n",
        "        \n",
        "        # Get direct response from LLM without parser\n",
        "        raw_response = llm_cld35.invoke(prompt_text)\n",
        "        \n",
        "        # Extract JSON from the raw response\n",
        "        result_cld = extract_json_from_response(raw_response)\n",
        "        # if result_cld:\n",
        "        #     print(json.dumps(result_cld, indent=4, ensure_ascii=False))\n",
        "        # else:\n",
        "        #     print(\"Failed to extract valid JSON from the response\")      \n",
        "    finally:\n",
        "        pred_results.append({\"msg\":msg_text, \"rag\":rag_context, \"res\":result_cld})\n",
        "\n",
        "    if len(pred_results)%5==0:\n",
        "      print(len(pred_results))\n",
        "\n",
        "    time.sleep(2.0)\n",
        "    # break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfeef0bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# pd.DataFrame(pred_results).to_csv(f\"./data/prod_graph.csv\", index=False, mode='w')\n",
        "# pd.read_csv(\"./data/prod_graph.csv\").shape\n",
        "\n",
        "# [item for sublist in list_of_lists for item in sublist]\n",
        "# [p for p in data if p['source_item']['name_in_message'] in stop_item_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "6e2ba86b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph_from_dict_list(dict_list):\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Keep track of nodes and edges we've added to avoid duplicates\n",
        "    added_nodes = set()\n",
        "    added_edges = set()\n",
        "    \n",
        "    for item in dict_list:\n",
        "        source = item['source_item']\n",
        "        target = item['target_item']\n",
        "        relation = item['relation']\n",
        "        \n",
        "        # Create unique identifiers for nodes\n",
        "        source_id = source['id']\n",
        "        target_id = target['id']\n",
        "        \n",
        "        # Add nodes if they don't exist already\n",
        "        if source_id not in added_nodes:\n",
        "            G.add_node(source_id, \n",
        "                       name_in_voca=source.get('name_in_voca', ''),\n",
        "                       name_in_message=source.get('name_in_message', ''),\n",
        "                       description=source.get('description', ''))\n",
        "            added_nodes.add(source_id)\n",
        "            \n",
        "        if target_id not in added_nodes:\n",
        "            G.add_node(target_id, \n",
        "                       name_in_voca=target.get('name_in_voca', ''),\n",
        "                       name_in_message=target.get('name_in_message', ''),\n",
        "                       description=target.get('description', ''))\n",
        "            added_nodes.add(target_id)\n",
        "        \n",
        "        # Create a unique edge identifier\n",
        "        edge_key = (source_id, target_id, relation['type'])\n",
        "        \n",
        "        # Add edge if it doesn't exist already\n",
        "        if edge_key not in added_edges:\n",
        "            G.add_edge(source_id, target_id, \n",
        "                      relation_type=relation['type'],\n",
        "                      description=relation.get('description', ''))\n",
        "            added_edges.add(edge_key)\n",
        "    \n",
        "    return G\n",
        "\n",
        "# Your list of dictionaries\n",
        "data = sum([d['res']['product'] for d in pred_results],[])\n",
        "data = [p for p in data if not p['source_item']['name_in_message'] in stop_item_names]\n",
        "\n",
        "# Build the graph\n",
        "G = build_graph_from_dict_list(data)\n",
        "\n",
        "# # Verify results\n",
        "# print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "# print(f\"Number of edges: {G.number_of_edges()}\")\n",
        "# print(\"\\nNodes:\")\n",
        "# for node, attributes in G.nodes(data=True):\n",
        "#     print(f\"  {node}: {attributes}\")\n",
        "# print(\"\\nEdges:\")\n",
        "# for source, target, attributes in G.edges(data=True):\n",
        "#     print(f\"  {source} -> {target}: {attributes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b715b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "def get_important_nodes(G, top_n=None, measures=None):\n",
        "    \"\"\"\n",
        "    Calculate importance metrics for nodes in graph G.\n",
        "    \n",
        "    Parameters:\n",
        "    - G: NetworkX graph\n",
        "    - top_n: Number of top nodes to return (None for all)\n",
        "    - measures: List of centrality measures to calculate\n",
        "               (None for all basic measures)\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame with nodes and their importance scores\n",
        "    \"\"\"\n",
        "    if measures is None:\n",
        "        measures = ['degree', 'in_degree', 'out_degree', 'betweenness', \n",
        "                   'closeness', 'eigenvector', 'pagerank']\n",
        "    \n",
        "    # Initialize results dictionary\n",
        "    results = {node: {'node_id': node} for node in G.nodes()}\n",
        "    \n",
        "    # Calculate selected centrality measures\n",
        "    if 'degree' in measures:\n",
        "        for node, degree in G.degree():\n",
        "            results[node]['degree'] = degree\n",
        "            \n",
        "    if 'in_degree' in measures and isinstance(G, nx.DiGraph):\n",
        "        for node, in_degree in G.in_degree():\n",
        "            results[node]['in_degree'] = in_degree\n",
        "            \n",
        "    if 'out_degree' in measures and isinstance(G, nx.DiGraph):\n",
        "        for node, out_degree in G.out_degree():\n",
        "            results[node]['out_degree'] = out_degree\n",
        "    \n",
        "    if 'betweenness' in measures:\n",
        "        betweenness = nx.betweenness_centrality(G)\n",
        "        for node, score in betweenness.items():\n",
        "            results[node]['betweenness'] = score\n",
        "    \n",
        "    if 'closeness' in measures:\n",
        "        try:\n",
        "            closeness = nx.closeness_centrality(G)\n",
        "            for node, score in closeness.items():\n",
        "                results[node]['closeness'] = score\n",
        "        except nx.NetworkXError:\n",
        "            # Closeness centrality is not defined for disconnected graphs\n",
        "            pass\n",
        "    \n",
        "    if 'eigenvector' in measures:\n",
        "        try:\n",
        "            eigenvector = nx.eigenvector_centrality(G, max_iter=100)\n",
        "            for node, score in eigenvector.items():\n",
        "                results[node]['eigenvector'] = score\n",
        "        except nx.PowerIterationFailedConvergence:\n",
        "            # Eigenvector centrality might not converge\n",
        "            pass\n",
        "    \n",
        "    if 'pagerank' in measures:\n",
        "        pagerank = nx.pagerank(G)\n",
        "        for node, score in pagerank.items():\n",
        "            results[node]['pagerank'] = score\n",
        "    \n",
        "    # Convert to DataFrame and sort\n",
        "    df = pd.DataFrame(list(results.values()))\n",
        "    \n",
        "    # Return top N nodes if specified\n",
        "    if top_n is not None and top_n < len(df):\n",
        "        # For each measure, get top_n nodes\n",
        "        important_nodes = set()\n",
        "        for measure in measures:\n",
        "            if measure in df.columns:\n",
        "                top_indices = df.nlargest(top_n, measure)['node_id'].values\n",
        "                important_nodes.update(top_indices)\n",
        "        \n",
        "        # Filter dataframe to only include important nodes\n",
        "        df = df[df['node_id'].isin(important_nodes)]\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# G = build_graph_from_dict_list(data)\n",
        "\n",
        "# Get all centrality measures\n",
        "importance_df = get_important_nodes(G)\n",
        "print(\"All centrality measures:\")\n",
        "print(importance_df)\n",
        "\n",
        "# Get only top 2 nodes based on degree and pagerank\n",
        "top_nodes = get_important_nodes(G, top_n=2, measures=['degree', 'pagerank'])\n",
        "print(\"\\nTop 2 nodes by degree and PageRank:\")\n",
        "print(top_nodes)\n",
        "\n",
        "# Access node attributes for important nodes\n",
        "print(\"\\nDetails of important nodes:\")\n",
        "for node_id in top_nodes['node_id']:\n",
        "    attributes = G.nodes[node_id]\n",
        "    print(f\"{node_id}: {attributes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1304058f",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3475d92b",
      "metadata": {},
      "outputs": [],
      "source": [
        "mms_pdf.query(\"msg.str.contains('콜링플러스') and msg.str.contains('통화 부가서비스')\")['msg_nm'].values[0]\n",
        "mms_pdf.query(\"msg.str.contains('콜링플러스') and msg.str.contains('통화 부가서비스')\")['mms_phrs'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d56ebf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "msg = f\"\"\"광고 제목:{mms_pdf.query(\"msg.str.contains('콜링플러스') and msg.str.contains('통화 부가서비스')\")['msg_nm'].values[0]}\n",
        "  \t광고 내용:{mms_pdf.query(\"msg.str.contains('콜링플러스') and msg.str.contains('통화 부가서비스')\")['mms_phrs'].values[0]}\n",
        "\"\"\"\n",
        "\n",
        "entity_list_test = [x for x in entity_list_trunc\n",
        "                #      if '우주' in x[0]\n",
        "                     ] #[\"T 우주패스\", \"우주패스\", \"T멤버십\", \"프리미엄패스\"]\n",
        "\n",
        "\n",
        "[x for x in find_entities_in_text(\n",
        "        replace_strings(msg, {x:'#' for x in stop_item_names}), \n",
        "        entity_list_test, \n",
        "        min_similarity=50,\n",
        "        high_score_threshold=50,\n",
        "        overlap_tolerance=0.5\n",
        ")]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "f4f0d77b",
      "metadata": {},
      "outputs": [],
      "source": [
        "similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "# Assign similarities to dataframe\n",
        "few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "few_shot_temp['sim'] = similarities\n",
        "few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "# Get examples with better structure - make sure each example shows JSON only in the result\n",
        "few_shot_exm = []\n",
        "top_examples = few_shot_temp[\n",
        "    (few_shot_temp['rank'] >= 2) & \n",
        "    (few_shot_temp['rank'] <= 2)\n",
        "].sort_values(\"rank\")\n",
        "\n",
        "for r in top_examples.to_dict(\"records\"):\n",
        "    msg = f\"\"\"\n",
        "    광고 제목:{r['msg_head']}\n",
        "    광고 내용:{r['msg_body']}\n",
        "    \"\"\"\n",
        "    result_cld = extraction_chain.invoke({\n",
        "        \"text\": msg, \n",
        "        \"schema\": str(schema_prd), \n",
        "        \"rag_context\": rag_context, \n",
        "        \"extraction_guide\": extraction_guide\n",
        "    })\n",
        "    example_json = json.dumps(result_cld, indent=4, ensure_ascii=False)\n",
        "\n",
        "    few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지_{_}]\n",
        "\\t* 광고 제목:{r['msg_head']}\n",
        "\\t* 광고 내용:{r['msg_body']}\n",
        "[학습용 정답 결과_{_}]\n",
        "\\t{example_json}\"\"\")\n",
        "\n",
        "few_shot_exm_str = (\"\\n\".join(few_shot_exm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfacca88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the system message with clear JSON output requirements\n",
        "system_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{json.dumps(schema_prd, indent=2, ensure_ascii=False)}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create message content with structured sections\n",
        "user_message = f\"\"\"### 분석 대상 광고 메세지 ###\n",
        "{msg_text}\n",
        "\n",
        "### 관련 참고 정보 ###\n",
        "{rag_context}\n",
        "\n",
        "### 학습용 예시 ###\n",
        "{few_shot_exm_str}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# print('%'*15+\" System Message \"+'%'*15+\"\\n\\n\")\n",
        "# print(system_message)\n",
        "print('%'*15+\" User Message \"+'%'*15+\"\\n\\n\")\n",
        "print(user_message)\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    print(\"\\n\\n\\n\"+'%'*15+\" LLM Result \"+'%'*15+\"\\n\\n\")\n",
        "    print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bcf7761",
      "metadata": {},
      "outputs": [],
      "source": [
        "mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "\n",
        "product_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3f1e1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_message = f\"\"\"\n",
        "다음은 광고 메시지에서 추출한 상품들의 정보입니다.\n",
        "각 상품별 광고 목적을 아래 분류 기준에 따라 추론하여, 상품 정보 JSON에 각 item별 \"purpose\" 태그로 추가해 주세요.\n",
        "\n",
        "### 광고 목적 분류 기준 ###\n",
        "1. 신규 가입 유도: 통신 서비스, 멤버십, 구독 서비스 등에 새롭게 가입하도록 유도\n",
        "2. 재가입/연장 유도: 만료된 서비스의 재가입이나 구독 연장을 유도\n",
        "3. 상품 구매: 휴대폰, 액세서리 등 물리적 제품의 구매를 유도\n",
        "4. 사전예약: 출시 예정 제품의 사전 예약을 유도\n",
        "5. 서비스 이용: 기존 서비스, 앱, 콘텐츠의 활성 사용을 장려\n",
        "6. 단기 프로모션 참여: 한정된 기간의 할인, 이벤트 등 참여 유도\n",
        "7. 매장 방문: 오프라인 매장 방문을 유도\n",
        "8. 브랜드 인지도 증진: 특정 제품이나 서비스보다 브랜드 자체의 인지도 향상이 목적\n",
        "9. 고객 충성도 강화: 기존 고객의 충성도를 높이기 위한 혜택 안내\n",
        "10. 교차판매/상향판매: 기존 고객에게 추가 상품이나 상위 서비스 판매 유도\n",
        "11. 정보 제공: 공지사항, 보안 경고 등 정보성 메시지\n",
        "12. 제외: 광고 대상 상품이 아니고, 단순히 광고 메시지 구성 요소인 경우\n",
        "\n",
        "각 광고는 주요 목적과 부가적 목적이 있을 수 있습니다. 가장 핵심적인 목적을 \"purpose\" 값으로 지정하되, 필요한 경우 \"secondary_purpose\"를\n",
        "추가하여 보조적인 목적도 표시해 주세요.\n",
        "\n",
        "### 상품 정보 ###\n",
        "{product_info}\n",
        "\n",
        "### 분석 대상 광고 메시지 ###\n",
        "{msg_text}\n",
        "\n",
        "분석 결과는 기존 JSON 구조를 유지하면서 각 항목에 \"purpose\"와 필요 시 \"secondary_purpose\" 필드를 추가해 주세요.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        # response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)\n",
        "\n",
        "    print(\"\\n\\n\\n\"+'%'*15+\" LLM Result \"+'%'*15+\"\\n\\n\")\n",
        "    print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")\n",
        "\n",
        "\n",
        "# extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"human\", \"{user_message}\")\n",
        "# ])\n",
        "\n",
        "# extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# result_cld = extraction_chain.invoke({\n",
        "#         \"user_message\": user_message, \n",
        "#     })\n",
        "\n",
        "# print(json.dumps(result_cld, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543794ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "mdf = mms_pdf.query(\"offer_dt>='20240101'\").rename(columns={'msg_nm':'head','mms_phrs':'body'})\n",
        "print(\"\\n\\n\".join([d['msg'] for d in mdf.sample(n=100).to_dict(\"records\")]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bbb040-d956-42ed-9786-e8c9c3013e2b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for index in range(len(result_cld['product'])):\n",
        "    item_name = result_cld['product'][index]['name'] #+ \" \" + result['광고 상품'][0]['상품 정보']\n",
        "    # get_relevant_context(item_name, vectorstore, topk=20)['title']\n",
        "\n",
        "    matches = find_entities_in_text(\n",
        "        result_cld['product'][index]['name'], \n",
        "        entity_list, \n",
        "        min_similarity=70,  # Lower threshold for more matches\n",
        "        ngram_size=3,       # Bigrams work well for Korean\n",
        "        min_entity_length=3 # Only consider entities with 3+ characters\n",
        "    )\n",
        "\n",
        "    schema_res = {\n",
        "        \"ext_item_nm\":{\"type\":\"string\",\"description\":\"기준 item의 name\"},\n",
        "        \"item_id\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 id\"},\n",
        "        \"item_nm\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 name\"},\n",
        "        \"domain\":{\"type\":\"string\",\"description\":\"해당 item의 도메인\"},    \n",
        "        \"reason\":{\"type\":\"string\",\"description\":\"선택한 이유\"}\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    아래 광고 메세지에서 추출한 상품 정보와 후보 상품 리스트 중에서 광고 메세지의 맥락에 가장 부합하는 상품을 선택해라.  다음과 같은 schema에 참고하여 JSON 개체로 만들어 주세요:\\n {schema_res}\n",
        "    ---상품 정보---\n",
        "    상품명: {result_cld['product'][index]['name']}\n",
        "    카테고리: {result_cld['product'][index]['category']}\n",
        "    혜택: {result_cld['product'][index]['benefit']}\n",
        "    조건: {result_cld['product'][index]['conditions']}\n",
        "    반응: {result_cld['product'][index]['action']}\n",
        "    \n",
        "    ---원본 광고 메세지---\n",
        "    {msg_text}\n",
        "\n",
        "    ---후보 상품 리스트---\n",
        "    {pd.DataFrame(matches)}\n",
        "    \"\"\"\n",
        "\n",
        "    res_cld = extract_json_objects(llm_cld37.invoke(prompt).content)[0]\n",
        "    print(json.dumps(res_cld, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72808d81-f3d8-40b7-b70f-afbb1d131f93",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "아래 광고 메세지의 구조 분석해라.\n",
        "분석의 목적은 메세지에서 구조화된 정보를 추출하기 위함이다.\n",
        "다양한 구조의 다른 메세지들도 분석할 예정인데, 이를 위한 prompt에 추가할 JSON이 아래와 같은 데, 개선점을 파악해서, 새로운 JSON을 추천해라.\n",
        "===광고 메세지 예시===\n",
        "{mdf[:15]}\n",
        "===PROMPT에 넣을 JSON===\n",
        "{schema_ax}\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\":prompt}\n",
        "    ],\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a65dcff-458d-4b3a-b359-0ede6bb11aca",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "\n",
        "def extract_store_code(text):\n",
        "    # Pattern to find store code in tworldfriends.co.kr URL\n",
        "    pattern1 = r'tworldfriends\\.co\\.kr/([D][0-9]{9})'\n",
        "    # Pattern to find store code in detail?code= parameter\n",
        "    pattern2 = r'code=([D][0-9\\-]+)'\n",
        "    \n",
        "    # Try first pattern\n",
        "    match = re.search(pattern1, text)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    # Try second pattern\n",
        "    match = re.search(pattern2, text)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_phone_numbers(text):\n",
        "    # Patterns for various phone number formats in Korea\n",
        "    patterns = [\n",
        "        r'(\\d{3,4}-\\d{3,4}-\\d{4})',  # Format: XXX-XXXX-XXXX or XXXX-XXXX-XXXX\n",
        "        r'(0\\d{1,2}-\\d{3,4}-\\d{4})',  # Format: 0X-XXXX-XXXX or 0XX-XXXX-XXXX\n",
        "        r'(0\\d{3}-\\d{3,4}-\\d{4})',    # Format: 0XXX-XXX-XXXX or 0XXX-XXXX-XXXX\n",
        "        r'(\\d{4}-\\d{4})',             # Format: XXXX-XXXX\n",
        "        r'(1\\d{3})',                  # Short numbers like 1588\n",
        "        r'(0507-\\d{4}-\\d{4})'         # Format: 0507-XXXX-XXXX (virtual numbers)\n",
        "    ]\n",
        "    \n",
        "    phone_numbers = []\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        phone_numbers.extend(matches)\n",
        "    \n",
        "    return phone_numbers\n",
        "\n",
        "def extract_unsubscribe_info(text):\n",
        "    # Look for unsubscribe text and associated number\n",
        "    unsubscribe_pattern = r'수신\\s*거부\\s*(\\d+)'\n",
        "    match = re.search(unsubscribe_pattern, text)\n",
        "    if match:\n",
        "        return {\n",
        "            \"type\": \"전화번호\",\n",
        "            \"value\": match.group(1),\n",
        "            \"action\": \"수신 거부\"\n",
        "        }\n",
        "    return None\n",
        "\n",
        "# Define your schema\n",
        "schema_ax = {\n",
        "    'properties': {\n",
        "    'message_info': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'title': {'type': 'string', 'description': '광고 제목 - 원본 그대로 추출'},\n",
        "            'main_theme': {'type': 'string', 'description': '광고의 핵심 주제와 가치 제안을 명확하게 설명'},\n",
        "            'period': {'type': 'string', 'description': '이벤트/프로모션 기간 - 명시적으로 언급된 경우에만 추출, 없으면 \"상시\"로 설정'}\n",
        "        }\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'target': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'segment': {'type': 'string', 'description': '타겟 고객층 - 광고에서 명시적으로 언급하거나 암시한 대상 고객'},\n",
        "                'characteristics': {'type': 'string', 'description': '해당 타겟의 특성과 니즈'},\n",
        "                'priority': {'type': 'integer', 'description': '광고 내용에서의 중요도에 따른 타겟팅 우선순위 (1이 최우선)'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름 - \"상품 후보 정보\" 목록에서 일치하는 항목 모두 선택'},\n",
        "                'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "                'category': {'type': 'string', 'description': '제품/서비스 카테고리 - \"상품 후보 정보\"에서 확인'},\n",
        "                'benefit': {\n",
        "                    'type': 'object', \n",
        "                    'properties': {\n",
        "                        'name': {'type': 'string', 'description': '제공되는 구체적인 혜택 내용 상세 설명'},\n",
        "                        'type': {'type': 'string', 'description': '혜택 유형: [할인, 쿠폰, 경품, 기타] 중에서 선택'}\n",
        "                    }\n",
        "                },\n",
        "                'conditions': {'type': 'string', 'description': '혜택/구매를 받기 위한 구체적인 조건'},\n",
        "                'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'primary': {'type': 'boolean', 'description': '주요 채널 여부 - 광고의 핵심 연락 채널이면 true'},\n",
        "                'availability': {'type': 'string', 'description': '채널 이용 가능 시간/조건 - 언급이 없으면 \"상시\"로 설정'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'metadata': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'message_type': {'type': 'string', 'description': '메시지 유형 - 항상 \"광고\"로 설정'},\n",
        "            'target_response': {'type': 'string', 'description': '광고주가 고객으로부터 기대하는 반응 요약'},\n",
        "            'success_metrics': {'type': 'array', 'description': '이 캠페인의 성공을 측정하기 위한 지표들 추출'}\n",
        "        }\n",
        "    }\n",
        "}, \n",
        "'required': ['purpose', 'target', 'product', 'channel', 'metadata'], \n",
        "'objectType': 'object'\n",
        "}\n",
        "\n",
        "# Improved extraction guidance with clear focus on JSON output\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 가이드 ###\n",
        "• 학습용 예시를 이용해서 few-shot 학습 기법을 사용합니다.\n",
        "• 광고 메시지에서 언급하는 모든 제품/서비스를 상품 정보 매핑에서 찾아 ID를 매칭하세요\n",
        "• 학습용 예시를 참고 정보로 사용하지 마세요. \n",
        "• 타겟 고객층과 혜택 내용을 명확히 식별하세요\n",
        "• 모든 채널 정보(URL, 전화번호 등)를 추출하세요\n",
        "• tworldfriends.co.kr URL에서 매장 코드(D+숫자 9자리)를 추출하세요\n",
        "• 명시되지 않은 정보는 적절히 추론하되, 추측은 최소화하세요\n",
        "\n",
        "### 응답 형식 ###\n",
        "순수 JSON만 반환하세요. 설명이나 추가 텍스트는 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Run the extraction with pre-processing\n",
        "msg_text = \"\"\"\n",
        "\\t광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "\\t광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Pre-process to extract helpful information\n",
        "store_code = extract_store_code(msg_text)\n",
        "phone_numbers = extract_phone_numbers(msg_text)\n",
        "unsubscribe_info = extract_unsubscribe_info(msg_text)\n",
        "\n",
        "# Prepare relevant context - This assumes you have the tfidf_desc and few_shot_pdf variables defined\n",
        "similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "# Assign similarities to dataframe\n",
        "few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "few_shot_temp['sim'] = similarities\n",
        "few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "# Get examples with clear JSON structure\n",
        "few_shot_exm = []\n",
        "top_examples = few_shot_temp[\n",
        "    (few_shot_temp['rank'] >= 1) & \n",
        "    (few_shot_temp['rank'] <= 1)  # Increased to 3 examples\n",
        "].sort_values(\"rank\")\n",
        "\n",
        "for _, r in top_examples.iterrows():\n",
        "    # Ensure examples show proper JSON format\n",
        "    example_json = r[f'res_{fs_model}']\n",
        "    \n",
        "    # Clean up examples to show proper JSON format\n",
        "    if not (example_json.startswith('{') and example_json.endswith('}')):\n",
        "        json_match = re.search(r'({[\\s\\S]*})', example_json)\n",
        "        if json_match:\n",
        "            example_json = json_match.group(1)\n",
        "    \n",
        "    example_json = extract_json_objects(example_json)[0]\n",
        "    # example_json.pop('product')\n",
        "    example_json = str(example_json)\n",
        "    \n",
        "    few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지_{_}]\n",
        "\\t* 광고 제목:{r['msg_head']}\n",
        "\\t* 광고 내용:{r['msg_body']}\n",
        "[학습용 정답 결과_{_}]\n",
        "\\t{example_json}\"\"\")\n",
        "\n",
        "few_shot_exm_str = (\"\\n\".join(few_shot_exm))\n",
        "\n",
        "# Enhanced entity matching - This assumes you have the find_entities_in_text and entity_list functions defined\n",
        "entity_list_trunc = [d for d in entity_list if d[1]['create_dt']>20230101]\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list_trunc, \n",
        "    min_similarity=65,  # Lower for more matches\n",
        "    ngram_size=3,      \n",
        "    min_entity_length=2  # Shorter for Korean words\n",
        ")\n",
        "\n",
        "stop_item_names = ['SK텔레콤','혜택안내','수신거부','멤버십','150','고객센터']\n",
        "mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "# product_info = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_markdown() #\n",
        "# product_info = convert_to_custom_format(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_id','category']].drop_duplicates().to_dict(orient='records'))\n",
        "\n",
        "# Improved context with structured auxiliary information\n",
        "rag_context = \"\\n\".join([\n",
        "f\"\\t* 상품 후보 정보: {product_info}\",\n",
        "f\"\\t* 매장 코드 정보: '{store_code}'. 이 코드는 type='대리점'인 channel 객체의 store_code 필드에 설정해주세요.\",\n",
        "f\"\\t* 수신 거부 정보: {unsubscribe_info}. 이 정보를 type='전화번호', action='수신 거부' channel 객체로 추가해주세요.\"\n",
        "])\n",
        "\n",
        "# Create the system message with clear JSON output requirements\n",
        "system_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{json.dumps(schema_ax, indent=2, ensure_ascii=False)}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create message content with structured sections\n",
        "user_message = f\"\"\"### 분석 대상 광고 메세지 ###\n",
        "{msg_text}\n",
        "\n",
        "### 관련 참고 정보 ###\n",
        "{rag_context}\n",
        "\n",
        "### 학습용 예시 ###\n",
        "{few_shot_exm_str}\n",
        "\"\"\"\n",
        "\n",
        "print('%'*15+\" System Message \"+'%'*15+\"\\n\\n\")\n",
        "print(system_message)\n",
        "print('%'*15+\" User Message \"+'%'*15+\"\\n\\n\")\n",
        "print(user_message)\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    print(\"\\n\\n\\n\"+'%'*15+\" LLM Result \"+'%'*15+\"\\n\\n\")\n",
        "    print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bd0b85",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"how to make a good prompt to extract information from a text given examples for few-shot learning and context information for candidate entities to extract based on the following guides. \\n\\n1. Do not use the examples for few-shot learning as context information for RAG. \"}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "    )\n",
        "    \n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f068bc-d4fc-451d-b569-f5e82a58e920",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "\n",
        "# Define your schema\n",
        "schema_ax_prd = {\n",
        "  \"schema\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"relationships\": {\n",
        "        \"type\": \"array\",\n",
        "        \"description\": \"메시지에서 추출한 상품 간의 관계 정보입니다.\",\n",
        "        \"items\": {\n",
        "          \"type\": \"object\",\n",
        "          \"description\": \"하나의 상품과 다른 상품 간의 관계를 나타냅니다.\",\n",
        "          \"properties\": {\n",
        "            \"source_item\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"관계의 주체가 되는 상품입니다. 보통 혜택이나 서비스를 제공하는 주체입니다.\",\n",
        "              \"properties\": {\n",
        "                \"id\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 고유 식별자(ID)입니다.\"\n",
        "                },\n",
        "                \"name\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 이름입니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품에 대한 간략한 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"id\", \"name\"]\n",
        "            },\n",
        "            \"relation\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"두 상품 간의 관계를 설명하는 객체입니다.\",\n",
        "              \"properties\": {\n",
        "                \"type\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"관계의 유형입니다. '제공', '협업', '활용', '포함', '결합', '부가', '제휴' 등의 값을 가질 수 있습니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"관계에 대한 상세 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"type\"]\n",
        "            },\n",
        "            \"target_item\": {\n",
        "              \"type\": \"object\",\n",
        "              \"description\": \"관계의 대상이 되는 상품입니다. 보통 혜택이나 서비스를 받는 대상입니다.\",\n",
        "              \"properties\": {\n",
        "                \"id\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 고유 식별자(ID)입니다.\"\n",
        "                },\n",
        "                \"name\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품의 이름입니다.\"\n",
        "                },\n",
        "                \"description\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"description\": \"상품에 대한 간략한 설명입니다.\"\n",
        "                }\n",
        "              },\n",
        "              \"required\": [\"id\", \"name\"]\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"source_item\", \"relation\", \"target_item\"]\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"relationships\"]\n",
        "  },\n",
        "  \"examples\": {\n",
        "    \"relation_types\": {\n",
        "      \"제공\": \"한 상품이 다른 상품에 혜택을 제공하는 관계입니다. 예: 0 day가 베어유 무료 수강 쿠폰을 제공\",\n",
        "      \"협업\": \"두 상품이 공동으로 이벤트나 프로모션을 진행하는 관계입니다. 예: 에이닷과 T 멤버십이 시크릿코드 이벤트 진행\",\n",
        "      \"활용\": \"한 상품이 다른 상품의 기능을 활용하는 관계입니다. 예: 에이닷이 T 멤버십 쿠폰함 기능을 활용\",\n",
        "      \"포함\": \"한 상품이 다른 상품을 포함하는 관계입니다. 예: T 우주 패스가 YouTube Premium을 포함\",\n",
        "      \"결합\": \"두 개 이상의 상품이 하나로 결합되어 새로운 가치를 창출하는 관계입니다. 예: 인터넷과 TV가 결합하여 할인 혜택 제공\",\n",
        "      \"부가\": \"기본 상품에 추가적인 상품이나 서비스가 부가되는 관계입니다. 예: SK텔레콤에 T 올케어플러스5가 부가서비스로 추가\",\n",
        "      \"제휴\": \"서로 다른 회사나 브랜드의 상품이 제휴 관계를 맺고 협력하는 관계입니다. 예: SK텔레콤과 Uber Taxi의 제휴\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Improved extraction guidance with clear focus on JSON output\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 가이드 ###\n",
        "순수 JSON만 반환하세요. 설명이나 추가 텍스트는 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "pred_results = []\n",
        "\n",
        "for m in mms_pdf.to_dict(orient='records'):\n",
        "    # Run the extraction with pre-processing\n",
        "  msg_text = f\"\"\"\n",
        "\\t광고 제목:{m['msg_nm']}\n",
        "\\t광고 내용:{m['mms_phrs']}\n",
        "  \"\"\"\n",
        "\n",
        "  # Enhanced entity matching - This assumes you have the find_entities_in_text and entity_list functions defined\n",
        "  entity_list_trunc = [d for d in entity_list if d[1]['create_dt']>20230101]\n",
        "  matches = find_entities_in_text(\n",
        "      msg_text, \n",
        "      entity_list_trunc, \n",
        "      min_similarity=65,  # Lower for more matches\n",
        "      ngram_size=3,      \n",
        "      min_entity_length=2  # Shorter for Korean words\n",
        "  )\n",
        "\n",
        "  stop_item_names = ['SK텔레콤','혜택안내','수신거부','멤버십','150','고객센터']\n",
        "  mdf = pd.DataFrame(matches).query(\"text.str.replace(' ', '') not in @stop_item_names\")\n",
        "\n",
        "  mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "  mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "  product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "  # product_info = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_markdown() #\n",
        "  # product_info = convert_to_custom_format(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_id','category']].drop_duplicates().to_dict(orient='records'))\n",
        "\n",
        "  # Improved context with structured auxiliary information\n",
        "  rag_context = \"\\n\".join([\n",
        "  f\"\\t* 상품 후보 정보: {product_info}\",\n",
        "  ])\n",
        "\n",
        "  # Create the system message with clear JSON output requirements\n",
        "  system_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 아래 상품 후보 정보들의 관게를 추출해서 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "  {schema_ax_prd}\n",
        "  \"\"\"\n",
        "\n",
        "  # Create message content with structured sections\n",
        "  user_message = f\"\"\"### 분석 대상 광고 메세지 ###\n",
        "  {msg_text}\n",
        "\n",
        "  ### 관련 참고 정보 ###\n",
        "  {rag_context}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # print('%'*15+\" System Message \"+'%'*15+\"\\n\\n\")\n",
        "  # print(system_message)\n",
        "  # print('%'*15+\" User Message \"+'%'*15+\"\\n\\n\")\n",
        "  # print(user_message)\n",
        "\n",
        "  try:\n",
        "      # Use OpenAI's ChatCompletion with the current API format\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": system_message},\n",
        "              {\"role\": \"user\", \"content\": user_message}\n",
        "          ],\n",
        "          temperature=0.0,\n",
        "          max_tokens=4000,\n",
        "          top_p=0.95,  # Reduces randomness\n",
        "          frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "          presence_penalty=0.0,\n",
        "          response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "      )\n",
        "      \n",
        "      # Extract the JSON from the response\n",
        "      result_json_text = response.choices[0].message.content\n",
        "      json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "      pred_results.append(json_objects)\n",
        "\n",
        "      # print(\"\\n\\n\\n\"+'%'*15+\" LLM Result \"+'%'*15+\"\\n\\n\")\n",
        "      # print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "      # print(result_json_text)\n",
        "              \n",
        "  except Exception as e:\n",
        "      print(f\"Error with API call: {e}\")\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ea84a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d12449f-7069-438e-b255-eb8f356af539",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def process_prompt(row):\n",
        "    \n",
        "    schema_ax = row['schema']\n",
        "    msg_text = row['msg_text']\n",
        "    \n",
        "    system_message_ax = f\"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 부합하도록 JSON 개체를 만들어 주세요:\\n {schema_ax}\"\n",
        "        \n",
        "    # Create the chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message_ax},\n",
        "            {\"role\": \"user\", \"content\": msg_text}\n",
        "        ],\n",
        "        temperature=0.0\n",
        "    )\n",
        "    \n",
        "    # print(response.choices[0].message.content)\n",
        "\n",
        "    # Parse the response\n",
        "    result_ax = extract_json_objects(response.choices[0].message.content)[0]\n",
        "\n",
        "    return result_ax\n",
        "\n",
        "results_ax_mul = []\n",
        "with ThreadPoolExecutor(max_workers=len(prompt_list)) as executor:  # More threads for I/O-bound tasks\n",
        "    results_ax_mul += list(executor.map(process_prompt, prompt_list))\n",
        "\n",
        "result_ax = {}\n",
        "for key in list(set(sum([list(d.keys()) for d in results_ax_mul], []))):\n",
        "    lst = []\n",
        "    for dic in results_ax_mul:\n",
        "        if key in dic:\n",
        "            lst.append(dic[key])\n",
        "    \n",
        "    dict_list = sum(lst,[])\n",
        "\n",
        "    if is_list_of_dicts(dict_list):\n",
        "        df = pd.DataFrame(dict_list)\n",
        "        df = df.drop_duplicates()\n",
        "        dict_list = df.to_dict('records')\n",
        "                \n",
        "    result_ax[key] = dict_list\n",
        "\n",
        "print(json.dumps(result_ax, indent=4, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0c7686-2177-4097-9d37-7459a624f31c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for index in range(len(result_ax['product'])):\n",
        "    item_name = result_ax['product'][index]['name'] #+ \" \" + result['광고 상품'][0]['상품 정보']\n",
        "    # get_relevant_context(item_name, vectorstore, topk=20)['title']\n",
        "\n",
        "    # item_pdf['sim'] = cosine_similarity(\n",
        "    #     tfidf_name.transform([result_ax['광고 상품'][index]['상품명'] ]),\n",
        "    #     tfidf_matrix_name\n",
        "    # )[0]\n",
        "    \n",
        "    matches = find_entities_in_text(\n",
        "        result_ax['product'][index]['name'], \n",
        "        entity_list, \n",
        "        min_similarity=70,  # Lower threshold for more matches\n",
        "        ngram_size=3,       # Bigrams work well for Korean\n",
        "        min_entity_length=3 # Only consider entities with 3+ characters\n",
        "    )\n",
        "\n",
        "    schema_res = {\n",
        "    \"ext_item_nm\":{\"type\":\"string\",\"description\":\"기준 item의 name\"},\n",
        "    \"item_id\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 id\"},\n",
        "    \"item_nm\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 name\"},\n",
        "    \"domain\":{\"type\":\"string\",\"description\":\"해당 item의 도메인\"},    \n",
        "    \"reason\":{\"type\":\"string\",\"description\":\"선택한 이유\"}\n",
        "    }\n",
        "\n",
        "    text = f\"\"\"\n",
        "    ---상품 정보---\n",
        "    상품명: {result_ax['product'][index]['name']}\n",
        "    상품 카테고리: {result_ax['product'][index]['category']}\n",
        "    상품 정보: {result_ax['product'][index]['info']}\n",
        "    \n",
        "    ---원본 광고 메세지---\n",
        "    {msg_text}\n",
        "\n",
        "    ---후보 상품 리스트---\n",
        "    {pd.DataFrame(matches)}\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    system_message_ax = f\"\"\"\n",
        "    아래 광고 메세지에서 추출한 상품 정보와 후보 상품 리스트 중에서 광고 메세지의 맥락에 가장 부합하는 상품을 선택해라. 다음과 같은 schema에 참고하여 JSON 개체로 만들어 주세요:\\n {schema_res}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message_ax},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "        \n",
        "    res_ax = extract_json_objects((response.choices[0].message.content).replace(\"'\",'\"'))[0]\n",
        "    print(json.dumps(res_ax, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "64230dab",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "schema_cld = {'properties': {\n",
        "    'message_info': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'title': {'type': 'string', 'description': '광고 제목 - 원본 그대로 추출'},\n",
        "            'main_theme': {'type': 'string', 'description': '광고의 핵심 주제와 가치 제안을 명확하게 설명'},\n",
        "            'period': {'type': 'string', 'description': '이벤트/프로모션 기간 - 명시적으로 언급된 경우에만 추출, 없으면 \"상시\"로 설정'}\n",
        "        }\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'target': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'segment': {'type': 'string', 'description': '타겟 고객층 - 광고에서 명시적으로 언급하거나 암시한 대상 고객'},\n",
        "                'characteristics': {'type': 'string', 'description': '해당 타겟의 특성과 니즈'},\n",
        "                'priority': {'type': 'integer', 'description': '광고 내용에서의 중요도에 따른 타겟팅 우선순위 (1이 최우선)'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름 - \"상품 후보 정보\" 목록에서 가장 일치하는 항목 선택'},\n",
        "                'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "                'category': {'type': 'string', 'description': '제품/서비스 카테고리 - \"상품 후보 정보\"에서 확인'},\n",
        "                'benefit': {\n",
        "                    'type': 'object', \n",
        "                    'properties': {\n",
        "                        'name': {'type': 'string', 'description': '제공되는 구체적인 혜택 내용 상세 설명'},\n",
        "                        'type': {'type': 'string', 'description': '혜택 유형: [할인, 쿠폰, 경품, 기타] 중에서 선택'}\n",
        "                    }\n",
        "                },\n",
        "                'conditions': {'type': 'string', 'description': '혜택/구매를 받기 위한 구체적인 조건'},\n",
        "                'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'primary': {'type': 'boolean', 'description': '주요 채널 여부 - 광고의 핵심 연락 채널이면 true'},\n",
        "                'availability': {'type': 'string', 'description': '채널 이용 가능 시간/조건 - 언급이 없으면 \"상시\"로 설정'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'metadata': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'message_type': {'type': 'string', 'description': '메시지 유형 - 항상 \"광고\"로 설정'},\n",
        "            'target_response': {'type': 'string', 'description': '광고주가 고객으로부터 기대하는 반응 요약'},\n",
        "            'success_metrics': {'type': 'array', 'description': '이 캠페인의 성공을 측정하기 위한 지표들 추출'}\n",
        "        }\n",
        "    }\n",
        "}, \n",
        "'required': ['purpose', 'target', 'product', 'channel', 'metadata'], \n",
        "'objectType': 'object'}\n",
        "\n",
        "# Improved extraction guidance\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 과정 ###\n",
        "1. 먼저 광고 메시지를 전체적으로 읽고 주요 내용, 목적, 혜택, 타겟을 파악하세요.\n",
        "2. \"상품 후보 정보\" 목록을 확인하여 광고에 언급된 제품/서비스와 가장 일치하는 항목을 선택하세요.\n",
        "3. 각 제품별 혜택과 조건을 명확하게 구분하여 추출하세요.\n",
        "4. 모든 채널 정보(URL, 전화번호, 대리점 등)를 빠짐없이 추출하고, tworldfriends.co.kr URL에서 매장 코드를 반드시 추출하세요.\n",
        "5. 각 필드에 적합한 값을 선택할 때 광고 내용에 명시적으로 언급된 정보를 우선하고, 없는 경우에만 적절히 추론하세요.\n",
        "6. 결과 JSON은 완전하고 정확해야 하며, schema에 정의된 모든 필수 필드를 포함해야 합니다.\n",
        "\n",
        "### JSON 응답 형식 ###\n",
        "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 {}여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Custom parser to extract JSON from text response\n",
        "class CustomJsonOutputParser(JsonOutputParser):\n",
        "    def parse(self, text):\n",
        "        # Try to find JSON in the response using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                raise OutputParserException(f\"Failed to parse JSON: {json_str}\")\n",
        "        else:\n",
        "            raise OutputParserException(f\"Could not find JSON in response: {text}\")\n",
        "\n",
        "# Create a prompt template with improved system message - emphasize JSON-only output\n",
        "prompt_cld = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "{schema}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "=====참고 정보=====\n",
        "{rag_context}\n",
        "\n",
        "이제 다음 광고 메시지를 분석하여 schema에 맞는 완전한 JSON 객체를 생성해 주세요.\n",
        "중요: 응답은 설명이나 추가 텍스트 없이 순수한 JSON 형식으로만 제공하세요. 응답은 '{{'로 시작하고 '}}'로 끝나야 합니다.\"\"\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Create the custom parser\n",
        "parser = CustomJsonOutputParser()\n",
        "\n",
        "# Wrapper function to handle parsing failures\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"Extract JSON from LLM response text even if it has explanatory text\"\"\"\n",
        "    try:\n",
        "        # First try to parse as pure JSON\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If that fails, try to extract JSON using regex\n",
        "        json_match = re.search(r'({[\\s\\S]*})', response_text)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Failed to parse extracted JSON pattern: {json_str}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Could not find JSON pattern in response: {response_text}\")\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "c62d55f5-1980-4217-adb4-45c3f4f379f6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "schema_ax = {'properties': {\n",
        "    'message_info': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'title': {'type': 'string', 'description': '광고 제목 - 원본 그대로 추출'},\n",
        "            'main_theme': {'type': 'string', 'description': '광고의 핵심 주제와 가치 제안을 명확하게 설명'},\n",
        "            'period': {'type': 'string', 'description': '이벤트/프로모션 기간 - 명시적으로 언급된 경우에만 추출, 없으면 \"상시\"로 설정'}\n",
        "        }\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'target': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'segment': {'type': 'string', 'description': '타겟 고객층 - 광고에서 명시적으로 언급하거나 암시한 대상 고객'},\n",
        "                'characteristics': {'type': 'string', 'description': '해당 타겟의 특성과 니즈'},\n",
        "                'priority': {'type': 'integer', 'description': '광고 내용에서의 중요도에 따른 타겟팅 우선순위 (1이 최우선)'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름 - \"상품 후보 정보\" 목록에서 가장 일치하는 항목 선택'},\n",
        "                'id': {'type': 'string', 'description': '제품/서비스 ID - \"상품 후보 정보\"에서 name에 일치하는 항목의 ID 선택'},\n",
        "                'category': {'type': 'string', 'description': '제품/서비스 카테고리 - \"상품 후보 정보\"에서 확인'},\n",
        "                'benefit': {\n",
        "                    'type': 'object', \n",
        "                    'properties': {\n",
        "                        'name': {'type': 'string', 'description': '제공되는 구체적인 혜택 내용 상세 설명'},\n",
        "                        'type': {'type': 'string', 'description': '혜택 유형: [할인, 쿠폰, 경품, 기타] 중에서 선택'}\n",
        "                    }\n",
        "                },\n",
        "                'conditions': {'type': 'string', 'description': '혜택/구매를 받기 위한 구체적인 조건'},\n",
        "                'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'primary': {'type': 'boolean', 'description': '주요 채널 여부 - 광고의 핵심 연락 채널이면 true'},\n",
        "                'availability': {'type': 'string', 'description': '채널 이용 가능 시간/조건 - 언급이 없으면 \"상시\"로 설정'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'metadata': {\n",
        "        'type': 'object', \n",
        "        'properties': {\n",
        "            'message_type': {'type': 'string', 'description': '메시지 유형 - 항상 \"광고\"로 설정'},\n",
        "            'target_response': {'type': 'string', 'description': '광고주가 고객으로부터 기대하는 반응 요약'},\n",
        "            'success_metrics': {'type': 'array', 'description': '이 캠페인의 성공을 측정하기 위한 지표들 추출'}\n",
        "        }\n",
        "    }\n",
        "}, \n",
        "'required': ['purpose', 'target', 'product', 'channel', 'metadata'], \n",
        "'objectType': 'object'}\n",
        "\n",
        "# Improved extraction guidance with clear focus on JSON output\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 과정 ###\n",
        "1. 먼저 광고 메시지를 전체적으로 읽고 주요 내용, 목적, 혜택, 타겟을 파악하세요.\n",
        "2. \"상품 후보 정보\" 목록을 확인하여 광고에 언급된 제품/서비스와 가장 일치하는 항목을 선택하세요.\n",
        "3. 각 제품별 혜택과 조건을 명확하게 구분하여 추출하세요.\n",
        "4. 모든 채널 정보(URL, 전화번호, 대리점 등)를 빠짐없이 추출하고, tworldfriends.co.kr URL에서 매장 코드를 반드시 추출하세요.\n",
        "5. 각 필드에 적합한 값을 선택할 때 광고 내용에 명시적으로 언급된 정보를 우선하고, 없는 경우에만 적절히 추론하세요.\n",
        "6. 결과 JSON은 완전하고 정확해야 하며, schema에 정의된 모든 필수 필드를 포함해야 합니다.\n",
        "\n",
        "### JSON 응답 형식 ###\n",
        "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 설명이나 추가 텍스트를 포함하지 마세요.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "dc181efc-5cea-4129-9261-ae350ac58bf6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_llm_result_by_fsl(msg_text, schema_ax, extraction_guide, rag_context, few_shot_exm_str, res_dic_empt, row, fs_model, num_fs=1):\n",
        "\n",
        "    try:\n",
        "                # Create the system message with clear JSON output requirements\n",
        "        system_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "        {json.dumps(schema_ax, indent=2, ensure_ascii=False)}\n",
        "\n",
        "        {extraction_guide}\n",
        "\n",
        "        중요: 응답은 설명이나 추가 텍스트 없이 순수한 JSON 형식으로만 제공하세요. 응답은 '{{'로 시작하고 '}}'로 끝나야 합니다.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create message content with structured sections\n",
        "        user_message = f\"\"\"=====분석 대상 광고 메세지=====\n",
        "        {msg_text}\n",
        "\n",
        "        =====관련 참고 정보=====\n",
        "        {rag_context}\n",
        "\n",
        "        =====학습용 예시=====\n",
        "        {few_shot_exm_str}\n",
        "        \"\"\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "        )\n",
        "    \n",
        "        # Extract the JSON from the response\n",
        "        result_json_text = response.choices[0].message.content\n",
        "        result = extract_json_objects(result_json_text)[0]\n",
        "        row[f'res_ax_f{num_fs}'] = result\n",
        "    except Exception as e:\n",
        "        print(f\"Error message when calling ax_f{num_fs}: \\n{result_json_text}\")\n",
        "        row[f'res_ax_f{num_fs}'] = res_dic_empt#.replace('\\n','')\n",
        "        \n",
        "    return row\n",
        "\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334e96f5-45d0-48b8-9b98-1dca5b6bc91f",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "build_few_shot = False\n",
        "\n",
        "job_resume = False\n",
        "\n",
        "ref_model = \"cld37\"\n",
        "n_sample_fsl = 500\n",
        "\n",
        "ref_model_fs = \"gpt\"\n",
        "\n",
        "max_workers = 1\n",
        "\n",
        "if ref_model == \"cld35\":\n",
        "    llm_modl = llm_cld35\n",
        "elif ref_model == \"cld37\":\n",
        "    llm_modl = llm_cld37\n",
        "else:\n",
        "    llm_modl = llm_chat\n",
        "\n",
        "n_fsl_sta = 2\n",
        "n_fsl_end = 2\n",
        "\n",
        "# Create the parser\n",
        "parser = JsonOutputParser()\n",
        "extraction_chain = prompt_cld | llm_modl | parser\n",
        "extraction_chain_cld35 = prompt_cld | llm_cld35 | parser\n",
        "\n",
        "few_shot_pdf = pd.read_csv(f\"./data/few_shot_data_gpt_250415_rd_500.csv\")\n",
        "few_shot_pdf['index'] = few_shot_pdf['index'].astype('str')\n",
        "few_shot_pdf['offer_dt'] = few_shot_pdf['offer_dt'].astype('str')\n",
        "tfidf_desc = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(3,5),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "tfidf_matrix_desc = tfidf_desc.fit_transform(few_shot_pdf[\"msg_body\"])\n",
        "\n",
        "rep_dict = {'[SK텔레콤]':'','[SKT]':''}\n",
        "special_symbols_to_remove = {'‘':'','’':'',':':'',\"'\":'','\"':'','_':'\\n'}\n",
        "\n",
        "def process_prompt(row, few_shot_pdf, ref_model):\n",
        "    res_dic_empt = {\n",
        "        \"purpose\": \"\",\n",
        "        \"target\": \"\",\n",
        "        \"product\": \"\",\n",
        "        \"channel\": \"\"\n",
        "    }\n",
        "    \n",
        "    head = row['head']\n",
        "    body = row['body']\n",
        "    \n",
        "    head = replace_strings(head, special_symbols_to_remove)\n",
        "    body = replace_strings(body, special_symbols_to_remove)\n",
        "            \n",
        "#     for k,v in special_symbols_to_remove.items():\n",
        "#         head = head.replace(k,v)\n",
        "#         body = body.replace(k,v)\n",
        "    \n",
        "    msg_text = f\"\"\"\n",
        "        광고 제목: {head}\n",
        "        광고 내용: {body}\n",
        "    \"\"\"\n",
        "        \n",
        "#     rag_context = get_relevant_context(text, vectorstore, topk=20)['title']\n",
        "    \n",
        "#     row['res_rag'] = rag_context\n",
        "\n",
        "    # Enhanced entity matching\n",
        "    matches = find_entities_in_text(\n",
        "        msg_text, \n",
        "        entity_list, \n",
        "        min_similarity=65,\n",
        "        ngram_size=4,\n",
        "        min_entity_length=2\n",
        "    )\n",
        "\n",
        "    product_info = \", \".join(set([match['text'] for match in matches]))\n",
        "\n",
        "    mdf = pd.DataFrame(matches)\n",
        "    mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    mdf['category'] = mdf['data'].apply(lambda x: x['category'])\n",
        "    product_info = json.dumps(mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','category']].drop_duplicates().to_dict(orient='records'), ensure_ascii=False)\n",
        "\n",
        "    if not build_few_shot:\n",
        "\n",
        "        store_code = extract_store_code(msg_text)\n",
        "        phone_numbers = extract_phone_numbers(msg_text)\n",
        "        unsubscribe_info = extract_unsubscribe_info(msg_text)\n",
        "\n",
        "        # Improved context with structured auxiliary information\n",
        "        rag_context = \"\\n\".join([\n",
        "            f\"\\t상품 후보 정보: {product_info}\",\n",
        "            f\"\\t매장 코드 정보: '{store_code}'. 이 코드는 type='대리점'인 channel 객체의 store_code 필드에 설정해주세요.\",\n",
        "            f\"\\t수신 거부 정보: {unsubscribe_info}. 이 정보를 type='전화번호', action='수신 거부' channel 객체로 추가해주세요.\"\n",
        "        ])\n",
        "        \n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "        # Assign similarities to dataframe\n",
        "        few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "        few_shot_temp['sim'] = similarities\n",
        "        few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "        # Get examples with better structure - make sure each example shows JSON only in the result\n",
        "        few_shot_exm = []\n",
        "        top_examples = few_shot_temp[\n",
        "            (few_shot_temp['rank'] >= 1) & \n",
        "            (few_shot_temp['rank'] <= 2)\n",
        "        ].sort_values(\"rank\")\n",
        "\n",
        "        for _, r in top_examples.iterrows():\n",
        "            # Ensure the examples show proper JSON-only outputs\n",
        "            example_json = r[f'res_{ref_model_fs}']\n",
        "            \n",
        "            # If the example contains explanatory text, try to extract just the JSON\n",
        "            if not (example_json.startswith('{') and example_json.endswith('}')):\n",
        "                json_match = re.search(r'({[\\s\\S]*})', example_json)\n",
        "                if json_match:\n",
        "                    example_json = json_match.group(1)\n",
        "            \n",
        "            few_shot_exm.append(f\"\"\"\n",
        "        \\t[학습용 광고 메세지_{_}]\n",
        "        \\t\\t광고 제목:{r['msg_head']}\n",
        "        \\t\\t광고 내용:{r['msg_body']}\n",
        "        \\t[학습용 정답 결과_{_}]\n",
        "        \\t\\t{example_json}\"\"\")\n",
        "\n",
        "        few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "        \n",
        "        for nf in range(n_fsl_sta, n_fsl_end+1):\n",
        "            row = get_llm_result_by_fsl(msg_text, schema_ax, extraction_guide, rag_context, few_shot_exm_str, res_dic_empt, row, ref_model, num_fs=nf)\n",
        "    \n",
        "    try:\n",
        "        rag_context = f\"\\t상품 후보 정보: {product_info}\"\n",
        "        result = extraction_chain.invoke({\n",
        "            \"text\": msg_text, \n",
        "            \"schema\": str(schema_cld), \n",
        "            \"rag_context\": rag_context, \n",
        "            # \"few_shot_exm_str\": few_shot_exm_str,\n",
        "            \"extraction_guide\": extraction_guide\n",
        "        })\n",
        "\n",
        "        # result = json.dumps(result, indent=4, ensure_ascii=False)\n",
        "        row[f'res_{ref_model}'] = result\n",
        "        \n",
        "    except Exception as e:\n",
        "        try:\n",
        "            result = extract_json_objects(e)[0]\n",
        "            row[f'res_{ref_model}'] = result\n",
        "        except Exception as e2:\n",
        "            print(f\"Error message when calling {ref_model}: \\n{e2}\")\n",
        "            row[f'res_{ref_model}'] = res_dic_empt\n",
        "\n",
        "    # try:\n",
        "    #     result = extraction_chain_cld35.invoke({\"text\": msg_text, \"schema\": str(schema_cld)\n",
        "    #                                       # , \"rag_context\":rag_context\n",
        "    #                                      })\n",
        "    #     # result = json.dumps(result, indent=4, ensure_ascii=False)\n",
        "    #     row[f'res_cld35'] = result\n",
        "        \n",
        "    # except Exception as e:\n",
        "    #     try:\n",
        "    #         result = extract_json_objects(e)[0]\n",
        "    #         row[f'res_cld35'] = result\n",
        "    #     except Exception as e2:\n",
        "    #         print(f\"Error message when calling cld35: \\n{e2}\")\n",
        "    #         row[f'res_cld35'] = res_dic_empt\n",
        "        \n",
        "                        \n",
        "    return row\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "import random\n",
        "from functools import partial\n",
        "\n",
        "# mdf = mms_pdf.rename(columns={'msg_nm':'head','mms_phrs':'body'}).to_dict(\"records\")\n",
        "\n",
        "if not job_resume:\n",
        "    test_pdf_list = []\n",
        "\n",
        "if build_few_shot:\n",
        "    mdf = (mms_pdf\n",
        "    # .merge(pd.DataFrame(test_pdf_list)[['index',f'res_{ref_model}']],on='index',how='left').query(f\"res_{ref_model}.isna()\").drop(f\"res_{ref_model}\",axis=1)\n",
        "    .rename(columns={'msg_nm':'head','mms_phrs':'body'}).sample(n=(n_sample_fsl-len(test_pdf_list))).to_dict(\"records\"))\n",
        "else:\n",
        "    mdf = mms_pdf.query(\"offer_dt>='20240101'\").merge(few_shot_pdf[['index',f'res_{ref_model_fs}']],on='index',how='left').query(f\"res_{ref_model_fs}.isna()\").drop(f\"res_{ref_model_fs}\",axis=1).rename(columns={'msg_nm':'head','mms_phrs':'body'})\n",
        "    mdf = mdf.sample(n=5).to_dict(\"records\")\n",
        "    # mdf = pd.read_excel(\"./data/mms_extraction_data_250415.xlsx\").rename(columns={'msg_head':'head','msg_body':'body'}).to_dict(\"records\")\n",
        "\n",
        "    \n",
        "# Create a partial function with few_shot_pdf\n",
        "process_prompt_with_pdf = partial(process_prompt, few_shot_pdf=few_shot_pdf, ref_model=ref_model)\n",
        "\n",
        "\n",
        "if max_workers>1:\n",
        "    print(\"Muli-processing starts\")\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        test_pdf_list += list(executor.map(process_prompt_with_pdf, mdf))\n",
        "else:\n",
        "    print(\"Single-processing starts\")\n",
        "    for row in mdf:\n",
        "        processed_row = process_prompt_with_pdf(row)\n",
        "        test_pdf_list.append(processed_row)\n",
        "\n",
        "        if len(test_pdf_list)%10==0:\n",
        "            print(len(test_pdf_list))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution took {elapsed_time:.4f} seconds\")\n",
        "    \n",
        "# test_pdf = pd.DataFrame(test_pdf_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27e5e67",
      "metadata": {},
      "outputs": [],
      "source": [
        "rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body'}).drop(['msg'], axis=1))\n",
        "rdf['res_cld37'] = rdf['res_cld37'].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "# print(rdf[''][0])\n",
        "\n",
        "print(rdf['msg_head'][0], \"\\n\",\n",
        "rdf['msg_body'][0])\n",
        "\n",
        "# msg_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "9472a430-1e30-43c0-bf0d-144fe79240af",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 1000)\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d%H\")\n",
        "\n",
        "if build_few_shot:\n",
        "    rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body'}).drop(['msg'], axis=1))\n",
        "    rdf = rdf[(rdf[f\"res_{ref_model}\"].apply(lambda x: x.get('purpose') != ''))]\n",
        "    rdf.to_csv(f\"./data/few_shot_data_{ref_model}_{timestamp}_rd_{n_sample_fsl}.csv\", index=False)\n",
        "else:\n",
        "    rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body',f'res_ax_f{n_fsl_end}':f'res_ax_{ref_model}'}))\n",
        "    rdf = rdf[[c for c in rdf.columns if not (c.startswith('fsl_') or c.startswith('cot_'))]]\n",
        "    rdf['msg_body'] = rdf['msg_body'].apply(remove_control_characters)\n",
        "    rdf = rdf[(rdf[f'res_ax_{ref_model}'].apply(lambda x: x.get('purpose') != '')) & (rdf[f\"res_{ref_model}\"].apply(lambda x: x.get('purpose') != ''))]\n",
        "    rdf[f'res_ax_{ref_model}'] = rdf[f'res_ax_{ref_model}'].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "    rdf[f\"res_{ref_model}\"] = rdf[f\"res_{ref_model}\"].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "\n",
        "    rdf.sort_values('offer_dt').to_excel(f\"./data/mms_extraction_data_{timestamp}.xlsx\", index=False, engine='openpyxl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "81db0618-52f0-4508-b1fa-7cc990209ff9",
      "metadata": {
        "tags": []
      },
      "source": [
        "import re\n",
        "\n",
        "tdf = few_shot_pdf.copy()\n",
        "\n",
        "# Function to remove control characters\n",
        "def remove_control_chars(s):\n",
        "    if isinstance(s, str):\n",
        "        return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', s)\n",
        "    return s\n",
        "\n",
        "# Apply to all object columns\n",
        "for col in tdf.select_dtypes(include=['object']).columns:\n",
        "    tdf[col] = tdf[col].apply(remove_control_chars)\n",
        "\n",
        "# tdf.to_excel(\"/home/skinet/myfiles/tos_ace/data/few_shot_data_250408.xlsx\", index=False, encoding='cp949', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99908dfe-e5e3-4b17-8b03-6c3a4e2b985c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body'}).drop(['msg'], axis=1))\n",
        "\n",
        "for nf in (list(range(n_fsl_sta, n_fsl_end+1))):\n",
        "    rdf = rdf[(rdf[f'res_ax_f{nf}'].apply(lambda x: x.get('purpose') != ''))]\n",
        "\n",
        "    # rdf = rdf[(rdf['res_ax_f'].apply(lambda x: x.get('purpose') != '')) & (rdf['res_ax_n'].apply(lambda x: x.get('purpose') != ''))]\n",
        "sdf = rdf.copy()\n",
        "# # sdf = pd.read_pickle(\"/home/skinet/myfiles/tos_ace/data/mms_extraction_data_vl_250403_01.pkl\")\n",
        "\n",
        "for nf in (list(range(n_fsl_sta, n_fsl_end+1))):\n",
        "    sdf[f'sim_f{nf}'] = sdf.apply(lambda x : calculate_dictionary_similarity(x[f'res_ax_f{nf}'], x[f'res_{ref_model}']), axis=1)\n",
        "    # sdf[f'sim_f{nf}'] = sdf.apply(lambda x : calculate_dictionary_similarity(x[f'res_ax_f{nf}'], x[f'res_cld35']), axis=1)\n",
        "    print(nf, \"\\n\", pd.json_normalize(sdf[f'sim_f{nf}']).mean())\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "cb4a7b40-83d0-4118-8aa0-8003b7860a42",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "1 \n",
        " overall_similarity         0.788906\n",
        "message_info_similarity    0.808221\n",
        "purpose_similarity         0.675500\n",
        "target_similarity          0.583016\n",
        "product_similarity         0.612335\n",
        "channel_similarity         0.731023\n",
        "meta_similarity            0.534437\n",
        "dtype: float64\n",
        "2 \n",
        " overall_similarity         0.821625\n",
        "message_info_similarity    0.828928\n",
        "purpose_similarity         0.692000\n",
        "target_similarity          0.624887\n",
        "product_similarity         0.655527\n",
        "channel_similarity         0.763915\n",
        "meta_similarity            0.542867\n",
        "dtype: float64\n",
        "3 \n",
        " overall_similarity         0.823776\n",
        "message_info_similarity    0.825154\n",
        "purpose_similarity         0.707167\n",
        "target_similarity          0.630087\n",
        "product_similarity         0.653435\n",
        "channel_similarity         0.764581\n",
        "meta_similarity            0.538457\n",
        "dtype: float64\n",
        "4 \n",
        " overall_similarity         0.836997\n",
        "message_info_similarity    0.838324\n",
        "purpose_similarity         0.720500\n",
        "target_similarity          0.631209\n",
        "product_similarity         0.675245\n",
        "channel_similarity         0.767200\n",
        "meta_similarity            0.552510\n",
        "dtype: float64\n",
        "5 \n",
        " overall_similarity         0.835741\n",
        "message_info_similarity    0.828243\n",
        "purpose_similarity         0.715333\n",
        "target_similarity          0.653488\n",
        "product_similarity         0.656675\n",
        "channel_similarity         0.768541\n",
        "meta_similarity            0.556425\n",
        "dtype: float64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "7cd60c94-84f2-4848-80ab-32cd506db132",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def json_to_custom_string(json_obj, indent=0):\n",
        "    result = []\n",
        "    indent_str = \"  \" * indent\n",
        "    \n",
        "    if isinstance(json_obj, dict):\n",
        "        for key, value in json_obj.items():\n",
        "            if isinstance(value, (dict, list)):\n",
        "                result.append(f\"{indent_str}{key}:\")\n",
        "                result.append(json_to_custom_string(value, indent + 1))\n",
        "            else:\n",
        "                result.append(f\"{indent_str}{key}: {value}\")\n",
        "    elif isinstance(json_obj, list):\n",
        "        for item in json_obj:\n",
        "            if isinstance(item, (dict, list)):\n",
        "                result.append(json_to_custom_string(item, indent))\n",
        "            else:\n",
        "                result.append(f\"{indent_str}- {item}\")\n",
        "    \n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "\n",
        "# json_to_custom_string(parsed_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1456f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3.12 install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d30a5394",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}