# %%
"""
MMS Extractor - ë°ì´í„° ë¡œë”© ë° ì´ˆê¸°í™” ëª¨ë“ˆ
===========================================

ì´ ëª¨ë“ˆì€ MMSExtractorì˜ ë°ì´í„° ë¡œë”© ë° ì´ˆê¸°í™” ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Mixin íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ MMSExtractor í´ë˜ìŠ¤ì— í†µí•©ë©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- LLM ëª¨ë¸ ì´ˆê¸°í™”
- ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”  
- Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
- ë°ì´í„° íŒŒì¼ ë¡œë“œ (ìƒí’ˆ, í”„ë¡œê·¸ë¨, ì¡°ì§ ì •ë³´)
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
"""

import time
import logging
import traceback
from typing import List
import os
import pandas as pd
import torch
from contextlib import contextmanager
import cx_Oracle
from kiwipiepy import Kiwi
from langchain_openai import ChatOpenAI
from joblib import Parallel, delayed
from dotenv import load_dotenv

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì„í¬íŠ¸
from utils import (
    log_performance,
    load_sentence_transformer,
    preprocess_text,
    select_most_comprehensive
)

# ì„¤ì • ì„í¬íŠ¸
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class MMSExtractorDataMixin:
    """
    MMS Extractor ë°ì´í„° ë¡œë”© ë° ì´ˆê¸°í™” Mixin
    
    ì´ í´ë˜ìŠ¤ëŠ” MMSExtractorì˜ ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”© ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def _set_default_config(self, model_path, data_dir, product_info_extraction_mode, 
                          entity_extraction_mode, offer_info_data_src, llm_model):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì ìš©"""
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else getattr(EMBEDDING_CONFIG, 'ko_sbert_model_path', 'jhgan/ko-sroberta-multitask')
        self.offer_info_data_src = offer_info_data_src
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'product_info_extraction_mode', 'nlp')
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'entity_extraction_mode', 'llm')
        self.llm_model_name = llm_model
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)

    @log_performance
    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    @log_performance
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì„¤ì • ë§¤í•‘
            model_mapping = {
                "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "gem": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "ax": getattr(MODEL_CONFIG, 'ax_model', 'ax-4'),
                "claude": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),
                "cld": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),
                "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'gpt-4')
            }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'gemini-pro'))
            
            # LLM ëª¨ë¸ë³„ ì¼ê´€ì„± ì„¤ì •
            model_kwargs = {
                "temperature": 0.0,
                "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ê°•í™”
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42
                
            self.llm_model = ChatOpenAI(**model_kwargs)
            
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ë¹„í™œì„±í™” ì˜µì…˜ í™•ì¸
        if MODEL_CONFIG.disable_embedding:
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¹„í™œì„±í™” ëª¨ë“œ (DISABLE_EMBEDDING=true)")
            self.emb_model = None
            return
            
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            try:
                self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)
            except Exception as e2:
                logger.error(f"Fallback ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                logger.warning("ì„ë² ë”© ëª¨ë¸ ì—†ì´ ë™ì‘ ëª¨ë“œë¡œ ì „í™˜")
                self.emb_model = None

    def _initialize_multiple_llm_models(self, model_names: List[str]) -> List:
        """
        ë³µìˆ˜ì˜ LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ
        
        Args:
            model_names (List[str]): ì´ˆê¸°í™”í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List: ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        llm_models = []
        
        # ëª¨ë¸ëª… ë§¤í•‘
        model_mapping = {
            "cld": getattr(MODEL_CONFIG, 'anthropic_model', 'amazon/anthropic/claude-sonnet-4-20250514'),
            "ax": getattr(MODEL_CONFIG, 'ax_model', 'skt/ax4'),
            "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'azure/openai/gpt-4o-2024-08-06'),
            "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gcp/gemini-2.5-flash')
        }
        
        for model_name in model_names:
            try:
                actual_model_name = model_mapping.get(model_name, model_name)
                
                # ëª¨ë¸ë³„ ì„¤ì •
                model_kwargs = {
                    "temperature": 0.0,
                    "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                    "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                    "model": actual_model_name,
                    "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
                }
                
                # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •
                if 'gpt' in actual_model_name.lower():
                    model_kwargs["seed"] = 42
                
                llm_model = ChatOpenAI(**model_kwargs)
                llm_models.append(llm_model)
                logger.info(f"âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name} ({actual_model_name})")
                
            except Exception as e:
                logger.error(f"âŒ LLM ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {model_name} - {e}")
                continue
        
        return llm_models

    @log_performance
    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _load_data(self):
        """í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ëª¨ë“œ: {self.offer_info_data_src}")
            
            # ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„
            logger.info("1ï¸âƒ£ ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì¤‘...")
            self._load_item_data()
            logger.info(f"ìƒí’ˆ ì •ë³´ ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì •ì§€ì–´ ë¡œë“œ
            logger.info("2ï¸âƒ£ ì •ì§€ì–´ ë¡œë“œ ì¤‘...")
            self._load_stopwords()
            logger.info(f"ë¡œë“œëœ ì •ì§€ì–´ ìˆ˜: {len(self.stop_item_names)}ê°œ")
            
            # Kiwiì— ìƒí’ˆëª… ë“±ë¡
            logger.info("3ï¸âƒ£ Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì¤‘...")
            self._register_items_in_kiwi()
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ
            logger.info("4ï¸âƒ£ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_program_data()
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì¡°ì§ ì •ë³´ ë¡œë“œ
            logger.info("5ï¸âƒ£ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_organization_data()
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
            
            # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ - ìµœì¢… ìƒíƒœ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ìƒí’ˆ ë°ì´í„°: {self.item_pdf_all.shape}")
            logger.info(f"âœ… í”„ë¡œê·¸ë¨ ë°ì´í„°: {self.pgm_pdf.shape}")
            logger.info(f"âœ… ì¡°ì§ ë°ì´í„°: {self.org_pdf.shape}")
            logger.info(f"âœ… ì •ì§€ì–´: {len(self.stop_item_names)}ê°œ")
            
            # ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒíƒœ ë¹„êµë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´
            if hasattr(self, 'item_pdf_all') and not self.item_pdf_all.empty:
                logger.info("=== ìƒí’ˆ ë°ì´í„° ìƒì„¸ ì •ë³´ ===")
                if 'item_nm' in self.item_pdf_all.columns:
                    unique_items = self.item_pdf_all['item_nm'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆëª… ìˆ˜: {unique_items}ê°œ")
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    unique_aliases = self.item_pdf_all['item_nm_alias'].nunique()
                    logger.info(f"ê³ ìœ  ë³„ì¹­ ìˆ˜: {unique_aliases}ê°œ")
                if 'item_id' in self.item_pdf_all.columns:
                    unique_ids = self.item_pdf_all['item_id'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆID ìˆ˜: {unique_ids}ê°œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_item_data(self):
        """
        ìƒí’ˆ ì •ë³´ ë¡œë“œ (ItemDataLoaderë¡œ ìœ„ì„)
        
        ê¸°ì¡´ 197ì¤„ì˜ ë³µì¡í•œ ë¡œì§ì„ ItemDataLoader ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬
        ì¬ì‚¬ìš©ì„±ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.
        """
        try:
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            # 1ë‹¨ê³„: ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ
            if self.offer_info_data_src == "local":
                logger.info("ğŸ“ ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ")
                csv_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/items.csv')
                item_pdf_raw = pd.read_csv(csv_path)
            elif self.offer_info_data_src == "db":
                logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ")
                # Import DATABASE_CONFIG
                from config.settings import DATABASE_CONFIG
                with self._database_connection() as conn:
                    sql = DATABASE_CONFIG.get_offer_table_query()
                    item_pdf_raw = pd.read_sql(sql, conn)
            
            logger.info(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {item_pdf_raw.shape}")
            
            # 2ë‹¨ê³„: ê³µí†µ ì „ì²˜ë¦¬
            item_pdf_raw['ITEM_DESC'] = item_pdf_raw['ITEM_DESC'].astype('str')
            
            # ë‹¨ë§ê¸°ì¸ ê²½ìš° ì„¤ëª…ì„ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©
            item_pdf_raw['ITEM_NM'] = item_pdf_raw.apply(
                lambda x: x['ITEM_DESC'] if x['ITEM_DMN']=='E' else x['ITEM_NM'], axis=1
            )
            
            # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
            item_pdf_all = item_pdf_raw.rename(columns={c: c.lower() for c in item_pdf_raw.columns})
            logger.info(f"ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")
            
            # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
            item_pdf_all['item_ctg'] = None
            item_pdf_all['item_emb_vec'] = None
            item_pdf_all['ofer_cd'] = item_pdf_all['item_id']
            item_pdf_all['oper_dt_hms'] = '20250101000000'
            
            # ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ í•„í„°ë§
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
            if excluded_domains:
                before_filter = len(item_pdf_all)
                item_pdf_all = item_pdf_all.query("item_dmn not in @excluded_domains")
                logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter} -> {len(item_pdf_all)}")
            
            # 3ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì²˜ë¦¬
            logger.info("ğŸ”— ë³„ì¹­ ê·œì¹™ ë¡œë“œ ì¤‘...")
            self.alias_pdf_raw = pd.read_csv(getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv'))
            alias_pdf = self.alias_pdf_raw.copy()
            alias_pdf['alias_1'] = alias_pdf['alias_1'].str.split("&&")
            alias_pdf['alias_2'] = alias_pdf['alias_2'].str.split("&&")
            alias_pdf = alias_pdf.explode('alias_1')
            alias_pdf = alias_pdf.explode('alias_2')
            
            # build íƒ€ì… ë³„ì¹­ í™•ì¥
            alias_list_ext = alias_pdf.query("type=='build'")[['alias_1','category','direction','type']].to_dict('records')
            for alias in alias_list_ext:
                adf = item_pdf_all.query(
                    "item_nm.str.contains(@alias['alias_1']) and item_dmn==@alias['category']"
                )[['item_nm','item_desc','item_dmn']].rename(
                    columns={'item_nm':'alias_2','item_desc':'description','item_dmn':'category'}
                ).drop_duplicates()
                adf['alias_1'] = alias['alias_1']
                adf['direction'] = alias['direction']
                adf['type'] = alias['type']
                adf = adf[alias_pdf.columns]
                alias_pdf = pd.concat([alias_pdf.query(f"alias_1!='{alias['alias_1']}'"), adf])
            
            alias_pdf = alias_pdf.drop_duplicates()
            
            # ì–‘ë°©í–¥(B) ë³„ì¹­ ì¶”ê°€
            alias_pdf = pd.concat([
                alias_pdf, 
                alias_pdf.query("direction=='B'").rename(
                    columns={'alias_1':'alias_2', 'alias_2':'alias_1'}
                )[alias_pdf.columns]
            ])
            
            alias_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2'], alias_pdf['type']))
            logger.info(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")
            
            # 4ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬)
            def apply_alias_rule_cascade_parallel(args_dict):
                """ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš©"""
                item_nm = args_dict['item_nm']
                max_depth = args_dict['max_depth']
                
                processed = set()
                result_dict = {item_nm: '#' * len(item_nm)}
                to_process = [(item_nm, 0, frozenset())]
                
                while to_process:
                    current_item, depth, path_applied_rules = to_process.pop(0)
                    
                    if depth >= max_depth or current_item in processed:
                        continue
                    
                    processed.add(current_item)
                    
                    for r in alias_rule_set:
                        alias_from, alias_to, alias_type = r[0], r[1], r[2]
                        rule_key = (alias_from, alias_to, alias_type)
                        
                        if rule_key in path_applied_rules:
                            continue
                        
                        # íƒ€ì…ì— ë”°ë¥¸ ë§¤ì¹­
                        if alias_type == 'exact':
                            matched = (current_item == alias_from)
                        else:
                            matched = (alias_from in current_item)
                        
                        if matched:
                            new_item = alias_to.strip() if alias_type == 'exact' else current_item.replace(alias_from.strip(), alias_to.strip())
                            
                            if new_item not in result_dict:
                                result_dict[new_item] = alias_from.strip()
                                to_process.append((new_item, depth + 1, path_applied_rules | {rule_key}))
                
                item_nm_list = [{'item_nm': k, 'item_nm_alias': v} for k, v in result_dict.items()]
                adf = pd.DataFrame(item_nm_list)
                selected_alias = select_most_comprehensive(adf['item_nm_alias'].tolist())
                result_aliases = list(adf.query("item_nm_alias in @selected_alias")['item_nm'].unique())
                
                if item_nm not in result_aliases:
                    result_aliases.append(item_nm)
                
                return {'item_nm': item_nm, 'item_nm_alias': result_aliases}
            
            def parallel_alias_rule_cascade(texts, max_depth=5, n_jobs=None):
                """ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš©"""
                if n_jobs is None:
                    n_jobs = min(os.cpu_count()-1, 4)
                
                batches = [{"item_nm": text, "max_depth": max_depth} for text in texts]
                with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                    batch_results = parallel(delayed(apply_alias_rule_cascade_parallel)(args) for args in batches)
                
                return pd.DataFrame(batch_results)
            
            logger.info("ğŸ”„ ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© ì¤‘...")
            item_alias_pdf = parallel_alias_rule_cascade(item_pdf_all['item_nm'], max_depth=3)
            
            # ë³„ì¹­ ë³‘í•© ë° explode
            item_pdf_all = item_pdf_all.merge(item_alias_pdf, on='item_nm', how='left')
            before_explode = len(item_pdf_all)
            item_pdf_all = item_pdf_all.explode('item_nm_alias').drop_duplicates()
            logger.info(f"ë³„ì¹­ explode: {before_explode} -> {len(item_pdf_all)}")
            
            # 5ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
            user_defined_entity = ['AIA Vitality', 'ë¶€ìŠ¤íŠ¸ íŒŒí¬ ê±´ëŒ€ì…êµ¬', 'Boost Park ê±´ëŒ€ì…êµ¬']
            item_pdf_ext = pd.DataFrame([{
                'item_nm': e, 'item_id': e, 'item_desc': e, 'item_dmn': 'user_defined',
                'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e
            } for e in user_defined_entity])
            item_pdf_all = pd.concat([item_pdf_all, item_pdf_ext])
            
            # 6ë‹¨ê³„: item_dmn_nm ì»¬ëŸ¼ ì¶”ê°€
            item_dmn_map = pd.DataFrame([
                {"item_dmn": 'P', 'item_dmn_nm': 'ìš”ê¸ˆì œ ë° ê´€ë ¨ ìƒí’ˆ'},
                {"item_dmn": 'E', 'item_dmn_nm': 'ë‹¨ë§ê¸°'},
                {"item_dmn": 'S', 'item_dmn_nm': 'êµ¬ë… ìƒí’ˆ'},
                {"item_dmn": 'C', 'item_dmn_nm': 'ì¿ í°'},
                {"item_dmn": 'X', 'item_dmn_nm': 'ê°€ìƒ ìƒí’ˆ'}
            ])
            item_pdf_all = item_pdf_all.merge(item_dmn_map, on='item_dmn', how='left')
            item_pdf_all['item_dmn_nm'] = item_pdf_all['item_dmn_nm'].fillna('ê¸°íƒ€')
            
            # 7ë‹¨ê³„: TEST í•„í„°ë§
            before_test = len(item_pdf_all)
            item_pdf_all = item_pdf_all.query("not item_nm_alias.str.contains('TEST', case=False, na=False)")
            logger.info(f"TEST í•„í„°ë§: {before_test} -> {len(item_pdf_all)}")
            
            self.item_pdf_all = item_pdf_all
            
            # ìµœì¢… í™•ì¸
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ì¤€ë¹„ ì™„ë£Œ ===")
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in self.item_pdf_all.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            else:
                logger.info("âœ… ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not self.item_pdf_all.empty:
                logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {self.item_pdf_all['item_nm'].dropna().head(3).tolist()}")
                logger.info(f"ë³„ì¹­ ìƒ˜í”Œ: {self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()}")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.item_pdf_all = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_nm_alias'])
            logger.warning("ë¹ˆ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")

    def _get_database_connection(self):
        """Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘ ===")
            
            username = os.getenv("DB_USERNAME")
            password = os.getenv("DB_PASSWORD")
            host = os.getenv("DB_HOST")
            port = os.getenv("DB_PORT")
            service_name = os.getenv("DB_NAME")
            
            # ì—°ê²° ì •ë³´ ë¡œê¹… (ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹)
            logger.info(f"DB ì—°ê²° ì •ë³´:")
            logger.info(f"  - ì‚¬ìš©ìëª…: {username if username else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ë¹„ë°€ë²ˆí˜¸: {'*' * len(password) if password else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í˜¸ìŠ¤íŠ¸: {host if host else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í¬íŠ¸: {port if port else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ì„œë¹„ìŠ¤ëª…: {service_name if service_name else '[ë¹„ì–´ìˆìŒ]'}")
            
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            missing_vars = []
            if not username: missing_vars.append('DB_USERNAME')
            if not password: missing_vars.append('DB_PASSWORD')
            if not host: missing_vars.append('DB_HOST')
            if not port: missing_vars.append('DB_PORT')
            if not service_name: missing_vars.append('DB_NAME')
            
            if missing_vars:
                logger.error(f"ëˆ„ë½ëœ í™˜ê²½ ë³€ìˆ˜: {missing_vars}")
                logger.error("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì„ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
                raise ValueError(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ëˆ„ë½: {missing_vars}")
            
            # DSN ìƒì„± ë° ë¡œê¹…
            logger.info(f"DSN ìƒì„± ì¤‘: {host}:{port}/{service_name}")
            dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
            logger.info(f"DSN ìƒì„± ì„±ê³µ: {dsn}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            conn = cx_Oracle.connect(user=username, password=password, dsn=dsn, encoding="UTF-8")
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            
            # LOB ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ outputtypehandler ì„¤ì •
            def output_type_handler(cursor, name, default_type, size, precision, scale):
                if default_type == cx_Oracle.CLOB:
                    return cursor.var(cx_Oracle.LONG_STRING, arraysize=cursor.arraysize)
                elif default_type == cx_Oracle.BLOB:
                    return cursor.var(cx_Oracle.LONG_BINARY, arraysize=cursor.arraysize)
            
            conn.outputtypehandler = output_type_handler
            
            # ì—°ê²° ì •ë³´ í™•ì¸
            logger.info(f"ì—°ê²°ëœ DB ë²„ì „: {conn.version}")
            
            return conn
            
        except cx_Oracle.DatabaseError as db_error:
            error_obj, = db_error.args
            logger.error(f"Oracle ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜:")
            logger.error(f"  - ì˜¤ë¥˜ ì½”ë“œ: {error_obj.code}")
            logger.error(f"  - ì˜¤ë¥˜ ë©”ì‹œì§€: {error_obj.message}")
            logger.error(f"  - ì „ì²´ ì˜¤ë¥˜: {db_error}")
            raise
        except ImportError as import_error:
            logger.error(f"cx_Oracle ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {import_error}")
            logger.error("ì½”ë§¨ë“œ: pip install cx_Oracle")
            raise
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    @contextmanager
    def _database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager"""
        conn = None
        start_time = time.time()
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager ì‹œì‘")
            conn = self._get_database_connection()
            connection_time = time.time() - start_time
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ ({connection_time:.2f}ì´ˆ)")
            yield conn
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise
        finally:
            if conn:
                try:
                    conn.close()
                    total_time = time.time() - start_time
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
                except Exception as close_error:
                    logger.warning(f"ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {close_error}")
            else:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _load_program_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            # Import DATABASE_CONFIG
            from config.settings import DATABASE_CONFIG
            
            with self._database_connection() as conn:
                # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ì¿¼ë¦¬
                where_clause = """DEL_YN = 'N' 
                         AND APRV_OP_RSLT_CD = 'APPR'
                         AND EXPS_YN = 'Y'
                         AND CMPGN_PGM_NUM like '2025%' 
                         AND RMK is not null"""
                sql = DATABASE_CONFIG.get_program_table_query(where_clause)
                
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.pgm_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ë“¤: {list(self.pgm_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.pgm_pdf.columns)
                self.pgm_pdf = self.pgm_pdf.rename(columns={c:c.lower() for c in self.pgm_pdf.columns})
                logger.info(f"í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.pgm_pdf.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.pgm_pdf.empty:
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.pgm_pdf.values
                        
                        # í”„ë¡œê·¸ë¨ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                        if 'pgm_nm' in self.pgm_pdf.columns:
                            sample_pgms = self.pgm_pdf['pgm_nm'].dropna().head(3).tolist()
                            logger.info(f"í”„ë¡œê·¸ë¨ëª… ìƒ˜í”Œ: {sample_pgms}")
                        
                        if 'clue_tag' in self.pgm_pdf.columns:
                            sample_clues = self.pgm_pdf['clue_tag'].dropna().head(3).tolist()
                            logger.info(f"í´ë£¨ íƒœê·¸ ìƒ˜í”Œ: {sample_clues}")
                            
                        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ")
                    except Exception as load_error:
                        logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            raise

    def _load_stopwords(self):
        """ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ"""
        try:
            self.stop_item_names = pd.read_csv(getattr(METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv'))['stop_words'].to_list()
            logger.info(f"ì •ì§€ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.stop_item_names)}ê°œ")
        except Exception as e:
            logger.warning(f"ì •ì§€ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stop_item_names = []

    def _register_items_in_kiwi(self):
        """Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡"""
        try:
            logger.info("=== Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… ë³„ì¹­ ë°ì´í„° í™•ì¸
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                return
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë“±ë¡í•  ê³ ìœ  ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # nullì´ ì•„ë‹Œ ìœ íš¨í•œ ë³„ì¹­ë“¤ë§Œ í•„í„°ë§
            valid_aliases = [w for w in unique_aliases if isinstance(w, str) and len(w.strip()) > 0]
            logger.info(f"ìœ íš¨í•œ ë³„ì¹­ ìˆ˜: {len(valid_aliases)}ê°œ")
            
            if len(valid_aliases) > 0:
                sample_aliases = valid_aliases[:5]
                logger.info(f"ë“±ë¡í•  ë³„ì¹­ ìƒ˜í”Œ: {sample_aliases}")
            
            registered_count = 0
            failed_count = 0
            
            for w in valid_aliases:
                try:
                    self.kiwi.add_user_word(w, "NNP")
                    registered_count += 1
                except Exception as reg_error:
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ë¡œê¹…
                        logger.warning(f"Kiwi ë“±ë¡ ì‹¤íŒ¨ - '{w}': {reg_error}")
            
            logger.info(f"Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"Kiwi ìƒí’ˆëª… ë“±ë¡ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

    def _load_program_data(self):
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
        try:
            logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë”© ì‹œì‘...")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                self.pgm_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'pgm_info_path', './data/program_info.csv'))
                logger.info(f"ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                self._load_program_from_database()
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
            if not self.pgm_pdf.empty:
                logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
                clue_texts = self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                    lambda x: preprocess_text(x['pgm_nm'].lower()) + " " + x['clue_tag'].lower(), axis=1
                ).tolist()
                
                if self.emb_model is not None:
                    self.clue_embeddings = self.emb_model.encode(
                        clue_texts, convert_to_tensor=True, show_progress_bar=False
                    )
                else:
                    logger.warning("ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë¹ˆ tensor ì‚¬ìš©")
                    self.clue_embeddings = torch.empty((0, 768))
                
                logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ í”„ë¡œê·¸ë¨")
            else:
                logger.warning("í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.clue_embeddings = torch.tensor([])
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            self.clue_embeddings = torch.tensor([])

    def _load_organization_data(self):
        """ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'org_info_path', './data/org_info_all_250605.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                org_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ì¡°ì§ ë°ì´í„° í¬ê¸°: {org_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(org_pdf_raw.columns)}")
                
                # ITEM_DMN='R' ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§
                if 'ITEM_DMN' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("ITEM_DMN=='R'").copy()
                elif 'item_dmn' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("item_dmn=='R'").copy()
                else:
                    logger.warning("ITEM_DMN/item_dmn ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.org_pdf = org_pdf_raw.copy()
                
                # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë¦¬ë„¤ì„
                self.org_pdf = self.org_pdf.rename(columns={c: c.lower() for c in self.org_pdf.columns})
                
                logger.info(f"ë¡œì»¬ ëª¨ë“œ: ITEM_DMN='R' í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_org_from_database()
            
            # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
            if not self.org_pdf.empty:
                sample_orgs = self.org_pdf.head(3).to_dict('records')
                logger.info(f"ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ (3ê°œ í–‰): {sample_orgs}")
            
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.org_pdf)}ê°œ ì¡°ì§ ===")
            logger.info(f"ìµœì¢… ì¡°ì§ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: {list(self.org_pdf.columns)}")
            
            # ì¡°ì§ ë°ì´í„° ìµœì¢… ê²€ì¦
            if not self.org_pdf.empty:
                critical_org_columns = ['item_nm', 'item_id']
                missing_org_columns = [col for col in critical_org_columns if col not in self.org_pdf.columns]
                if missing_org_columns:
                    logger.error(f"ì¡°ì§ ë°ì´í„°ì—ì„œ ì¤‘ìš” ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_org_columns}")
                    logger.error("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    logger.info("ëª¨ë“  ì¤‘ìš” ì¡°ì§ ì»¬ëŸ¼ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¡°ì§/ë§¤ì¥ ì¶”ì¶œì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ ì¡°ì§ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")
            logger.warning("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    def _load_org_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ (ITEM_DMN='R')"""
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            
            # Import DATABASE_CONFIG
            from config.settings import DATABASE_CONFIG
            
            with self._database_connection() as conn:
                sql = DATABASE_CONFIG.get_offer_table_query("ITEM_DMN='R'")
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.org_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.org_pdf.columns)
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì›ë³¸ ì»¬ëŸ¼ë“¤: {original_columns}")
                
                # ì¡°ì§ ë°ì´í„°ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ë§¤í•‘
                column_mapping = {c: c.lower() for c in self.org_pdf.columns}
                
                self.org_pdf = self.org_pdf.rename(columns=column_mapping)
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ì»¬ëŸ¼ëª… ë§¤í•‘ ì™„ë£Œ: {dict(zip(original_columns, self.org_pdf.columns))}")
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ë° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
                if not self.org_pdf.empty:
                    logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìµœì¢… í¬ê¸°: {self.org_pdf.shape}")
                    
                    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    required_columns = ['item_nm', 'item_id']
                    missing_columns = [col for col in required_columns if col not in self.org_pdf.columns]
                    if missing_columns:
                        logger.error(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                        logger.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                    else:
                        logger.info("ëª¨ë“  í•„ìˆ˜ ì¡°ì§ ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    
                    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
                    if 'item_nm' in self.org_pdf.columns:
                        sample_orgs = self.org_pdf['item_nm'].dropna().head(5).tolist()
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ëª… ìƒ˜í”Œ: {sample_orgs}")
                    else:
                        logger.error("item_nm ì»¬ëŸ¼ì´ ì—†ì–´ ìƒ˜í”Œì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ í‘œì‹œ
                        sample_data = self.org_pdf.head(3).to_dict('records')
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ: {sample_data}")
                else:
                    logger.warning("DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                
                logger.info(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.org_pdf)}ê°œ ì¡°ì§")
                
        except Exception as e:
            logger.error(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"DB ì¡°ì§ ë¡œë“œ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ì¡°ì§ ë°ì´í„° DB ë¡œë“œ ì‹¤íŒ¨ë¡œ ë¹ˆ DataFrame ì‚¬ìš©")
            
            raise
