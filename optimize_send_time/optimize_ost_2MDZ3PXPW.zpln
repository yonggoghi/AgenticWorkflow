{
  "paragraphs": [
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768175047641_1348097503",
      "id": "paragraph_1768175047641_1348097503",
      "dateCreated": "2026-01-11T23:44:07+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2286",
      "text": "%spark-local\r\n\r\nimport org.apache.spark.sql.{DataFrame, SparkSession, Row}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport com.google.ortools.Loader\r\nimport com.google.ortools.linearsolver.{MPConstraint, MPObjective, MPSolver, MPVariable}\r\nimport scala.collection.mutable\r\nimport java.text.DecimalFormat\r\nimport org.apache.spark.sql.expressions.Window\r\n",
      "dateUpdated": "2026-01-14T01:33:12+0000",
      "dateFinished": "2026-01-14T01:33:39+0000",
      "dateStarted": "2026-01-14T01:33:12+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.{DataFrame, SparkSession, Row}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport com.google.ortools.Loader\nimport com.google.ortools.linearsolver.{MPConstraint, MPObjective, MPSolver, MPVariable}\nimport scala.collection.mutable\nimport java.text.DecimalFormat\nimport org.apache.spark.sql.expressions.Window\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768175027528_175907068",
      "id": "paragraph_1768175027528_175907068",
      "dateCreated": "2026-01-11T23:43:47+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2219",
      "text": "%spark-local\r\n\r\ncase class AllocationResult(\r\n  svc_mgmt_num: String,\r\n  assigned_hour: Int,\r\n  score: Double\r\n)\r\n\r\nval numFormatter = new DecimalFormat(\"#,###\")\r\n\r\n/**\r\n * DataFrame에서 사용자 데이터 수집\r\n */\r\ndef collectUserData(df: DataFrame): Map[String, Map[Int, Double]] = {\r\n  df.collect()\r\n    .groupBy(_.getAs[String](\"svc_mgmt_num\"))\r\n    .map { case (userId, rows) =>\r\n      userId -> rows.map { row =>\r\n        row.getAs[Int](\"send_hour\") -> row.getAs[Double](\"propensity_score\")\r\n      }.toMap\r\n    }\r\n    .toMap\r\n}\r\n\r\n/**\r\n * 의사결정 변수 생성\r\n */\r\ndef createVariables(\r\n  solver: MPSolver,\r\n  users: Array[String],\r\n  hours: Array[Int],\r\n  userData: Map[String, Map[Int, Double]]\r\n): mutable.Map[(String, Int), MPVariable] = {\r\n  \r\n  val variables = mutable.Map[(String, Int), MPVariable]()\r\n  \r\n  for {\r\n    user <- users\r\n    hour <- hours\r\n    if userData(user).contains(hour)\r\n  } {\r\n    val varName = s\"x_${user}_$hour\"\r\n    variables((user, hour)) = solver.makeBoolVar(varName)\r\n  }\r\n  \r\n  variables\r\n}\r\n\r\n/**\r\n * 목적 함수 설정\r\n */\r\ndef setObjective(\r\n  solver: MPSolver,\r\n  variables: mutable.Map[(String, Int), MPVariable],\r\n  userData: Map[String, Map[Int, Double]]\r\n): Unit = {\r\n  \r\n  val objective: MPObjective = solver.objective()\r\n  \r\n  for (((user, hour), variable) <- variables) {\r\n    val score = userData(user)(hour)\r\n    objective.setCoefficient(variable, score)\r\n  }\r\n  \r\n  objective.setMaximization()\r\n}\r\n\r\n/**\r\n * 제약 조건 추가\r\n */\r\ndef addConstraints(\r\n  solver: MPSolver,\r\n  variables: mutable.Map[(String, Int), MPVariable],\r\n  users: Array[String],\r\n  hours: Array[Int],\r\n  capacityPerHour: Int\r\n): Unit = {\r\n  \r\n  // Constraint 1: 각 사용자는 정확히 1개 시간대에 할당\r\n  for (user <- users) {\r\n    val constraint: MPConstraint = solver.makeConstraint(\r\n      1.0, 1.0, s\"user_$user\"\r\n    )\r\n    \r\n    for (hour <- hours if variables.contains((user, hour))) {\r\n      constraint.setCoefficient(variables((user, hour)), 1.0)\r\n    }\r\n  }\r\n  \r\n  // Constraint 2: 시간대별 용량 제약\r\n  for (hour <- hours) {\r\n    val constraint: MPConstraint = solver.makeConstraint(\r\n      0.0, capacityPerHour.toDouble, s\"hour_$hour\"\r\n    )\r\n    \r\n    for (user <- users if variables.contains((user, hour))) {\r\n      constraint.setCoefficient(variables((user, hour)), 1.0)\r\n    }\r\n  }\r\n}\r\n\r\n/**\r\n * 솔루션 추출\r\n */\r\ndef extractSolution(\r\n  variables: mutable.Map[(String, Int), MPVariable],\r\n  userData: Map[String, Map[Int, Double]],\r\n  users: Array[String],\r\n  hours: Array[Int]\r\n): Array[AllocationResult] = {\r\n  \r\n  val results = mutable.ArrayBuffer[AllocationResult]()\r\n  \r\n  for (user <- users) {\r\n    for (hour <- hours if variables.contains((user, hour))) {\r\n      val variable = variables((user, hour))\r\n      if (variable.solutionValue() > 0.5) {\r\n        val score = userData(user)(hour)\r\n        results += AllocationResult(user, hour, score)\r\n      }\r\n    }\r\n  }\r\n  \r\n  results.toArray\r\n}\r\n\r\n/**\r\n * 통계 출력\r\n */\r\ndef printStatistics(\r\n  results: Array[AllocationResult],\r\n  hours: Array[Int],\r\n  capacityPerHour: Int\r\n): Unit = {\r\n  \r\n  println(\"\\n\" + \"=\" * 80)\r\n  println(\"Allocation Statistics\")\r\n  println(\"=\" * 80)\r\n  \r\n  val totalAssigned = results.length\r\n  val totalScore = results.map(_.score).sum\r\n  val avgScore = totalScore / totalAssigned\r\n  \r\n  println(s\"Total assigned: ${numFormatter.format(totalAssigned)} users\")\r\n  println(f\"Total score: $totalScore%,.2f\")\r\n  println(f\"Average score: $avgScore%.4f\")\r\n  \r\n  println(\"\\nHour-wise allocation:\")\r\n  println(\"-\" * 60)\r\n  println(f\"${\"Hour\"}%-8s ${\"Count\"}%12s ${\"Total Score\"}%15s ${\"Avg Score\"}%12s\")\r\n  println(\"-\" * 60)\r\n  \r\n  val hourStats = results.groupBy(_.assigned_hour)\r\n  \r\n  for (hour <- hours) {\r\n    val hourResults = hourStats.getOrElse(hour, Array.empty)\r\n    val count = hourResults.length\r\n    val hourTotalScore = hourResults.map(_.score).sum\r\n    val hourAvgScore = if (count > 0) hourTotalScore / count else 0.0\r\n    val utilization = count.toDouble / capacityPerHour * 100\r\n    \r\n    println(f\"$hour%-8d ${numFormatter.format(count)}%12s $hourTotalScore%,15.2f $hourAvgScore%12.4f ($utilization%5.1f%%)\")\r\n  }\r\n  println(\"-\" * 60)\r\n}\r\n\r\nprintln(\"✓ Helper functions defined\")",
      "dateUpdated": "2026-01-14T01:33:39+0000",
      "dateFinished": "2026-01-14T01:33:41+0000",
      "dateStarted": "2026-01-14T01:33:39+0000",
      "title": "Helper Func.",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "✓ Helper functions defined\ndefined class AllocationResult\n\u001b[1m\u001b[34mnumFormatter\u001b[0m: \u001b[1m\u001b[32mjava.text.DecimalFormat\u001b[0m = java.text.DecimalFormat@674dc\n\u001b[1m\u001b[34mcollectUserData\u001b[0m: \u001b[1m\u001b[32m(df: org.apache.spark.sql.DataFrame)Map[String,Map[Int,Double]]\u001b[0m\n\u001b[1m\u001b[34mcreateVariables\u001b[0m: \u001b[1m\u001b[32m(solver: com.google.ortools.linearsolver.MPSolver, users: Array[String], hours: Array[Int], userData: Map[String,Map[Int,Double]])scala.collection.mutable.Map[(String, Int),com.google.ortools.linearsolver.MPVariable]\u001b[0m\n\u001b[1m\u001b[34msetObjective\u001b[0m: \u001b[1m\u001b[32m(solver: com.google.ortools.linearsolver.MPSolver, variables: scala.collection.mutable.Map[(String, Int),com.google.ortools.linearsolver.MPVariable], userData: Map[String,Map[Int,Double]])Unit\u001b[0m\n\u001b[1m\u001b[34maddConstraints\u001b[0m: \u001b[1m\u001b[32m(solver: com.google.ortools.line...\n"
          }
        ]
      }
    },
    {
      "text": "%spark-local\r\n\r\n/**\r\n * Preprocessing: 사용자별 선택지 제한\r\n * 각 사용자의 상위 N개 시간대만 선택지로 제한\r\n */\r\ndef preprocessTopChoices(\r\n  df: DataFrame,\r\n  topN: Int = 5\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(s\"\\n${\"=\" * 80}\")\r\n  println(s\"Preprocessing: Limiting to top $topN choices per user\")\r\n  println(s\"${\"=\" * 80}\")\r\n  \r\n  val originalCount = df.count()\r\n  val originalUsers = df.select(\"svc_mgmt_num\").distinct().count()\r\n  \r\n  println(s\"Original data:\")\r\n  println(s\"  Total rows: ${numFormatter.format(originalCount)}\")\r\n  println(s\"  Unique users: ${numFormatter.format(originalUsers)}\")\r\n  \r\n  // 각 사용자별로 propensity_score 상위 topN개만 선택\r\n  val windowSpec = Window.partitionBy(\"svc_mgmt_num\")\r\n    .orderBy(desc(\"propensity_score\"))\r\n  \r\n  val filtered = df\r\n    .withColumn(\"rank\", row_number().over(windowSpec))\r\n    .filter($\"rank\" <= topN)\r\n    .drop(\"rank\")\r\n  \r\n  val filteredCount = filtered.count()\r\n  val filteredUsers = filtered.select(\"svc_mgmt_num\").distinct().count()\r\n  \r\n  println(s\"\\nFiltered data:\")\r\n  println(s\"  Total rows: ${numFormatter.format(filteredCount)}\")\r\n  println(s\"  Unique users: ${numFormatter.format(filteredUsers)}\")\r\n  println(s\"  Reduction: ${(1 - filteredCount.toDouble / originalCount) * 100}%.1f%%\")\r\n  \r\n  // 시간대별 분포 확인\r\n  println(s\"\\nHour distribution after filtering:\")\r\n  val hourDist = filtered.groupBy(\"send_hour\")\r\n    .agg(countDistinct(\"svc_mgmt_num\").as(\"user_count\"))\r\n    .orderBy(\"send_hour\")\r\n    .collect()\r\n  \r\n  hourDist.foreach { row =>\r\n    val hour = row.getInt(0)\r\n    val count = row.getLong(1)\r\n    println(f\"  Hour $hour: ${numFormatter.format(count)} users\")\r\n  }\r\n  \r\n  // 사용자별 평균 선택지 수\r\n  val avgChoices = filtered.groupBy(\"svc_mgmt_num\")\r\n    .count()\r\n    .agg(avg(\"count\"))\r\n    .first()\r\n    .getDouble(0)\r\n  \r\n  println(f\"\\nAverage choices per user: $avgChoices%.2f (max: $topN)\")\r\n  println(s\"${\"=\" * 80}\\n\")\r\n  \r\n  filtered\r\n}\r\n\r\n/**\r\n * 1. 시간대별 차등 용량을 적용한 최적화 (FEASIBLE 수용 + Preprocessing)\r\n */\r\ndef allocateUsersWithHourlyCapacity(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  timeLimit: Int = 300,\r\n  batchSize: Int = 500000,\r\n  topChoices: Int = 5,  // 추가: 사용자별 상위 N개 시간대만 사용\r\n  enablePreprocessing: Boolean = true  // 추가: preprocessing 활성화 여부\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"=\" * 80)\r\n  println(\"Robust Allocation with hourly capacity\")\r\n  println(s\"Preprocessing: ${if (enablePreprocessing) s\"Enabled (top $topChoices)\" else \"Disabled\"}\")\r\n  println(\"FEASIBLE solution: Accepted\")\r\n  println(\"=\" * 80)\r\n  \r\n  // Preprocessing: 상위 N개 선택지만 사용\r\n  val processedDf = if (enablePreprocessing) {\r\n    preprocessTopChoices(df, topChoices)\r\n  } else {\r\n    df\r\n  }\r\n  \r\n  Loader.loadNativeLibraries()\r\n  \r\n  val userData = collectUserData(processedDf)\r\n  val users = userData.keys.toArray.sorted\r\n  val hours = processedDf.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n  \r\n  println(s\"\\n[INPUT INFO]\")\r\n  println(s\"Users: ${numFormatter.format(users.length)}\")\r\n  println(s\"Hours: ${hours.length}\")\r\n  \r\n  if (users.length > batchSize) {\r\n    println(s\"⚠ Warning: User count (${users.length}) exceeds batchSize ($batchSize)\")\r\n  }\r\n  \r\n  // 데이터 분포 분석\r\n  println(s\"\\n[DATA DISTRIBUTION]\")\r\n  val hourUserCounts = userData.values.flatMap(_.keys).groupBy(identity).mapValues(_.size)\r\n  hours.sorted.foreach { hour =>\r\n    val userCount = hourUserCounts.getOrElse(hour, 0)\r\n    println(f\"  Hour $hour: $userCount%,6d users have this hour as an option\")\r\n  }\r\n  \r\n  val avgChoicesPerUser = userData.values.map(_.size).sum.toDouble / users.length\r\n  println(f\"\\n  Average choices per user: $avgChoicesPerUser%.2f hours\")\r\n  \r\n  // Solver 설정\r\n  val solver = MPSolver.createSolver(\"SCIP\")\r\n  solver.setTimeLimit(timeLimit * 1000L)\r\n  \r\n  val variables = createVariables(solver, users, hours, userData)\r\n  setObjective(solver, variables, userData)\r\n  \r\n  // 제약 조건 1: 사용자당 정확히 1회 할당\r\n  println(s\"\\n[CONSTRAINT 1: User Assignment]\")\r\n  for (user <- users) {\r\n    val constraint = solver.makeConstraint(1.0, 1.0, s\"user_$user\")\r\n    for (hour <- hours if variables.contains((user, hour))) {\r\n      constraint.setCoefficient(variables((user, hour)), 1.0)\r\n    }\r\n  }\r\n  println(s\"  ✓ Each user must be assigned to exactly 1 hour\")\r\n  \r\n  // 제약 조건 2: 시간대별 용량 (독립 제약)\r\n  println(s\"\\n[CONSTRAINT 2: Hourly Capacity]\")\r\n  for (hour <- hours) {\r\n    val hourCapacity = capacityPerHour.getOrElse(hour, 0)\r\n    val constraint = solver.makeConstraint(0.0, hourCapacity.toDouble, s\"hour_$hour\")\r\n    \r\n    var varCount = 0\r\n    for (user <- users if variables.contains((user, hour))) {\r\n      constraint.setCoefficient(variables((user, hour)), 1.0)\r\n      varCount += 1\r\n    }\r\n    \r\n    println(f\"  Hour $hour: capacity = ${numFormatter.format(hourCapacity)}, candidates = ${numFormatter.format(varCount)}\")\r\n  }\r\n  \r\n  val totalCapacity = capacityPerHour.values.sum\r\n  println(f\"\\n  Total capacity: ${numFormatter.format(totalCapacity)}\")\r\n  println(f\"  Users to assign: ${numFormatter.format(users.length)}\")\r\n  println(f\"  Capacity ratio: ${totalCapacity.toDouble / users.length}%.2fx\")\r\n  \r\n  println(s\"\\n[SOLVER INFO]\")\r\n  println(s\"  Variables: ${numFormatter.format(variables.size)}\")\r\n  println(s\"  Constraints: ${numFormatter.format(solver.numConstraints())}\")\r\n  println(s\"  Time limit: ${timeLimit}s\")\r\n  \r\n  println(\"\\nSolving...\")\r\n  val startTime = System.currentTimeMillis()\r\n  val status = solver.solve()\r\n  val solveTime = (System.currentTimeMillis() - startTime) / 1000.0\r\n  \r\n  status match {\r\n    case MPSolver.ResultStatus.OPTIMAL =>\r\n      println(s\"✓ OPTIMAL solution found in ${solveTime}s\")\r\n      val objValue = solver.objective().value()\r\n      println(f\"Objective value: $objValue%,.2f\")\r\n      \r\n      val results = extractSolution(variables, userData, users, hours)\r\n      validateAndPrintResults(results, users, hours, capacityPerHour)\r\n      results.toSeq.toDF()\r\n      \r\n    case MPSolver.ResultStatus.FEASIBLE =>\r\n      println(s\"⚠ FEASIBLE solution found in ${solveTime}s (timeout, but solution accepted)\")\r\n      val objValue = solver.objective().value()\r\n      println(f\"Objective value: $objValue%,.2f\")\r\n      \r\n      val results = extractSolution(variables, userData, users, hours)\r\n      val assignedCount = results.length\r\n      \r\n      println(f\"\\nFeasible solution quality:\")\r\n      println(f\"  Assigned: ${numFormatter.format(assignedCount)} / ${numFormatter.format(users.length)}\")\r\n      println(f\"  Coverage: ${assignedCount.toDouble / users.length * 100}%.2f%%\")\r\n      \r\n      if (assignedCount > 0) {\r\n        validateAndPrintResults(results, users, hours, capacityPerHour)\r\n        results.toSeq.toDF()\r\n      } else {\r\n        println(\"  ✗ No assignments in feasible solution\")\r\n        throw new RuntimeException(\"Empty feasible solution\")\r\n      }\r\n      \r\n    case _ =>\r\n      println(s\"✗ Solver failed with status: $status (${solveTime}s)\")\r\n      throw new RuntimeException(s\"Solver failed: $status\")\r\n  }\r\n}\r\n\r\n/**\r\n * 결과 검증 및 출력\r\n */\r\ndef validateAndPrintResults(\r\n  results: Array[AllocationResult],\r\n  users: Array[String],\r\n  hours: Array[Int],\r\n  capacityPerHour: Map[Int, Int]\r\n): Unit = {\r\n  \r\n  println(s\"\\n${\"=\" * 80}\")\r\n  println(\"[VALIDATION: Hourly Assignment vs Capacity]\")\r\n  println(s\"${\"=\" * 80}\")\r\n  \r\n  val hourlyAssignment = results.groupBy(_.assigned_hour).mapValues(_.length)\r\n  val totalCapacity = capacityPerHour.values.sum\r\n  \r\n  var totalAssigned = 0\r\n  var violationDetected = false\r\n  \r\n  hours.sorted.foreach { hour =>\r\n    val assigned = hourlyAssignment.getOrElse(hour, 0)\r\n    val capacity = capacityPerHour.getOrElse(hour, 0)\r\n    val utilizationPct = if (capacity > 0) assigned.toDouble / capacity * 100 else 0.0\r\n    \r\n    val status = if (assigned <= capacity) \"✓\" else \"✗ VIOLATION\"\r\n    \r\n    println(f\"  Hour $hour: assigned=${numFormatter.format(assigned).padTo(8, ' ')} / capacity=${numFormatter.format(capacity).padTo(8, ' ')} (${utilizationPct}%5.1f%%) $status\")\r\n    \r\n    totalAssigned += assigned\r\n    \r\n    if (assigned > capacity) {\r\n      violationDetected = true\r\n      println(s\"    ⚠⚠⚠ ERROR: Over capacity by ${numFormatter.format(assigned - capacity)}!\")\r\n    }\r\n  }\r\n  \r\n  println(s\"\\n[SUMMARY]\")\r\n  println(f\"  Total assigned: ${numFormatter.format(totalAssigned)}\")\r\n  println(f\"  Total users: ${numFormatter.format(users.length)}\")\r\n  println(f\"  Assignment rate: ${totalAssigned.toDouble / users.length * 100}%.2f%%\")\r\n  println(f\"  Total capacity: ${numFormatter.format(totalCapacity)}\")\r\n  println(f\"  Capacity utilization: ${totalAssigned.toDouble / totalCapacity * 100}%.2f%%\")\r\n  \r\n  if (violationDetected) {\r\n    println(\"\\n✗✗✗ CRITICAL ERROR: Capacity constraints violated!\")\r\n    throw new RuntimeException(\"Capacity constraints not enforced correctly\")\r\n  } else {\r\n    println(\"\\n✓✓✓ All capacity constraints satisfied\")\r\n  }\r\n  \r\n  println(s\"${\"=\" * 80}\\n\")\r\n}\r\n\r\n/**\r\n * 2. Hybrid 할당: OR-Tools + Greedy 조합\r\n */\r\ndef allocateUsersHybrid(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  timeLimit: Int = 300,\r\n  batchSize: Int = 500000,\r\n  topChoices: Int = 5,\r\n  enablePreprocessing: Boolean = true\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  try {\r\n    val optimizerResult = allocateUsersWithHourlyCapacity(\r\n      df, \r\n      capacityPerHour, \r\n      timeLimit, \r\n      batchSize,\r\n      topChoices,\r\n      enablePreprocessing\r\n    )\r\n    \r\n    // 미할당 사용자 확인\r\n    val assignedUsers = optimizerResult.select(\"svc_mgmt_num\").collect().map(_.getString(0)).toSet\r\n    val allUsers = df.select(\"svc_mgmt_num\").distinct().collect().map(_.getString(0)).toSet\r\n    val unassignedUsers = allUsers -- assignedUsers\r\n    \r\n    if (unassignedUsers.isEmpty) {\r\n      println(\"✓ All users assigned by optimizer\")\r\n      return optimizerResult\r\n    }\r\n    \r\n    println(s\"\\n${numFormatter.format(unassignedUsers.size)} users unassigned, running Greedy for remainder...\")\r\n    \r\n    // 남은 용량 계산\r\n    val usedCapacity = optimizerResult.groupBy(\"assigned_hour\").count().collect()\r\n      .map(r => r.getInt(0) -> r.getLong(1).toInt).toMap\r\n    \r\n    val remainingCapacity = capacityPerHour.map { case (hour, cap) =>\r\n      hour -> Math.max(0, cap - usedCapacity.getOrElse(hour, 0))\r\n    }\r\n    \r\n    println(\"\\nRemaining capacity after optimizer:\")\r\n    remainingCapacity.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n      println(s\"  Hour $hour: ${numFormatter.format(cap)}\")\r\n    }\r\n    \r\n    // 미할당 사용자를 원본 데이터에서 가져옴 (preprocessing 없이)\r\n    val unassignedDf = df.filter($\"svc_mgmt_num\".isin(unassignedUsers.toSeq: _*))\r\n    val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n    val greedyResult = allocateGreedySimple(unassignedDf, hours, remainingCapacity)\r\n    \r\n    if (greedyResult.count() == 0) {\r\n      println(\"Greedy assigned 0 users\")\r\n      return optimizerResult\r\n    }\r\n    \r\n    val combined = optimizerResult.union(greedyResult)\r\n    val totalAssigned = combined.count()\r\n    println(s\"\\nTotal assigned: ${numFormatter.format(totalAssigned)} / ${numFormatter.format(allUsers.size)}\")\r\n    \r\n    combined\r\n    \r\n  } catch {\r\n    case e: Exception =>\r\n      println(s\"\\nOptimizer failed: ${e.getMessage}\")\r\n      println(\"Running full Greedy allocation...\")\r\n      val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n      allocateGreedySimple(df, hours, capacityPerHour)\r\n  }\r\n}\r\n\r\n/**\r\n * 3. 안정적인 배치 처리 함수 - Hybrid 방식 (가치 기반 정렬)\r\n */\r\ndef allocateLargeScaleHybrid(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  batchSize: Int = 500000,\r\n  timeLimit: Int = 300,\r\n  topChoices: Int = 5,\r\n  enablePreprocessing: Boolean = true\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"=\" * 80)\r\n  println(\"Batch Allocation for Large Scale Data (Hybrid Mode)\")\r\n  println(s\"Preprocessing: ${if (enablePreprocessing) s\"Enabled (top $topChoices)\" else \"Disabled\"}\")\r\n  println(\"FEASIBLE solution: Accepted\")\r\n  println(\"=\" * 80)\r\n  \r\n  // 가치 기반 정렬\r\n  val userPriority = df.groupBy(\"svc_mgmt_num\")\r\n    .agg(max(\"propensity_score\").as(\"max_prob\"))\r\n\r\n  val totalUsers = userPriority.count()\r\n  val numBatches = Math.ceil(totalUsers.toDouble / batchSize).toInt\r\n  \r\n  println(s\"\\n[BATCH SETUP]\")\r\n  println(s\"Total users: ${numFormatter.format(totalUsers)}\")\r\n  println(s\"Batch size: ${numFormatter.format(batchSize)}\")\r\n  println(s\"Number of batches: $numBatches\")\r\n  \r\n  // 시간대별 용량 출력\r\n  println(\"\\n[CAPACITY PER HOUR]\")\r\n  capacityPerHour.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n    println(s\"  Hour $hour: ${numFormatter.format(cap)}\")\r\n  }\r\n  val totalCapacity = capacityPerHour.values.sum\r\n  println(s\"Total capacity: ${numFormatter.format(totalCapacity)}\")\r\n  \r\n  // 가치 기반 배치 분할\r\n  val allUsers = userPriority\r\n    .withColumn(\"row_id\", row_number().over(Window.orderBy(desc(\"max_prob\"))))\r\n    .withColumn(\"batch_id\", (($\"row_id\" - 1) / batchSize).cast(\"int\"))\r\n    .select(\"svc_mgmt_num\", \"batch_id\")\r\n    .cache()\r\n  \r\n  println(\"\\n[BATCH DISTRIBUTION]\")\r\n  val batchCounts = allUsers.groupBy(\"batch_id\").count().collect()\r\n    .map(r => r.getInt(0) -> r.getLong(1))\r\n    .sortBy(_._1)\r\n  \r\n  batchCounts.foreach { case (bid, cnt) =>\r\n    println(s\"  Batch $bid: ${numFormatter.format(cnt)} users\")\r\n  }\r\n  \r\n  var remainingCapacity = capacityPerHour.toMap\r\n  val allResults = mutable.ArrayBuffer[DataFrame]()\r\n  var totalAssignedSoFar = 0L\r\n  \r\n  // 각 배치별로 최적화\r\n  for (batchId <- 0 until numBatches) {\r\n    println(s\"\\n${\"=\" * 80}\")\r\n    println(s\"Processing Batch ${batchId + 1}/$numBatches\")\r\n    println(s\"${\"=\" * 80}\")\r\n    \r\n    val batchUsers = allUsers.filter($\"batch_id\" === batchId)\r\n    \r\n    // 용량이 남아있는 시간대만 처리\r\n    val availableHours = remainingCapacity.filter(_._2 > 0).keys.toSeq\r\n    \r\n    if (availableHours.isEmpty) {\r\n      println(\"⚠ No capacity left in any hour. Stopping.\")\r\n      val unassignedCount = totalUsers - totalAssignedSoFar\r\n      println(s\"Unassigned users: ${numFormatter.format(unassignedCount)}\")\r\n      \r\n    } else {\r\n      println(s\"\\nAvailable hours: ${availableHours.sorted.mkString(\", \")}\")\r\n      println(\"\\nRemaining capacity:\")\r\n      remainingCapacity.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n        val status = if (availableHours.contains(hour)) \"✓\" else \"✗\"\r\n        println(s\"  Hour $hour: ${numFormatter.format(cap)} $status\")\r\n      }\r\n      \r\n      val batchDf = df.join(batchUsers, Seq(\"svc_mgmt_num\"))\r\n        .filter($\"send_hour\".isin(availableHours: _*))\r\n      \r\n      val batchUserCount = batchDf.select(\"svc_mgmt_num\").distinct().count()\r\n      println(s\"\\nBatch users with available hours: ${numFormatter.format(batchUserCount)}\")\r\n      \r\n      if (batchUserCount == 0) {\r\n        println(\"⚠ No users can be assigned\")\r\n      } else {\r\n        val startTime = System.currentTimeMillis()\r\n        val batchResult = allocateUsersHybrid(\r\n          batchDf, \r\n          remainingCapacity, \r\n          timeLimit, \r\n          batchSize,\r\n          topChoices,\r\n          enablePreprocessing\r\n        )\r\n        val solveTime = (System.currentTimeMillis() - startTime) / 1000.0\r\n        \r\n        println(f\"\\nBatch completed in $solveTime%.2f seconds\")\r\n        \r\n        val assignedCount = batchResult.count()\r\n        \r\n        if (assignedCount > 0) {\r\n          totalAssignedSoFar += assignedCount\r\n          \r\n          // 용량 차감\r\n          val allocatedPerHour = batchResult.groupBy(\"assigned_hour\").count().collect()\r\n            .map(row => row.getInt(0) -> row.getLong(1).toInt).toMap\r\n          \r\n          println(\"\\n[CAPACITY UPDATE]\")\r\n          val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n          hours.foreach { hour =>\r\n            val allocated = allocatedPerHour.getOrElse(hour, 0)\r\n            val before = remainingCapacity.getOrElse(hour, 0)\r\n            val after = Math.max(0, before - allocated)\r\n            \r\n            if (allocated > 0) {\r\n              println(f\"  Hour $hour: ${numFormatter.format(before)} - ${numFormatter.format(allocated)} = ${numFormatter.format(after)}\")\r\n            }\r\n          }\r\n          \r\n          remainingCapacity = remainingCapacity.map { case (hour, cap) =>\r\n            hour -> Math.max(0, cap - allocatedPerHour.getOrElse(hour, 0))\r\n          }\r\n          \r\n          val batchScore = batchResult.agg(sum(\"score\")).first().getDouble(0)\r\n          println(f\"\\nBatch score: $batchScore%,.2f\")\r\n          println(s\"Batch assigned: ${numFormatter.format(assignedCount)}\")\r\n          \r\n          allResults += batchResult\r\n        } else {\r\n          println(\"⚠ No users assigned in this batch\")\r\n        }\r\n      }\r\n    }\r\n    \r\n    // 진행률\r\n    val progress = totalAssignedSoFar.toDouble / totalUsers * 100\r\n    val coverageVsCapacity = totalAssignedSoFar.toDouble / totalCapacity * 100\r\n    println(f\"\\n[PROGRESS]\")\r\n    println(f\"  Assigned: ${numFormatter.format(totalAssignedSoFar)} / ${numFormatter.format(totalUsers)} users ($progress%.1f%%)\")\r\n    println(f\"  Capacity used: ${numFormatter.format(totalAssignedSoFar)} / ${numFormatter.format(totalCapacity)} ($coverageVsCapacity%.1f%%)\")\r\n  }\r\n  \r\n  allUsers.unpersist()\r\n  \r\n  // 최종 결과\r\n  if (allResults.isEmpty) {\r\n    println(\"\\n⚠ No results generated!\")\r\n    return spark.emptyDataFrame\r\n  }\r\n  \r\n  val finalResult = allResults.reduce(_.union(_))\r\n  printFinalStatistics(finalResult, totalUsers)\r\n  \r\n  finalResult\r\n}\r\n\r\n/**\r\n * 4. 간단한 Greedy 할당\r\n */\r\ndef allocateGreedySimple(\r\n  df: DataFrame,\r\n  hours: Array[Int],\r\n  initialCapacity: Map[Int, Int]\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"\\n\" + \"=\" * 80)\r\n  println(\"Running Greedy allocation\")\r\n  println(\"=\" * 80)\r\n  \r\n  println(\"\\nInitial capacity:\")\r\n  initialCapacity.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n    println(s\"  Hour $hour: ${numFormatter.format(cap)}\")\r\n  }\r\n  \r\n  val userData = collectUserData(df)\r\n  val users = userData.keys.toArray\r\n  \r\n  println(s\"\\nUsers to assign: ${numFormatter.format(users.length)}\")\r\n  \r\n  // 사용자를 최고 점수 순으로 정렬\r\n  val userBestScores = users.map { user =>\r\n    val bestScore = userData(user).values.max\r\n    (user, bestScore)\r\n  }.sortBy(-_._2)\r\n  \r\n  val hourCapacity = mutable.Map(initialCapacity.toSeq: _*)\r\n  val assignments = mutable.ArrayBuffer[AllocationResult]()\r\n  \r\n  for ((user, _) <- userBestScores) {\r\n    val choices = userData(user).toSeq.sortBy(-_._2)\r\n    \r\n    var assigned = false\r\n    for ((hour, score) <- choices if !assigned) {\r\n      val currentCapacity = hourCapacity.getOrElse(hour, 0)\r\n      \r\n      if (currentCapacity > 0) {\r\n        assignments += AllocationResult(user, hour, score)\r\n        hourCapacity(hour) = currentCapacity - 1\r\n        assigned = true\r\n      }\r\n    }\r\n  }\r\n  \r\n  println(s\"\\nGreedy assigned: ${numFormatter.format(assignments.size)} / ${numFormatter.format(users.length)}\")\r\n  \r\n  if (assignments.nonEmpty) {\r\n    println(\"\\n[GREEDY ALLOCATION BY HOUR]\")\r\n    val hourlyAssignment = assignments.groupBy(_.assigned_hour).mapValues(_.length)\r\n    \r\n    hours.sorted.foreach { hour =>\r\n      val assigned = hourlyAssignment.getOrElse(hour, 0)\r\n      val initialCap = initialCapacity.getOrElse(hour, 0)\r\n      val remaining = hourCapacity.getOrElse(hour, 0)\r\n      \r\n      println(f\"  Hour $hour: assigned=${numFormatter.format(assigned)}, capacity=${numFormatter.format(initialCap)}, remaining=${numFormatter.format(remaining)}\")\r\n    }\r\n    \r\n    println(\"=\" * 80 + \"\\n\")\r\n  }\r\n  \r\n  if (assignments.isEmpty) {\r\n    spark.emptyDataFrame\r\n  } else {\r\n    assignments.toSeq.toDF()\r\n  }\r\n}\r\n\r\n/**\r\n * 5. 최종 통계 출력\r\n */\r\ndef printFinalStatistics(result: DataFrame, totalUsers: Long): Unit = {\r\n  println(s\"\\n${\"=\" * 80}\")\r\n  println(\"Final Allocation Statistics\")\r\n  println(s\"${\"=\" * 80}\")\r\n  \r\n  val totalAssigned = result.count()\r\n  val coverage = totalAssigned.toDouble / totalUsers * 100\r\n  \r\n  println(s\"\\nTotal assigned: ${numFormatter.format(totalAssigned)} / ${numFormatter.format(totalUsers)} ($coverage%.2f%%)\")\r\n  \r\n  if (totalAssigned > 0) {\r\n    val totalScore = result.agg(sum(\"score\")).first().getDouble(0)\r\n    val avgScore = totalScore / totalAssigned\r\n    \r\n    println(f\"Total score: $totalScore%,.2f\")\r\n    println(f\"Average score: $avgScore%.4f\")\r\n    \r\n    println(\"\\nHour-wise allocation:\")\r\n    result.groupBy(\"assigned_hour\")\r\n      .agg(\r\n        count(\"*\").as(\"count\"),\r\n        sum(\"score\").as(\"total_score\"),\r\n        avg(\"score\").as(\"avg_score\")\r\n      )\r\n      .orderBy(\"assigned_hour\")\r\n      .show(false)\r\n  }\r\n  \r\n  println(\"=\" * 80)\r\n}\r\n\r\nprintln(\"✓ Opt 3 with FEASIBLE acceptance and preprocessing defined\")",
      "user": "anonymous",
      "dateUpdated": "2026-01-14T01:35:07+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768294211464_1590897454",
      "id": "paragraph_1768294211464_1590897454",
      "dateCreated": "2026-01-13T08:50:11+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:820599",
      "dateFinished": "2026-01-14T01:33:45+0000",
      "dateStarted": "2026-01-14T01:33:44+0000",
      "title": "Google OR",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "✓ Opt 3 with FEASIBLE acceptance and preprocessing defined\n\u001b[1m\u001b[34mpreprocessTopChoices\u001b[0m: \u001b[1m\u001b[32m(df: org.apache.spark.sql.DataFrame, topN: Int)org.apache.spark.sql.DataFrame\u001b[0m\n\u001b[1m\u001b[34mallocateUsersWithHourlyCapacity\u001b[0m: \u001b[1m\u001b[32m(df: org.apache.spark.sql.DataFrame, capacityPerHour: Map[Int,Int], timeLimit: Int, batchSize: Int, topChoices: Int, enablePreprocessing: Boolean)org.apache.spark.sql.DataFrame\u001b[0m\n\u001b[1m\u001b[34mvalidateAndPrintResults\u001b[0m: \u001b[1m\u001b[32m(results: Array[AllocationResult], users: Array[String], hours: Array[Int], capacityPerHour: Map[Int,Int])Unit\u001b[0m\n\u001b[1m\u001b[34mallocateUsersHybrid\u001b[0m: \u001b[1m\u001b[32m(df: org.apache.spark.sql.DataFrame, capacityPerHour: Map[Int,Int], timeLimit: Int, batchSize: Int, topChoices: Int, enablePreprocessing: Boolean)org.apache.spark.sql.DataFrame\u001b[0m\n\u001b[1m\u001b[34mallocateLargeScaleHybrid\u001b[0m: \u001b[1m\u001b[32m(...\n"
          }
        ]
      }
    },
    {
      "text": "%spark-local\r\n\r\nimport scala.collection.mutable\r\nimport scala.util.Random\r\nimport org.apache.spark.sql.DataFrame\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.expressions.Window\r\n\r\n/**\r\n * 메타 휴리스틱 기반 사용자 할당 시스템\r\n * Simulated Annealing 알고리즘 사용\r\n */\r\n\r\n/**\r\n * 해(Solution) 클래스: 사용자-시간대 할당 정보\r\n */\r\ncase class Solution(\r\n  assignments: Map[String, Int],  // userId -> hour\r\n  score: Double,\r\n  hourUsage: Map[Int, Int]        // hour -> count\r\n) {\r\n  def isValid(capacityPerHour: Map[Int, Int]): Boolean = {\r\n    hourUsage.forall { case (hour, count) =>\r\n      count <= capacityPerHour.getOrElse(hour, 0)\r\n    }\r\n  }\r\n}\r\n\r\n/**\r\n * 1. Simulated Annealing 기반 최적화\r\n */\r\ndef allocateUsersWithSimulatedAnnealing(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  maxIterations: Int = 100000,\r\n  initialTemperature: Double = 100.0,\r\n  coolingRate: Double = 0.995,\r\n  batchSize: Int = 500000\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"=\" * 80)\r\n  println(\"Simulated Annealing Optimization\")\r\n  println(\"=\" * 80)\r\n  \r\n  val userData = collectUserData(df)\r\n  val users = userData.keys.toArray.sorted\r\n  val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n  \r\n  println(s\"\\n[INPUT INFO]\")\r\n  println(s\"Users: ${numFormatter.format(users.length)}\")\r\n  println(s\"Hours: ${hours.length}\")\r\n  \r\n  if (users.length > batchSize) {\r\n    println(s\"⚠ Warning: User count (${users.length}) exceeds batchSize ($batchSize)\")\r\n  }\r\n  \r\n  // 데이터 분포 분석\r\n  println(s\"\\n[DATA DISTRIBUTION]\")\r\n  val hourUserCounts = userData.values.flatMap(_.keys).groupBy(identity).mapValues(_.size)\r\n  hours.sorted.foreach { hour =>\r\n    val userCount = hourUserCounts.getOrElse(hour, 0)\r\n    println(f\"  Hour $hour: $userCount%,6d users have this hour as an option\")\r\n  }\r\n  \r\n  println(s\"\\n[CAPACITY PER HOUR]\")\r\n  capacityPerHour.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n    println(s\"  Hour $hour: ${numFormatter.format(cap)}\")\r\n  }\r\n  val totalCapacity = capacityPerHour.values.sum\r\n  println(f\"\\n  Total capacity: ${numFormatter.format(totalCapacity)}\")\r\n  println(f\"  Users to assign: ${numFormatter.format(users.length)}\")\r\n  println(f\"  Capacity ratio: ${totalCapacity.toDouble / users.length}%.2fx\")\r\n  \r\n  println(s\"\\n[ALGORITHM PARAMETERS]\")\r\n  println(s\"  Max iterations: ${numFormatter.format(maxIterations)}\")\r\n  println(f\"  Initial temperature: $initialTemperature%.1f\")\r\n  println(f\"  Cooling rate: $coolingRate%.4f\")\r\n  \r\n  // 초기 해 생성 (Greedy)\r\n  println(\"\\nGenerating initial solution...\")\r\n  val startTime = System.currentTimeMillis()\r\n  val initialSolution = generateInitialSolution(users, userData, capacityPerHour, hours)\r\n  val initTime = (System.currentTimeMillis() - startTime) / 1000.0\r\n  \r\n  println(f\"Initial solution generated in $initTime%.2f seconds\")\r\n  println(f\"  Initial score: ${initialSolution.score}%,.2f\")\r\n  println(f\"  Assigned users: ${numFormatter.format(initialSolution.assignments.size)}\")\r\n  \r\n  // Simulated Annealing 실행\r\n  println(\"\\nRunning Simulated Annealing...\")\r\n  val saStartTime = System.currentTimeMillis()\r\n  val bestSolution = runSimulatedAnnealing(\r\n    initialSolution,\r\n    userData,\r\n    capacityPerHour,\r\n    hours,\r\n    maxIterations,\r\n    initialTemperature,\r\n    coolingRate\r\n  )\r\n  val saTime = (System.currentTimeMillis() - saStartTime) / 1000.0\r\n  \r\n  println(f\"\\nOptimization completed in $saTime%.2f seconds\")\r\n  println(f\"  Best score: ${bestSolution.score}%,.2f\")\r\n  println(f\"  Improvement: ${(bestSolution.score - initialSolution.score)}%,.2f\")\r\n  println(f\"  Improvement rate: ${((bestSolution.score - initialSolution.score) / initialSolution.score * 100)}%.2f%%\")\r\n  \r\n  // 결과 검증\r\n  validateSolution(bestSolution, capacityPerHour, hours, users.length)\r\n  \r\n  // DataFrame 변환\r\n  val results = bestSolution.assignments.map { case (user, hour) =>\r\n    val score = userData(user)(hour)\r\n    AllocationResult(user, hour, score)\r\n  }.toSeq\r\n  \r\n  results.toDF()\r\n}\r\n\r\n/**\r\n * 2. 초기 해 생성 (Greedy 방식)\r\n */\r\ndef generateInitialSolution(\r\n  users: Array[String],\r\n  userData: Map[String, Map[Int, Double]],\r\n  capacityPerHour: Map[Int, Int],\r\n  hours: Array[Int]\r\n): Solution = {\r\n  \r\n  val hourCapacity = mutable.Map(capacityPerHour.toSeq: _*)\r\n  val assignments = mutable.Map[String, Int]()\r\n  val hourUsage = mutable.Map[Int, Int]().withDefaultValue(0)\r\n  var totalScore = 0.0\r\n  \r\n  // 사용자를 최고 점수 순으로 정렬\r\n  val sortedUsers = users.sortBy { user =>\r\n    -userData(user).values.max\r\n  }\r\n  \r\n  for (user <- sortedUsers) {\r\n    val choices = userData(user).toSeq.sortBy(-_._2)\r\n    \r\n    var assigned = false\r\n    for ((hour, score) <- choices if !assigned) {\r\n      if (hourCapacity(hour) > 0) {\r\n        assignments(user) = hour\r\n        hourUsage(hour) += 1\r\n        hourCapacity(hour) -= 1\r\n        totalScore += score\r\n        assigned = true\r\n      }\r\n    }\r\n  }\r\n  \r\n  Solution(assignments.toMap, totalScore, hourUsage.toMap)\r\n}\r\n\r\n/**\r\n * 3. Simulated Annealing 메인 루프\r\n */\r\ndef runSimulatedAnnealing(\r\n  initialSolution: Solution,\r\n  userData: Map[String, Map[Int, Double]],\r\n  capacityPerHour: Map[Int, Int],\r\n  hours: Array[Int],\r\n  maxIterations: Int,\r\n  initialTemp: Double,\r\n  coolingRate: Double\r\n): Solution = {\r\n  \r\n  val random = new Random(42)\r\n  var currentSolution = initialSolution\r\n  var bestSolution = initialSolution\r\n  var temperature = initialTemp\r\n  \r\n  var acceptedMoves = 0\r\n  var rejectedMoves = 0\r\n  var improvements = 0\r\n  \r\n  val reportInterval = maxIterations / 10\r\n  \r\n  for (iteration <- 1 to maxIterations) {\r\n    // 이웃 해 생성\r\n    val neighborSolution = generateNeighbor(\r\n      currentSolution,\r\n      userData,\r\n      capacityPerHour,\r\n      hours,\r\n      random\r\n    )\r\n    \r\n    if (neighborSolution.isDefined) {\r\n      val neighbor = neighborSolution.get\r\n      val delta = neighbor.score - currentSolution.score\r\n      \r\n      // 수락 여부 결정\r\n      if (delta > 0 || random.nextDouble() < Math.exp(delta / temperature)) {\r\n        currentSolution = neighbor\r\n        acceptedMoves += 1\r\n        \r\n        if (neighbor.score > bestSolution.score) {\r\n          bestSolution = neighbor\r\n          improvements += 1\r\n        }\r\n      } else {\r\n        rejectedMoves += 1\r\n      }\r\n    }\r\n    \r\n    // 온도 감소\r\n    temperature *= coolingRate\r\n    \r\n    // 진행 상황 출력\r\n    if (iteration % reportInterval == 0) {\r\n      val progress = iteration.toDouble / maxIterations * 100\r\n      val acceptRate = acceptedMoves.toDouble / (acceptedMoves + rejectedMoves) * 100\r\n      println(f\"  Iteration ${numFormatter.format(iteration)} ($progress%.0f%%) - \" +\r\n        f\"Best: ${bestSolution.score}%,.2f, Current: ${currentSolution.score}%,.2f, \" +\r\n        f\"Temp: $temperature%.2f, Accept: $acceptRate%.1f%%, Improvements: $improvements\")\r\n    }\r\n  }\r\n  \r\n  val totalMoves = acceptedMoves + rejectedMoves\r\n  val finalAcceptRate = if (totalMoves > 0) acceptedMoves.toDouble / totalMoves * 100 else 0.0\r\n  \r\n  println(f\"\\n[SA STATISTICS]\")\r\n  println(f\"  Total moves evaluated: ${numFormatter.format(totalMoves)}\")\r\n  println(f\"  Accepted moves: ${numFormatter.format(acceptedMoves)} ($finalAcceptRate%.1f%%)\")\r\n  println(f\"  Improvements found: ${numFormatter.format(improvements)}\")\r\n  \r\n  bestSolution\r\n}\r\n\r\n/**\r\n * 4. 이웃 해 생성 (여러 전략)\r\n */\r\ndef generateNeighbor(\r\n  current: Solution,\r\n  userData: Map[String, Map[Int, Double]],\r\n  capacityPerHour: Map[Int, Int],\r\n  hours: Array[Int],\r\n  random: Random\r\n): Option[Solution] = {\r\n  \r\n  val strategy = random.nextInt(3)\r\n  \r\n  strategy match {\r\n    case 0 => reassignSingleUser(current, userData, capacityPerHour, random)\r\n    case 1 => swapTwoUsers(current, userData, random)\r\n    case 2 => reassignMultipleUsers(current, userData, capacityPerHour, random, 3)\r\n    case _ => reassignSingleUser(current, userData, capacityPerHour, random)\r\n  }\r\n}\r\n\r\n/**\r\n * 전략 1: 단일 사용자 재할당\r\n */\r\ndef reassignSingleUser(\r\n  current: Solution,\r\n  userData: Map[String, Map[Int, Double]],\r\n  capacityPerHour: Map[Int, Int],\r\n  random: Random\r\n): Option[Solution] = {\r\n  \r\n  if (current.assignments.isEmpty) return None\r\n  \r\n  val users = current.assignments.keys.toArray\r\n  val user = users(random.nextInt(users.length))\r\n  val currentHour = current.assignments(user)\r\n  val currentScore = userData(user)(currentHour)\r\n  \r\n  // 다른 시간대 중 선택\r\n  val availableHours = userData(user).keys.filter { hour =>\r\n    hour != currentHour &&\r\n    current.hourUsage.getOrElse(hour, 0) < capacityPerHour.getOrElse(hour, 0)\r\n  }.toArray\r\n  \r\n  if (availableHours.isEmpty) return None\r\n  \r\n  val newHour = availableHours(random.nextInt(availableHours.length))\r\n  val newScore = userData(user)(newHour)\r\n  \r\n  // 새로운 해 생성\r\n  val newAssignments = current.assignments + (user -> newHour)\r\n  val newHourUsage = current.hourUsage +\r\n    (currentHour -> (current.hourUsage(currentHour) - 1)) +\r\n    (newHour -> (current.hourUsage.getOrElse(newHour, 0) + 1))\r\n  val newTotalScore = current.score - currentScore + newScore\r\n  \r\n  Some(Solution(newAssignments, newTotalScore, newHourUsage))\r\n}\r\n\r\n/**\r\n * 전략 2: 두 사용자 교환\r\n */\r\ndef swapTwoUsers(\r\n  current: Solution,\r\n  userData: Map[String, Map[Int, Double]],\r\n  random: Random\r\n): Option[Solution] = {\r\n  \r\n  if (current.assignments.size < 2) return None\r\n  \r\n  val users = current.assignments.keys.toArray\r\n  val user1 = users(random.nextInt(users.length))\r\n  val user2 = users(random.nextInt(users.length))\r\n  \r\n  if (user1 == user2) return None\r\n  \r\n  val hour1 = current.assignments(user1)\r\n  val hour2 = current.assignments(user2)\r\n  \r\n  if (hour1 == hour2) return None\r\n  \r\n  // 두 사용자가 서로의 시간대를 가질 수 있는지 확인\r\n  if (!userData(user1).contains(hour2) || !userData(user2).contains(hour1)) {\r\n    return None\r\n  }\r\n  \r\n  val oldScore = userData(user1)(hour1) + userData(user2)(hour2)\r\n  val newScore = userData(user1)(hour2) + userData(user2)(hour1)\r\n  \r\n  val newAssignments = current.assignments +\r\n    (user1 -> hour2) +\r\n    (user2 -> hour1)\r\n  val newTotalScore = current.score - oldScore + newScore\r\n  \r\n  Some(Solution(newAssignments, newTotalScore, current.hourUsage))\r\n}\r\n\r\n/**\r\n * 전략 3: 다중 사용자 재할당\r\n */\r\ndef reassignMultipleUsers(\r\n  current: Solution,\r\n  userData: Map[String, Map[Int, Double]],\r\n  capacityPerHour: Map[Int, Int],\r\n  random: Random,\r\n  numUsers: Int\r\n): Option[Solution] = {\r\n  \r\n  if (current.assignments.size < numUsers) return None\r\n  \r\n  val users = current.assignments.keys.toArray\r\n  val selectedUsers = random.shuffle(users.toSeq).take(numUsers)\r\n  \r\n  var newSolution = current\r\n  var success = false\r\n  \r\n  for (user <- selectedUsers) {\r\n    val result = reassignSingleUser(newSolution, userData, capacityPerHour, random)\r\n    if (result.isDefined) {\r\n      newSolution = result.get\r\n      success = true\r\n    }\r\n  }\r\n  \r\n  if (success) Some(newSolution) else None\r\n}\r\n\r\n/**\r\n * 5. 해 검증\r\n */\r\ndef validateSolution(\r\n  solution: Solution,\r\n  capacityPerHour: Map[Int, Int],\r\n  hours: Array[Int],\r\n  totalUsers: Int\r\n): Unit = {\r\n  \r\n  println(s\"\\n${\"=\" * 80}\")\r\n  println(\"[SOLUTION VALIDATION]\")\r\n  println(s\"${\"=\" * 80}\")\r\n  \r\n  val assignedUsers = solution.assignments.size\r\n  println(f\"\\nTotal assigned: ${numFormatter.format(assignedUsers)} / ${numFormatter.format(totalUsers)}\")\r\n  println(f\"Assignment rate: ${assignedUsers.toDouble / totalUsers * 100}%.2f%%\")\r\n  \r\n  println(\"\\n[HOURLY ALLOCATION vs CAPACITY]\")\r\n  var violationDetected = false\r\n  \r\n  hours.sorted.foreach { hour =>\r\n    val assigned = solution.hourUsage.getOrElse(hour, 0)\r\n    val capacity = capacityPerHour.getOrElse(hour, 0)\r\n    val utilizationPct = if (capacity > 0) assigned.toDouble / capacity * 100 else 0.0\r\n    \r\n    val status = if (assigned <= capacity) \"✓\" else \"✗ VIOLATION\"\r\n    \r\n    println(f\"  Hour $hour: assigned=${numFormatter.format(assigned).padTo(8, ' ')} / \" +\r\n      f\"capacity=${numFormatter.format(capacity).padTo(8, ' ')} (${utilizationPct}%5.1f%%) $status\")\r\n    \r\n    if (assigned > capacity) {\r\n      violationDetected = true\r\n      println(s\"    ⚠⚠⚠ ERROR: Over capacity by ${numFormatter.format(assigned - capacity)}!\")\r\n    }\r\n  }\r\n  \r\n  if (violationDetected) {\r\n    println(\"\\n✗✗✗ CRITICAL ERROR: Capacity constraints violated!\")\r\n    throw new RuntimeException(\"Capacity constraints not satisfied\")\r\n  } else {\r\n    println(\"\\n✓✓✓ All capacity constraints satisfied\")\r\n  }\r\n  \r\n  println(s\"${\"=\" * 80}\\n\")\r\n}\r\n\r\n/**\r\n * 6. Hybrid 할당: SA + Greedy 조합\r\n */\r\ndef allocateUsersHybridSA(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  maxIterations: Int = 100000,\r\n  batchSize: Int = 500000\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  try {\r\n    val saResult = allocateUsersWithSimulatedAnnealing(\r\n      df,\r\n      capacityPerHour,\r\n      maxIterations,\r\n      100.0,\r\n      0.995,\r\n      batchSize\r\n    )\r\n    \r\n    val assignedUsers = saResult.select(\"svc_mgmt_num\").collect().map(_.getString(0)).toSet\r\n    val allUsers = df.select(\"svc_mgmt_num\").distinct().collect().map(_.getString(0)).toSet\r\n    val unassignedUsers = allUsers -- assignedUsers\r\n    \r\n    if (unassignedUsers.isEmpty) {\r\n      println(\"✓ All users assigned by SA optimizer\")\r\n      return saResult\r\n    }\r\n    \r\n    println(s\"\\n${numFormatter.format(unassignedUsers.size)} users unassigned, running Greedy for remainder...\")\r\n    \r\n    val usedCapacity = saResult.groupBy(\"assigned_hour\").count().collect()\r\n      .map(r => r.getInt(0) -> r.getLong(1).toInt).toMap\r\n    \r\n    val remainingCapacity = capacityPerHour.map { case (hour, cap) =>\r\n      hour -> Math.max(0, cap - usedCapacity.getOrElse(hour, 0))\r\n    }\r\n    \r\n    val unassignedDf = df.filter($\"svc_mgmt_num\".isin(unassignedUsers.toSeq: _*))\r\n    val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n    val greedyResult = allocateGreedySimple(unassignedDf, hours, remainingCapacity)\r\n    \r\n    if (greedyResult.count() == 0) {\r\n      return saResult\r\n    }\r\n    \r\n    saResult.union(greedyResult)\r\n    \r\n  } catch {\r\n    case e: Exception =>\r\n      println(s\"\\nSA optimizer failed: ${e.getMessage}\")\r\n      println(\"Running full Greedy allocation...\")\r\n      val hours = df.select(\"send_hour\").distinct().collect().map(_.getInt(0)).sorted\r\n      allocateGreedySimple(df, hours, capacityPerHour)\r\n  }\r\n}\r\n\r\n/**\r\n * 7. 대규모 배치 처리 (SA 기반)\r\n */\r\ndef allocateLargeScaleSA(\r\n  df: DataFrame,\r\n  capacityPerHour: Map[Int, Int],\r\n  batchSize: Int = 500000,\r\n  maxIterations: Int = 100000\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"=\" * 80)\r\n  println(\"Batch Allocation for Large Scale Data (SA Mode)\")\r\n  println(\"=\" * 80)\r\n  \r\n  val userPriority = df.groupBy(\"svc_mgmt_num\")\r\n    .agg(max(\"propensity_score\").as(\"max_prob\"))\r\n\r\n  val totalUsers = userPriority.count()\r\n  val numBatches = Math.ceil(totalUsers.toDouble / batchSize).toInt\r\n  \r\n  println(s\"\\n[BATCH SETUP]\")\r\n  println(s\"Total users: ${numFormatter.format(totalUsers)}\")\r\n  println(s\"Batch size: ${numFormatter.format(batchSize)}\")\r\n  println(s\"Number of batches: $numBatches\")\r\n  \r\n  val allUsers = userPriority\r\n    .withColumn(\"row_id\", row_number().over(Window.orderBy(desc(\"max_prob\"))))\r\n    .withColumn(\"batch_id\", (($\"row_id\" - 1) / batchSize).cast(\"int\"))\r\n    .select(\"svc_mgmt_num\", \"batch_id\")\r\n    .cache()\r\n  \r\n  var remainingCapacity = capacityPerHour.toMap\r\n  val allResults = mutable.ArrayBuffer[DataFrame]()\r\n  var totalAssignedSoFar = 0L\r\n  \r\n  for (batchId <- 0 until numBatches) {\r\n    println(s\"\\n${\"=\" * 80}\")\r\n    println(s\"Processing Batch ${batchId + 1}/$numBatches\")\r\n    println(s\"${\"=\" * 80}\")\r\n    \r\n    val batchUsers = allUsers.filter($\"batch_id\" === batchId)\r\n    val availableHours = remainingCapacity.filter(_._2 > 0).keys.toSeq\r\n    \r\n    if (availableHours.isEmpty) {\r\n      println(\"⚠ No capacity left in any hour.\")\r\n      \r\n    } else {\r\n      val batchDf = df.join(batchUsers, Seq(\"svc_mgmt_num\"))\r\n        .filter($\"send_hour\".isin(availableHours: _*))\r\n      \r\n      val batchResult = allocateUsersHybridSA(batchDf, remainingCapacity, maxIterations, batchSize)\r\n      val assignedCount = batchResult.count()\r\n      \r\n      if (assignedCount > 0) {\r\n        totalAssignedSoFar += assignedCount\r\n        \r\n        val allocatedPerHour = batchResult.groupBy(\"assigned_hour\").count().collect()\r\n          .map(row => row.getInt(0) -> row.getLong(1).toInt).toMap\r\n        \r\n        remainingCapacity = remainingCapacity.map { case (hour, cap) =>\r\n          hour -> Math.max(0, cap - allocatedPerHour.getOrElse(hour, 0))\r\n        }\r\n        \r\n        allResults += batchResult\r\n      }\r\n    }\r\n  }\r\n  \r\n  allUsers.unpersist()\r\n  \r\n  if (allResults.isEmpty) {\r\n    spark.emptyDataFrame\r\n  } else {\r\n    val finalResult = allResults.reduce(_.union(_))\r\n    printFinalStatistics(finalResult, totalUsers)\r\n    finalResult\r\n  }\r\n}\r\n\r\ndef allocateGreedySimple(\r\n  df: DataFrame,\r\n  hours: Array[Int],\r\n  initialCapacity: Map[Int, Int]\r\n): DataFrame = {\r\n  \r\n  import spark.implicits._\r\n  \r\n  println(\"\\n\" + \"=\" * 80)\r\n  println(\"Running Greedy allocation\")\r\n  println(\"=\" * 80)\r\n  \r\n  println(\"\\nInitial capacity:\")\r\n  initialCapacity.toSeq.sortBy(_._1).foreach { case (hour, cap) =>\r\n    println(s\"  Hour $hour: ${numFormatter.format(cap)}\")\r\n  }\r\n  \r\n  val userData = collectUserData(df)\r\n  val users = userData.keys.toArray\r\n  \r\n  println(s\"\\nUsers to assign: ${numFormatter.format(users.length)}\")\r\n  \r\n  // 사용자를 최고 점수 순으로 정렬\r\n  val userBestScores = users.map { user =>\r\n    val bestScore = userData(user).values.max\r\n    (user, bestScore)\r\n  }.sortBy(-_._2)\r\n  \r\n  val hourCapacity = mutable.Map(initialCapacity.toSeq: _*)\r\n  val assignments = mutable.ArrayBuffer[AllocationResult]()\r\n  \r\n  for ((user, _) <- userBestScores) {\r\n    val choices = userData(user).toSeq.sortBy(-_._2)\r\n    \r\n    var assigned = false\r\n    for ((hour, score) <- choices if !assigned) {\r\n      val currentCapacity = hourCapacity.getOrElse(hour, 0)\r\n      \r\n      if (currentCapacity > 0) {\r\n        assignments += AllocationResult(user, hour, score)\r\n        hourCapacity(hour) = currentCapacity - 1\r\n        assigned = true\r\n      }\r\n    }\r\n  }\r\n  \r\n  println(s\"\\nGreedy assigned: ${numFormatter.format(assignments.size)} / ${numFormatter.format(users.length)}\")\r\n  \r\n  if (assignments.nonEmpty) {\r\n    println(\"\\n[GREEDY ALLOCATION BY HOUR]\")\r\n    val hourlyAssignment = assignments.groupBy(_.assigned_hour).mapValues(_.length)\r\n    \r\n    hours.sorted.foreach { hour =>\r\n      val assigned = hourlyAssignment.getOrElse(hour, 0)\r\n      val initialCap = initialCapacity.getOrElse(hour, 0)\r\n      val remaining = hourCapacity.getOrElse(hour, 0)\r\n      \r\n      println(f\"  Hour $hour: assigned=${numFormatter.format(assigned)}, capacity=${numFormatter.format(initialCap)}, remaining=${numFormatter.format(remaining)}\")\r\n    }\r\n    \r\n    println(\"=\" * 80 + \"\\n\")\r\n  }\r\n  \r\n  if (assignments.isEmpty) {\r\n    spark.emptyDataFrame\r\n  } else {\r\n    assignments.toSeq.toDF()\r\n  }\r\n}\r\n\r\nprintln(\"✓ Simulated Annealing optimizer defined (ready for Java conversion)\")",
      "user": "anonymous",
      "dateUpdated": "2026-01-14T01:36:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768287717428_140681656",
      "id": "paragraph_1768287717428_140681656",
      "dateCreated": "2026-01-13T07:01:57+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:803368",
      "dateFinished": "2026-01-14T01:33:49+0000",
      "dateStarted": "2026-01-14T01:33:49+0000",
      "title": "Simulated Anealing",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "✓ Simulated Annealing optimizer defined (ready for Java conversion)\nimport scala.collection.mutable\nimport scala.util.Random\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\ndefined class Solution\n\u001b[1m\u001b[34mallocateUsersWithSimulatedAnnealing\u001b[0m: \u001b[1m\u001b[32m(df: org.apache.spark.sql.DataFrame, capacityPerHour: Map[Int,Int], maxIterations: Int, initialTemperature: Double, coolingRate: Double, batchSize: Int)org.apache.spark.sql.DataFrame\u001b[0m\n\u001b[1m\u001b[34mgenerateInitialSolution\u001b[0m: \u001b[1m\u001b[32m(users: Array[String], userData: Map[String,Map[Int,Double]], capacityPerHour: Map[Int,Int], hours: Array[Int])Solution\u001b[0m\n\u001b[1m\u001b[34mrunSimulatedAnnealing\u001b[0m: \u001b[1m\u001b[32m(initialSolution: Solution, userData: Map[String,Map[Int,Double]], capacityPerHour: Map[Int,Int], hours: Array[Int], maxIterations...\n"
          }
        ]
      }
    },
    {
      "text": "%spark-local\n\nval dfAll = spark.read.parquet(\"aos/sto/propensityScoreDF\")\n// .filter(\"suffix=0\")\n.cache()\n// dfAll.sort(\"svc_mgmt_num\",\"send_hour\").show()",
      "user": "anonymous",
      "dateUpdated": "2026-01-14T01:38:10+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ats-01-16:30431/jobs/job?id=0",
              "$$hashKey": "object:846090"
            }
          ],
          "interpreterSettingId": "spark-local"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768267623870_408515033",
      "id": "paragraph_1768267623870_408515033",
      "dateCreated": "2026-01-13T01:27:03+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:697748",
      "dateFinished": "2026-01-14T01:33:48+0000",
      "dateStarted": "2026-01-14T01:33:45+0000",
      "title": "Data Load",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mdfAll\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [svc_mgmt_num: string, prob_click: double ... 5 more fields]\n"
          }
        ]
      }
    },
    {
      "text": "%spark-local\r\n\r\n// ============================================================================\r\n// Simulated Annealing 기반 사용자 할당 최적화 - 사용법 예시\r\n// ============================================================================\r\n\r\nval df = dfAll.filter(\"svc_mgmt_num like '%00'\")\r\n\r\nprintln(s\"Total records: ${df.count()}\")\r\n\r\nval runtime = Runtime.getRuntime\r\nval maxMemoryGB = runtime.maxMemory() / (1024.0 * 1024 * 1024)\r\nprintln(f\"Available memory: $maxMemoryGB%.2f GB\")\r\n\r\nval safeBatchSize = (maxMemoryGB * 1000000 / 10 * 0.05).toInt\r\nprintln(s\"Safe batch size: ${numFormatter.format(safeBatchSize)}\")\r\n\r\n// 1. 설정\r\nval userCnt = df.select(\"svc_mgmt_num\").distinct().count()\r\nval capacityPerHour = 10000 // 시간대별 기본 용량\r\n\r\n// 시간대별 용량 맵 생성 (9시~18시)\r\nval capacityPerHourMap = Map(\r\n  9 -> capacityPerHour,\r\n  10 -> capacityPerHour,\r\n  11 -> capacityPerHour,\r\n  12 -> capacityPerHour,\r\n  13 -> capacityPerHour,\r\n  14 -> capacityPerHour,\r\n  15 -> capacityPerHour,\r\n  16 -> capacityPerHour,\r\n  17 -> capacityPerHour,\r\n  18 -> capacityPerHour\r\n)\r\n\r\n// SA 알고리즘 파라미터\r\nval maxIterations = 100000      // 최대 반복 횟수\r\nval initialTemp = 1000.0          // 초기 온도\r\nval coolingRate = 0.9995        // 냉각 비율\r\nval batchSize = safeBatchSize           // 배치 크기 (대규모 데이터용)\r\n\r\nval result = allocateUsersWithSimulatedAnnealing(\r\n  df = df,\r\n  capacityPerHour = capacityPerHourMap,\r\n  maxIterations = maxIterations,\r\n  initialTemperature = initialTemp,\r\n  coolingRate = coolingRate,\r\n  batchSize = batchSize\r\n)\r\n\r\n// val result = allocateLargeScaleHybrid(\r\n//   df = df,\r\n//   capacityPerHour = capacityPerHourMap,\r\n//   batchSize = safeBatchSize,\r\n//   timeLimit = timeLimit,\r\n//   topChoices = 5,          // 사용자별 상위 5개 시간대만 사용\r\n//   enablePreprocessing = true\r\n// )\r\n\r\n// 결과 확인\r\nresult.show(20, false)\r\n\r\n// 시간대별 할당 현황\r\nresult.groupBy(\"assigned_hour\")\r\n  .agg(\r\n    count(\"*\").as(\"count\"),\r\n    sum(\"score\").as(\"total_score\"),\r\n    avg(\"score\").as(\"avg_score\")\r\n  )\r\n  .orderBy(\"assigned_hour\")\r\n  .show(false)\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "2026-01-14T01:37:29+0000",
      "progress": 0,
      "config": {
        "lineNumbers": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 10,
        "title": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 87.2,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=2",
              "$$hashKey": "object:846033"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=3",
              "$$hashKey": "object:846034"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=4",
              "$$hashKey": "object:846035"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=5",
              "$$hashKey": "object:846036"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=6",
              "$$hashKey": "object:846037"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=7",
              "$$hashKey": "object:846038"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=8",
              "$$hashKey": "object:846039"
            },
            {
              "jobUrl": "http://ats-01-16:30432/jobs/job?id=9",
              "$$hashKey": "object:846040"
            }
          ],
          "interpreterSettingId": "spark-local"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768350168375_2087564907",
      "id": "paragraph_1768350168375_2087564907",
      "dateCreated": "2026-01-14T00:22:48+0000",
      "status": "ABORT",
      "focus": true,
      "$$hashKey": "object:841720",
      "dateFinished": "2026-01-14T01:31:45+0000",
      "dateStarted": "2026-01-14T01:20:31+0000",
      "title": "Test",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.RuntimeException\n\tat org.apache.zeppelin.interpreter.remote.PooledRemoteClient.callRemoteFunction(PooledRemoteClient.java:119)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:100)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:208)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:484)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:69)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          }
        ]
      }
    },
    {
      "text": "%spark-local\n",
      "user": "anonymous",
      "dateUpdated": "2026-01-14T00:24:12+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1768350252909_1380838956",
      "id": "paragraph_1768350252909_1380838956",
      "dateCreated": "2026-01-14T00:24:12+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:841814"
    }
  ],
  "name": "optimize_ost",
  "id": "2MDZ3PXPW",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/STO/optimize_ost"
}