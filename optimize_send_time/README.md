# Optimize Send Time - Work Guide

## ğŸ“‚ Directory Overview

This directory contains Spark/Scala-based machine learning models for optimizing message send time prediction.

**File Structure:**
- `*.zpln` - Zeppelin notebook files (original source)
- `*.scala` - Pure Scala code files (converted from notebooks)
- `*.java` - Java implementation (GreedyAllocator for large-scale processing)

## ğŸ¯ Project Purpose

- **Goal**: Predict optimal send times for marketing messages using XGBoost regression
- **Tech Stack**: Apache Spark 3.1.3, Scala 2.12.18, XGBoost
- **Data Source**: Campaign reaction data (MMS/RCS channels)

## ğŸš€ Quick Start

### 1. Environment Setup (First Time Only)

```bash
cd /Users/yongwook/workspace/AgenticWorkflow/optimize_send_time

# Spark í™˜ê²½ ì„¤ì¹˜
./setup_spark_env.sh
source ~/.zshrc

# ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ë¡œë“œ
# jMetal ë‹¤ìš´ë¡œë“œ (ë‹¤ëª©ì  ìµœì í™”ìš© - NEW!)
./download_jmetal.sh

# OR-ToolsëŠ” ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ lib/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
# wget https://repo1.maven.org/maven2/com/google/ortools/ortools-java/9.4.1874/ortools-java-9.4.1874.jar -P lib/

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ê¶Œì¥)
source setup_all_optimizers.sh
# ì´ì œ $JMETAL_JARS, $ORTOOLS_JARS, $ALL_OPTIMIZER_JARS ì‚¬ìš© ê°€ëŠ¥
```

### 2. Generate Sample Data

```bash
# Option 1: Generate full dataset (100K users, 1M records)
./generate_sample_data.sh
# Select option 1

# Option 2: Generate small test dataset (1K users, 10K records)
./generate_sample_data.sh
# Select option 2
```

### 3. Run Quick Test

```bash
# Complete test with sample data
./quick_test_with_sample_data.sh
```

### 4. Interactive Development (ê¶Œì¥ â­)

```bash
# ë°©ë²• 1: Interactive Shell ì‹œì‘
./run_interactive.sh

# Spark Shell ë‚´ë¶€ì—ì„œ:
scala> :load optimize_ost.scala
scala> import OptimizeSendTime._
scala> val dfAll = spark.read.parquet("aos/sto/propensityScoreDF").cache()
```

> **ì¤‘ìš”**: spark-shellì„ `-i optimize_ost.scala`ë¡œ ì‹œì‘í•œ ê²½ìš°, ê°™ì€ ì„¸ì…˜ì—ì„œ ë‹¤ì‹œ `:load optimize_ost.scala`ë¥¼ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”.  
> ë™ì¼ ì •ì˜ ì¬ë¡œë”©ìœ¼ë¡œ ì¸í•´ spark-shellì´ í¬ë˜ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì¬ë¡œë”©ì´ í•„ìš”í•˜ë©´ `:quit` í›„ ì¬ì‹œì‘ì´ ê°€ì¥ ì•ˆì „í•©ë‹ˆë‹¤.)

```bash
# ë°©ë²• 2: ìë™ í…ŒìŠ¤íŠ¸
spark-shell -i load_and_test.scala

# ë°©ë²• 3: Quick Run (ëŒ€í™”í˜• ë©”ë‰´)
./quick_run.sh
```

## ğŸ“Š Sample Data Schema

Generated sample data follows this schema:

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `svc_mgmt_num` | String | User ID | s:0063c2994b5452d... |
| `send_ym` | String | Send year-month | 202512 |
| `send_hour` | Int | Send hour (9-18) | 10 |
| `propensity_score` | Double | Response probability | 0.7234 |

**Dataset Characteristics:**
- 100,000 users
- Each user has 10 records (one per hour: 9-18)
- Total: 1,000,000 records
- Propensity scores: 0.1 ~ 0.99
- Realistic distribution (users have preferred hours)

## ğŸ“‹ AI Assistant Work Rules

### 1. Zeppelin Notebook (.zpln) Handling

#### Default Behavior
When working with `.zpln` files:
- **DO NOT** directly edit notebook files manually
- **ALWAYS** use Python scripts for automated conversion
- Convert to `.scala` files for version control and code review
- Maintain paragraph IDs in converted files

#### Conversion Process
```bash
python3 -c "
import json

with open('notebook.zpln', 'r') as f:
    notebook = json.load(f)

scala_code = []
for para in notebook['paragraphs']:
    text = para.get('text', '')
    para_id = para.get('id', '')
    
    # Skip PySpark paragraphs
    if text.strip().startswith('%pyspark'):
        continue
    
    # Remove Zeppelin directives
    if text.strip().startswith('%'):
        text = '\\n'.join(text.split('\\n')[1:])
    
    # Remove Zeppelin-specific code
    if 'z.show(' in text or 'z.input(' in text:
        continue
        
    if text.strip():
        scala_code.append(f'// ===== {para_id} =====')
        scala_code.append(text)
        scala_code.append('')

with open('output.scala', 'w') as f:
    f.write('\\n'.join(scala_code))
"
```

### 2. Zeppelin-Specific Code Removal

When converting `.zpln` to `.scala`, **ALWAYS** remove or replace:

#### Must Remove:
- `z.show()` - Zeppelin display function
- `z.run()` - Zeppelin paragraph execution
- `z.angular()` - Zeppelin angular binding
- `%sql`, `%md`, `%pyspark` - Interpreter directives

#### Must Replace:
- `z.input("name", "default")` â†’ `val name = "default"` (with comment explaining source)

#### Example Replacement:
```scala
// Original (Zeppelin):
val smnSuffix = z.input("suffix", "0").toString
z.show(dataDF)

// Converted (Pure Scala):
// Default suffix value (previously from Zeppelin input widget)
val smnSuffix = "0"
// To view data, use: dataDF.show()
```

### 3. Paragraph ID Preservation

**ALWAYS** preserve original paragraph IDs when converting:

```scala
// ===== paragraph_1764658338256_686533166 =====
import com.microsoft.azure.synapse.ml.causal
// ... code ...

// ===== paragraph_1764742922351_426209997 =====
def getPreviousMonths(startMonthStr: String, periodM: Int): Array[String] = {
// ... code ...
}
```

**Why?**
- Easy mapping between notebook and Scala files
- Better debugging and issue tracking
- Clear code organization by logical sections

### 4. Spark Environment Setup

#### Local Spark Installation
Location: `~/spark-local/`
- Spark 3.1.3
- Scala 2.12.18
- Java 8 (OpenJDK 1.8.0_292)

#### Environment Activation
```bash
source ~/spark-local/spark-env.sh
```

#### Running Scala Code
```bash
# Interactive Shell
spark-shell

# Execute Scala file
spark-shell -i /path/to/file.scala

# With specific memory settings
spark-shell --driver-memory 4g --executor-memory 4g
```

### 5. XGBoost Model Configuration

When modifying XGBoost models:

#### Standard Parameters
```scala
val xgbParamR = Map(
  "eta" -> 0.01,                    // Learning rate
  "max_depth" -> 6,                 // Tree depth
  "objective" -> "reg:squarederror", // Regression task
  "num_round" -> 100,               // Number of iterations
  "num_workers" -> 10,              // Parallel workers
  "eval_metric" -> "rmse"           // Evaluation metric
)
```

#### Feature Interaction Constraints
For forcing specific features (e.g., `send_hournum_cd`) as primary split:

```scala
// Find feature indices
val assemblerInputCols = vectorAssembler.getInputCols
val sendHournumIdx = assemblerInputCols.indexOf("send_hournum_cd_enc")

// Apply constraints
val xgbParamWithConstraints = xgbParamR + (
  "interaction_constraints" -> s"[[$sendHournumIdx],[0,1,2,...]]"
)
```

### 6. Code Style Guidelines

#### Imports Organization
```scala
// 1. External libraries
import com.microsoft.azure.synapse.ml.causal
import ml.dmlc.xgboost4j.scala.spark._

// 2. Spark libraries
import org.apache.spark.ml.classification._
import org.apache.spark.sql.functions.{col, lit, expr}

// 3. Java standard libraries
import java.time.format.DateTimeFormatter

// 4. Scala standard libraries
import scala.collection.mutable.ListBuffer
```

#### Variable Naming
- Use `camelCase` for variables: `sendMonth`, `featureYmList`
- Use `PascalCase` for classes/objects: `XGBoostRegressor`
- Use descriptive names: `resDFFiltered` not `df1`

#### SQL Queries
```scala
// Prefer multiline formatting for readability
val df = spark.sql("""
  SELECT 
    svc_mgmt_num,
    send_dt,
    click_yn
  FROM tos.od_tcam_cmpgn_obj_cont
  WHERE send_ym = '202512'
""")
```

### 7. Git Workflow

#### Before Committing
1. Convert `.zpln` files to `.scala`
2. Remove all Zeppelin-specific code
3. Verify Scala syntax (no compilation errors)
4. Test key functionality if possible

#### Commit Message Format
```bash
git commit -m "feat: Add XGBoost model with feature constraints

- Implement interaction constraints for send_hournum_cd
- Update parameter tuning for better RMSE
- Files: predict_ost_251221.scala
"
```

#### Files to Track
- âœ… `.scala` files (always)
- âœ… `.zpln` files (for reference)
- âŒ `.metals/`, `.scala-build/` (ignore)

### 8. Testing & Validation

#### Quick Syntax Check
```bash
# Try to load in Spark shell
spark-shell -i predict_ost_251221.scala
```

#### Common Issues
1. **Missing variable**: Check if Zeppelin `z.input()` was removed
   - Fix: Add default value declaration
   
2. **Import errors**: Verify all imports are valid for local Spark
   - Fix: Remove Synapse-specific imports if needed

3. **SQL syntax errors**: Check string interpolation
   - Fix: Use proper `s"..."` or `f"..."` syntax

## ğŸ§ª Usage Examples

### Example 1: Greedy Allocation (Fastest)

```scala
import OptimizeSendTime._

val dfAll = spark.read.parquet("data/sample/propensityScoreDF").cache()
val df = dfAll.limit(10000)

val capacity = Map(
  9 -> 1000, 10 -> 1000, 11 -> 1000, 12 -> 1000, 13 -> 1000,
  14 -> 1000, 15 -> 1000, 16 -> 1000, 17 -> 1000, 18 -> 1000
)

val result = allocateGreedySimple(df, Array(9,10,11,12,13,14,15,16,17,18), capacity)
result.show()
```

### Example 2: Simulated Annealing (Balanced)

```scala
import OptimizeSendTime._

val dfAll = spark.read.parquet("data/sample/propensityScoreDF").cache()
val df = dfAll.limit(10000)

val capacityMap = Map(
  9 -> 1000, 10 -> 1000, 11 -> 1000, 12 -> 1000, 13 -> 1000,
  14 -> 1000, 15 -> 1000, 16 -> 1000, 17 -> 1000, 18 -> 1000
)

val result = allocateUsersWithSimulatedAnnealing(
  df = df,
  capacityPerHour = capacityMap,
  maxIterations = 100000,
  initialTemperature = 1000.0,
  coolingRate = 0.9995,
  batchSize = 500000
)

result.groupBy("assigned_hour").count().orderBy("assigned_hour").show()
```

### Example 3: OR-Tools Optimization (Most Accurate)

```scala
import OptimizeSendTime._

val dfAll = spark.read.parquet("data/sample/propensityScoreDF").cache()
val df = dfAll.limit(5000)  // OR-Tools works best with smaller datasets

val capacityMap = Map(
  9 -> 500, 10 -> 500, 11 -> 500, 12 -> 500, 13 -> 500,
  14 -> 500, 15 -> 500, 16 -> 500, 17 -> 500, 18 -> 500
)

val result = allocateUsersWithHourlyCapacity(
  df = df,
  capacityPerHour = capacityMap,
  timeLimit = 300,
  topChoices = 5,
  enablePreprocessing = true
)

result.show()
```

### Example 3-1: jMetal NSGA-II (Multi-Objective - NEW!)

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì²˜ìŒ í•œ ë²ˆ)
source setup_all_optimizers.sh

# Spark shell ì‹œì‘
spark-shell --jars $JMETAL_JARS -i optimize_ost.scala
```

```scala
import OptimizeSendTime._

val dfAll = spark.read.parquet("data/sample/propensityScoreDF").cache()
val df = dfAll.limit(10000)

val capacityMap = Map(
  9 -> 1000, 10 -> 1000, 11 -> 1000, 12 -> 1000, 13 -> 1000,
  14 -> 1000, 15 -> 1000, 16 -> 1000, 17 -> 1000, 18 -> 1000
)

// ë‹¤ëª©ì  ìµœì í™”: ì ìˆ˜ ìµœëŒ€í™” + ë¶€í•˜ ë¶„ì‚°
val result = allocateUsersWithJMetalNSGAII(
  df = df,
  capacityPerHour = capacityMap,
  populationSize = 100,
  maxEvaluations = 25000
)

result.groupBy("assigned_hour").count().orderBy("assigned_hour").show()
```

### Example 4: Large Scale Processing

```scala
import OptimizeSendTime._

val dfAll = spark.read.parquet("data/sample/propensityScoreDF").cache()

val capacityMap = Map(
  9 -> 10000, 10 -> 10000, 11 -> 10000, 12 -> 10000, 13 -> 10000,
  14 -> 10000, 15 -> 10000, 16 -> 10000, 17 -> 10000, 18 -> 10000
)

val result = allocateLargeScaleHybrid(
  df = dfAll,
  capacityPerHour = capacityMap,
  batchSize = 50000,
  timeLimit = 300,
  topChoices = 5,
  enablePreprocessing = true
)

// Save results
result.write.mode("overwrite").parquet("data/results/allocation_result")
```

## ğŸ“š Common Operations

### Generate Sample Data
```bash
# Full dataset (100K users)
./generate_sample_data.sh

# Or in Spark Shell
import GenerateSampleData._
val df = generateSampleData(spark, numUsers = 100000, sendYm = "202512")
```

### Convert New Zeppelin Notebook
```bash
# 1. Place .zpln file in optimize_send_time/
# 2. Run conversion script (see section 1)
# 3. Remove Zeppelin-specific code
# 4. Add default values for z.input() parameters
# 5. Verify paragraph IDs are preserved
```

### Update Existing Model
```bash
# 1. Edit .scala file directly
# 2. Test changes in spark-shell
# 3. Update corresponding .zpln if needed
# 4. Commit both files
```

### Add New Features
```bash
# 1. Add feature engineering code
# 2. Update VectorAssembler input columns
# 3. Adjust XGBoost parameters if needed
# 4. Document changes in comments
```

## ğŸ” Key Files

### Optimization System (New)
| File | Purpose | Status |
|------|---------|--------|
| `optimize_ost.scala` | User allocation optimizer (OR-Tools, SA, jMetal) | **Production** |
| `generate_sample_data.scala` | Sample data generator | Development |
| `setup_spark_env.sh` | Environment setup script | Setup |
| `setup_ortools_jars.sh` | OR-Tools JAR downloader | Setup |
| `download_jmetal.sh` | â­ jMetal JAR downloader (NEW) | **Setup** |
| `setup_jmetal_env.sh` | jMetal í™˜ê²½ ë³€ìˆ˜ ì„¤ì • | Setup |
| `setup_all_optimizers.sh` | â­ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ í™˜ê²½ ë³€ìˆ˜ í†µí•© (NEW) | **Recommended** |
| `spark-shell-jmetal.sh` | jMetal ì§€ì› Spark shell ë˜í¼ | Convenience |
| `example_jmetal.scala` | jMetal ì‚¬ìš© ì˜ˆì œ | Example |
| `run_optimize_ost.sh` | Execution script | Development |
| `generate_sample_data.sh` | Data generation script | Development |
| `generate_data_simple.sh` | Simple data generator | Development |
| `generate_data_with_backup.sh` | Data generator with backup | Development |
| `quick_test_with_sample_data.sh` | Quick test script | Testing |
| `quick_run.sh` | Quick run with auto-detection | Development |
| `run_interactive.sh` | â­ Interactive shell launcher | **Recommended** |
| `load_and_test.scala` | Auto-load and test script | Testing |

### Prediction Models (Original)
| File | Purpose | Status |
|------|---------|--------|
| `predict_ost_251221.zpln` | Latest notebook (Dec 2024) | Source |
| `predict_ost_251221.scala` | Converted Scala code | Archive |
| `predict_ost_25121711.zpln` | Previous version | Archive |
| `predict_ost_25121711.scala` | Previous Scala code | Archive |
| `predict_ost_25121710.scala` | Oldest version | Archive |

### Documentation
| File | Purpose |
|------|---------|
| `INSTALLATION_GUIDE.md` | Detailed installation guide (macOS) |
| `LINUX_DEPLOYMENT_GUIDE.md` | Linux server deployment guide |
| `DATA_GENERATION_GUIDE.md` | Sample data generation guide |
| `TROUBLESHOOTING.md` | â­ Problem solving guide |
| `QUICK_START.md` | 5-minute quick start guide |
| `JMETAL_SETUP.md` | â­ jMetal ì„¤ì • ë° ì‚¬ìš© ê°€ì´ë“œ (NEW) |
| `QUICK_START_JMETAL.md` | jMetal ë¹ ë¥¸ ì‹œì‘ (NEW) |
| `ENV_SETUP_GUIDE.md` | â­ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ê°€ì´ë“œ (NEW) |
| `README_SBT.md` | SBT project guide (after setup) |

## ğŸ¯ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ (NEW!)

| ì•Œê³ ë¦¬ì¦˜ | ëª©ì  | ì†ë„ | í’ˆì§ˆ | ë©”ëª¨ë¦¬ | ê¶Œì¥ ì‚¬ìš© |
|---------|------|------|------|--------|----------|
| **Greedy** | ë¹ ë¥¸ í• ë‹¹ | âš¡âš¡âš¡âš¡âš¡ | â­â­ | ğŸ’¾ | ì´ˆê¸° í…ŒìŠ¤íŠ¸, ëŒ€ìš©ëŸ‰ |
| **Simulated Annealing** | ì¤€ìµœì í•´ íƒìƒ‰ | âš¡âš¡âš¡âš¡ | â­â­â­â­ | ğŸ’¾ğŸ’¾ | ë°¸ëŸ°ìŠ¤í˜•, ì¤‘ëŒ€ê·œëª¨ |
| **OR-Tools** | ì •í™•í•œ ìµœì í•´ | âš¡âš¡ | â­â­â­â­â­ | ğŸ’¾ğŸ’¾ | ì†Œê·œëª¨, ìµœê³  í’ˆì§ˆ |
| **jMetal NSGA-II** | ë‹¤ëª©ì  ìµœì í™” | âš¡âš¡âš¡ | â­â­â­â­ | ğŸ’¾ğŸ’¾ğŸ’¾ | ë¶€í•˜ë¶„ì‚°+ì ìˆ˜ |
| **jMetal MOEA/D** | ë¹ ë¥¸ ë‹¤ëª©ì  | âš¡âš¡âš¡âš¡ | â­â­â­ | ğŸ’¾ğŸ’¾ | ë¹ ë¥¸ ìˆ˜ë ´ í•„ìš”ì‹œ |

### ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ê°€ì´ë“œ

```bash
# ìƒí™© 1: 100ë§Œ+ ì‚¬ìš©ì, ë¹ ë¥¸ ê²°ê³¼ í•„ìš”
â†’ Greedy ë˜ëŠ” Hybrid (Greedy ê¸°ë°˜)

# ìƒí™© 2: 10ë§Œ ì‚¬ìš©ì, ë†’ì€ í’ˆì§ˆ í•„ìš”
â†’ Simulated Annealing ë˜ëŠ” Hybrid (SA ê¸°ë°˜)

# ìƒí™© 3: 5ë§Œ ì´í•˜ ì‚¬ìš©ì, ìµœê³  í’ˆì§ˆ í•„ìš”
â†’ OR-Tools ë˜ëŠ” Hybrid (OR-Tools ê¸°ë°˜)

# ìƒí™© 4: ì ìˆ˜ì™€ ë¶€í•˜ë¶„ì‚° ëª¨ë‘ ì¤‘ìš”
â†’ jMetal NSGA-II ë˜ëŠ” MOEA/D

# ìƒí™© 5: ëŒ€ìš©ëŸ‰ + ë†’ì€ í’ˆì§ˆ
â†’ allocateLargeScaleHybrid ë˜ëŠ” allocateLargeScaleJMetal
```

## ğŸ’¡ Best Practices

### For Optimization System
1. **Start small** - Test with limited data first (limit 1000-10000)
2. **Use Greedy for quick tests** - Fastest way to validate data and logic
3. **Monitor memory** - Watch Spark UI at http://localhost:4040
4. **Adjust batch size** - Reduce if you encounter OOM errors
5. **Use preprocessing** - Enable `topChoices` to reduce problem size
6. **Save intermediate results** - Cache DataFrames for reuse

### For Zeppelin Notebooks
1. **Always preserve paragraph IDs** - Enables mapping between notebooks and code
2. **Remove Zeppelin dependencies** - Ensures code runs in pure Spark environment
3. **Add explanatory comments** - Especially for replaced Zeppelin widgets
4. **Test before committing** - At minimum, verify Scala syntax
5. **Use descriptive variable names** - Makes code maintainable
6. **Format SQL queries** - Multi-line format for readability
7. **Document model parameters** - Explain non-obvious hyperparameter choices

## ğŸš¨ Common Pitfalls

### Optimization System
1. âŒ **Out of Memory** â†’ Reduce batch size or limit dataset
2. âŒ **Slow execution** â†’ Use preprocessing, reduce iterations, or use Greedy
3. âŒ **Wrong data path** â†’ Verify parquet file location
4. âŒ **Java not found** â†’ Run `source ~/.zshrc` after setup
5. âŒ **No results** â†’ Check capacity vs user count ratio

### Zeppelin Notebooks
1. âŒ Leaving `z.show()` in code â†’ Replace with `.show()` or remove
2. âŒ Undefined variables from removed `z.input()` â†’ Add default declarations
3. âŒ Wrong Scala version â†’ Use Scala 2.12.18
4. âŒ Missing environment activation â†’ Source `spark-env.sh` first
5. âŒ Lost paragraph IDs â†’ Always extract from original .zpln

## ğŸ“ Support

For issues specific to this directory:
1. Check paragraph ID in .scala file
2. Find corresponding paragraph in .zpln file
3. Review original Zeppelin code
4. Verify environment setup (Spark/Scala versions)

## ğŸ“ Learning Path

1. âœ… **Setup** - Run `./setup_spark_env.sh`
2. âœ… **Generate Data** - Run `./generate_sample_data.sh`
3. âœ… **Quick Test** - Run `./quick_test_with_sample_data.sh`
4. âœ… **Try Greedy** - Test with 10K records
5. âœ… **Try SA** - Test with 1K records, 10K iterations
6. âœ… **Parameter Tuning** - Adjust iterations, temperature, batch size
7. âœ… **Scale Up** - Test with full dataset
8. âœ… **Production** - Deploy to Red Hat server

## ğŸ“– Additional Resources

- **Detailed Setup**: See `INSTALLATION_GUIDE.md`
- **Quick Examples**: See `QUICK_START.md`
- **SBT Build**: See `README_SBT.md` (after running `create_sbt_project.sh`)
- **Spark UI**: http://localhost:4040 (during execution)
- **Spark Docs**: https://spark.apache.org/docs/3.1.3/

---

## â˜• Java ë²„ì „ (NEW)

Scalaì˜ `greedy_allocation.scala`ë¥¼ Javaë¡œ ì™„ì „ ë³€í™˜í•œ êµ¬í˜„ì´ ì œê³µë©ë‹ˆë‹¤.

### íŠ¹ì§•
- âœ… Java 8+ í˜¸í™˜
- âœ… Spark Java API ì‚¬ìš©
- âœ… ë™ì¼í•œ ì„±ëŠ¥ (~1-5% ì°¨ì´)
- âœ… Maven/Gradle ë¹Œë“œ ì§€ì›

### ë¹Œë“œ ë° ì‹¤í–‰
```bash
# ë¹Œë“œ
./build_java.sh

# ì‹¤í–‰
spark-submit \
  --class optimize_send_time.GreedyAllocatorTest \
  --driver-memory 16g \
  --executor-memory 16g \
  build/greedy-allocator.jar
```

### ì‚¬ìš© ì˜ˆì œ
```java
// 1. Allocator ìƒì„±
GreedyAllocator allocator = new GreedyAllocator();

// 2. ë°ì´í„° ë¡œë“œ
Dataset<Row> df = spark.read().parquet("aos/sto/propensityScoreDF");

// 3. ìš©ëŸ‰ ì„¤ì •
int[] hours = {9, 10, 11, 12, 13, 14, 15, 16, 17, 18};
Map<Integer, Integer> capacity = new HashMap<>();
for (int h : hours) capacity.put(h, 2500000);

// 4. í• ë‹¹ ì‹¤í–‰
Dataset<Row> result = allocator.allocateLargeScale(
    df, hours, capacity, 1000000
);
```

**ìì„¸í•œ ê°€ì´ë“œ**: `JAVA_USAGE.md`

---

**Last Updated**: 2026-01-14
**Maintainer**: Data Science Team

