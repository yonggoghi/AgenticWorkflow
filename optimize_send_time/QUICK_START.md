# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### 1ï¸âƒ£ í™˜ê²½ ì„¤ì • (ìµœì´ˆ 1íšŒ)

```bash
cd /Users/yongwook/workspace/AgenticWorkflow/optimize_send_time

# Spark í™˜ê²½ ì„¤ì¹˜
./setup_spark_env.sh
source ~/.zshrc

# OR-Tools JAR ë‹¤ìš´ë¡œë“œ
./setup_ortools_jars.sh
```

### 2ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```bash
# ë°©ë²• 1: ê°„ë‹¨í•œ ë°©ë²• (ì¶”ì²œ)
./generate_data_simple.sh

# ë°©ë²• 2: ì‚¬ìš©ì ìˆ˜ ì§€ì •
./generate_data_simple.sh 10000  # 10,000ëª…

# ë°©ë²• 3: ëŒ€í™”í˜• ë©”ë‰´
./generate_sample_data.sh
```

### 3ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ë°©ë²• 1: ìë™ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
spark-shell --driver-memory 4g -i load_and_test.scala

# ë°©ë²• 2: ìˆ˜ë™ ë¡œë“œ (ê¶Œì¥ - Interactive)
./run_interactive.sh
# Spark Shellì—ì„œ:
scala> :load optimize_ost.scala
scala> import OptimizeSendTime._

# ë°©ë²• 3: ê¸°ì¡´ ë°©ì‹ (deprecated)
spark-shell --driver-memory 4g -i optimize_ost.scala
```

> **ì¤‘ìš”**: `-i optimize_ost.scala`ë¡œ ì‹œì‘í•œ ì„¸ì…˜ì—ì„œëŠ” ê°™ì€ íŒŒì¼ì„ ë‹¤ì‹œ `:load optimize_ost.scala`ë¡œ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”.  
> ë™ì¼ ì •ì˜ ì¬ë¡œë”©ìœ¼ë¡œ spark-shellì´ í¬ë˜ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¬ë¡œë”©ì´ í•„ìš”í•˜ë©´ `:quit` í›„ ì¬ì‹œì‘ì´ ê°€ì¥ ì•ˆì „í•©ë‹ˆë‹¤.

### 4ï¸âƒ£ Interactive ì‚¬ìš©ë²•

#### Spark Shellì—ì„œ ìˆ˜ë™ ë¡œë“œ
```bash
# 1. Spark Shell ì‹œì‘
./run_interactive.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
source ortools_env.sh
spark-shell --jars "$ORTOOLS_JARS" --driver-memory 4g
```

```scala
// 2. Spark Shell ë‚´ë¶€ì—ì„œ
scala> :load optimize_ost.scala
// âœ“ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€ í‘œì‹œë¨

scala> import OptimizeSendTime._

// 3. ë°ì´í„° ë¡œë“œ
scala> val dfAll = spark.read.parquet("aos/sto/propensityScoreDF").cache()
scala> val df = dfAll.limit(1000)

// 4. ìš©ëŸ‰ ì„¤ì •
scala> val capacity = Map(
     | 9 -> 100, 10 -> 100, 11 -> 100, 12 -> 100, 13 -> 100,
     | 14 -> 100, 15 -> 100, 16 -> 100, 17 -> 100, 18 -> 100
     | )

// 5. Greedy ì‹¤í–‰
scala> val result = allocateGreedySimple(df, Array(9,10,11,12,13,14,15,16,17,18), capacity)

// 6. ê²°ê³¼ í™•ì¸
scala> result.groupBy("assigned_hour").count().orderBy("assigned_hour").show()
```

#### ìë™ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```bash
# ëª¨ë“  ê²ƒì„ ìë™ìœ¼ë¡œ ì‹¤í–‰
spark-shell --driver-memory 4g -i load_and_test.scala
```

### 5ï¸âƒ£ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸

```scala
// ì´ë¯¸ ë¡œë“œëœ ë°ì´í„° ì‚¬ìš©
val df = dfAll.limit(10000)

// ìš©ëŸ‰ ì„¤ì •
val capacityPerHourMap = Map(
  9 -> 1000, 10 -> 1000, 11 -> 1000, 12 -> 1000,
  13 -> 1000, 14 -> 1000, 15 -> 1000, 16 -> 1000,
  17 -> 1000, 18 -> 1000
)

// SA ìµœì í™” ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
import OptimizeSendTime._
val result = allocateUsersWithSimulatedAnnealing(
  df = df,
  capacityPerHour = capacityPerHourMap,
  maxIterations = 10000,  // ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
  initialTemperature = 100.0,
  coolingRate = 0.99,
  batchSize = 500000
)

// ê²°ê³¼ í™•ì¸
result.show()
result.groupBy("assigned_hour").count().orderBy("assigned_hour").show()
```

## ğŸ¯ ì£¼ìš” í•¨ìˆ˜

### 1. Greedy í• ë‹¹ (ê°€ì¥ ë¹ ë¦„)
```scala
allocateGreedySimple(df, hours, capacityMap)
```

### 2. OR-Tools ìµœì í™” (ì •í™•í•¨)
```scala
allocateUsersWithHourlyCapacity(df, capacityMap, timeLimit = 300)
```

### 3. Simulated Annealing (ê· í˜•)
```scala
allocateUsersWithSimulatedAnnealing(df, capacityMap, maxIterations = 100000)
```

### 4. Hybrid (OR-Tools + Greedy)
```scala
allocateUsersHybrid(df, capacityMap)
```

### 5. ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬
```scala
allocateLargeScaleHybrid(df, capacityMap, batchSize = 500000)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë°©ë²• | ì†ë„ | í’ˆì§ˆ | ë©”ëª¨ë¦¬ | ì¶”ì²œ ìš©ë„ |
|------|------|------|--------|-----------|
| Greedy | âš¡âš¡âš¡ | â­â­ | ì ìŒ | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ |
| OR-Tools | âš¡ | â­â­â­â­â­ | ë§ìŒ | ì†Œê·œëª¨ ìµœì í™” |
| SA | âš¡âš¡ | â­â­â­â­ | ì¤‘ê°„ | ê· í˜• ì¡íŒ ì„ íƒ |
| Hybrid | âš¡âš¡ | â­â­â­â­ | ì¤‘ê°„ | í”„ë¡œë•ì…˜ |
| Batch | âš¡ | â­â­â­â­ | ì ìŒ | ëŒ€ê·œëª¨ ë°ì´í„° |

## ğŸ”§ ë¬¸ì œ í•´ê²°

### âŒ "Java not found"
```bash
export JAVA_HOME=$(/usr/libexec/java_home -v 11)
source ~/.zshrc
```

### âŒ "Spark not found"
```bash
export SPARK_HOME=/Users/yongwook/spark-local/spark-3.1.3-bin-hadoop3.2
export PATH=$SPARK_HOME/bin:$PATH
source ~/.zshrc
```

### âŒ "Out of Memory"
```bash
# ë©”ëª¨ë¦¬ ì¦ê°€
spark-shell --driver-memory 8g --executor-memory 8g -i optimize_ost.scala

# ë˜ëŠ” ë°ì´í„° í¬ê¸° ì¤„ì´ê¸°
val df = dfAll.sample(0.1)  // 10% ìƒ˜í”Œë§
```

## ğŸ“Œ íŒ

1. **ì²˜ìŒ ì‹¤í–‰**: Greedyë¡œ ì‹œì‘í•´ì„œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
2. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: batchSizeë¥¼ ì¤„ì´ê±°ë‚˜ ë°ì´í„° ìƒ˜í”Œë§
3. **ëŠë¦° ì‹¤í–‰**: maxIterationsë¥¼ ì¤„ì´ê±°ë‚˜ preprocessing í™œì„±í™”
4. **ìµœê³  í’ˆì§ˆ**: OR-Tools ì‚¬ìš© (ì†Œê·œëª¨ ë°ì´í„°ë§Œ)
5. **ëŒ€ê·œëª¨ ë°ì´í„°**: Batch ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©

## ğŸ“ í•™ìŠµ ìˆœì„œ

1. âœ… í™˜ê²½ ì„¤ì • í™•ì¸
2. âœ… ìƒ˜í”Œ ë°ì´í„°ë¡œ Greedy í…ŒìŠ¤íŠ¸
3. âœ… ì†Œê·œëª¨ ì‹¤ì œ ë°ì´í„°ë¡œ SA í…ŒìŠ¤íŠ¸
4. âœ… íŒŒë¼ë¯¸í„° íŠœë‹ (maxIterations, coolingRate ë“±)
5. âœ… ì „ì²´ ë°ì´í„°ë¡œ Batch ì²˜ë¦¬

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

- `INSTALLATION_GUIDE.md`: ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ
- `TROUBLESHOOTING.md`: â­ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
- `LINUX_DEPLOYMENT_GUIDE.md`: ë¦¬ëˆ…ìŠ¤ ì„œë²„ ë°°í¬
- `DATA_GENERATION_GUIDE.md`: ë°ì´í„° ìƒì„± ê°€ì´ë“œ
- `README_SBT.md`: SBT í”„ë¡œì íŠ¸ ê°€ì´ë“œ
- Spark UI: http://localhost:4040 (ì‹¤í–‰ ì¤‘ì¼ ë•Œ)

## ğŸ†˜ ë¬¸ì œê°€ ìˆë‚˜ìš”?

**TROUBLESHOOTING.md**ë¥¼ ì°¸ê³ í•˜ì„¸ìš”! ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤:
- âœ… `$ORTOOLS_JARS` ë³€ìˆ˜ ë¬¸ì œ
- âœ… Out of Memory í•´ê²°
- âœ… JAR íŒŒì¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
- âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë¬¸ì œ
- âœ… ë°ì´í„° ê²½ë¡œ ë¬¸ì œ
- âœ… ê¸°íƒ€ ì¼ë°˜ì ì¸ ì˜¤ë¥˜
