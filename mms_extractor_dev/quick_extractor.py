#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Quick Extractor - ë¹ ë¥¸ ë©”ì‹œì§€ ì •ë³´ ì¶”ì¶œê¸°
- ë©”ì‹œì§€ì—ì„œ ì œëª©ê³¼ ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ë¥¼ ë¹ ë¥´ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
- NLP ê¸°ë²• ë° LLMì„ í™œìš©í•˜ì—¬ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import pandas as pd
import json
import re
import os
from typing import Dict, List, Optional
from collections import Counter
import numpy as np

# mms_extractor.pyì™€ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©
try:
    from config.settings import API_CONFIG, MODEL_CONFIG
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("âš ï¸  config/settings.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# LLM ê´€ë ¨ import (ì„ íƒì )
try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import PromptTemplate
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    print("âš ï¸  LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install langchain langchain-openai' ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")


class MessageInfoExtractor:
    """ë©”ì‹œì§€ì—ì„œ ì œëª©ê³¼ ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, csv_path: str, use_llm: bool = False, llm_model: str = 'gpt'):
        """
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ ('gpt', 'claude', 'gemini' ë“±)
        """
        self.csv_path = csv_path
        self.df = None
        self.use_llm = use_llm and LLM_AVAILABLE
        self.llm_model = None
        self.llm_model_name = llm_model
        
        if self.use_llm:
            self._initialize_llm()
    
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™” (mms_extractor.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)"""
        try:
            print("ğŸ¤– LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            
            # mms_extractor.pyì™€ ë™ì¼í•œ ëª¨ë¸ ë§¤í•‘ (config/settings.py ìš°ì„ , í™˜ê²½ë³€ìˆ˜ fallback)
            if CONFIG_AVAILABLE:
                model_mapping = {
                    "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                    "gem": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                    "ax": getattr(MODEL_CONFIG, 'ax_model', 'skt/ax4'),
                    "claude": getattr(MODEL_CONFIG, 'claude_model', 'amazon/anthropic/claude-sonnet-4-20250514'),
                    "cld": getattr(MODEL_CONFIG, 'claude_model', 'amazon/anthropic/claude-sonnet-4-20250514'),
                    "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gcp/gemini-2.5-flash'),
                    "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gcp/gemini-2.5-flash'),
                    "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'azure/openai/gpt-4o-2024-08-06')
                }
            else:
                # config/settings.pyê°€ ì—†ëŠ” ê²½ìš° í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©
                model_mapping = {
                    "gemma": os.getenv('GEMMA_MODEL', 'gemma-7b'),
                    "gem": os.getenv('GEMMA_MODEL', 'gemma-7b'),
                    "ax": os.getenv('AX_MODEL', 'skt/ax4'),
                    "claude": os.getenv('CLAUDE_MODEL', 'amazon/anthropic/claude-sonnet-4-20250514'),
                    "cld": os.getenv('CLAUDE_MODEL', 'amazon/anthropic/claude-sonnet-4-20250514'),
                    "gemini": os.getenv('GEMINI_MODEL', 'gcp/gemini-2.5-flash'),
                    "gen": os.getenv('GEMINI_MODEL', 'gcp/gemini-2.5-flash'),
                    "gpt": os.getenv('GPT_MODEL', 'azure/openai/gpt-4o-2024-08-06')
                }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'gcp/gemini-2.5-flash') if CONFIG_AVAILABLE else 'gcp/gemini-2.5-flash')
            
            # mms_extractor.pyì™€ ë™ì¼í•œ LLM ì´ˆê¸°í™” (API_CONFIG ì‚¬ìš©)
            if CONFIG_AVAILABLE:
                api_key = getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY'))
                api_base = getattr(API_CONFIG, 'llm_api_url', None)
            else:
                api_key = os.getenv('OPENAI_API_KEY')
                api_base = os.getenv('OPENAI_API_BASE')
            
            model_kwargs = {
                "temperature": 0.0,  # mms_extractor.pyì™€ ë™ì¼í•˜ê²Œ ì™„ì „ ê²°ì •ì  ì¶œë ¥
                "openai_api_key": api_key,
                "openai_api_base": api_base,
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 1000) if CONFIG_AVAILABLE else 1000
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì • (mms_extractor.pyì™€ ë™ì¼)
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42  # ê³ ì • ì‹œë“œë¡œ ì¼ê´€ì„± ë³´ì¥
            
            self.llm_model = ChatOpenAI(**model_kwargs)
            print(f"âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            if CONFIG_AVAILABLE:
                print(f"   ğŸ“‹ ì„¤ì • ì†ŒìŠ¤: config/settings.py (mms_extractor.pyì™€ ë™ì¼)")
            else:
                print(f"   ğŸ“‹ ì„¤ì • ì†ŒìŠ¤: í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©")
            
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ NLP ë°©ë²•ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.use_llm = False
    
    def load_data(self) -> pd.DataFrame:
        """CSV ë˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_ext = os.path.splitext(self.csv_path)[1].lower()
        
        if file_ext == '.txt':
            # í…ìŠ¤íŠ¸ íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë©”ì‹œì§€
            print(f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ í˜•ì‹ ê°ì§€: {self.csv_path}")
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                messages = [line.strip() for line in f if line.strip()]
            
            # \nì„ ì‹¤ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
            messages = [msg.replace('\\n', '\n').replace('\\t', '\t') for msg in messages]
            
            # DataFrameìœ¼ë¡œ ë³€í™˜ (message ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
            self.df = pd.DataFrame({
                'message': messages
            })
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œì˜ ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ íŒŒì¼)")
        else:
            # CSV íŒŒì¼: ê¸°ì¡´ ë°©ì‹
            print(f"ğŸ“Š CSV íŒŒì¼ í˜•ì‹ ê°ì§€: {self.csv_path}")
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œì˜ ë©”ì‹œì§€ (CSV)")
        
        return self.df
    
    def extract_unsubscribe_phone(self, text: str) -> Optional[str]:
        """
        ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            text: ë©”ì‹œì§€ ë³¸ë¬¸
            
        Returns:
            ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ ë˜ëŠ” None
        """
        if pd.isna(text):
            return None
            
        # íŒ¨í„´ 1: "ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ [ì „í™”ë²ˆí˜¸]"
        pattern1 = r'ë¬´ë£Œ\s*ìˆ˜ì‹ \s*ê±°ë¶€\s*([0-9\-]+)'
        match = re.search(pattern1, text)
        if match:
            return match.group(1)
        
        # íŒ¨í„´ 2: "ìˆ˜ì‹ ê±°ë¶€ [ì „í™”ë²ˆí˜¸]"
        pattern2 = r'ìˆ˜ì‹ \s*ê±°ë¶€\s*([0-9\-]+)'
        match = re.search(pattern2, text)
        if match:
            return match.group(1)
            
        return None
    
    def extract_title_by_llm(self, text: str) -> str:
        """
        LLMì„ í™œìš©í•˜ì—¬ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            text: ë©”ì‹œì§€ ë³¸ë¬¸
            
        Returns:
            ì¶”ì¶œëœ ì œëª©
        """
        if pd.isna(text):
            return ""
        
        if not self.use_llm or self.llm_model is None:
            print("âš ï¸  LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TextRank ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._extract_by_textrank(text)
        
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt_template = """ë‹¹ì‹ ì€ ê´‘ê³  ë©”ì‹œì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ MMS ê´‘ê³  ë©”ì‹œì§€ì—ì„œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•œ ì œëª©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì§€ì¹¨:
1. ê´‘ê³ ì˜ í•µì‹¬ ë‚´ìš©(í˜œíƒ, ìƒí’ˆ, ì´ë²¤íŠ¸ ë“±)ì„ ëª…í™•íˆ ë‹´ì•„ì•¼ í•©ë‹ˆë‹¤
2. í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤ (ìµœëŒ€ 50ì)
3. "(ê´‘ê³ )", "[SKT]" ê°™ì€ ë¼ë²¨ì€ ì œì™¸í•©ë‹ˆë‹¤
4. íŠ¹ìˆ˜ë¬¸ì(__,  ë“±)ëŠ” ì œê±°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤
5. ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨í•©ë‹ˆë‹¤
6. ì œëª©ì€ ê°œì¡°ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

## ì¶œë ¥ í˜•ì‹:
- ì œëª©ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”
- JSONì´ë‚˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì—†ì´ ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”

## MMS ë©”ì‹œì§€:
{message}

## ì¶”ì¶œëœ ì œëª©:"""

            # í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
            prompt = PromptTemplate(
                input_variables=["message"],
                template=prompt_template
            )
            
            chain = prompt | self.llm_model
            response = chain.invoke({"message": text[:1000]})  # ê¸´ ë©”ì‹œì§€ëŠ” ì•ë¶€ë¶„ë§Œ
            
            # ì‘ë‹µ ì¶”ì¶œ
            title = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            # í›„ì²˜ë¦¬: ë”°ì˜´í‘œ ì œê±°, ë„ˆë¬´ ê¸´ ê²½ìš° ìë¥´ê¸°
            title = title.strip('"\'')
            if len(title) > 150:
                title = title[:150] + '...'
            
            # ë¹ˆ ì‘ë‹µì¸ ê²½ìš° fallback
            if not title or len(title) < 5:
                print("âš ï¸  LLMì´ ìœ íš¨í•œ ì œëª©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. TextRankë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._extract_by_textrank(text)
            
            return title
            
        except Exception as e:
            print(f"âš ï¸  LLM ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            print("TextRank ë°©ë²•ìœ¼ë¡œ fallbackí•©ë‹ˆë‹¤.")
            return self._extract_by_textrank(text)
    
    def extract_title_by_nlp(self, text: str, method: str = 'textrank') -> str:
        """
        NLP ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            text: ë©”ì‹œì§€ ë³¸ë¬¸
            method: ì¶”ì¶œ ë°©ë²• ('textrank', 'tfidf', 'first_bracket', 'llm')
            
        Returns:
            ì¶”ì¶œëœ ì œëª©
        """
        if pd.isna(text):
            return ""
        
        # LLM ë°©ë²•
        if method == 'llm':
            return self.extract_title_by_llm(text)
        
        # ë¨¼ì € ëŒ€ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ì œëª© í›„ë³´ë¡œ ì¶”ì¶œ
        bracket_pattern = r'\[([^\]]+)\]'
        bracket_matches = re.findall(bracket_pattern, text)
        
        # ê´‘ê³  ë¼ë²¨ ì œê±°
        if bracket_matches:
            for match in bracket_matches:
                if 'ê´‘ê³ ' not in match and 'SKT' not in match:
                    # ê´‘ê³ /SKTê°€ ì•„ë‹Œ ëŒ€ê´„í˜¸ ë‚´ìš©ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                    return match.strip()
        
        if method == 'first_bracket':
            # ì²« ë²ˆì§¸ ëŒ€ê´„í˜¸ ë‚´ìš© (ê´‘ê³  ì œì™¸)
            for match in bracket_matches:
                if match.strip() not in ['ê´‘ê³ ', 'SKí…”ë ˆì½¤', 'SKT']:
                    return match.strip()
        
        elif method == 'textrank':
            return self._extract_by_textrank(text)
        
        elif method == 'tfidf':
            return self._extract_by_tfidf(text)
        
        # ê¸°ë³¸: ì²« ë¬¸ì¥ (ì–¸ë”ë°” ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬)
        sentences = text.split('_')
        if len(sentences) > 1:
            # (ê´‘ê³ )[SKT] ë¶€ë¶„ ì œê±°
            first_sentence = sentences[1].strip() if len(sentences) > 1 else sentences[0].strip()
            # ëŒ€ê´„í˜¸ ì œê±°
            first_sentence = re.sub(r'\[.*?\]', '', first_sentence).strip()
            # (ê´‘ê³ ) ì œê±°
            first_sentence = re.sub(r'\(ê´‘ê³ \)', '', first_sentence).strip()
            return first_sentence
        
        return text[:50]  # ê¸°ë³¸ê°’: ì²˜ìŒ 50ì
    
    def _extract_by_textrank(self, text: str) -> str:
        """
        TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ìš” ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê°„ë‹¨í•œ êµ¬í˜„: ë‹¨ì–´ ë¹ˆë„ì™€ ë¬¸ì¥ ê¸¸ì´ë¥¼ ê³ ë ¤
        
        Args:
            text: ë©”ì‹œì§€ ë³¸ë¬¸
            
        Returns:
            ì¶”ì¶œëœ ì œëª©
        """
        # ì–¸ë”ë°”ë¡œ ë¬¸ì¥ ë¶„ë¦¬
        sentences = [s.strip() for s in text.split('_') if s.strip()]
        
        if not sentences:
            return text[:50]
        
        # ê´‘ê³ /SKT í¬í•¨ ë¬¸ì¥ ì œê±°
        filtered_sentences = []
        for sent in sentences:
            if '(ê´‘ê³ )' not in sent and '[ê´‘ê³ ]' not in sent:
                # ëŒ€ê´„í˜¸ ì œê±°
                clean_sent = re.sub(r'\[.*?\]', '', sent).strip()
                if clean_sent and len(clean_sent) > 5:
                    filtered_sentences.append(clean_sent)
        
        if not filtered_sentences:
            return text[:50]
        
        # ê° ë¬¸ì¥ì˜ ì¤‘ìš”ë„ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        scores = []
        for sent in filtered_sentences:
            # ê¸¸ì´ì™€ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ ì ìˆ˜ ê³„ì‚°
            score = 0
            
            # ì ì ˆí•œ ê¸¸ì´ì˜ ë¬¸ì¥ ì„ í˜¸ (10-100ì)
            if 10 <= len(sent) <= 100:
                score += 10
            
            # ì¤‘ìš” í‚¤ì›Œë“œ í¬í•¨ ì‹œ ê°€ì¤‘ì¹˜
            keywords = ['í˜œíƒ', 'ì•ˆë‚´', 'ì´ë²¤íŠ¸', 'í• ì¸', 'ë¬´ë£Œ', 'íŠ¹ë³„', 'ì„œë¹„ìŠ¤']
            for keyword in keywords:
                if keyword in sent:
                    score += 5
            
            # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ê°ì 
            if len(sent) > 150:
                score -= 10
                
            scores.append(score)
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë¬¸ì¥ ì„ íƒ
        if scores:
            best_idx = np.argmax(scores)
            title = filtered_sentences[best_idx]
            
            # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 100ì)
            if len(title) > 100:
                title = title[:100] + '...'
            
            return title
        
        return filtered_sentences[0][:100] if filtered_sentences else text[:50]
    
    def _extract_by_tfidf(self, text: str) -> str:
        """
        TF-IDF ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê°„ë‹¨í•œ êµ¬í˜„: ë¬¸ì ë¹ˆë„ ê¸°ë°˜
        
        Args:
            text: ë©”ì‹œì§€ ë³¸ë¬¸
            
        Returns:
            ì¶”ì¶œëœ ì œëª©
        """
        # ì–¸ë”ë°”ë¡œ ë¬¸ì¥ ë¶„ë¦¬
        sentences = [s.strip() for s in text.split('_') if s.strip()]
        
        if not sentences:
            return text[:50]
        
        # ê´‘ê³  ë¬¸ì¥ í•„í„°ë§
        filtered_sentences = []
        for sent in sentences:
            if '(ê´‘ê³ )' not in sent:
                clean_sent = re.sub(r'\[.*?\]', '', sent).strip()
                if clean_sent and len(clean_sent) > 5:
                    filtered_sentences.append(clean_sent)
        
        if not filtered_sentences:
            return text[:50]
        
        # ê°„ë‹¨í•œ TF ê³„ì‚° (ë¬¸ì ê¸°ë°˜)
        all_chars = ''.join(filtered_sentences)
        char_freq = Counter(all_chars)
        
        # ê° ë¬¸ì¥ì˜ TF ì ìˆ˜ ê³„ì‚°
        scores = []
        for sent in filtered_sentences:
            score = sum(char_freq[c] for c in sent if c.isalnum())
            # ë¬¸ì¥ ê¸¸ì´ë¡œ ì •ê·œí™”
            if len(sent) > 0:
                score = score / len(sent)
            scores.append(score)
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë¬¸ì¥ ì„ íƒ
        if scores:
            best_idx = np.argmax(scores)
            title = filtered_sentences[best_idx]
            
            # ê¸¸ì´ ì œí•œ
            if len(title) > 100:
                title = title[:100] + '...'
            
            return title
        
        return filtered_sentences[0][:100] if filtered_sentences else text[:50]
    
    def extract_all(self, title_method: str = 'textrank') -> List[Dict]:
        """
        ëª¨ë“  ë©”ì‹œì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            title_method: ì œëª© ì¶”ì¶œ ë°©ë²•
            
        Returns:
            ì¶”ì¶œëœ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        import time
        
        if self.df is None:
            self.load_data()
        
        results = []
        
        # íŒŒì¼ íƒ€ì… í™•ì¸ (CSV vs í…ìŠ¤íŠ¸)
        is_text_file = 'message' in self.df.columns and 'mms_phrs' not in self.df.columns
        
        for idx, row in self.df.iterrows():
            # ë©”ì‹œì§€ë³„ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
            start_time = time.time()
            
            # ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (íŒŒì¼ í˜•ì‹ì— ë”°ë¼)
            if is_text_file:
                # í…ìŠ¤íŠ¸ íŒŒì¼: 'message' ì»¬ëŸ¼ ì‚¬ìš©
                message_text = row.get('message', '')
            else:
                # CSV íŒŒì¼: 'mms_phrs' ì»¬ëŸ¼ ì‚¬ìš©
                message_text = row.get('mms_phrs', '')
            
            # ì œëª© ì¶”ì¶œ
            title = self.extract_title_by_nlp(message_text, method=title_method)
            
            # ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
            unsubscribe_phone = self.extract_unsubscribe_phone(message_text)
            
            # ë©”ì‹œì§€ë³„ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ êµ¬ì„± (íŒŒì¼ í˜•ì‹ì— ë”°ë¼)
            if is_text_file:
                # í…ìŠ¤íŠ¸ íŒŒì¼: ë©”ì‹œì§€ ë³¸ë¬¸ë§Œ ì €ì¥
                result = {
                    'msg_id': int(idx),
                    'title': title,
                    'unsubscribe_phone': unsubscribe_phone,
                    'message': message_text,
                    'processing_time_seconds': round(processing_time, 3)
                }
            else:
                # CSV íŒŒì¼: ê¸°ì¡´ êµ¬ì¡° ìœ ì§€
                result = {
                    'msg_id': int(idx),
                    'offer_date': str(row.get('offer_dt', '')),
                    'title': title,
                    'unsubscribe_phone': unsubscribe_phone,
                    'original_message_name': str(row.get('msg_nm', '')),
                    'processing_time_seconds': round(processing_time, 3)
                }
            
            results.append(result)
        
        return results
    
    def save_to_json(self, results: List[Dict], output_path: str):
        """
        ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            results: ì¶”ì¶œëœ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ì´ {len(results)}ê°œì˜ ë©”ì‹œì§€ ì •ë³´ ì¶”ì¶œ")
    
    def process_single_message(self, message: str, method: str = 'textrank') -> Dict:
        """
        ë‹¨ì¼ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (APIìš©)
        
        Args:
            message: ì²˜ë¦¬í•  ë©”ì‹œì§€ í…ìŠ¤íŠ¸
            method: ì œëª© ì¶”ì¶œ ë°©ë²• ('textrank', 'tfidf', 'first_bracket', 'llm')
            
        Returns:
            JSON í˜•ì‹ì˜ ì¶”ì¶œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì œëª© ì¶”ì¶œ
        title = self.extract_title_by_nlp(message, method=method)
        
        # ìˆ˜ì‹  ê±°ë¶€ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
        unsubscribe_phone = self.extract_unsubscribe_phone(message)
        
        # JSON ê²°ê³¼ êµ¬ì„±
        result = {
            'success': True,
            'data': {
                'title': title,
                'unsubscribe_phone': unsubscribe_phone,
                'message': message
            },
            'metadata': {
                'method': method,
                'message_length': len(message)
            }
        }
        
        return result
    
    def process_batch_file(self, file_path: str, method: str = 'textrank') -> Dict:
        """
        ë°°ì¹˜ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (APIìš©)
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ (CSV ë˜ëŠ” í…ìŠ¤íŠ¸)
            method: ì œëª© ì¶”ì¶œ ë°©ë²• ('textrank', 'tfidf', 'first_bracket', 'llm')
            
        Returns:
            JSON í˜•ì‹ì˜ ì¶”ì¶œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(file_path):
            return {
                'success': False,
                'error': f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}',
                'data': None
            }
        
        try:
            # ë°ì´í„° ë¡œë“œ
            self.load_data()
            
            # ì •ë³´ ì¶”ì¶œ
            results = self.extract_all(title_method=method)
            
            # í†µê³„ ê³„ì‚°
            total = len(results)
            with_phone = sum(1 for r in results if r.get('unsubscribe_phone'))
            
            # ì²˜ë¦¬ ì‹œê°„ í†µê³„
            processing_times = [r.get('processing_time_seconds', 0) for r in results]
            total_time = sum(processing_times)
            avg_time = total_time / total if total > 0 else 0
            min_time = min(processing_times) if processing_times else 0
            max_time = max(processing_times) if processing_times else 0
            
            # JSON ê²°ê³¼ êµ¬ì„±
            result = {
                'success': True,
                'data': {
                    'messages': results,
                    'statistics': {
                        'total_messages': total,
                        'with_unsubscribe_phone': with_phone,
                        'extraction_rate': round(with_phone / total * 100, 2) if total > 0 else 0,
                        'total_processing_time_seconds': round(total_time, 3),
                        'avg_processing_time_seconds': round(avg_time, 3),
                        'min_processing_time_seconds': round(min_time, 3),
                        'max_processing_time_seconds': round(max_time, 3)
                    }
                },
                'metadata': {
                    'method': method,
                    'file_path': file_path,
                    'file_type': 'text' if file_path.endswith('.txt') else 'csv'
                }
            }
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'data': None
            }


def main():
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜ (mms_extractor.pyì™€ ë™ì¼í•œ ë°©ì‹)
    
    ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
    python quick_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€ ë‚´ìš©" --method llm --llm-model gpt
    
    # ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ (CSV ë˜ëŠ” í…ìŠ¤íŠ¸)
    python quick_extractor.py --batch-file ./data/messages.csv --method textrank --output results.json
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ (ê¸°ë³¸ ë°°ì¹˜ íŒŒì¼)
    python quick_extractor.py
    """
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(
        description='Quick Extractor - ë¹ ë¥¸ ë©”ì‹œì§€ ì •ë³´ ì¶”ì¶œê¸° (mms_extractor.pyì™€ ë™ì¼í•œ ì…ë ¥ ë°©ì‹)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (LLM ì‚¬ìš©)
  python quick_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --method llm --llm-model gpt
  
  # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (NLP ë°©ë²•)
  python quick_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --method textrank
  
  # ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ (CSV ë˜ëŠ” í…ìŠ¤íŠ¸)
  python quick_extractor.py --batch-file ./data/messages.csv --output results.json
  python quick_extractor.py --batch-file ./data/messages.txt --method llm --llm-model ax
  
  # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
  python quick_extractor.py
        """
    )
    
    # ì…ë ¥ ì†ŒìŠ¤ (mms_extractor.pyì™€ ë™ì¼)
    parser.add_argument('--message', type=str, help='ì²˜ë¦¬í•  ë‹¨ì¼ ë©”ì‹œì§€ í…ìŠ¤íŠ¸')
    parser.add_argument('--batch-file', type=str, default='./data/mms_data_251023.csv', 
                       help='ë°°ì¹˜ ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ (CSV ë˜ëŠ” í…ìŠ¤íŠ¸, ê¸°ë³¸ê°’: ./data/mms_data_251023.csv)')
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument('--output', type=str, default='./quick_extracted_info.json',
                       help='ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ (ë°°ì¹˜ íŒŒì¼ ëª¨ë“œ ì „ìš©, ê¸°ë³¸ê°’: ./quick_extracted_info.json)')
    
    # ì¶”ì¶œ ë°©ë²• ì˜µì…˜
    parser.add_argument('--method', type=str, default='llm',
                       choices=['textrank', 'tfidf', 'first_bracket', 'llm'],
                       help='ì œëª© ì¶”ì¶œ ë°©ë²• (ê¸°ë³¸ê°’: llm)')
    
    # LLM ì˜µì…˜ (mms_extractor.pyì™€ ë™ì¼)
    parser.add_argument('--llm-model', type=str, default='ax',
                       choices=['gpt', 'claude', 'gemini', 'ax', 'gem', 'gen', 'cld'],
                       help='LLM ëª¨ë¸ ì„ íƒ (llm ë°©ë²• ì‚¬ìš© ì‹œ, ê¸°ë³¸ê°’: ax)')
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²€ì¦
    if args.message and args.batch_file != './data/mms_data_251023.csv':
        print("âš ï¸  --messageì™€ --batch-fileì„ ë™ì‹œì— ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --message ìš°ì„  ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    print(f"\n{'='*60}")
    print(f"Quick Extractor - ë©”ì‹œì§€ ì •ë³´ ì¶”ì¶œê¸°")
    print(f"{'='*60}")
    
    use_llm = (args.method == 'llm')
    
    if use_llm:
        print(f"ğŸ¤– LLM ëª¨ë“œ í™œì„±í™”: {args.llm_model}")
    
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (mms_extractor.pyì™€ ë™ì¼í•œ ë°©ì‹)
    if args.message:
        print(f"\nğŸ“ ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ëª¨ë“œ")
        print(f"ì œëª© ì¶”ì¶œ ë°©ë²•: {args.method}")
        if use_llm:
            print(f"LLM ëª¨ë¸: {args.llm_model}")
        
        # CSV ê²½ë¡œê°€ ì—†ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ None ì „ë‹¬
        extractor = MessageInfoExtractor(csv_path=None, use_llm=use_llm, llm_model=args.llm_model)
        
        # ë‹¨ì¼ ë©”ì‹œì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
        print(f"\nì²˜ë¦¬ ì¤‘...")
        title = extractor.extract_title_by_nlp(args.message, method=args.method)
        unsubscribe_phone = extractor.extract_unsubscribe_phone(args.message)
        
        # ê²°ê³¼ ì¶œë ¥
        result = {
            'title': title,
            'unsubscribe_phone': unsubscribe_phone,
            'original_message': args.message[:100] + '...' if len(args.message) > 100 else args.message
        }
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ì¶”ì¶œ ê²°ê³¼")
        print(f"{'='*60}")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        print(f"{'='*60}\n")
        
    else:
        # ë°°ì¹˜ íŒŒì¼ ì „ì²´ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
        batch_file = args.batch_file
        output_path = args.output
        
        print(f"\nğŸ“ ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ")
        print(f"ì…ë ¥ íŒŒì¼: {batch_file}")
        print(f"ì¶œë ¥ íŒŒì¼: {output_path}")
        
        if not os.path.exists(batch_file):
            print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_file}")
            sys.exit(1)
        
        extractor = MessageInfoExtractor(batch_file, use_llm=use_llm, llm_model=args.llm_model)
        
        # ë°ì´í„° ë¡œë“œ
        extractor.load_data()
        
        # ì •ë³´ ì¶”ì¶œ
        method_names = {
            'textrank': 'TextRank (ë¬¸ì¥ ì¤‘ìš”ë„ ê¸°ë°˜)',
            'tfidf': 'TF-IDF (ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)',
            'first_bracket': 'ì²« ë²ˆì§¸ ëŒ€ê´„í˜¸ ë‚´ìš©',
            'llm': 'LLM (Large Language Model)'
        }
        print(f"\nì œëª© ì¶”ì¶œ ë°©ë²•: {method_names.get(args.method, args.method)}")
        results = extractor.extract_all(title_method=args.method)
        
        # JSONìœ¼ë¡œ ì €ì¥
        extractor.save_to_json(results, output_path)
        
        # ìƒ˜í”Œ ì¶œë ¥ (íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í‘œì‹œ)
        print("\n=== ì¶”ì¶œ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ) ===")
        is_text_file = 'message' in results[0] if results else False
        
        for i, result in enumerate(results[:5]):
            print(f"\n[ë©”ì‹œì§€ {i+1}] (ì²˜ë¦¬ì‹œê°„: {result.get('processing_time_seconds', 0)}ì´ˆ)")
            if is_text_file:
                # í…ìŠ¤íŠ¸ íŒŒì¼ ì¶œë ¥
                print(f"  - ì¶”ì¶œëœ ì œëª©: {result['title']}")
                print(f"  - ìˆ˜ì‹ ê±°ë¶€ ë²ˆí˜¸: {result['unsubscribe_phone']}")
                print(f"  - ë©”ì‹œì§€ ë¯¸ë¦¬ë³´ê¸°: {result['message']}")
            else:
                # CSV íŒŒì¼ ì¶œë ¥
                print(f"  - ë‚ ì§œ: {result.get('offer_date', 'N/A')}")
                print(f"  - ì¶”ì¶œëœ ì œëª©: {result['title']}")
                print(f"  - ìˆ˜ì‹ ê±°ë¶€ ë²ˆí˜¸: {result['unsubscribe_phone']}")
                print(f"  - ì›ë³¸ ì œëª©: {result.get('original_message_name', 'N/A')}")
        
        # í†µê³„ ì¶œë ¥
        print("\n=== ì¶”ì¶œ í†µê³„ ===")
        total = len(results)
        with_phone = sum(1 for r in results if r['unsubscribe_phone'])
        
        # ì²˜ë¦¬ ì‹œê°„ í†µê³„
        processing_times = [r.get('processing_time_seconds', 0) for r in results]
        total_time = sum(processing_times)
        avg_time = total_time / total if total > 0 else 0
        min_time = min(processing_times) if processing_times else 0
        max_time = max(processing_times) if processing_times else 0
        
        print(f"ì „ì²´ ë©”ì‹œì§€: {total}ê°œ")
        print(f"ìˆ˜ì‹ ê±°ë¶€ ë²ˆí˜¸ ì¶”ì¶œ: {with_phone}ê°œ ({with_phone/total*100:.1f}%)")
        print(f"\nì²˜ë¦¬ ì‹œê°„:")
        print(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.3f}ì´ˆ/ë©”ì‹œì§€")
        print(f"  - ìµœì†Œ ì²˜ë¦¬ ì‹œê°„: {min_time:.3f}ì´ˆ")
        print(f"  - ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {max_time:.3f}ì´ˆ")
        
        # ìˆ˜ì‹ ê±°ë¶€ ë²ˆí˜¸ ë¶„í¬
        phone_counter = Counter(r['unsubscribe_phone'] for r in results if r['unsubscribe_phone'])
        print("\nìˆ˜ì‹ ê±°ë¶€ ë²ˆí˜¸ ë¶„í¬:")
        for phone, count in phone_counter.most_common():
            print(f"  - {phone}: {count}ê°œ")
        
        print(f"\n{'='*60}")
        print(f"ì™„ë£Œ! ê²°ê³¼ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"{'='*60}\n")


if __name__ == '__main__':
    main()

