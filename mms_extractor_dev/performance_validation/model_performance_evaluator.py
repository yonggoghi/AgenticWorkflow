#!/usr/bin/env python3
"""
ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
=====================

ëª¨ë¸ ë¹„êµ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
1. ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„± (3ê°œ ëª¨ë¸ ê²°ê³¼ì˜ ì¢…í•© ìœ ì‚¬ë„ 90% ì´ìƒì¸ ê²½ìš°ì˜ claude ê²°ê³¼)
2. gemma, ax ëª¨ë¸ì„ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ í‰ê°€
"""

import os
import sys
import pandas as pd
import json
import pickle
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import numpy as np
from collections import defaultdict

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤
from difflib import SequenceMatcher

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'model_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ (ì‚¬ìš©ì ì œê³µ ì½”ë“œ) ===

def calculate_list_similarity(list1, list2):
    """Calculate Jaccard similarity between two lists"""
    if isinstance(list1, dict):
        list1 = [str(item) for item in list1.values()]
    if isinstance(list2, dict):
        list2 = [str(item) for item in list2.values()]
    # Ensure lists contain strings
    list1 = [str(item) for item in list1]
    list2 = [str(item) for item in list2]
    # Convert lists to sets for comparison
    set1 = set(sorted(set(list1)))
    set2 = set(sorted(set(list2)))
    # Calculate Jaccard similarity
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union if union > 0 else 0

def calculate_text_similarity(text1, text2):
    """Calculate text similarity using SequenceMatcher"""
    return SequenceMatcher(None, str(text1), str(text2)).ratio()

def calculate_product_similarity(prod1, prod2):
    """Calculate similarity between product dictionaries with detailed structure"""
    if not isinstance(prod1, dict) or not isinstance(prod2, dict):
        return 0.0
    
    # Calculate similarity for each field
    item_name_message_sim = calculate_text_similarity(
        prod1.get('item_name_in_message', '#'),
        prod2.get('item_name_in_message', '&')
    )
    item_name_voca_sim = calculate_text_similarity(
        prod1.get('item_name_in_voca', '#'),
        prod2.get('item_name_in_voca', '&')
    )
    item_id_sim = calculate_text_similarity(
        prod1.get('item_id', '#'),
        prod2.get('item_id', '&')
    )
    domain_sim = calculate_text_similarity(
        prod1.get('domain', '#'),
        prod2.get('domain', '&')
    )
    name_sim = calculate_text_similarity(
        prod1.get('name', '#'),
        prod2.get('name', '&')
    )
    action_sim = calculate_text_similarity(
        prod1.get('action', '#'),
        prod2.get('action', '&')
    )
    
    # Weighted average - item_id and domain are more distinctive
    similarity = (
        item_name_message_sim +
        item_name_voca_sim +
        item_id_sim +
        domain_sim +
        name_sim +
        action_sim
    )/len(prod1.keys())
    return similarity

def calculate_channel_similarity(chan1, chan2):
    """Calculate similarity between channel dictionaries"""
    if not isinstance(chan1, dict) or not isinstance(chan2, dict):
        return 0.0
    type_sim = calculate_text_similarity(chan1.get('type', ''), chan2.get('type', ''))
    value_sim = calculate_text_similarity(chan1.get('value', ''), chan2.get('value', ''))
    action_sim = calculate_text_similarity(chan1.get('action', ''), chan2.get('action', ''))
    return (type_sim + value_sim + action_sim) / 3

def calculate_pgm_similarity(pgm1, pgm2):
    """Calculate similarity between program dictionaries"""
    if isinstance(pgm1, dict) and isinstance(pgm2, dict):
        pgm_nm_sim = calculate_text_similarity(pgm1.get('pgm_nm', ''), pgm2.get('pgm_nm', ''))
        pgm_id_sim = calculate_text_similarity(pgm1.get('pgm_id', ''), pgm2.get('pgm_id', ''))
        pgm_sim = pgm_nm_sim * 0.4 + pgm_id_sim * 0.6
    else:
        pgm_sim = 0.0
    return pgm_sim

def calculate_products_list_similarity(products1, products2):
    """Calculate similarity between two lists of product dictionaries"""
    if not products1 or not products2:
        return 0.0
    
    # For each product in list1, find best match in list2
    similarities = []
    for p1 in products1:
        best_match = 0.0
        for p2 in products2:
            similarity = calculate_product_similarity(p1, p2)
            best_match = max(best_match, similarity)
        similarities.append(best_match)
    
    # Also check reverse direction to handle different list sizes
    reverse_similarities = []
    for p2 in products2:
        best_match = 0.0
        for p1 in products1:
            similarity = calculate_product_similarity(p1, p2)
            best_match = max(best_match, similarity)
        reverse_similarities.append(best_match)
    
    # Take average of both directions
    forward_avg = sum(similarities) / len(similarities)
    reverse_avg = sum(reverse_similarities) / len(reverse_similarities)
    return (forward_avg + reverse_avg) / 2

def calculate_channels_list_similarity(channels1, channels2):
    """Calculate similarity between two lists of channel dictionaries"""
    if not channels1 or not channels2:
        return 0.0
    similarities = []
    for c1 in channels1:
        best_match = 0.0
        for c2 in channels2:
            similarity = calculate_channel_similarity(c1, c2)
            best_match = max(best_match, similarity)
        similarities.append(best_match)
    return sum(similarities) / len(similarities)

def calculate_pgms_list_similarity(pgms1, pgms2):
    """Calculate similarity between two lists of program dictionaries"""
    if not pgms1 or not pgms2:
        return 0.0
    if isinstance(pgms1, list) and isinstance(pgms2, list):
        pgm_sim = calculate_list_similarity(pgms1, pgms2)
        return pgm_sim
    
    # For each pgm in list1, find best match in list2
    similarities = []
    for p1 in pgms1:
        best_match = 0.0
        for p2 in pgms2:
            similarity = calculate_pgm_similarity(p1, p2)
            best_match = max(best_match, similarity)
        similarities.append(best_match)
    
    # Also check reverse direction
    reverse_similarities = []
    for p2 in pgms2:
        best_match = 0.0
        for p1 in pgms1:
            similarity = calculate_pgm_similarity(p1, p2)
            best_match = max(best_match, similarity)
        reverse_similarities.append(best_match)
    
    # Take average of both directions
    forward_avg = sum(similarities) / len(similarities)
    reverse_avg = sum(reverse_similarities) / len(reverse_similarities)
    return (forward_avg + reverse_avg) / 2

def calculate_dictionary_similarity(dict1, dict2):
    """
    Calculate similarity between two dictionaries with generalized structure:
    {
        'title': str,
        'purpose': [list of strings],
        'product': [list of product dicts],
        'channel': [list of channel dicts],
        'pgm': [list of program dicts]
    }
    """
    if not isinstance(dict1, dict) or not isinstance(dict2, dict):
        return {'overall_similarity': 0.0, 'error': 'Both inputs must be dictionaries'}
    
    # Calculate title similarity
    title_similarity = calculate_text_similarity(
        dict1.get('title', ''),
        dict2.get('title', '')
    )
    
    # Calculate purpose similarity (list of strings)
    purpose_similarity = calculate_list_similarity(
        dict1.get('purpose', []),
        dict2.get('purpose', [])
    )
    
    # Calculate product similarity (list of product dicts)
    product_similarity = calculate_products_list_similarity(
        dict1.get('product', []),
        dict2.get('product', [])
    )
    
    # Calculate channel similarity (list of channel dicts)
    channel_similarity = calculate_channels_list_similarity(
        dict1.get('channel', []),
        dict2.get('channel', [])
    )
    
    # Calculate pgm similarity (list of program dicts)
    pgm_similarity = calculate_pgms_list_similarity(
        dict1.get('pgm', []),
        dict2.get('pgm', [])
    )
    
    # Calculate overall similarity (weighted average)
    # Adjusted weights to reflect importance of each component
    overall_similarity = (
        title_similarity * 0.2 +
        purpose_similarity * 0.15 +
        product_similarity * 0.35 +
        channel_similarity * 0.15 +
        pgm_similarity * 0.15
    )
    
    return {
        'overall_similarity': overall_similarity,
        'title_similarity': title_similarity,
        'purpose_similarity': purpose_similarity,
        'product_similarity': product_similarity,
        'channel_similarity': channel_similarity,
        'pgm_similarity': pgm_similarity
    }

class ModelPerformanceEvaluator:
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, results_dir: str = "results"):
        self.results_dir = Path(results_dir)
        self.evaluation_dir = self.results_dir / "evaluation"
        self.results_dir.mkdir(exist_ok=True)  # ìƒìœ„ ë””ë ‰í† ë¦¬ë„ ìƒì„±
        self.evaluation_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.extraction_results = {}
        self.ground_truth_data = []
        self.evaluation_results = {}
        
        # í‰ê°€ ëŒ€ìƒ ëª¨ë¸ë“¤
        self.reference_models = ['gemini', 'gpt', 'claude']  # ì •ë‹µ ìƒì„±ìš©
        self.target_models = ['gemma', 'ax']  # í‰ê°€ ëŒ€ìƒ
        self.all_models = ['gemma', 'gemini', 'claude', 'ax', 'gpt']
    
    def load_extraction_results(self):
        """ì¶”ì¶œ ê²°ê³¼ ë¡œë”©"""
        logger.info("ì¶”ì¶œ ê²°ê³¼ ë¡œë”© ì¤‘...")
        
        # í”¼í´ íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        pickle_file = self.results_dir / "combined_extraction_results.pkl"
        if pickle_file.exists():
            with open(pickle_file, 'rb') as f:
                self.extraction_results = pickle.load(f)
            logger.info("í”¼í´ íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë”© ì™„ë£Œ")
        else:
            # JSON íŒŒì¼ ì‚¬ìš©
            json_file = self.results_dir / "combined_extraction_results.json"
            if json_file.exists():
                with open(json_file, 'r', encoding='utf-8') as f:
                    self.extraction_results = json.load(f)
                logger.info("JSON íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë”© ì™„ë£Œ")
            else:
                raise FileNotFoundError("ì¶”ì¶œ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„° ê²€ì¦
        for model in self.all_models:
            if model not in self.extraction_results:
                logger.warning(f"{model} ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            else:
                success_count = len([r for r in self.extraction_results[model] if r.get('success', False)])
                total_count = len(self.extraction_results[model])
                logger.info(f"{model}: {success_count}/{total_count} ì„±ê³µ")
    
    def generate_ground_truth_dataset(self, similarity_threshold: float = 0.9, min_message_length: int = 300):
        """ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„±"""
        logger.info(f"ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘ (ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}, ìµœì†Œ ë©”ì‹œì§€ ê¸¸ì´: {min_message_length}ì)")
        
        # ë©”ì‹œì§€ IDë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™”
        message_groups = defaultdict(dict)
        message_texts = {}  # ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ì €ì¥
        
        for model in self.reference_models:
            if model in self.extraction_results:
                for result in self.extraction_results[model]:
                    if result.get('success', False):
                        msg_id = result['msg_id']
                        message_groups[msg_id][model] = result['json_objects']
                        message_texts[msg_id] = result['msg']  # ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ì €ì¥
        
        # 3ê°œ ëª¨ë¸ ëª¨ë‘ ì„±ê³µí•œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
        valid_messages = []
        for msg_id, results in message_groups.items():
            if len(results) == len(self.reference_models):
                valid_messages.append(msg_id)
        
        logger.info(f"3ê°œ ì°¸ì¡° ëª¨ë¸ ëª¨ë‘ ì„±ê³µí•œ ë©”ì‹œì§€: {len(valid_messages)}ê°œ")
        
        # ì¶”ê°€ í•„í„°ë§ ì ìš©
        filtered_messages = []
        
        for msg_id in valid_messages:
            msg_text = message_texts.get(msg_id, "")
            claude_result = message_groups[msg_id].get('claude', {})
            
            # 1. ë©”ì‹œì§€ ê¸¸ì´ ì¡°ê±´ í™•ì¸ (ìµœì†Œ 300ì)
            if len(msg_text) < min_message_length:
                logger.debug(f"ë©”ì‹œì§€ {msg_id}: ê¸¸ì´ ë¶€ì¡± ({len(msg_text)}ì < {min_message_length}ì)")
                continue
            
            # 2. 1st depth íƒœê·¸ë“¤ ê°’ì´ ì±„ì›Œì ¸ ìˆëŠ”ì§€ í™•ì¸
            required_fields = ['title', 'purpose', 'product', 'channel', 'pgm']
            if not self._validate_first_depth_tags(claude_result, required_fields):
                logger.debug(f"ë©”ì‹œì§€ {msg_id}: 1st depth íƒœê·¸ ê°’ ë¶€ì¡±")
                continue
            
            filtered_messages.append(msg_id)
        
        logger.info(f"ì¶”ê°€ ì¡°ê±´ ì ìš© í›„ ìœ íš¨í•œ ë©”ì‹œì§€: {len(filtered_messages)}ê°œ")
        
        # ìœ ì‚¬ë„ 90% ì´ìƒì¸ ë©”ì‹œì§€ë“¤ ì°¾ê¸°
        high_similarity_messages = []
        
        for msg_id in filtered_messages:
            results = message_groups[msg_id]
            
            # 3ê°œ ëª¨ë¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            models = list(results.keys())
            
            for i in range(len(models)):
                for j in range(i+1, len(models)):
                    model1, model2 = models[i], models[j]
                    sim_result = calculate_dictionary_similarity(results[model1], results[model2])
                    similarities.append(sim_result['overall_similarity'])
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean(similarities)
            
            if avg_similarity >= similarity_threshold:
                # claude ê²°ê³¼ë¥¼ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©
                ground_truth_record = {
                    'msg_id': msg_id,
                    'msg': message_texts[msg_id],
                    'msg_length': len(message_texts[msg_id]),
                    'ground_truth': results['claude'],
                    'avg_similarity': avg_similarity,
                    'similarities': dict(zip([f"{models[i]}_{models[j]}" for i in range(len(models)) for j in range(i+1, len(models))], similarities))
                }
                
                high_similarity_messages.append(ground_truth_record)
                self.ground_truth_data.append(ground_truth_record)
        
        logger.info(f"ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(self.ground_truth_data)}ê°œ ë©”ì‹œì§€")
        
        # ì •ë‹µ ë°ì´í„°ì…‹ ì €ì¥
        ground_truth_file = self.evaluation_dir / "ground_truth_dataset.json"
        with open(ground_truth_file, 'w', encoding='utf-8') as f:
            json.dump(self.ground_truth_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì •ë‹µ ë°ì´í„°ì…‹ ì €ì¥: {ground_truth_file}")
        
        return self.ground_truth_data
    
    def _validate_first_depth_tags(self, json_result: Dict, required_fields: List[str]) -> bool:
        """1st depth íƒœê·¸ë“¤ì˜ ê°’ì´ ìœ íš¨í•˜ê²Œ ì±„ì›Œì ¸ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            for field in required_fields:
                if field not in json_result:
                    return False
                
                value = json_result[field]
                
                # ê°’ì´ Noneì´ê±°ë‚˜ ë¹ˆ ê°’ì¸ì§€ í™•ì¸
                if value is None:
                    return False
                
                # ë¬¸ìì—´ì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ê³µë°±ë§Œ ìˆëŠ”ì§€ í™•ì¸
                if isinstance(value, str):
                    if not value.strip():
                        return False
                
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ì´ê±°ë‚˜ ëª¨ë“  ìš”ì†Œê°€ ë¹ˆ ê°’ì¸ì§€ í™•ì¸
                elif isinstance(value, list):
                    if not value:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                        return False
                    # ëª¨ë“  ìš”ì†Œê°€ ìœ íš¨í•œì§€ í™•ì¸
                    for item in value:
                        if item is None:
                            return False
                        if isinstance(item, str) and not item.strip():
                            return False
                        elif isinstance(item, dict) and not item:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
                            return False
                
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                elif isinstance(value, dict):
                    if not value:
                        return False
                
            return True
            
        except Exception as e:
            logger.debug(f"1st depth íƒœê·¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def evaluate_target_models(self):
        """ëŒ€ìƒ ëª¨ë¸ë“¤ì„ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ í‰ê°€"""
        logger.info("ëŒ€ìƒ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        if not self.ground_truth_data:
            raise ValueError("ì •ë‹µ ë°ì´í„°ì…‹ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ì •ë‹µ ë°ì´í„°ì…‹ì˜ ë©”ì‹œì§€ IDë“¤
        ground_truth_msg_ids = set(record['msg_id'] for record in self.ground_truth_data)
        
        for model in self.target_models:
            logger.info(f"=== {model} ëª¨ë¸ í‰ê°€ ===")
            
            if model not in self.extraction_results:
                logger.warning(f"{model} ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            model_evaluations = []
            
            for ground_truth_record in self.ground_truth_data:
                msg_id = ground_truth_record['msg_id']
                ground_truth = ground_truth_record['ground_truth']
                
                # í•´ë‹¹ ë©”ì‹œì§€ì— ëŒ€í•œ ëª¨ë¸ ê²°ê³¼ ì°¾ê¸°
                model_result = None
                for result in self.extraction_results[model]:
                    if result['msg_id'] == msg_id and result.get('success', False):
                        model_result = result['json_objects']
                        break
                
                if model_result is None:
                    logger.warning(f"{model} ëª¨ë¸ - ë©”ì‹œì§€ {msg_id} ê²°ê³¼ ì—†ìŒ")
                    continue
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarity_result = calculate_dictionary_similarity(ground_truth, model_result)
                
                evaluation_record = {
                    'msg_id': msg_id,
                    'model': model,
                    'similarity_result': similarity_result,
                    'ground_truth_avg_similarity': ground_truth_record['avg_similarity']
                }
                
                model_evaluations.append(evaluation_record)
            
            # ëª¨ë¸ë³„ í†µê³„ ê³„ì‚°
            if model_evaluations:
                overall_similarities = [eval_record['similarity_result']['overall_similarity'] for eval_record in model_evaluations]
                
                model_stats = {
                    'model': model,
                    'total_evaluated': len(model_evaluations),
                    'avg_overall_similarity': np.mean(overall_similarities),
                    'std_overall_similarity': np.std(overall_similarities),
                    'min_overall_similarity': np.min(overall_similarities),
                    'max_overall_similarity': np.max(overall_similarities),
                    'median_overall_similarity': np.median(overall_similarities),
                    'detailed_evaluations': model_evaluations
                }
                
                self.evaluation_results[model] = model_stats
                
                logger.info(f"{model} í‰ê°€ ì™„ë£Œ:")
                logger.info(f"  - í‰ê°€ ë©”ì‹œì§€ ìˆ˜: {model_stats['total_evaluated']}")
                logger.info(f"  - í‰ê·  ìœ ì‚¬ë„: {model_stats['avg_overall_similarity']:.4f}")
                logger.info(f"  - í‘œì¤€í¸ì°¨: {model_stats['std_overall_similarity']:.4f}")
                logger.info(f"  - ìµœì†Œ/ìµœëŒ€: {model_stats['min_overall_similarity']:.4f}/{model_stats['max_overall_similarity']:.4f}")
            else:
                logger.warning(f"{model} ëª¨ë¸ì— ëŒ€í•œ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥
        evaluation_file = self.evaluation_dir / "model_evaluation_results.json"
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary_report = {
            'evaluation_date': datetime.now().isoformat(),
            'ground_truth_count': len(self.ground_truth_data),
            'model_comparisons': {}
        }
        
        for model, stats in self.evaluation_results.items():
            summary_report['model_comparisons'][model] = {
                'total_evaluated': stats['total_evaluated'],
                'avg_overall_similarity': stats['avg_overall_similarity'],
                'std_overall_similarity': stats['std_overall_similarity'],
                'performance_grade': self._get_performance_grade(stats['avg_overall_similarity'])
            }
        
        # ì„±ëŠ¥ ìˆœìœ„
        if len(self.evaluation_results) > 1:
            ranking = sorted(
                self.evaluation_results.items(),
                key=lambda x: x[1]['avg_overall_similarity'],
                reverse=True
            )
            summary_report['performance_ranking'] = [model for model, _ in ranking]
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_file = self.evaluation_dir / "performance_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_text_report(summary_report)
        
        logger.info(f"ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {self.evaluation_dir}")
        
        return summary_report
    
    def _get_performance_grade(self, similarity: float) -> str:
        """ìœ ì‚¬ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ë“±ê¸‰ ë°˜í™˜"""
        if similarity >= 0.9:
            return "A+ (Excellent)"
        elif similarity >= 0.8:
            return "A (Very Good)"
        elif similarity >= 0.7:
            return "B+ (Good)"
        elif similarity >= 0.6:
            return "B (Fair)"
        elif similarity >= 0.5:
            return "C+ (Below Average)"
        else:
            return "C (Poor)"
    
    def _generate_text_report(self, summary_report: Dict):
        """í…ìŠ¤íŠ¸ í˜•íƒœì˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸")
        report_lines.append("=" * 60)
        report_lines.append(f"í‰ê°€ ì¼ì‹œ: {summary_report['evaluation_date']}")
        report_lines.append(f"ì •ë‹µ ë°ì´í„°ì…‹ í¬ê¸°: {summary_report['ground_truth_count']}ê°œ ë©”ì‹œì§€")
        report_lines.append("")
        
        report_lines.append("í‰ê°€ ê²°ê³¼:")
        report_lines.append("-" * 40)
        
        for model, stats in summary_report['model_comparisons'].items():
            report_lines.append(f"ğŸ“Š {model.upper()} ëª¨ë¸:")
            report_lines.append(f"   í‰ê°€ ë©”ì‹œì§€ ìˆ˜: {stats['total_evaluated']}")
            report_lines.append(f"   í‰ê·  ìœ ì‚¬ë„: {stats['avg_overall_similarity']:.4f}")
            report_lines.append(f"   í‘œì¤€í¸ì°¨: {stats['std_overall_similarity']:.4f}")
            report_lines.append(f"   ì„±ëŠ¥ ë“±ê¸‰: {stats['performance_grade']}")
            report_lines.append("")
        
        if 'performance_ranking' in summary_report:
            report_lines.append("ì„±ëŠ¥ ìˆœìœ„:")
            report_lines.append("-" * 20)
            for i, model in enumerate(summary_report['performance_ranking'], 1):
                stats = summary_report['model_comparisons'][model]
                report_lines.append(f"{i}. {model.upper()}: {stats['avg_overall_similarity']:.4f}")
            report_lines.append("")
        
        # ìƒì„¸ ë¶„ì„
        report_lines.append("ìƒì„¸ ë¶„ì„:")
        report_lines.append("-" * 20)
        
        for model, model_stats in self.evaluation_results.items():
            similarities = [eval_record['similarity_result'] for eval_record in model_stats['detailed_evaluations']]
            
            # ê° í•„ë“œë³„ í‰ê·  ìœ ì‚¬ë„
            avg_title = np.mean([sim['title_similarity'] for sim in similarities])
            avg_purpose = np.mean([sim['purpose_similarity'] for sim in similarities])
            avg_product = np.mean([sim['product_similarity'] for sim in similarities])
            avg_channel = np.mean([sim['channel_similarity'] for sim in similarities])
            avg_pgm = np.mean([sim['pgm_similarity'] for sim in similarities])
            
            report_lines.append(f"{model.upper()} ëª¨ë¸ í•„ë“œë³„ ì„±ëŠ¥:")
            report_lines.append(f"  ì œëª©(title): {avg_title:.4f}")
            report_lines.append(f"  ëª©ì (purpose): {avg_purpose:.4f}")
            report_lines.append(f"  ìƒí’ˆ(product): {avg_product:.4f}")
            report_lines.append(f"  ì±„ë„(channel): {avg_channel:.4f}")
            report_lines.append(f"  í”„ë¡œê·¸ë¨(pgm): {avg_pgm:.4f}")
            report_lines.append("")
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥
        text_report_file = self.evaluation_dir / "performance_report.txt"
        with open(text_report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        # ì½˜ì†”ì—ë„ ì¶œë ¥
        print('\n'.join(report_lines))

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰")
    parser.add_argument('--results-dir', type=str, default='results', help='ê²°ê³¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--similarity-threshold', type=float, default=0.9, help='ì •ë‹µ ìƒì„± ìœ ì‚¬ë„ ì„ê³„ê°’')
    parser.add_argument('--min-message-length', type=int, default=300, help='ì •ë‹µìš© ë©”ì‹œì§€ ìµœì†Œ ê¸¸ì´')
    
    args = parser.parse_args()
    
    logger.info("=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ ===")
    
    try:
        # í‰ê°€ê¸° ê°ì²´ ìƒì„±
        evaluator = ModelPerformanceEvaluator(results_dir=args.results_dir)
        
        # 1. ì¶”ì¶œ ê²°ê³¼ ë¡œë”©
        logger.info("1ë‹¨ê³„: ì¶”ì¶œ ê²°ê³¼ ë¡œë”©")
        evaluator.load_extraction_results()
        
        # 2. ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„±
        logger.info("2ë‹¨ê³„: ì •ë‹µ ë°ì´í„°ì…‹ ìƒì„±")
        evaluator.generate_ground_truth_dataset(
            similarity_threshold=args.similarity_threshold,
            min_message_length=args.min_message_length
        )
        
        # 3. ëŒ€ìƒ ëª¨ë¸ í‰ê°€
        logger.info("3ë‹¨ê³„: ëŒ€ìƒ ëª¨ë¸ í‰ê°€")
        evaluator.evaluate_target_models()
        
        # 4. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        logger.info("4ë‹¨ê³„: ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±")
        evaluator.generate_performance_report()
        
        logger.info("=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    main()
