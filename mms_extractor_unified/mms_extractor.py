# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor) - ê°œì„ ëœ ë²„ì „
==============================================

ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì  ë“±ì„ 
ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” ê°œì„ ì‚¬í•­:
- ì½”ë“œ ëª¨ë“ˆí™”: ëŒ€í˜• ë©”ì†Œë“œë“¤ì„ ê¸°ëŠ¥ë³„ë¡œ ë¶„ë¦¬
- ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”: ì•ˆì „í•œ LLM í˜¸ì¶œ ë° ì—ëŸ¬ ë³µêµ¬
- ë¡œê¹… ì‹œìŠ¤í…œ ì¶”ê°€: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… ì§€ì›
- ë°ì´í„° ê²€ì¦ ì¶”ê°€: ì¶”ì¶œ ê²°ê³¼ í’ˆì§ˆ ë³´ì¥
- ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›: ìƒí’ˆ ì •ë³´ì™€ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ëª¨ë‘ DBì—ì„œ ë¡œë“œ ê°€ëŠ¥
"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
from functools import wraps
from typing import List, Tuple, Union, Dict, Any, Optional
from abc import ABC, abstractmethod
import traceback
import json
import re
import ast
import glob
import os
import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle
from contextlib import contextmanager

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from rapidfuzz import fuzz, process
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
from entity_dag_extractor import DAGParser, extract_dag, create_dag_diagram, sha256_hash

# ì„¤ì • ë° ì˜ì¡´ì„± ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë¡œê¹… ì„¤ì • - api.pyì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê¸°ë³¸ ì„¤ì • ì ìš©
logger = logging.getLogger(__name__)

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë¡œê¹… ì„¤ì • (api.pyì—ì„œ ì„í¬íŠ¸ë  ë•ŒëŠ” api.pyì˜ ì„¤ì • ì‚¬ìš©)
if __name__ == '__main__':
    import sys
    from pathlib import Path
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'mms_extractor.log'),
            logging.StreamHandler()
        ]
    )

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ë°ì½”ë ˆì´í„° ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def log_performance(func):
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ì„ ë¡œê¹…í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            logger.info(f"{func.__name__} ì‹¤í–‰ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            return result
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{func.__name__} ì‹¤í–‰ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            raise
    return wrapper

def safe_execute(func, *args, default_return=None, max_retries=2, **kwargs):
    """ì•ˆì „í•œ í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°"""
    for attempt in range(max_retries + 1):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries:
                logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
                return default_return
            else:
                logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    return default_return

def validate_text_input(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì…ë ¥ ê²€ì¦ ë° ì •ë¦¬"""
    if not isinstance(text, str):
        raise ValueError(f"í…ìŠ¤íŠ¸ ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(text)}")
    
    text = text.strip()
    if not text:
        raise ValueError("ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    if len(text) > 10000:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œí•œ
        logger.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(text)} ë¬¸ì). ì²˜ìŒ 10000ìë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        text = text[:10000]
    
    return text

# ===== ì›ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìœ ì§€) =====

def dataframe_to_markdown_prompt(df, max_rows=None):
    """DataFrameì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    if max_rows is not None and len(df) > max_rows:
        display_df = df.head(max_rows)
        truncation_note = f"\n[Note: Only showing first {max_rows} of {len(df)} rows]"
    else:
        display_df = df
        truncation_note = ""
    df_markdown = display_df.to_markdown()
    prompt = f"\n\n    {df_markdown}\n    {truncation_note}\n\n    "
    return prompt

def clean_segment(segment):
    """ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ì—ì„œ ë‚´ë¶€ì˜ ë™ì¼í•œ ë”°ì˜´í‘œ ì œê±°"""
    segment = segment.strip()
    if len(segment) >= 2 and segment[0] in ['"', "'"] and segment[-1] == segment[0]:
        q = segment[0]
        inner = segment[1:-1].replace(q, '')
        return q + inner + q
    return segment

def split_key_value(text):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ ì²« ë²ˆì§¸ ì½œë¡ ì„ ê¸°ì¤€ìœ¼ë¡œ í‚¤-ê°’ ë¶„ë¦¬"""
    in_quote = False
    quote_char = ''
    for i, char in enumerate(text):
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
        elif char == ':' and not in_quote:
            return text[:i], text[i+1:]
    return text, ''

def split_outside_quotes(text, delimiter=','):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ êµ¬ë¶„ìë¡œë§Œ í…ìŠ¤íŠ¸ ë¶„ë¦¬"""
    parts = []
    current = []
    in_quote = False
    quote_char = ''
    for char in text:
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
            current.append(char)
        elif char == delimiter and not in_quote:
            parts.append(''.join(current).strip())
            current = []
        else:
            current.append(char)
    if current:
        parts.append(''.join(current).strip())
    return parts

def clean_ill_structured_json(text):
    """ì˜ëª» êµ¬ì¡°í™”ëœ JSON í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    parts = split_outside_quotes(text, delimiter=',')
    cleaned_parts = []
    for part in parts:
        key, value = split_key_value(part)
        key_clean = clean_segment(key)
        value_clean = clean_segment(value) if value.strip() != "" else ""
        if value_clean:
            cleaned_parts.append(f"{key_clean}: {value_clean}")
        else:
            cleaned_parts.append(key_clean)
    return ', '.join(cleaned_parts)

def repair_json(broken_json):
    """ì†ìƒëœ JSON ë¬¸ìì—´ ë³µêµ¬"""
    json_str = broken_json
    # ë”°ì˜´í‘œ ì—†ëŠ” í‚¤ì— ë”°ì˜´í‘œ ì¶”ê°€
    json_str = re.sub(r'([{,])\s*([a-zA-Z0-9_]+)\s*:', r'\1 "\2":', json_str)
    # ë”°ì˜´í‘œ ì—†ëŠ” ê°’ ì²˜ë¦¬
    parts = json_str.split('"')
    for i in range(0, len(parts), 2):
        parts[i] = re.sub(r':\s*([a-zA-Z0-9_]+)(?=\s*[,\]\}])', r': "\1"', parts[i])
    json_str = '"'.join(parts)
    # í›„í–‰ ì‰¼í‘œ ì œê±°
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    return json_str

def extract_json_objects(text):
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì¶”ì¶œ"""
    pattern = r'(\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\})'
    result = []
    for match in re.finditer(pattern, text):
        potential_json = match.group(0)
        try:
            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))
            result.append(json_obj)
        except (json.JSONDecodeError, SyntaxError, ValueError):
            pass
    return result

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ê·œí™”)"""
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ===== ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ (ì›ë³¸ ìœ ì§€) =====

def fuzzy_similarities(text, entities):
    """í¼ì§€ ë§¤ì¹­ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    results = []
    for entity in entities:
        scores = {
            'ratio': fuzz.ratio(text, entity) / 100,
            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,
            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,
            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100
        }
        max_score = max(scores.values())
        results.append((entity, max_score))
    return results

def get_fuzzy_similarities(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜"""
    text = args_dict['text']
    entities = args_dict['entities']
    threshold = args_dict['threshold']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    
    text_processed = preprocess_text(text.lower())
    similarities = fuzzy_similarities(text_processed, entities)
    
    filtered_results = [
        {
            text_col_nm: text,
            item_col_nm: entity, 
            "sim": score
        } 
        for entity, score in similarities 
        if score >= threshold
    ]
    return filtered_results

def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, len(entities) // (n_jobs * 2))
    
    batches = []
    for text in texts:
        for i in range(0, len(entities), batch_size):
            batch = entities[i:i + batch_size]
            batches.append({"text": text, "entities": batch, "threshold": threshold, "text_col_nm": text_col_nm, "item_col_nm": item_col_nm})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def longest_common_subsequence_ratio(s1, s2, normalizaton_value):
    """ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ë¹„ìœ¨ ê³„ì‚°"""
    def lcs_length(x, y):
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i-1] == y[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_len = lcs_length(s1, s2)
    if normalizaton_value == 'max':
        max_len = max(len(s1), len(s2))
        return lcs_len / max_len if max_len > 0 else 1.0
    elif normalizaton_value == 'min':
        min_len = min(len(s1), len(s2))
        return lcs_len / min_len if min_len > 0 else 1.0
    elif normalizaton_value == 's1':
        return lcs_len / len(s1) if len(s1) > 0 else 1.0
    elif normalizaton_value == 's2':
        return lcs_len / len(s2) if len(s2) > 0 else 1.0
    else:
        raise ValueError(f"Invalid normalization value: {normalizaton_value}")

def sequence_matcher_similarity(s1, s2, normalizaton_value):
    """SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    matcher = difflib.SequenceMatcher(None, s1, s2)
    matches = sum(triple.size for triple in matcher.get_matching_blocks())
    
    normalization_length = min(len(s1), len(s2))
    if normalizaton_value == 'max':
        normalization_length = max(len(s1), len(s2))
    elif normalizaton_value == 's1':
        normalization_length = len(s1)
    elif normalizaton_value == 's2':
        normalization_length = len(s2)
        
    if normalization_length == 0: 
        return 0.0
    
    return matches / normalization_length

def substring_aware_similarity(s1, s2, normalizaton_value):
    """ë¶€ë¶„ë¬¸ìì—´ ê´€ê³„ë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    if s1 in s2 or s2 in s1:
        shorter = min(s1, s2, key=len)
        longer = max(s1, s2, key=len)
        base_score = len(shorter) / len(longer)
        return min(0.95 + base_score * 0.05, 1.0)
    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)

def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\s_\-]+'):
    """í† í° ì‹œí€€ìŠ¤ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    tokens1 = [t for t in re.split(separator_pattern, s1.strip()) if t]
    tokens2 = [t for t in re.split(separator_pattern, s2.strip()) if t]
    
    if not tokens1 or not tokens2:
        return 0.0
    
    def token_lcs_length(t1, t2):
        m, n = len(t1), len(t2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if t1[i-1] == t2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_tokens = token_lcs_length(tokens1, tokens2)
    normalization_tokens = max(len(tokens1), len(tokens2))
    if normalizaton_value == 'min':
        normalization_tokens = min(len(tokens1), len(tokens2))
    elif normalizaton_value == 's1':
        normalization_tokens = len(tokens1)
    elif normalizaton_value == 's2':
        normalization_tokens = len(tokens2)
    
    return lcs_tokens / normalization_tokens  

def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):
    """ì—¬ëŸ¬ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ì„ ê²°í•©í•œ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°"""
    if weights is None:
        weights = {'substring': 0.4, 'sequence_matcher': 0.4, 'token_sequence': 0.2}
    
    similarities = {
        'substring': substring_aware_similarity(s1, s2, normalizaton_value),
        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),
        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)
    }
    
    return sum(similarities[key] * weights[key] for key in weights), similarities

def calculate_seq_similarity(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    sent_item_batch = args_dict['sent_item_batch']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    normalizaton_value = args_dict['normalizaton_value']
    
    results = []
    for sent_item in sent_item_batch:
        sent = sent_item[text_col_nm]
        item = sent_item[item_col_nm]
        try:
            sent_processed = preprocess_text(sent.lower())
            item_processed = preprocess_text(item.lower())
            similarity = combined_sequence_similarity(sent_processed, item_processed, normalizaton_value=normalizaton_value)[0]
            results.append({text_col_nm:sent, item_col_nm:item, "sim":similarity})
        except Exception as e:
            logger.error(f"Error processing {item}: {e}")
            results.append({text_col_nm:sent, item_col_nm:item, "sim":0.0})
    
    return results

def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, normalizaton_value='s2'):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))
    
    batches = []
    for i in range(0, sent_item_pdf.shape[0], batch_size):
        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')
        batches.append({"sent_item_batch": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'normalizaton_value': normalizaton_value})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def load_sentence_transformer(model_path, device=None):
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Loading model from {model_path}...")
    model = SentenceTransformer(model_path).to(device)
    logger.info(f"Model loaded on {device}")
    return model

# ===== Kiwi í˜•íƒœì†Œ ë¶„ì„ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ (ì›ë³¸ ìœ ì§€) =====

class Token:
    """í˜•íƒœì†Œ ë¶„ì„ í† í° í´ë˜ìŠ¤"""
    def __init__(self, form, tag, start, length):
        self.form = form      # í† í° í˜•íƒœ
        self.tag = tag        # í’ˆì‚¬ íƒœê·¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.len = length     # ê¸¸ì´

class Sentence:
    """í˜•íƒœì†Œ ë¶„ì„ ë¬¸ì¥ í´ë˜ìŠ¤"""
    def __init__(self, text, start, end, tokens, subs=None):
        self.text = text      # ë¬¸ì¥ í…ìŠ¤íŠ¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.end = end        # ë ìœ„ì¹˜
        self.tokens = tokens  # í† í° ë¦¬ìŠ¤íŠ¸
        self.subs = subs or []  # í•˜ìœ„ ë¬¸ì¥ë“¤

def filter_text_by_exc_patterns(sentence, exc_tag_patterns):
    """ì œì™¸í•  í’ˆì‚¬ íŒ¨í„´ì— ë”°ë¼ í…ìŠ¤íŠ¸ í•„í„°ë§"""
    # ê°œë³„ íƒœê·¸ì™€ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ë¦¬
    individual_tags = set()
    sequences = []
    
    for pattern in exc_tag_patterns:
        if isinstance(pattern, list):
            if len(pattern) == 1:
                individual_tags.add(pattern[0])
            else:
                sequences.append(pattern)
        else:
            individual_tags.add(pattern)
    
    # ì œì™¸í•  í† í° ì¸ë±ìŠ¤ ìˆ˜ì§‘
    tokens_to_exclude = set()
    
    # ê°œë³„ íƒœê·¸ ë§¤ì¹­ í™•ì¸
    for i, token in enumerate(sentence.tokens):
        if token.tag in individual_tags:
            tokens_to_exclude.add(i)
    
    # ì‹œí€€ìŠ¤ íŒ¨í„´ ë§¤ì¹­ í™•ì¸
    for sequence in sequences:
        seq_len = len(sequence)
        for i in range(len(sentence.tokens) - seq_len + 1):
            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):
                for j in range(seq_len):
                    tokens_to_exclude.add(i + j)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸í•  í† í° ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    result_chars = list(sentence.text)
    for i, token in enumerate(sentence.tokens):
        if i in tokens_to_exclude:
            start_pos = token.start - sentence.start
            end_pos = start_pos + token.len
            for j in range(start_pos, end_pos):
                if j < len(result_chars) and result_chars[j] != ' ':
                    result_chars[j] = ' '
    
    filtered_text = ''.join(result_chars)
    return re.sub(r'\s+', ' ', filtered_text)

def filter_specific_terms(strings: List[str]) -> List[str]:
    """ì¤‘ë³µë˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì— ìˆëŠ” ìš©ì–´ë“¤ í•„í„°ë§"""
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)
    
    filtered = []
    for s in unique_strings:
        if not any(s in other for other in filtered):
            filtered.append(s)
    
    return filtered

def convert_df_to_json_list(df):
    """DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜"""
    result = []
    grouped = df.groupby('item_name_in_msg')
    
    for item_name_in_msg, group in grouped:
        item_dict = {
            'item_name_in_msg': item_name_in_msg,
            'item_in_voca': []
        }
        
        item_nm_groups = group.groupby('item_nm')
        for item_nm, item_group in item_nm_groups:
            item_ids = list(item_group['item_id'].unique())
            voca_item = {
                'item_nm': item_nm,
                'item_id': item_ids
            }
            item_dict['item_in_voca'].append(voca_item)
        result.append(item_dict)
    
    return result

# ===== ì¶”ìƒ í´ë˜ìŠ¤ ë° ì „ëµ íŒ¨í„´ =====

class EntityExtractionStrategy(ABC):
    """ì—”í‹°í‹° ì¶”ì¶œ ì „ëµ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def extract(self, text: str, **kwargs) -> pd.DataFrame:
        """ì—”í‹°í‹° ì¶”ì¶œ ë©”ì†Œë“œ"""
        pass

class DataLoader(ABC):
    """ë°ì´í„° ë¡œë” ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def load_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë“œ ë©”ì†Œë“œ"""
        pass

# ===== ê°œì„ ëœ MMSExtractor í´ë˜ìŠ¤ =====

class MMSExtractor:
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ë©”ì¸ í´ë˜ìŠ¤ - ê°œì„ ëœ ë²„ì „
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    - ë©”ì†Œë“œ ëª¨ë“ˆí™”: í° ë©”ì†Œë“œë“¤ì„ ê¸°ëŠ¥ë³„ë¡œ ë¶„ë¦¬
    - ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”: ì•ˆì „í•œ LLM í˜¸ì¶œ ë° ì—ëŸ¬ ë³µêµ¬
    - ë¡œê¹… ì‹œìŠ¤í…œ: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…
    - ë°ì´í„° ê²€ì¦: ì…ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ ë³´ì¥
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='gemma', extract_entity_dag=False):
        """MMSExtractor ì´ˆê¸°í™”"""
        logger.info("MMSExtractor ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # ê¸°ë³¸ ì„¤ì • ì ìš©
            self._set_default_config(model_path, data_dir, product_info_extraction_mode, 
                                   entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag)
            
            # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
            load_dotenv()
            
            # ë‹¨ê³„ë³„ ì´ˆê¸°í™”
            self._initialize_device()
            self._initialize_llm()
            self._initialize_embedding_model()
            self._initialize_kiwi()
            self._load_data()
            
            logger.info("MMSExtractor ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"MMSExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise

    def _set_default_config(self, model_path, data_dir, product_info_extraction_mode, 
                          entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì ìš©"""
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else getattr(EMBEDDING_CONFIG, 'ko_sbert_model_path', 'jhgan/ko-sroberta-multitask')
        self.offer_info_data_src = offer_info_data_src
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'product_info_extraction_mode', 'nlp')
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'entity_extraction_mode', 'llm')
        self.llm_model_name = llm_model
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)
        self.extract_entity_dag = extract_entity_dag
        
        # DAG ì¶”ì¶œ ì„¤ì • ë¡œê¹…
        # extract_entity_dag: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ DAG(Directed Acyclic Graph)ë¡œ ì¶”ì¶œ
        # Trueì¸ ê²½ìš° ì¶”ê°€ì ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹° ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ 
        # NetworkX + Graphvizë¥¼ í†µí•´ ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±
        if self.extract_entity_dag:
            logger.info("ğŸ¯ DAG ì¶”ì¶œ ëª¨ë“œ í™œì„±í™”ë¨")
        else:
            logger.info("ğŸ“‹ í‘œì¤€ ì¶”ì¶œ ëª¨ë“œ (DAG ë¹„í™œì„±í™”)")

    @log_performance
    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    @log_performance
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì„¤ì • ë§¤í•‘
            model_mapping = {
                "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "ax": getattr(MODEL_CONFIG, 'ax_model', 'gpt-4'),
                "claude": getattr(MODEL_CONFIG, 'claude_model', 'claude-3'),
                "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'gpt-4')
            }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'gpt-4'))
            
            # LLM ëª¨ë¸ë³„ ì¼ê´€ì„± ì„¤ì •
            model_kwargs = {
                "temperature": 0.0,  # ì™„ì „ ê²°ì •ì  ì¶œë ¥ì„ ìœ„í•´ 0.0 ê³ ì •
                "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ê°•í™”
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42  # ê³ ì • ì‹œë“œë¡œ ì¼ê´€ì„± ë³´ì¥
                
            self.llm_model = ChatOpenAI(**model_kwargs)
            
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)

    @log_performance
    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _load_data(self):
        """í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ëª¨ë“œ: {self.offer_info_data_src}")
            
            # ìƒí’ˆ ì •ë³´ ë¡œë“œ
            logger.info("1ï¸âƒ£ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_item_data()
            logger.info(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ë³„ì¹­ ê·œì¹™ ì ìš©
            logger.info("2ï¸âƒ£ ë³„ì¹­ ê·œì¹™ ì ìš© ì¤‘...")
            self._apply_alias_rules()
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            
            # ì •ì§€ì–´ ë¡œë“œ
            logger.info("3ï¸âƒ£ ì •ì§€ì–´ ë¡œë“œ ì¤‘...")
            self._load_stop_words()
            logger.info(f"ë¡œë“œëœ ì •ì§€ì–´ ìˆ˜: {len(self.stop_item_names)}ê°œ")
            
            # Kiwiì— ìƒí’ˆëª… ë“±ë¡
            logger.info("4ï¸âƒ£ Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì¤‘...")
            self._register_items_to_kiwi()
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ
            logger.info("5ï¸âƒ£ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_program_data()
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì¡°ì§ ì •ë³´ ë¡œë“œ
            logger.info("6ï¸âƒ£ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_organization_data()
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
            
            # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ - ìµœì¢… ìƒíƒœ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ìƒí’ˆ ë°ì´í„°: {self.item_pdf_all.shape}")
            logger.info(f"âœ… í”„ë¡œê·¸ë¨ ë°ì´í„°: {self.pgm_pdf.shape}")
            logger.info(f"âœ… ì¡°ì§ ë°ì´í„°: {self.org_pdf.shape}")
            logger.info(f"âœ… ì •ì§€ì–´: {len(self.stop_item_names)}ê°œ")
            
            # ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒíƒœ ë¹„êµë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´
            if hasattr(self, 'item_pdf_all') and not self.item_pdf_all.empty:
                logger.info("=== ìƒí’ˆ ë°ì´í„° ìƒì„¸ ì •ë³´ ===")
                if 'item_nm' in self.item_pdf_all.columns:
                    unique_items = self.item_pdf_all['item_nm'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆëª… ìˆ˜: {unique_items}ê°œ")
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    unique_aliases = self.item_pdf_all['item_nm_alias'].nunique()
                    logger.info(f"ê³ ìœ  ë³„ì¹­ ìˆ˜: {unique_aliases}ê°œ")
                if 'item_id' in self.item_pdf_all.columns:
                    unique_ids = self.item_pdf_all['item_id'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆID ìˆ˜: {unique_ids}ê°œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_item_data(self):
        """ìƒí’ˆ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/items.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                item_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ë°ì´í„° í¬ê¸°: {item_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(item_pdf_raw.columns)}")
                
                self.item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','item_dmn']].copy()
                logger.info(f"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
                
                # ì¶”ê°€ ì»¬ëŸ¼ë“¤ ìƒì„±
                self.item_pdf_all['item_ctg'] = None
                self.item_pdf_all['item_emb_vec'] = None
                self.item_pdf_all['ofer_cd'] = self.item_pdf_all['item_id']
                self.item_pdf_all['oper_dt_hms'] = '20250101000000'
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.item_pdf_all.columns)
                self.item_pdf_all = self.item_pdf_all.rename(columns={c:c.lower() for c in self.item_pdf_all.columns})
                logger.info(f"ë¡œì»¬ ëª¨ë“œ ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.item_pdf_all.columns))}")
                
                # ë¡œì»¬ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                if not self.item_pdf_all.empty:
                    sample_items = self.item_pdf_all['item_nm'].dropna().head(5).tolist()
                    logger.info(f"ë¡œì»¬ ëª¨ë“œ ìƒí’ˆëª… ìƒ˜í”Œ: {sample_items}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_item_from_database()
            
            # ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ í•„í„°ë§
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
            if excluded_domains:
                before_filter_size = len(self.item_pdf_all)
                self.item_pdf_all = self.item_pdf_all.query("item_dmn not in @excluded_domains")
                after_filter_size = len(self.item_pdf_all)
                logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter_size} -> {after_filter_size} (ì œì™¸ëœ ë„ë©”ì¸: {excluded_domains})")
                
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.item_pdf_all)}ê°œ ìƒí’ˆ ===")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.item_pdf_all = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")

    def _get_database_connection(self):
        """Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        try:
            username = os.getenv("DB_USERNAME")
            password = os.getenv("DB_PASSWORD")
            host = os.getenv("DB_HOST")
            port = os.getenv("DB_PORT")
            service_name = os.getenv("DB_NAME")
            
            if not all([username, password, host, port, service_name]):
                raise ValueError("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤")
            
            dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
            conn = cx_Oracle.connect(user=username, password=password, dsn=dsn, encoding="UTF-8")
            
            # LOB ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ outputtypehandler ì„¤ì •
            def output_type_handler(cursor, name, default_type, size, precision, scale):
                if default_type == cx_Oracle.CLOB:
                    return cursor.var(cx_Oracle.LONG_STRING, arraysize=cursor.arraysize)
                elif default_type == cx_Oracle.BLOB:
                    return cursor.var(cx_Oracle.LONG_BINARY, arraysize=cursor.arraysize)
            
            conn.outputtypehandler = output_type_handler
            
            return conn
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    @contextmanager
    def _database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager"""
        conn = None
        try:
            conn = self._get_database_connection()
            yield conn
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        finally:
            if conn:
                try:
                    conn.close()
                    logger.debug("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ ì¢…ë£Œ")
                except Exception as close_error:
                    logger.warning(f"ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {close_error}")

    def _load_item_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            with self._database_connection() as conn:
                sql = "SELECT * FROM TCAM_RC_OFER_MST WHERE ROWNUM <= 1000000"
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.item_pdf_all = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì›ë³¸ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
                
                # ë°ì´í„° íƒ€ì… ì •ë³´ ë¡œê¹…
                logger.info("=== DB ë°ì´í„° íƒ€ì… ì •ë³´ ===")
                for col in self.item_pdf_all.columns:
                    dtype = self.item_pdf_all[col].dtype
                    null_count = self.item_pdf_all[col].isnull().sum()
                    logger.info(f"  {col}: {dtype}, nullê°’: {null_count}ê°œ")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.item_pdf_all.columns)
                self.item_pdf_all = self.item_pdf_all.rename(columns={c:c.lower() for c in self.item_pdf_all.columns})
                logger.info(f"ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.item_pdf_all.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.item_pdf_all.empty:
                    logger.info("DataFrame ë°ì´í„° ê°•ì œ ë¡œë“œ ì‹œì‘...")
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.item_pdf_all.values  # ëª¨ë“  ë°ì´í„° ì ‘ê·¼í•˜ì—¬ LOB ë¡œë“œ ìœ ë„
                        logger.info("DataFrame ë°ì´í„° ê°•ì œ ë¡œë“œ ì™„ë£Œ")
                        
                        # ì£¼ìš” ì»¬ëŸ¼ë“¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸
                        if 'item_nm' in self.item_pdf_all.columns:
                            sample_items = self.item_pdf_all['item_nm'].dropna().head(5).tolist()
                            logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {sample_items}")
                        
                        if 'item_id' in self.item_pdf_all.columns:
                            sample_ids = self.item_pdf_all['item_id'].dropna().head(5).tolist()
                            logger.info(f"ìƒí’ˆID ìƒ˜í”Œ: {sample_ids}")
                            
                        logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.item_pdf_all)}ê°œ ìƒí’ˆ")
                    except Exception as load_error:
                        logger.error(f"ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_program_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            with self._database_connection() as conn:
                # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ì¿¼ë¦¬
                sql = """SELECT CMPGN_PGM_NUM pgm_id, CMPGN_PGM_NM pgm_nm, RMK clue_tag 
                         FROM TCAM_CMPGN_PGM_INFO
                         WHERE DEL_YN = 'N' 
                         AND APRV_OP_RSLT_CD = 'APPR'
                         AND EXPS_YN = 'Y'
                         AND CMPGN_PGM_NUM like '2025%' 
                         AND RMK is not null"""
                
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.pgm_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ë“¤: {list(self.pgm_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.pgm_pdf.columns)
                self.pgm_pdf = self.pgm_pdf.rename(columns={c:c.lower() for c in self.pgm_pdf.columns})
                logger.info(f"í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.pgm_pdf.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.pgm_pdf.empty:
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.pgm_pdf.values  # ëª¨ë“  ë°ì´í„° ì ‘ê·¼í•˜ì—¬ LOB ë¡œë“œ ìœ ë„
                        
                        # í”„ë¡œê·¸ë¨ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                        if 'pgm_nm' in self.pgm_pdf.columns:
                            sample_pgms = self.pgm_pdf['pgm_nm'].dropna().head(3).tolist()
                            logger.info(f"í”„ë¡œê·¸ë¨ëª… ìƒ˜í”Œ: {sample_pgms}")
                        
                        if 'clue_tag' in self.pgm_pdf.columns:
                            sample_clues = self.pgm_pdf['clue_tag'].dropna().head(3).tolist()
                            logger.info(f"í´ë£¨ íƒœê·¸ ìƒ˜í”Œ: {sample_clues}")
                            
                        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ")
                    except Exception as load_error:
                        logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            raise

    def _apply_alias_rules(self):
        """ë³„ì¹­ ê·œì¹™ ì ìš©"""
        try:
            logger.info("=== ë³„ì¹­ ê·œì¹™ ì ìš© ì‹œì‘ ===")
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì „ ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            
            alias_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv'))
            alias_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))
            logger.info(f"ë¡œë“œëœ ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")

            def apply_alias_rule(item_nm):
                if pd.isna(item_nm) or not isinstance(item_nm, str):
                    return [item_nm] if not pd.isna(item_nm) else []
                    
                item_nm_list = [item_nm]
                for r in alias_rule_set:
                    if r[0] in item_nm:
                        item_nm_list.append(item_nm.replace(r[0], r[1]))
                    if r[1] in item_nm:
                        item_nm_list.append(item_nm.replace(r[1], r[0]))
                return item_nm_list

            # ë³„ì¹­ ê·œì¹™ ì ìš© ì „ ë°ì´í„° ìƒíƒœ í™•ì¸
            if 'item_nm' in self.item_pdf_all.columns:
                non_null_items = self.item_pdf_all['item_nm'].dropna()
                logger.info(f"nullì´ ì•„ë‹Œ ìƒí’ˆëª… ìˆ˜: {len(non_null_items)}ê°œ")
                if len(non_null_items) > 0:
                    sample_before = non_null_items.head(3).tolist()
                    logger.info(f"ë³„ì¹­ ì ìš© ì „ ìƒí’ˆëª… ìƒ˜í”Œ: {sample_before}")
            
            self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm'].apply(apply_alias_rule)
            
            # explode ì „í›„ í¬ê¸° ë¹„êµ
            before_explode_size = len(self.item_pdf_all)
            self.item_pdf_all = self.item_pdf_all.explode('item_nm_alias')
            after_explode_size = len(self.item_pdf_all)
            
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© í›„ ë°ì´í„° í¬ê¸°: {before_explode_size} -> {after_explode_size}")
            
            # ë³„ì¹­ ì ìš© í›„ ìƒ˜í”Œ í™•ì¸
            if 'item_nm_alias' in self.item_pdf_all.columns:
                non_null_aliases = self.item_pdf_all['item_nm_alias'].dropna()
                if len(non_null_aliases) > 0:
                    sample_after = non_null_aliases.head(5).tolist()
                    logger.info(f"ë³„ì¹­ ì ìš© í›„ ìƒ˜í”Œ: {sample_after}")
            
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì™„ë£Œ: {len(alias_rule_set)}ê°œ ê·œì¹™")
            
        except Exception as e:
            logger.warning(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨: {e}")
            logger.warning(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì›ë³¸ ì´ë¦„ì„ ë³„ì¹­ìœ¼ë¡œ ì‚¬ìš©
            if 'item_nm' in self.item_pdf_all.columns:
                self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm']
                logger.info("ì›ë³¸ ìƒí’ˆëª…ì„ ë³„ì¹­ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")

    def _load_stop_words(self):
        """ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ"""
        try:
            self.stop_item_names = pd.read_csv(getattr(METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv'))['stop_words'].to_list()
            logger.info(f"ì •ì§€ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.stop_item_names)}ê°œ")
        except Exception as e:
            logger.warning(f"ì •ì§€ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stop_item_names = []

    def _register_items_to_kiwi(self):
        """Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡"""
        try:
            logger.info("=== Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… ë³„ì¹­ ë°ì´í„° í™•ì¸
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                return
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë“±ë¡í•  ê³ ìœ  ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # nullì´ ì•„ë‹Œ ìœ íš¨í•œ ë³„ì¹­ë“¤ë§Œ í•„í„°ë§
            valid_aliases = [w for w in unique_aliases if isinstance(w, str) and len(w.strip()) > 0]
            logger.info(f"ìœ íš¨í•œ ë³„ì¹­ ìˆ˜: {len(valid_aliases)}ê°œ")
            
            if len(valid_aliases) > 0:
                sample_aliases = valid_aliases[:5]
                logger.info(f"ë“±ë¡í•  ë³„ì¹­ ìƒ˜í”Œ: {sample_aliases}")
            
            registered_count = 0
            failed_count = 0
            
            for w in valid_aliases:
                try:
                    self.kiwi.add_user_word(w, "NNP")
                    registered_count += 1
                except Exception as reg_error:
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ë¡œê¹…
                        logger.warning(f"Kiwi ë“±ë¡ ì‹¤íŒ¨ - '{w}': {reg_error}")
            
            logger.info(f"Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"Kiwi ìƒí’ˆëª… ë“±ë¡ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

    def _load_program_data(self):
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
        try:
            logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë”© ì‹œì‘...")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                self.pgm_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'pgm_info_path', './data/program_info.csv'))
                logger.info(f"ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                self._load_program_from_database()
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
            if not self.pgm_pdf.empty:
                logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
                clue_texts = self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                    lambda x: preprocess_text(x['pgm_nm'].lower()) + " " + x['clue_tag'].lower(), axis=1
                ).tolist()
                
                self.clue_embeddings = self.emb_model.encode(
                    clue_texts, convert_to_tensor=True, show_progress_bar=False
                )
                
                logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ í”„ë¡œê·¸ë¨")
            else:
                logger.warning("í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.clue_embeddings = torch.tensor([])
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            self.clue_embeddings = torch.tensor([])

    def _load_organization_data(self):
        """ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ"""
        try:
            self.org_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'org_info_path', './data/org_info.csv'), encoding='cp949')
            self.org_pdf['sub_org_cd'] = self.org_pdf['sub_org_cd'].apply(lambda x: str(x).zfill(4))
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.org_pdf)}ê°œ ì¡°ì§")
        except Exception as e:
            logger.warning(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.org_pdf = pd.DataFrame(columns=['org_nm', 'org_cd', 'sub_org_cd', 'org_abbr_nm'])

    def _safe_llm_invoke(self, prompt: str, max_retries: int = 3) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì†Œë“œ"""
        for attempt in range(max_retries):
            try:
                # LLM í˜¸ì¶œ
                response = self.llm_model.invoke(prompt)
                result_text = response.content if hasattr(response, 'content') else str(response)
                
                # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
                json_objects_list = extract_json_objects(result_text)
                if json_objects_list:
                    json_objects = json_objects_list[-1]
                    if self._detect_schema_response(json_objects):
                        logger.warning(f"ì‹œë„ {attempt + 1}: LLMì´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        
                        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µì¸ ê²½ìš° ë” ê°•í•œ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            enhanced_prompt = self._enhance_prompt_for_retry(prompt)
                            response = self.llm_model.invoke(enhanced_prompt)
                            result_text = response.content if hasattr(response, 'content') else str(response)
                
                return result_text
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_extraction(prompt)
                else:
                    logger.warning(f"LLM í˜¸ì¶œ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return ""

    def _enhance_prompt_for_retry(self, original_prompt: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”"""
        enhanced_instruction = """
ğŸš¨ CRITICAL INSTRUCTION ğŸš¨
You MUST return actual extracted data, NOT the schema definition.

DO NOT return:
- Schema structures like {"type": "array", "items": {...}}
- Template definitions
- Example formats

DO return:
- Real extracted values from the advertisement
- Actual product names, purposes, channels found in the text
- Concrete data only

For example:
WRONG: {"purpose": {"type": "array", "items": {"type": "string"}}}
CORRECT: {"purpose": ["ìƒí’ˆ ê°€ì… ìœ ë„", "í˜œíƒ ì•ˆë‚´"]}

WRONG: {"product": {"type": "array", "items": {"type": "object"}}}
CORRECT: {"product": [{"name": "ZEMí°", "action": "ê°€ì…"}]}

"""
        return enhanced_instruction + "\n" + original_prompt

    def _fallback_extraction(self, prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ fallback ì¶”ì¶œ ë¡œì§"""
        logger.info("Fallback ì¶”ì¶œ ë¡œì§ ì‹¤í–‰")
        
        # ê¸°ë³¸ì ì¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ ì‹œë„
        fallback_result = {
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "product": [],
            "channel": [],
            "pgm": []
        }
        
        return json.dumps(fallback_result, ensure_ascii=False)

    @log_performance
    def extract_entities_from_kiwi(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            logger.info("=== Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘ ===")
            mms_msg = validate_text_input(mms_msg)
            logger.info(f"ì²˜ë¦¬í•  ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # ìƒí’ˆ ë°ì´í„° ìƒíƒœ í™•ì¸
            if self.item_pdf_all.empty:
                logger.error("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë§¤ì¹­í•  ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # ë¬¸ì¥ ë¶„í•  ë° í•˜ìœ„ ë¬¸ì¥ ì²˜ë¦¬
            sentences = sum(self.kiwi.split_into_sents(
                re.split(r"_+", mms_msg), return_tokens=True, return_sub_sents=True
            ), [])
            
            sentences_all = []
            for sent in sentences:
                if sent.subs:
                    sentences_all.extend(sent.subs)
                else:
                    sentences_all.append(sent)
            
            logger.info(f"ë¶„í• ëœ ë¬¸ì¥ ìˆ˜: {len(sentences_all)}ê°œ")
            
            # ì œì™¸ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ í•„í„°ë§
            sentence_list = [
                filter_text_by_exc_patterns(sent, self.exc_tag_patterns) 
                for sent in sentences_all
            ]
            
            logger.info(f"í•„í„°ë§ëœ ë¬¸ì¥ë“¤: {sentence_list[:3]}...")  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…

            # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
            result_msg = self.kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)
            all_tokens = [(token.form, token.tag) for token in result_msg]
            logger.info(f"ì „ì²´ í† í° ìˆ˜: {len(all_tokens)}ê°œ")
            
            # NNP íƒœê·¸ í† í°ë“¤ë§Œ ì¶”ì¶œ
            nnp_tokens = [token.form for token in result_msg if token.tag == 'NNP']
            logger.info(f"NNP íƒœê·¸ í† í°ë“¤: {nnp_tokens}")
            
            entities_from_kiwi = [
                token.form for token in result_msg 
                if token.tag == 'NNP' and 
                   token.form not in self.stop_item_names + ['-'] and 
                   len(token.form) >= 2 and 
                   not token.form.lower() in self.stop_item_names
            ]
            entities_from_kiwi = filter_specific_terms(entities_from_kiwi)
            
            logger.info(f"í•„í„°ë§ í›„ Kiwi ì¶”ì¶œ ì—”í‹°í‹°: {list(set(entities_from_kiwi))}")

            # í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ìœ ì‚¬ ìƒí’ˆëª… ì°¾ê¸°
            logger.info("í¼ì§€ ë§¤ì¹­ ì‹œì‘...")
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                sentence_list, 
                unique_aliases,
                threshold=getattr(PROCESSING_CONFIG, 'fuzzy_threshold', 0.5),
                text_col_nm='sent', 
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            logger.info(f"í¼ì§€ ë§¤ì¹­ ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if similarities_fuzzy.empty:
                logger.warning("í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©
                cand_item_list = entities_from_kiwi
                logger.info(f"Kiwi ê¸°ë°˜ í›„ë³´ ì•„ì´í…œ: {cand_item_list}")
                
                if cand_item_list:
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                    logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
                else:
                    extra_item_pdf = pd.DataFrame()
                    logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                
                return cand_item_list, extra_item_pdf
            else:
                logger.info(f"í¼ì§€ ë§¤ì¹­ ì„±ê³µ: {len(similarities_fuzzy)}ê°œ ê²°ê³¼")
                if not similarities_fuzzy.empty:
                    sample_fuzzy = similarities_fuzzy.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                    logger.info(f"í¼ì§€ ë§¤ì¹­ ìƒ˜í”Œ: {sample_fuzzy}")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ í†µí•œ ì •ë°€ ë§¤ì¹­
            logger.info("ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")
            similarities_seq = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='sent',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )
            
            logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê²°ê³¼ í¬ê¸°: {similarities_seq.shape if not similarities_seq.empty else 'ë¹„ì–´ìˆìŒ'}")
            if not similarities_seq.empty:
                sample_seq = similarities_seq.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ìƒ˜í”Œ: {sample_seq}")
            
            # ì„ê³„ê°’ ì´ìƒì˜ í›„ë³´ ì•„ì´í…œë“¤ í•„í„°ë§
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold', 0.2)
            logger.info(f"ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
            
            cand_items = similarities_seq.query(
                "sim >= @similarity_threshold and "
                "item_nm_alias.str.contains('', case=False) and "
                "item_nm_alias not in @self.stop_item_names"
            )
            logger.info(f"ì„ê³„ê°’ í•„í„°ë§ í›„ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_items)}ê°œ")
            
            # Kiwiì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ ì¶”ê°€
            entities_from_kiwi_pdf = self.item_pdf_all.query("item_nm_alias in @entities_from_kiwi")[
                ['item_nm','item_nm_alias']
            ]
            entities_from_kiwi_pdf['sim'] = 1.0
            logger.info(f"Kiwi ì—”í‹°í‹° ë§¤ì¹­ ê²°ê³¼: {len(entities_from_kiwi_pdf)}ê°œ")

            # ê²°ê³¼ í†µí•© ë° ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            cand_item_pdf = pd.concat([cand_items, entities_from_kiwi_pdf])
            logger.info(f"í†µí•©ëœ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_item_pdf)}ê°œ")
            
            if not cand_item_pdf.empty:
                cand_item_list = cand_item_pdf.sort_values('sim', ascending=False).groupby([
                    "item_nm_alias"
                ])['sim'].max().reset_index(name='final_sim').sort_values(
                    'final_sim', ascending=False
                ).query("final_sim >= 0.2")['item_nm_alias'].unique()
                
                logger.info(f"ìµœì¢… í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {list(cand_item_list)}")
                
                extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                    ['item_nm','item_nm_alias','item_id']
                ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                
                logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ DataFrame í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    sample_final = extra_item_pdf.head(3).to_dict('records')
                    logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ìƒ˜í”Œ: {sample_final}")
            else:
                logger.warning("í†µí•©ëœ í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                cand_item_list = []
                extra_item_pdf = pd.DataFrame()

            return cand_item_list, extra_item_pdf
            
        except Exception as e:
            logger.error(f"Kiwi ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [], pd.DataFrame()

    def extract_entities_by_logic(self, cand_entities: List[str], threshold_for_fuzzy: float = 0.8) -> pd.DataFrame:
        """ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            if not cand_entities:
                return pd.DataFrame()
            
            # í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entities,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=threshold_for_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _calculate_combined_similarity(self, similarities_fuzzy: pd.DataFrame) -> pd.DataFrame:
        """s1, s2 ì •ê·œí™” ë°©ì‹ìœ¼ë¡œ ê°ê° ê³„ì‚° í›„ í•©ì‚°"""
        try:
            # s1 ì •ê·œí™”
            sim_s1 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s1',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s2',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s2'})
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            if not sim_s1.empty and not sim_s2.empty:
                combined = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
                combined = combined.groupby(['item_name_in_msg', 'item_nm_alias'])[
                    ['sim_s1', 'sim_s2']
                ].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')
                return combined
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    @log_performance
    def extract_entities_by_llm(self, msg_text: str, rank_limit: int = 5) -> pd.DataFrame:
        """LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            msg_text = validate_text_input(msg_text)
            
            # ë¡œì§ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ í›„ë³´ ì—”í‹°í‹° ë¨¼ì € ì¶”ì¶œ
            cand_entities_by_sim = sorted([
                e.strip() for e in self.extract_entities_by_logic([msg_text], threshold_for_fuzzy=0.7)['item_nm_alias'].unique() 
                if e.strip() not in self.stop_item_names and len(e.strip()) >= 2
            ])

            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
            {getattr(PROCESSING_CONFIG, 'entity_extraction_prompt', 'ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ìƒí’ˆëª…ì„ ì¶”ì¶œí•˜ì„¸ìš”.')}

            ## message:                
            {msg_text}

            ## Candidate entities:
            {cand_entities_by_sim}
            """
            
            # LLM í˜¸ì¶œ
            cand_entities = self._safe_llm_invoke(prompt)
            
            # LLM ì‘ë‹µ íŒŒì‹± ë° ì •ë¦¬
            cand_entity_list = [e.strip() for e in cand_entities.split(',') if e.strip()]
            cand_entity_list = [e for e in cand_entity_list if e not in self.stop_item_names and len(e) >= 2]

            if not cand_entity_list:
                return pd.DataFrame()

            # í›„ë³´ ì—”í‹°í‹°ë“¤ê³¼ ìƒí’ˆ DB ë§¤ì¹­
            return self._match_entities_with_products(cand_entity_list, rank_limit)
            
        except Exception as e:
            logger.error(f"LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _match_entities_with_products(self, cand_entity_list: List[str], rank_limit: int) -> pd.DataFrame:
        """í›„ë³´ ì—”í‹°í‹°ë“¤ì„ ìƒí’ˆ DBì™€ ë§¤ì¹­"""
        try:
            # í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entity_list,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=0.6,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì •ì§€ì–´ í•„í„°ë§
            similarities_fuzzy = similarities_fuzzy[
                ~similarities_fuzzy['item_nm_alias'].isin(self.stop_item_names)
            ]

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ë§¤ì¹­
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            if cand_entities_sim.empty:
                return pd.DataFrame()
            
            cand_entities_sim = cand_entities_sim.query("sim >= 1.5").copy()

            # ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ
            cand_entities_sim["rank"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(
                method='first', ascending=False
            )
            cand_entities_sim = cand_entities_sim.query(f"rank <= {rank_limit}").sort_values(
                ['item_name_in_msg', 'rank'], ascending=[True, True]
            )

            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _extract_entities(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """ì—”í‹°í‹° ì¶”ì¶œ (Kiwi ë˜ëŠ” LLM ë°©ì‹)"""
        try:
            if self.entity_extraction_mode == 'logic':
                # Kiwi ê¸°ë°˜ ì¶”ì¶œ
                return self.extract_entities_from_kiwi(mms_msg)
            else:
                # LLM ê¸°ë°˜ ì¶”ì¶œì„ ìœ„í•´ ë¨¼ì € Kiwië¡œ ê¸°ë³¸ ì¶”ì¶œ
                cand_item_list, extra_item_pdf = self.extract_entities_from_kiwi(mms_msg)
                return cand_item_list, extra_item_pdf
                
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [], pd.DataFrame()

    def _classify_programs(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
        try:
            if self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _build_extraction_prompt(self, msg: str, rag_context: str, product_element: Optional[List[Dict]]) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì‚¬ê³  ê³¼ì • ì •ì˜ (ëª¨ë“œë³„ ìµœì í™”)
        if self.product_info_extraction_mode == 'llm':
            chain_of_thought = """
1. Identify the advertisement's purpose first, using expressions as they appear in the original text.
2. Extract ONLY explicitly mentioned product/service names from the text, using exact original expressions.
3. For each product, assign a standardized action from: [êµ¬ë§¤, ê°€ì…, ì‚¬ìš©, ë°©ë¬¸, ì°¸ì—¬, ì½”ë“œì…ë ¥, ì¿ í°ë‹¤ìš´ë¡œë“œ, ê¸°íƒ€].
4. Avoid inferring or adding products not directly mentioned in the text.
5. Provide channel information considering the extracted product information, preserving original text expressions.
"""
        else:
            chain_of_thought = """
1. Identify the advertisement's purpose first, using expressions as they appear in the original text.
2. Extract product names based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.
3. Provide channel information considering the extracted product information, preserving original text expressions.
"""

        # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
        schema_prd = {
            "title": "Advertisement title, using the exact expressions as they appear in the original text.",
            "purpose": {
                "type": "array",
                "items": {
                    "type": "string",
                    "enum": ["ìƒí’ˆ ê°€ì… ìœ ë„", "ëŒ€ë¦¬ì /ë§¤ì¥ ë°©ë¬¸ ìœ ë„", "ì›¹/ì•± ì ‘ì† ìœ ë„", "ì´ë²¤íŠ¸ ì‘ëª¨ ìœ ë„", 
                           "í˜œíƒ ì•ˆë‚´", "ì¿ í° ì œê³µ ì•ˆë‚´", "ê²½í’ˆ ì œê³µ ì•ˆë‚´", "ìˆ˜ì‹  ê±°ë¶€ ì•ˆë‚´", "ê¸°íƒ€ ì •ë³´ ì œê³µ"]
                },
                "description": "Primary purpose(s) of the advertisement."
            },
            "product": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string", "description": "Name of the advertised product or service."},
                        "action": {
                            "type": "string",
                            "enum": ["êµ¬ë§¤", "ê°€ì…", "ì‚¬ìš©", "ë°©ë¬¸", "ì°¸ì—¬", "ì½”ë“œì…ë ¥", "ì¿ í°ë‹¤ìš´ë¡œë“œ", "ê¸°íƒ€"],
                            "description": "Expected customer action for the product."
                        }
                    }
                },
                "description": "Extract all product names from the advertisement."
            },
            "channel": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "type": {
                            "type": "string",
                            "enum": ["URL", "ì „í™”ë²ˆí˜¸", "ì•±", "ëŒ€ë¦¬ì "],
                            "description": "Channel type."
                        },
                        "value": {"type": "string", "description": "Specific information for the channel."},
                        "action": {
                            "type": "string",
                            "enum": ["ê°€ì…", "ì¶”ê°€ ì •ë³´", "ë¬¸ì˜", "ìˆ˜ì‹ ", "ìˆ˜ì‹  ê±°ë¶€"],
                            "description": "Purpose of the channel."
                        }
                    }
                },
                "description": "Channels provided in the advertisement."
            },
            "pgm": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Select the two most relevant pgm_nm from the advertising classification criteria."
            }
        }

        # ì¶”ì¶œ ê°€ì´ë“œë¼ì¸ ì„¤ì •
        prd_ext_guide = """
* Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering.
* Extract all information (title, purpose, product, channel, pgm) using the exact expressions as they appear in the original text without translation, as specified in the schema.
* If the advertisement purpose includes encouraging agency/store visits, provide agency channel information.
"""

        # ì œí’ˆ ì •ë³´ ëª¨ë“œì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ì¡°ì •
        if self.product_info_extraction_mode == 'nlp' and product_element:
            schema_prd['product'] = product_element
            chain_of_thought = """
1. Identify the advertisement's purpose first, using expressions as they appear in the original text.
2. Extract product information based on the identified purpose, ensuring only distinct offerings are included.
3. Extract the action field for each product based on the provided name information.
4. Provide channel information considering the extracted product information.
"""
            prd_ext_guide += """
* Extract the action field for each product based on the identified product names, using the original text context.
"""
        
        # ëª¨ë“œë³„ ê°€ì´ë“œë¼ì¸ ì¶”ê°€
        if "### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###" in rag_context:
            # RAG ëª¨ë“œ: í›„ë³´ ëª©ë¡ì„ ê°•ì œ ì°¸ì¡°
            prd_ext_guide += """
* Use the provided candidate product names as a reference to guide product extraction, ensuring alignment with the advertisement content and using exact expressions from the original text.
"""
        elif "### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###" in rag_context:
            # LLM ëª¨ë“œ: í›„ë³´ ëª©ë¡ì„ ì°¸ê³ í•˜ë˜ ì¼ê´€ì„± ê°•í™”
            prd_ext_guide += """
* Refer to the candidate product names list as guidance, but extract products based on your understanding of the advertisement content.
* Maintain consistency by using standardized product naming conventions.
* If multiple similar products exist, choose the most specific and relevant one to reduce variability.
"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ìŠ¤í‚¤ë§ˆë¥¼ ë” ëª…í™•í•˜ê²Œ ì„¤ëª…
        schema_prompt = f"""
Return your response as a JSON object that follows this exact structure:

{json.dumps(schema_prd, indent=4, ensure_ascii=False)}

IMPORTANT: 
- Do NOT return the schema definition itself
- Return actual extracted data in the specified format
- For "purpose": return an array of strings from the enum values
- For "product": return an array of objects with "name" and "action" fields
- For "channel": return an array of objects with "type", "value", and "action" fields
- For "pgm": return an array of strings

Example response format:
{{
    "title": "ì‹¤ì œ ê´‘ê³  ì œëª©",
    "purpose": ["ìƒí’ˆ ê°€ì… ìœ ë„", "í˜œíƒ ì•ˆë‚´"],
    "product": [
        {{"name": "ì‹¤ì œ ìƒí’ˆëª…", "action": "ê°€ì…"}},
        {{"name": "ë‹¤ë¥¸ ìƒí’ˆëª…", "action": "êµ¬ë§¤"}}
    ],
    "channel": [
        {{"type": "URL", "value": "ì‹¤ì œ URL", "action": "ê°€ì…"}}
    ],
    "pgm": ["ì‹¤ì œ í”„ë¡œê·¸ë¨ëª…"]
}}
"""

        # LLM ëª¨ë“œì—ì„œ ì¼ê´€ì„± ê°•í™”ë¥¼ ìœ„í•œ ì¶”ê°€ ì§€ì‹œì‚¬í•­
        consistency_note = ""
        if self.product_info_extraction_mode == 'llm':
            consistency_note = """

### ì¼ê´€ì„± ìœ ì§€ ì§€ì¹¨ ###
* ë™ì¼í•œ ê´‘ê³  ë©”ì‹œì§€ì— ëŒ€í•´ì„œëŠ” í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
* ì• ë§¤í•œ í‘œí˜„ì´ ìˆì„ ë•ŒëŠ” ê°€ì¥ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í•´ì„ì„ ì„ íƒí•˜ì„¸ìš”.
* ìƒí’ˆëª…ì€ ì›ë¬¸ì—ì„œ ì •í™•íˆ ì–¸ê¸‰ëœ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

        prompt = f"""
Extract the advertisement purpose and product names from the provided advertisement text.

### Advertisement Message ###
{msg}

### Extraction Steps ###
{chain_of_thought}

### Extraction Guidelines ###
{prd_ext_guide}{consistency_note}

{schema_prompt}

### OUTPUT FORMAT REQUIREMENT ###
You MUST respond with a valid JSON object containing actual extracted data.
Do NOT include schema definitions, type specifications, or template structures.
Return only the concrete extracted information in the specified JSON format.

{rag_context}

### FINAL REMINDER ###
Return a JSON object with actual data, not schema definitions!
"""

        # ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (LLM ëª¨ë“œì—ì„œë§Œ)
        if self.product_info_extraction_mode == 'llm':
            logger.debug(f"LLM ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í›„ë³´ ìƒí’ˆ ëª©ë¡ í¬í•¨ ì—¬ë¶€: {'ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡' in rag_context}")
            
        return prompt

    def _extract_channels(self, json_objects: Dict, msg: str) -> List[Dict]:
        """ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¹­"""
        try:
            channel_tag = []
            channel_items = json_objects.get('channel', [])
            if isinstance(channel_items, dict):
                channel_items = channel_items.get('items', [])

            for d in channel_items:
                if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                    # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                    store_info = self._match_store_info(d['value'])
                    d['store_info'] = store_info
                else:
                    d['store_info'] = []
                channel_tag.append(d)

            return channel_tag
            
        except Exception as e:
            logger.error(f"ì±„ë„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _match_store_info(self, store_name: str) -> List[Dict]:
        """ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­"""
        try:
            # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
            org_pdf_cand = safe_execute(
                parallel_fuzzy_similarity,
                [preprocess_text(store_name.lower())],
                self.org_pdf['org_abbr_nm'].unique(),
                threshold=0.5,
                text_col_nm='org_nm_in_msg',
                item_col_nm='org_abbr_nm',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )

            if org_pdf_cand.empty:
                return []

            org_pdf_cand = org_pdf_cand.drop('org_nm_in_msg', axis=1)
            org_pdf_cand = self.org_pdf.merge(org_pdf_cand, on=['org_abbr_nm'])
            org_pdf_cand['sim'] = org_pdf_cand.apply(
                lambda x: combined_sequence_similarity(store_name, x['org_nm'])[0], axis=1
            ).round(5)
            
            # ëŒ€ë¦¬ì  ì½”ë“œ('D'ë¡œ ì‹œì‘) ìš°ì„  ê²€ìƒ‰
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold', 0.2)
            org_pdf_tmp = org_pdf_cand.query(
                "org_cd.str.startswith('D') & sim >= @similarity_threshold", engine='python'
            ).sort_values('sim', ascending=False)
            
            if org_pdf_tmp.empty:
                # ëŒ€ë¦¬ì ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰
                org_pdf_tmp = org_pdf_cand.query("sim >= @similarity_threshold").sort_values('sim', ascending=False)
            
            if not org_pdf_tmp.empty:
                # ìµœê³  ìˆœìœ„ ì¡°ì§ë“¤ì˜ ì •ë³´ ì¶”ì¶œ
                org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense', ascending=False)
                org_pdf_tmp['org_cd_full'] = org_pdf_tmp.apply(lambda x: x['org_cd'] + x['sub_org_cd'], axis=1)
                org_info = org_pdf_tmp.query("rank == 1").groupby('org_nm')['org_cd_full'].apply(list).reset_index(name='org_cd').to_dict('records')
                return org_info
            else:
                return []
                
        except Exception as e:
            logger.error(f"ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []

    def _validate_extraction_result(self, result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'purpose', 'product', 'channel']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    result[field] = [] if field != 'title' else "ê´‘ê³  ë©”ì‹œì§€"

            # ìƒí’ˆëª… ê¸¸ì´ ê²€ì¦
            validated_products = []
            for product in result.get('product', []):
                if isinstance(product, dict):
                    item_name = product.get('item_name_in_msg', product.get('name', ''))
                    if len(item_name) >= 2 and item_name not in self.stop_item_names:
                        validated_products.append(product)
                    else:
                        logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìƒí’ˆëª… ì œì™¸: {item_name}")
            
            result['product'] = validated_products

            # ì±„ë„ ì •ë³´ ê²€ì¦
            validated_channels = []
            for channel in result.get('channel', []):
                if isinstance(channel, dict) and channel.get('value'):
                    validated_channels.append(channel)
            
            result['channel'] = validated_channels

            return result
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return result

    @log_performance
    def process_message(self, mms_msg: str) -> Dict[str, Any]:
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜)
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ MMS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë©”ì‹œì§€ ë‚´ìš©: {mms_msg[:200]}...")
            logger.info(f"ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # í˜„ì¬ ì„¤ì • ìƒíƒœ ë¡œê¹…
            logger.info("=== í˜„ì¬ ì¶”ì¶œê¸° ì„¤ì • ===")
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.offer_info_data_src}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: {self.product_info_extraction_mode}")
            logger.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ: {self.entity_extraction_mode}")
            logger.info(f"LLM ëª¨ë¸: {self.llm_model_name}")
            logger.info(f"ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì…ë ¥ ê²€ì¦
            msg = validate_text_input(mms_msg)
            
            # 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("=" * 30 + " 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ " + "=" * 30)
            
            # DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨ ì‹œì‘")
                logger.info(f"ìƒí’ˆ ë°ì´í„° ìƒíƒœ: {self.item_pdf_all.shape}")
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                required_columns = ['item_nm', 'item_id', 'item_nm_alias']
                missing_columns = [col for col in required_columns if col not in self.item_pdf_all.columns]
                if missing_columns:
                    logger.error(f"ğŸš¨ DB ëª¨ë“œì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                
                # ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    null_aliases = self.item_pdf_all['item_nm_alias'].isnull().sum()
                    total_aliases = len(self.item_pdf_all)
                    logger.info(f"DB ëª¨ë“œ ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ: {total_aliases - null_aliases}/{total_aliases} ìœ íš¨")
            
            cand_item_list, extra_item_pdf = self._extract_entities(msg)
            logger.info(f"ì¶”ì¶œëœ í›„ë³´ ì—”í‹°í‹°: {cand_item_list}")
            logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
            
            # DB ëª¨ë“œì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ íŠ¹ë³„ ë¶„ì„
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
                if not cand_item_list:
                    logger.error("ğŸš¨ DB ëª¨ë“œì—ì„œ í›„ë³´ ì—”í‹°í‹°ê°€ ì „í˜€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
                    logger.error("1. ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ìƒí’ˆì´ ì—†ìŒ")
                    logger.error("2. ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨")
                    logger.error("3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ")
                    logger.error("4. Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨")
            
            # 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜
            logger.info("=" * 30 + " 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ " + "=" * 30)
            pgm_info = self._classify_programs(msg)
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ê²°ê³¼ í‚¤: {list(pgm_info.keys())}")
            
            # 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            logger.info("=" * 30 + " 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± " + "=" * 30)
            rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_info['pgm_cand_info']}" if self.num_cand_pgms > 0 else ""
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ (ëª¨ë“œë³„ ì²˜ë¦¬)
            logger.info("=" * 30 + " 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ " + "=" * 30)
            product_element = None
            if len(cand_item_list) > 0:
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°: {len(cand_item_list)}ê°œ")
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                # extra_item_pdf ìƒíƒœ í™•ì¸
                logger.info(f"extra_item_pdf í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    logger.info(f"extra_item_pdf ì»¬ëŸ¼ë“¤: {list(extra_item_pdf.columns)}")
                    logger.info(f"extra_item_pdf ìƒ˜í”Œ: {extra_item_pdf.head(2).to_dict('records')}")
                
                if self.product_info_extraction_mode == 'rag':
                    rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'llm':
                    # LLM ëª¨ë“œì—ë„ í›„ë³´ ëª©ë¡ ì œê³µí•˜ì—¬ ì¼ê´€ì„± í–¥ìƒ
                    rag_context += f"\n\n### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("LLM ëª¨ë“œ: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'nlp':
                    if not extra_item_pdf.empty and 'item_nm' in extra_item_pdf.columns:
                        product_df = extra_item_pdf.rename(columns={'item_nm': 'name'}).query(
                            "not name in @self.stop_item_names"
                        )[['name']]
                        product_df['action'] = 'ê¸°íƒ€'
                        product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
                        logger.info(f"NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ - {len(product_element) if product_element else 0}ê°œ")
                        if product_element:
                            logger.info(f"NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ ìƒ˜í”Œ: {product_element[:2]}")
                    else:
                        logger.warning("NLP ëª¨ë“œ: extra_item_pdfê°€ ë¹„ì–´ìˆê±°ë‚˜ item_nm ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            else:
                logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                logger.warning("ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                logger.warning("1. ìƒí’ˆ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                logger.warning("2. ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨") 
                logger.warning("3. ìœ ì‚¬ë„ ë§¤ì¹­ ì„ê³„ê°’ ë¬¸ì œ")

            # 5ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹¤í–‰
            logger.info("=" * 30 + " 5ë‹¨ê³„: LLM í˜¸ì¶œ " + "=" * 30)
            prompt = self._build_extraction_prompt(msg, rag_context, product_element)
            logger.info(f"êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€: {'í›„ë³´ ìƒí’ˆ' in rag_context}")
            
            result_json_text = self._safe_llm_invoke(prompt)
            logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(result_json_text)} ë¬¸ì")
            logger.info(f"LLM ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {result_json_text[:500]}...")
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            logger.info("=" * 30 + " 6ë‹¨ê³„: JSON íŒŒì‹± " + "=" * 30)
            json_objects_list = extract_json_objects(result_json_text)
            logger.info(f"ì¶”ì¶œëœ JSON ê°ì²´ ìˆ˜: {len(json_objects_list)}ê°œ")
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ: {result_json_text}")
                return self._create_fallback_result(msg)
            
            json_objects = json_objects_list[-1]
            logger.info(f"íŒŒì‹±ëœ JSON ê°ì²´ í‚¤: {list(json_objects.keys())}")
            logger.info(f"íŒŒì‹±ëœ JSON ë‚´ìš©: {json_objects}")
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ë° ì²˜ë¦¬
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.error("ğŸš¨ LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤! ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                logger.error("ì¬ì‹œë„ ë˜ëŠ” fallback ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._create_fallback_result(msg)
            
            # 7ë‹¨ê³„: ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±
            logger.info("=" * 30 + " 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„± " + "=" * 30)
            final_result = self._build_final_result(json_objects, msg, pgm_info)
            
            # 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
            logger.info("=" * 30 + " 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ " + "=" * 30)
            final_result = self._validate_extraction_result(final_result)

            # DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ (ì„ íƒì )
            # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
            # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
            dag_section = ""
            if self.extract_entity_dag:
                logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
                try:
                    dag_start_time = time.time()
                    # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
                    extract_dag_result = extract_dag(DAGParser(), msg, self.llm_model)
                    dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
                    dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
                    dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
                    
                    # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
                    dag_filename = f'dag_{sha256_hash(msg)}'
                    create_dag_diagram(dag, filename=dag_filename)
                    dag_processing_time = time.time() - dag_start_time
                    
                    logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")
                    logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
                    logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
                    if dag_section:
                        logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
                    else:
                        logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                        
                except Exception as e:
                    logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    dag_section = ""

            # ìµœì¢… ê²°ê³¼ì— DAG ì •ë³´ ì¶”ê°€ (ë¹„ì–´ìˆì„ ìˆ˜ë„ ìˆìŒ)
            final_result['entity_dag'] = dag_section
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
            logger.info("=" * 60)
            logger.info("âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"ì œëª©: {final_result.get('title', 'N/A')}")
            logger.info(f"ëª©ì : {final_result.get('purpose', [])}")
            logger.info(f"ìƒí’ˆ ìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info(f"ì±„ë„ ìˆ˜: {len(final_result.get('channel', []))}ê°œ")
            logger.info(f"í”„ë¡œê·¸ë¨ ìˆ˜: {len(final_result.get('pgm', []))}ê°œ")
            
            return final_result
            
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_fallback_result(mms_msg)

    def _detect_schema_response(self, json_objects: Dict) -> bool:
        """LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ ê°ì§€"""
        try:
            # purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            purpose = json_objects.get('purpose', {})
            if isinstance(purpose, dict) and 'type' in purpose and purpose.get('type') == 'array':
                logger.warning("purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸  
            product = json_objects.get('product', {})
            if isinstance(product, dict) and 'type' in product and product.get('type') == 'array':
                logger.warning("product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            channel = json_objects.get('channel', {})
            if isinstance(channel, dict) and 'type' in channel and channel.get('type') == 'array':
                logger.warning("channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _create_fallback_result(self, msg: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "product": [],
            "channel": [],
            "pgm": []
        }

    def _build_final_result(self, json_objects: Dict, msg: str, pgm_info: Dict) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        try:
            final_result = json_objects.copy()
            
            # ìƒí’ˆ ì •ë³´ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            product_items = json_objects.get('product', [])
            if isinstance(product_items, dict):
                product_items = product_items.get('items', [])
            
            logger.info(f"LLM ì¶”ì¶œ ì—”í‹°í‹°: {[x.get('name', '') for x in product_items]}")

            # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if self.entity_extraction_mode == 'logic':
                # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
                cand_entities = [item.get('name', '') for item in product_items if item.get('name')]
                similarities_fuzzy = self.extract_entities_by_logic(cand_entities)
            else:
                # LLM ê¸°ë°˜: LLMì„ í†µí•œ ì—”í‹°í‹° ì¶”ì¶œ
                similarities_fuzzy = self.extract_entities_by_llm(msg)

            # ìƒí’ˆ ì •ë³´ ë§¤í•‘
            if not similarities_fuzzy.empty:
                final_result['product'] = self._map_products_with_similarity(similarities_fuzzy, json_objects)
            else:
                # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                final_result['product'] = [
                    {
                        'item_name_in_msg': d.get('name', ''), 
                        'expected_action': d.get('action', 'ê¸°íƒ€'),
                        'item_in_voca': [{'item_name_in_voca': d.get('name', ''), 'item_id': ['#']}]
                    } 
                    for d in product_items 
                    if d.get('name') and d['name'] not in self.stop_item_names
                ]

            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
            final_result['pgm'] = self._map_program_classification(json_objects, pgm_info)
            
            # ì±„ë„ ì •ë³´ ì²˜ë¦¬
            final_result['channel'] = self._extract_channels(json_objects, msg)

            return final_result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return json_objects

    def _map_products_with_similarity(self, similarities_fuzzy: pd.DataFrame, json_objects: Dict = None) -> List[Dict]:
        """ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ë§¤í•‘"""
        try:
            # ë†’ì€ ìœ ì‚¬ë„ ì•„ì´í…œë“¤ í•„í„°ë§
            high_sim_items = similarities_fuzzy.query('sim >= 1.5')['item_nm_alias'].unique()
            filtered_similarities = similarities_fuzzy[
                (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &
                (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &
                (~similarities_fuzzy['item_name_in_msg'].isin(self.stop_item_names))
            ]
            
            # ìƒí’ˆ ì •ë³´ì™€ ë§¤í•‘í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
            product_tag = convert_df_to_json_list(
                self.item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])
            )
            
            # Add action information from original json_objects
            if json_objects:
                action_mapping = self._create_action_mapping(json_objects)
                for product in product_tag:
                    item_name = product.get('item_name_in_msg', '')
                    product['expected_action'] = action_mapping.get(item_name, 'ê¸°íƒ€')
            
            return product_tag
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

    def _create_action_mapping(self, json_objects: Dict) -> Dict[str, str]:
        """LLM ì‘ë‹µì—ì„œ ìƒí’ˆëª…-ì•¡ì…˜ ë§¤í•‘ ìƒì„±"""
        try:
            action_mapping = {}
            product_data = json_objects.get('product', [])
            
            if isinstance(product_data, list):
                # ì •ìƒì ì¸ ë°°ì—´ êµ¬ì¡°
                for item in product_data:
                    if isinstance(item, dict) and 'name' in item and 'action' in item:
                        action_mapping[item['name']] = item['action']
            elif isinstance(product_data, dict):
                # ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ë˜ëŠ” ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                if 'items' in product_data:
                    # ìŠ¤í‚¤ë§ˆ êµ¬ì¡°: {"items": [...]}
                    items = product_data.get('items', [])
                    for item in items:
                        if isinstance(item, dict) and 'name' in item and 'action' in item:
                            action_mapping[item['name']] = item['action']
                elif 'type' in product_data and product_data.get('type') == 'array':
                    # ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡°ëŠ” ê±´ë„ˆë›°ê¸°
                    logger.debug("ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡° ê°ì§€ë¨, ì•¡ì…˜ ë§¤í•‘ ê±´ë„ˆë›°ê¸°")
                else:
                    # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                    if 'name' in product_data and 'action' in product_data:
                        action_mapping[product_data['name']] = product_data['action']
            
            logger.debug(f"ìƒì„±ëœ ì•¡ì…˜ ë§¤í•‘: {action_mapping}")
            return action_mapping
            
        except Exception as e:
            logger.error(f"ì•¡ì…˜ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _map_program_classification(self, json_objects: Dict, pgm_info: Dict) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘"""
        try:
            if (self.num_cand_pgms > 0 and 
                'pgm' in json_objects and 
                isinstance(json_objects['pgm'], list) and
                not pgm_info.get('pgm_pdf_tmp', pd.DataFrame()).empty):
                
                pgm_json = pgm_info['pgm_pdf_tmp'][
                    pgm_info['pgm_pdf_tmp']['pgm_nm'].apply(
                        lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm'])
                    )
                ][['pgm_nm', 'pgm_id']].to_dict('records')
                
                return pgm_json
            
            return []
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []


def main():
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ì¶”ì¶œê¸° ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° - ê°œì„ ëœ ë²„ì „')
    parser.add_argument('--message', type=str, help='í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€')
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='local',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm', 'rag'], default='nlp',
                       help='ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ (nlp: í˜•íƒœì†Œë¶„ì„, llm: LLM ê¸°ë°˜, rag: ê²€ìƒ‰ì¦ê°•ìƒì„±)')
    parser.add_argument('--entity-matching-mode', choices=['logic', 'llm'], default='llm',
                       help='ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œ (logic: ë¡œì§ ê¸°ë°˜, llm: LLM ê¸°ë°˜)')
    parser.add_argument('--llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='ax',
                       help='ì‚¬ìš©í•  LLM ëª¨ë¸ (gem: Gemma, ax: ax, cld: Claude, gen: Gemini, gpt: GPT)')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                       help='ë¡œê·¸ ë ˆë²¨ ì„¤ì •')
    parser.add_argument('--extract-entity-dag', action='store_true', default=False, help='Entity DAG extraction (default: False)')

    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    try:
        # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        logger.info("MMS ì¶”ì¶œê¸° ì´ˆê¸°í™” ì¤‘...")
        extractor = MMSExtractor(
            offer_info_data_src=args.offer_data_source,
            product_info_extraction_mode=args.product_info_extraction_mode,
            entity_extraction_mode=args.entity_matching_mode,
            llm_model=args.llm_model,
            extract_entity_dag=args.extract_entity_dag
        )
        
        # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì„¤ì •
        test_message = args.message if args.message else """
        [SKT] ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3 ì•ˆë‚´
        (ê´‘ê³ )[SKT] ìš°ë¦¬ ì•„ì´ ì²« ë²ˆì§¸ ìŠ¤ë§ˆíŠ¸í°, ZEM í‚¤ì¦ˆí°__#04 ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”!
        ìš°ë¦¬ ì•„ì´ ìŠ¤ë§ˆíŠ¸í° ê³ ë¯¼ ì¤‘ì´ì…¨ë‹¤ë©´, ìë…€ ìŠ¤ë§ˆíŠ¸í° ê´€ë¦¬ ì•± ZEMì´ ì„¤ì¹˜ëœ SKTë§Œì˜ ì•ˆì „í•œ í‚¤ì¦ˆí°,
        ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3ìœ¼ë¡œ ìš°ë¦¬ ì•„ì´ ì·¨í–¥ì„ ì €ê²©í•´ ë³´ì„¸ìš”!
        ì‹ í•™ê¸°ë¥¼ ë§ì´í•˜ì—¬ SKí…”ë ˆì½¤ ê³µì‹ ì¸ì¦ ëŒ€ë¦¬ì ì—ì„œ í’ì„±í•œ í˜œíƒì„ ì œê³µí•´ ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤!
        â–¶ ì£¼ìš” ê¸°ëŠ¥
        1. ì‹¤ì‹œê°„ ìœ„ì¹˜ ì¡°íšŒ
        2. ëª¨ë¥´ëŠ” íšŒì„  ìë™ ì°¨ë‹¨
        3. ìŠ¤ë§ˆíŠ¸í° ì‚¬ìš© ì‹œê°„ ì œí•œ
        4. IP68 ë°©ìˆ˜ ë°©ì§„
        5. ìˆ˜ì—… ì‹œê°„ ìë™ ë¬´ìŒëª¨ë“œ
        6. ìœ í•´ ì½˜í…ì¸  ì°¨ë‹¨
        â–¶ ê°€ê¹Œìš´ SKí…”ë ˆì½¤ ê³µì‹ ì¸ì¦ ëŒ€ë¦¬ì  ì°¾ê¸°
        http://t-mms.kr/t.do?m=#61&s=30684&a=&u=https://bit.ly/3yQF2hx
        â–¶ ë¬¸ì˜ : SKT ê³ ê°ì„¼í„°(1558, ë¬´ë£Œ)
        ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ 1504
        """
        
        # ë©”ì‹œì§€ ì²˜ë¦¬ ë° ê²°ê³¼ ì¶œë ¥
        logger.info("ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
        result = extractor.process_message(test_message)
            
        print("\n" + "="*50)
        print("ğŸ¯ ìµœì¢… ì¶”ì¶œëœ ì •ë³´")
        print("="*50)
        print(json.dumps(result, indent=4, ensure_ascii=False))
        
        # ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ")
        print("="*50)
        print(f"âœ… ì œëª©: {result.get('title', 'N/A')}")
        print(f"âœ… ëª©ì : {len(result.get('purpose', []))}ê°œ")
        print(f"âœ… ìƒí’ˆ: {len(result.get('product', []))}ê°œ")
        print(f"âœ… ì±„ë„: {len(result.get('channel', []))}ê°œ")
        print(f"âœ… í”„ë¡œê·¸ë¨: {len(result.get('pgm', []))}ê°œ")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        exit(1)


if __name__ == '__main__':
    main()