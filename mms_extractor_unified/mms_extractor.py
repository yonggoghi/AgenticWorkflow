
# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor)
=========================

ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì  ë“±ì„ 
ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê°œì²´ëª… ì¶”ì¶œ
- í¼ì§€ ë§¤ì¹­ ë° ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ìƒí’ˆëª… ë§¤ì¹­
- LLMì„ í™œìš©í•œ ê´‘ê³  ì •ë³´ êµ¬ì¡°í™” ì¶”ì¶œ
- ëŒ€ë¦¬ì /ë§¤ì¥ ì •ë³´ ë§¤ì¹­
"""

from concurrent.futures import ThreadPoolExecutor
import time
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
import json
import re
# from pygments import highlight
# from pygments.lexers import JsonLexer
# from pygments.formatters import HtmlFormatter
# from IPython.display import HTML
import pandas as pd
# from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from openai import OpenAI
from typing import List, Tuple, Union, Dict, Any
import ast
from rapidfuzz import fuzz, process
import re
import json
import glob
import os
from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def dataframe_to_markdown_prompt(df, max_rows=None):
    """
    DataFrameì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    
    Args:
        df: ë³€í™˜í•  DataFrame
        max_rows: ìµœëŒ€ í–‰ ìˆ˜ ì œí•œ (Noneì´ë©´ ëª¨ë“  í–‰ í¬í•¨)
    
    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í…Œì´ë¸” ë¬¸ìì—´
    """
    if max_rows is not None and len(df) > max_rows:
        display_df = df.head(max_rows)
        truncation_note = f"\n[Note: Only showing first {max_rows} of {len(df)} rows]"
    else:
        display_df = df
        truncation_note = ""
    df_markdown = display_df.to_markdown()
    prompt = f"\n\n    {df_markdown}\n    {truncation_note}\n\n    "
    return prompt

def clean_segment(segment):
    """
    ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ì—ì„œ ë‚´ë¶€ì˜ ë™ì¼í•œ ë”°ì˜´í‘œ ì œê±°
    
    Args:
        segment: ì •ë¦¬í•  ë¬¸ìì—´ ì„¸ê·¸ë¨¼íŠ¸
    
    Returns:
        str: ì •ë¦¬ëœ ë¬¸ìì—´
    """
    segment = segment.strip()
    if len(segment) >= 2 and segment[0] in ['"', "'"] and segment[-1] == segment[0]:
        q = segment[0]
        inner = segment[1:-1].replace(q, '')
        return q + inner + q
    return segment

def split_key_value(text):
    """
    ë”°ì˜´í‘œ ì™¸ë¶€ì˜ ì²« ë²ˆì§¸ ì½œë¡ ì„ ê¸°ì¤€ìœ¼ë¡œ í‚¤-ê°’ ë¶„ë¦¬
    
    Args:
        text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
    
    Returns:
        tuple: (í‚¤, ê°’) íŠœí”Œ
    """
    in_quote = False
    quote_char = ''
    for i, char in enumerate(text):
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
        elif char == ':' and not in_quote:
            return text[:i], text[i+1:]
    return text, ''

def split_outside_quotes(text, delimiter=','):
    """
    ë”°ì˜´í‘œ ì™¸ë¶€ì˜ êµ¬ë¶„ìë¡œë§Œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
    
    Args:
        text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
        delimiter: êµ¬ë¶„ì (ê¸°ë³¸ê°’: ì‰¼í‘œ)
    
    Returns:
        list: ë¶„ë¦¬ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    parts = []
    current = []
    in_quote = False
    quote_char = ''
    for char in text:
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
            current.append(char)
        elif char == delimiter and not in_quote:
            parts.append(''.join(current).strip())
            current = []
        else:
            current.append(char)
    if current:
        parts.append(''.join(current).strip())
    return parts

def clean_ill_structured_json(text):
    """
    ì˜ëª» êµ¬ì¡°í™”ëœ JSON í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì •ë¦¬
    
    Args:
        text: ì •ë¦¬í•  JSON í˜•ì‹ í…ìŠ¤íŠ¸
    
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    parts = split_outside_quotes(text, delimiter=',')
    cleaned_parts = []
    for part in parts:
        key, value = split_key_value(part)
        key_clean = clean_segment(key)
        value_clean = clean_segment(value) if value.strip() != "" else ""
        if value_clean:
            cleaned_parts.append(f"{key_clean}: {value_clean}")
        else:
            cleaned_parts.append(key_clean)
    return ', '.join(cleaned_parts)

def repair_json(broken_json):
    """
    ì†ìƒëœ JSON ë¬¸ìì—´ ë³µêµ¬
    
    Args:
        broken_json: ë³µêµ¬í•  JSON ë¬¸ìì—´
    
    Returns:
        str: ë³µêµ¬ëœ JSON ë¬¸ìì—´
    """
    json_str = broken_json
    # ë”°ì˜´í‘œ ì—†ëŠ” í‚¤ì— ë”°ì˜´í‘œ ì¶”ê°€
    json_str = re.sub(r'([{,])\s*([a-zA-Z0-9_]+)\s*:', r'\1 "\2":', json_str)
    # ë”°ì˜´í‘œ ì—†ëŠ” ê°’ ì²˜ë¦¬
    parts = json_str.split('"')
    for i in range(0, len(parts), 2):
        parts[i] = re.sub(r':\s*([a-zA-Z0-9_]+)(?=\s*[,\]\}])', r': "\1"', parts[i])
    json_str = '"'.join(parts)
    # í›„í–‰ ì‰¼í‘œ ì œê±°
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    return json_str

def extract_json_objects(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì¶”ì¶œ
    
    Args:
        text: JSONì´ í¬í•¨ëœ í…ìŠ¤íŠ¸
    
    Returns:
        list: ì¶”ì¶œëœ JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    pattern = r'(\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\})'
    result = []
    for match in re.finditer(pattern, text):
        potential_json = match.group(0)
        try:
            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))
            result.append(json_obj)
        except (json.JSONDecodeError, SyntaxError, ValueError):
            pass
    return result

def preprocess_text(text):
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ê·œí™”)
    
    Args:
        text: ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
    
    Returns:
        str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ===== ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ =====

def fuzzy_similarities(text, entities):
    """
    í¼ì§€ ë§¤ì¹­ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        text: ë¹„êµí•  í…ìŠ¤íŠ¸
        entities: ë¹„êµ ëŒ€ìƒ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: (ì—”í‹°í‹°, ìµœëŒ€ìœ ì‚¬ë„ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    for entity in entities:
        scores = {
            'ratio': fuzz.ratio(text, entity) / 100,
            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,
            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,
            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100
        }
        max_score = max(scores.values())
        results.append((entity, max_score))
    return results

def get_fuzzy_similarities(args_dict):
    """
    ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
    
    Args:
        args_dict: ì²˜ë¦¬ ì¸ì ë”•ì…”ë„ˆë¦¬
            - text: ë¹„êµí•  í…ìŠ¤íŠ¸
            - entities: ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            - threshold: ì„ê³„ê°’
            - text_col_nm: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            - item_col_nm: ì•„ì´í…œ ì»¬ëŸ¼ëª…
    
    Returns:
        list: í•„í„°ë§ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    text = args_dict['text']
    entities = args_dict['entities']
    threshold = args_dict['threshold']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    
    text_processed = preprocess_text(text.lower())
    similarities = fuzzy_similarities(text_processed, entities)
    
    filtered_results = [
        {
            text_col_nm: text,
            item_col_nm: entity, 
            "sim": score
        } 
        for entity, score in similarities 
        if score >= threshold
    ]
    return filtered_results

def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        texts: ë¹„êµí•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        entities: ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        text_col_nm: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        item_col_nm: ì•„ì´í…œ ì»¬ëŸ¼ëª…
        n_jobs: ë³‘ë ¬ ì‘ì—… ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        DataFrame: ìœ ì‚¬ë„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, len(entities) // (n_jobs * 2))
    
    batches = []
    for text in texts:
        for i in range(0, len(entities), batch_size):
            batch = entities[i:i + batch_size]
            batches.append({"text": text, "entities": batch, "threshold": threshold, "text_col_nm": text_col_nm, "item_col_nm": item_col_nm})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def longest_common_subsequence_ratio(s1, s2, normalizaton_value):
    """
    ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ë¹„ìœ¨ ê³„ì‚°
    
    Args:
        s1, s2: ë¹„êµí•  ë‘ ë¬¸ìì—´
        normalizaton_value: ì •ê·œí™” ë°©ì‹ ('max', 'min', 's1', 's2')
    
    Returns:
        float: LCS ë¹„ìœ¨ (0.0-1.0)
    """
    def lcs_length(x, y):
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i-1] == y[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_len = lcs_length(s1, s2)
    if normalizaton_value == 'max':
        max_len = max(len(s1), len(s2))
        return lcs_len / max_len if max_len > 0 else 1.0
    elif normalizaton_value == 'min':
        min_len = min(len(s1), len(s2))
        return lcs_len / min_len if min_len > 0 else 1.0
    elif normalizaton_value == 's1':
        return lcs_len / len(s1) if len(s1) > 0 else 1.0
    elif normalizaton_value == 's2':
        return lcs_len / len(s2) if len(s2) > 0 else 1.0
    else:
        raise ValueError(f"Invalid normalization value: {normalizaton_value}")

def sequence_matcher_similarity(s1, s2, normalizaton_value):
    """
    SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        s1, s2: ë¹„êµí•  ë‘ ë¬¸ìì—´
        normalizaton_value: ì •ê·œí™” ë°©ì‹
    
    Returns:
        float: ìœ ì‚¬ë„ ì ìˆ˜ (0.0-1.0)
    """
    matcher = difflib.SequenceMatcher(None, s1, s2)
    matches = sum(triple.size for triple in matcher.get_matching_blocks())
    
    normalization_length = min(len(s1), len(s2))
    if normalizaton_value == 'max':
        normalization_length = max(len(s1), len(s2))
    elif normalizaton_value == 's1':
        normalization_length = len(s1)
    elif normalizaton_value == 's2':
        normalization_length = len(s2)
        
    if normalization_length == 0: 
        return 0.0
    
    return matches / normalization_length

def substring_aware_similarity(s1, s2, normalizaton_value):
    """
    ë¶€ë¶„ë¬¸ìì—´ ê´€ê³„ë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        s1, s2: ë¹„êµí•  ë‘ ë¬¸ìì—´
        normalizaton_value: ì •ê·œí™” ë°©ì‹
    
    Returns:
        float: ìœ ì‚¬ë„ ì ìˆ˜ (0.0-1.0)
    """
    # í•œ ë¬¸ìì—´ì´ ë‹¤ë¥¸ ë¬¸ìì—´ì˜ ë¶€ë¶„ë¬¸ìì—´ì¸ì§€ í™•ì¸
    if s1 in s2 or s2 in s1:
        shorter = min(s1, s2, key=len)
        longer = max(s1, s2, key=len)
        base_score = len(shorter) / len(longer)
        return min(0.95 + base_score * 0.05, 1.0)
    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)

def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\s_\-]+'):
    """
    í† í° ì‹œí€€ìŠ¤ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        s1, s2: ë¹„êµí•  ë‘ ë¬¸ìì—´
        normalizaton_value: ì •ê·œí™” ë°©ì‹
        separator_pattern: í† í° ë¶„ë¦¬ íŒ¨í„´
    
    Returns:
        float: í† í° ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ (0.0-1.0)
    """
    tokens1 = [t for t in re.split(separator_pattern, s1.strip()) if t]
    tokens2 = [t for t in re.split(separator_pattern, s2.strip()) if t]
    
    if not tokens1 or not tokens2:
        return 0.0
    
    def token_lcs_length(t1, t2):
        m, n = len(t1), len(t2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if t1[i-1] == t2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_tokens = token_lcs_length(tokens1, tokens2)
    normalization_tokens = max(len(tokens1), len(tokens2))
    if normalizaton_value == 'min':
        normalization_tokens = min(len(tokens1), len(tokens2))
    elif normalizaton_value == 's1':
        normalization_tokens = len(tokens1)
    elif normalizaton_value == 's2':
        normalization_tokens = len(tokens2)
    
    return lcs_tokens / normalization_tokens  

def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):
    """
    ì—¬ëŸ¬ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ì„ ê²°í•©í•œ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        s1, s2: ë¹„êµí•  ë‘ ë¬¸ìì—´
        weights: ê° ë©”íŠ¸ë¦­ì˜ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
        normalizaton_value: ì •ê·œí™” ë°©ì‹
    
    Returns:
        tuple: (ì¢…í•©ìœ ì‚¬ë„, ê°œë³„ìœ ì‚¬ë„ë”•ì…”ë„ˆë¦¬)
    """
    if weights is None:
        weights = {'substring': 0.4, 'sequence_matcher': 0.4, 'token_sequence': 0.2}
    
    similarities = {
        'substring': substring_aware_similarity(s1, s2, normalizaton_value),
        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),
        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)
    }
    
    return sum(similarities[key] * weights[key] for key in weights), similarities

def calculate_seq_similarity(args_dict):
    """
    ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        args_dict: ì²˜ë¦¬ ì¸ì ë”•ì…”ë„ˆë¦¬
    
    Returns:
        list: ìœ ì‚¬ë„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    sent_item_batch = args_dict['sent_item_batch']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    normalizaton_value = args_dict['normalizaton_value']
    
    results = []
    for sent_item in sent_item_batch:
        sent = sent_item[text_col_nm]
        item = sent_item[item_col_nm]
        try:
            sent_processed = preprocess_text(sent.lower())
            item_processed = preprocess_text(item.lower())
            similarity = combined_sequence_similarity(sent_processed, item_processed, normalizaton_value=normalizaton_value)[0]
            results.append({text_col_nm:sent, item_col_nm:item, "sim":similarity})
        except Exception as e:
            print(f"Error processing {item}: {e}")
            results.append({text_col_nm:sent, item_col_nm:item, "sim":0.0})
    
    return results

def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, normalizaton_value='s2'):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        sent_item_pdf: ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„
        text_col_nm: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        item_col_nm: ì•„ì´í…œ ì»¬ëŸ¼ëª…
        n_jobs: ë³‘ë ¬ ì‘ì—… ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        normalizaton_value: ì •ê·œí™” ë°©ì‹
    
    Returns:
        DataFrame: ìœ ì‚¬ë„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))
    
    batches = []
    for i in range(0, sent_item_pdf.shape[0], batch_size):
        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')
        batches.append({"sent_item_batch": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'normalizaton_value': normalizaton_value})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def load_sentence_transformer(model_path, device=None):
    """
    SentenceTransformer ëª¨ë¸ ë¡œë“œ
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
    
    Returns:
        SentenceTransformer: ë¡œë“œëœ ëª¨ë¸
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Loading model from {model_path}...")
    model = SentenceTransformer(model_path).to(device)
    print(f"Model loaded on {device}")
    return model

# ===== Kiwi í˜•íƒœì†Œ ë¶„ì„ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ =====

class Token:
    """í˜•íƒœì†Œ ë¶„ì„ í† í° í´ë˜ìŠ¤"""
    def __init__(self, form, tag, start, length):
        self.form = form      # í† í° í˜•íƒœ
        self.tag = tag        # í’ˆì‚¬ íƒœê·¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.len = length     # ê¸¸ì´

class Sentence:
    """í˜•íƒœì†Œ ë¶„ì„ ë¬¸ì¥ í´ë˜ìŠ¤"""
    def __init__(self, text, start, end, tokens, subs=None):
        self.text = text      # ë¬¸ì¥ í…ìŠ¤íŠ¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.end = end        # ë ìœ„ì¹˜
        self.tokens = tokens  # í† í° ë¦¬ìŠ¤íŠ¸
        self.subs = subs or []  # í•˜ìœ„ ë¬¸ì¥ë“¤

def filter_text_by_exc_patterns(sentence, exc_tag_patterns):
    """
    ì œì™¸í•  í’ˆì‚¬ íŒ¨í„´ì— ë”°ë¼ í…ìŠ¤íŠ¸ í•„í„°ë§
    
    Args:
        sentence: í•„í„°ë§í•  ë¬¸ì¥ ê°ì²´
        exc_tag_patterns: ì œì™¸í•  í’ˆì‚¬ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        str: í•„í„°ë§ëœ í…ìŠ¤íŠ¸
    """
    # ê°œë³„ íƒœê·¸ì™€ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ë¦¬
    individual_tags = set()
    sequences = []
    
    for pattern in exc_tag_patterns:
        if isinstance(pattern, list):
            if len(pattern) == 1:
                individual_tags.add(pattern[0])
            else:
                sequences.append(pattern)
        else:
            individual_tags.add(pattern)
    
    # ì œì™¸í•  í† í° ì¸ë±ìŠ¤ ìˆ˜ì§‘
    tokens_to_exclude = set()
    
    # ê°œë³„ íƒœê·¸ ë§¤ì¹­ í™•ì¸
    for i, token in enumerate(sentence.tokens):
        if token.tag in individual_tags:
            tokens_to_exclude.add(i)
    
    # ì‹œí€€ìŠ¤ íŒ¨í„´ ë§¤ì¹­ í™•ì¸
    for sequence in sequences:
        seq_len = len(sequence)
        for i in range(len(sentence.tokens) - seq_len + 1):
            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):
                for j in range(seq_len):
                    tokens_to_exclude.add(i + j)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸í•  í† í° ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    result_chars = list(sentence.text)
    for i, token in enumerate(sentence.tokens):
        if i in tokens_to_exclude:
            start_pos = token.start - sentence.start
            end_pos = start_pos + token.len
            for j in range(start_pos, end_pos):
                if j < len(result_chars) and result_chars[j] != ' ':
                    result_chars[j] = ' '
    
    filtered_text = ''.join(result_chars)
    return re.sub(r'\s+', ' ', filtered_text)

def filter_specific_terms(strings: List[str]) -> List[str]:
    """
    ì¤‘ë³µë˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì— ìˆëŠ” ìš©ì–´ë“¤ í•„í„°ë§
    
    Args:
        strings: í•„í„°ë§í•  ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: í•„í„°ë§ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)
    
    filtered = []
    for s in unique_strings:
        if not any(s in other for other in filtered):
            filtered.append(s)
    
    return filtered

def convert_df_to_json_list(df):
    """
    DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜
    
    Args:
        df: ë³€í™˜í•  DataFrame
    
    Returns:
        list: ë³€í™˜ëœ JSON êµ¬ì¡° ë¦¬ìŠ¤íŠ¸
    """
    result = []
    grouped = df.groupby('item_name_in_msg')
    
    for item_name_in_msg, group in grouped:
        item_dict = {
            'item_name_in_msg': item_name_in_msg,
            'item_in_voca': []
        }
        
        item_nm_groups = group.groupby('item_nm')
        for item_nm, item_group in item_nm_groups:
            item_ids = list(item_group['item_id'].unique())
            voca_item = {
                'item_nm': item_nm,
                'item_id': item_ids
            }
            item_dict['item_in_voca'].append(voca_item)
        result.append(item_dict)
    
    return result


class MMSExtractor:
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ë©”ì¸ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” MMS ê´‘ê³  ë©”ì‹œì§€ì—ì„œ ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì  ë“±ì„ 
    ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='gemma'):
        """
        MMSExtractor ì´ˆê¸°í™”
        
        Args:
            model_path: ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            product_info_extraction_mode: ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ ('rag', 'llm', 'nlp')
            entity_extraction_mode: ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ ('llm', 'logic')
            offer_info_data_src: ìƒí’ˆ ì •ë³´ ë°ì´í„° ì†ŒìŠ¤ ('local', 'db')
            llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ ('gemma', 'gpt', 'claude')
        """
        # ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else EMBEDDING_CONFIG.ko_sbert_model_path
        self.offer_info_data_src = offer_info_data_src  # 'local' ë˜ëŠ” 'db'
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else PROCESSING_CONFIG.product_info_extraction_mode
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else PROCESSING_CONFIG.entity_extraction_mode
        self.llm_model_name = llm_model
        self.num_cand_pgms = PROCESSING_CONFIG.num_candidate_programs
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        # ì´ˆê¸°í™” ë‹¨ê³„ë³„ ì‹¤í–‰
        self._initialize_device()
        self._initialize_llm()
        self._initialize_embedding_model(self.model_path)
        self._initialize_kiwi()
        self._load_data()

    def _initialize_device(self):
        """
        ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™” (MPS > CUDA > CPU ìˆœì„œë¡œ ì„ íƒ)
        """
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"  # Apple Silicon Mac
        elif torch.cuda.is_available():
            self.device = "cuda"  # NVIDIA GPU
        else:
            self.device = "cpu"   # CPU
        print(f"Using device: {self.device}")

    def _initialize_llm(self):
        """
        ì„ íƒëœ LLM ëª¨ë¸ ì´ˆê¸°í™”
        """
        if self.llm_model_name == "gemma":
            self.llm_model = ChatOpenAI(
                temperature=MODEL_CONFIG.temperature,
                openai_api_key=API_CONFIG.llm_api_key,
                openai_api_base=API_CONFIG.llm_api_url,
                model=MODEL_CONFIG.gemma_model,
                max_tokens=MODEL_CONFIG.llm_max_tokens
            )
        elif self.llm_model_name == "gpt":
            self.llm_model = ChatOpenAI(
                temperature=MODEL_CONFIG.temperature,
                openai_api_key=API_CONFIG.openai_api_key,
                model=MODEL_CONFIG.gpt_model,
                max_tokens=MODEL_CONFIG.llm_max_tokens
            )
        elif self.llm_model_name == "claude":
            self.llm_model = ChatAnthropic(
                temperature=MODEL_CONFIG.temperature,
                api_key=API_CONFIG.anthropic_api_key,
                model=MODEL_CONFIG.claude_model,
                max_tokens=MODEL_CONFIG.llm_max_tokens
            )

        print(f"Initialized LLM: {self.llm_model_name}")

    def _initialize_embedding_model(self, model_path):
        """
        ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ
        """
        self.emb_model = load_sentence_transformer(model_path, self.device)

    def _initialize_kiwi(self):
        """
        Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì œì™¸ íŒ¨í„´ ì„¤ì •
        """
        self.kiwi = Kiwi()
        
        # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
        self.exc_tag_patterns = [
            ['SN', 'NNB'],    # ìˆ«ì + ì˜ì¡´ëª…ì‚¬
            ['W_SERIAL'],     # ì¼ë ¨ë²ˆí˜¸
            ['JKO'],          # ëª©ì ê²© ì¡°ì‚¬
            ['W_URL'],        # URL
            ['W_EMAIL'],      # ì´ë©”ì¼
            ['XSV', 'EC'],    # ë™ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬ + ì—°ê²°ì–´ë¯¸
            ['VV', 'EC'],     # ë™ì‚¬ + ì—°ê²°ì–´ë¯¸
            ['VCP', 'ETM'],   # ê¸ì • ì§€ì •ì‚¬ + ê´€í˜•í˜• ì „ì„±ì–´ë¯¸
            ['XSA', 'ETM'],   # í˜•ìš©ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬ + ê´€í˜•í˜• ì „ì„±ì–´ë¯¸
            ['VV', 'ETN'],    # ë™ì‚¬ + ëª…ì‚¬í˜• ì „ì„±ì–´ë¯¸
            ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], ['SS'], ['SE'], ['SO'], ['SB'], ['SH'],  # ê°ì¢… ê¸°í˜¸
            ['W_HASHTAG']     # í•´ì‹œíƒœê·¸
        ]
        print("Initialized Kiwi morphological analyzer.")

    def _load_data(self):
        """
        í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ (ìƒí’ˆ ì •ë³´, ë³„ì¹­ ê·œì¹™, ì •ì§€ì–´, í”„ë¡œê·¸ë¨ ë¶„ë¥˜, ì¡°ì§ ì •ë³´)
        """
        print("Loading data...")
        
        # ìƒí’ˆ ì •ë³´ ë¡œë“œ (ë¡œì»¬ íŒŒì¼ ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤)
        if self.offer_info_data_src == "local":
            item_pdf_raw = pd.read_csv(METADATA_CONFIG.offer_data_path)
            self.item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','item_dmn']].copy()
            self.item_pdf_all['item_ctg'] = None
            self.item_pdf_all['item_emb_vec'] = None
            self.item_pdf_all['ofer_cd'] = self.item_pdf_all['item_id']
            self.item_pdf_all['oper_dt_hms'] = '20250101000000'
            self.item_pdf_all = self.item_pdf_all.rename(columns={c:c.lower() for c in self.item_pdf_all.columns})
                        
        elif self.offer_info_data_src == "db":
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
            username = os.getenv("DB_USERNAME")
            password = os.getenv("DB_PASSWORD")
            host = os.getenv("DB_HOST")
            port = os.getenv("DB_PORT")
            service_name = os.getenv("DB_NAME")
            
            # Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
            conn = cx_Oracle.connect(user=username, password=password, dsn=dsn, encoding="UTF-8")
            
            # ìƒí’ˆ ì •ë³´ ì¡°íšŒ (ìµœëŒ€ 100ë§Œ ê±´)
            sql = "SELECT * FROM TCAM_RC_OFER_MST WHERE ROWNUM <= 1000000"
            self.item_pdf_all = pd.read_sql(sql, conn)
            conn.close()
            
            # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
            self.item_pdf_all = self.item_pdf_all.rename(columns={c:c.lower() for c in self.item_pdf_all.columns})

        if PROCESSING_CONFIG.excluded_domain_codes_for_items:
            self.item_pdf_all = self.item_pdf_all.query("item_dmn not in @PROCESSING_CONFIG.excluded_domain_codes_for_items")
        # else:
        #     self.item_pdf_all = self.item_pdf_all.copy()

        # ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì ìš©
        alias_pdf = pd.read_csv(METADATA_CONFIG.alias_rules_path)
        alia_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))

        def apply_alias_rule(item_nm):
            """ìƒí’ˆëª…ì— ë³„ì¹­ ê·œì¹™ ì ìš©"""
            item_nm_list = [item_nm]
            for r in alia_rule_set:
                if r[0] in item_nm:
                    item_nm_list.append(item_nm.replace(r[0], r[1]))
                if r[1] in item_nm:
                    item_nm_list.append(item_nm.replace(r[1], r[0]))
            return item_nm_list

        self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm'].apply(apply_alias_rule)
        self.item_pdf_all = self.item_pdf_all.explode('item_nm_alias')
        
        # ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
        user_defined_entity = PROCESSING_CONFIG.user_defined_entities
        item_pdf_ext = pd.DataFrame([
            {'item_nm':e,'item_id':e,'item_desc':e, 'item_dmn':'user_defined', 
             'start_dt':20250101, 'end_dt':99991231, 'rank':1, 'item_nm_alias':e} 
            for e in user_defined_entity
        ])
        # ì£¼ì„: ì»¬ëŸ¼ ë¶ˆì¼ì¹˜ ë¬¸ì œë¡œ ì¸í•´ í˜„ì¬ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        # self.item_pdf_all = pd.concat([self.item_pdf_all,item_pdf_ext])
        
        # ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ
        self.stop_item_names = pd.read_csv(METADATA_CONFIG.stop_items_path)['stop_words'].to_list()

        # Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡
        for w in self.item_pdf_all['item_nm_alias'].unique():
            self.kiwi.add_user_word(w, "NNP")

        # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±
        print("ğŸ”„ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
        self.pgm_pdf = pd.read_csv(METADATA_CONFIG.pgm_info_path)
        self.clue_embeddings = self.emb_model.encode(
            self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                lambda x: preprocess_text(x['pgm_nm'].lower())+" "+x['clue_tag'].lower(), axis=1
            ).tolist(),
            convert_to_tensor=True, show_progress_bar=False
        )
        print("âœ… í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")

        # ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ
        self.org_pdf = pd.read_csv(METADATA_CONFIG.org_info_path, encoding='cp949')
        self.org_pdf['sub_org_cd'] = self.org_pdf['sub_org_cd'].apply(lambda x: str(x).zfill(4))
        print("Data loading complete.")

    def extract_entities_from_kiwi(self, mms_msg):
        """
        Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ
        
        Args:
            mms_msg: ë¶„ì„í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            tuple: (í›„ë³´_ì•„ì´í…œ_ë¦¬ìŠ¤íŠ¸, ì¶”ê°€_ì•„ì´í…œ_ë°ì´í„°í”„ë ˆì„)
        """
        # ë¬¸ì¥ ë¶„í•  ë° í•˜ìœ„ ë¬¸ì¥ ì²˜ë¦¬
        sentences = sum(self.kiwi.split_into_sents(re.split(r"_+", mms_msg), return_tokens=True, return_sub_sents=True), [])
        sentences_all = []
        
        for sent in sentences:
            if sent.subs:
                sentences_all.extend(sent.subs)
            else:
                sentences_all.append(sent)
        
        # ì œì™¸ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ í•„í„°ë§
        sentence_list = [filter_text_by_exc_patterns(sent, self.exc_tag_patterns) for sent in sentences_all]

        # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
        result_msg = self.kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)
        entities_from_kiwi = [
            token.form for token in result_msg 
            if token.tag == 'NNP' and 
               token.form not in self.stop_item_names+['-'] and 
               len(token.form)>=2 and 
               not token.form.lower() in self.stop_item_names
        ]
        entities_from_kiwi = filter_specific_terms(entities_from_kiwi)
        print("ì¶”ì¶œëœ ê°œì²´ëª… (Kiwi):", list(set(entities_from_kiwi)))

        # í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ìœ ì‚¬ ìƒí’ˆëª… ì°¾ê¸°
        similarities_fuzzy = parallel_fuzzy_similarity(
            sentence_list, self.item_pdf_all['item_nm_alias'].unique(), 
            threshold=PROCESSING_CONFIG.fuzzy_threshold,
            text_col_nm='sent', item_col_nm='item_nm_alias', 
            n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=30
        )
        
        if similarities_fuzzy.empty:
            # í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©
            cand_item_list = entities_from_kiwi
            extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[['item_nm','item_nm_alias','item_id']].groupby(["item_nm"])['item_id'].apply(list).reset_index()
            return cand_item_list, extra_item_pdf

        # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ í†µí•œ ì •ë°€ ë§¤ì¹­
        similarities_seq = parallel_seq_similarity(
            sent_item_pdf=similarities_fuzzy, text_col_nm='sent', item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=PROCESSING_CONFIG.batch_size
        )
        
        # ì„ê³„ê°’ ì´ìƒì˜ í›„ë³´ ì•„ì´í…œë“¤ í•„í„°ë§
        cand_items = similarities_seq.query("sim>=@PROCESSING_CONFIG.similarity_threshold and item_nm_alias.str.contains('', case=False) and item_nm_alias not in @self.stop_item_names")
        
        # Kiwiì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ ì¶”ê°€ (ë†’ì€ ì‹ ë¢°ë„ë¡œ ì„¤ì •)
        entities_from_kiwi_pdf = self.item_pdf_all.query("item_nm_alias in @entities_from_kiwi")[['item_nm','item_nm_alias']]
        entities_from_kiwi_pdf['sim'] = 1.0

        # ê²°ê³¼ í†µí•© ë° ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        cand_item_pdf = pd.concat([cand_items, entities_from_kiwi_pdf])
        cand_item_list = cand_item_pdf.sort_values('sim', ascending=False).groupby(["item_nm_alias"])['sim'].max().reset_index(name='final_sim').sort_values('final_sim', ascending=False).query("final_sim>=0.2")['item_nm_alias'].unique()
        extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[['item_nm','item_nm_alias','item_id']].groupby(["item_nm"])['item_id'].apply(list).reset_index()

        return cand_item_list, extra_item_pdf

    def extract_entities_by_logic(self, cand_entities, threshold_for_fuzzy=0.8):
        """
        ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ (í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„)
        
        Args:
            cand_entities: í›„ë³´ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            threshold_for_fuzzy: í¼ì§€ ë§¤ì¹­ ì„ê³„ê°’
        
        Returns:
            DataFrame: ì¶”ì¶œëœ ì—”í‹°í‹°ì™€ ìœ ì‚¬ë„ ì •ë³´
        """
        # í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
        similarities_fuzzy = parallel_fuzzy_similarity(
            cand_entities, self.item_pdf_all['item_nm_alias'].unique(), 
            threshold=threshold_for_fuzzy,
            text_col_nm='item_name_in_msg', item_col_nm='item_nm_alias', 
            n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=30
        )
        
        if similarities_fuzzy.empty:
            return pd.DataFrame()
        
        # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° (s1, s2 ì •ê·œí™” ë°©ì‹ìœ¼ë¡œ ê°ê° ê³„ì‚° í›„ í•©ì‚°)
        cand_entities_sim = parallel_seq_similarity(
            sent_item_pdf=similarities_fuzzy, text_col_nm='item_name_in_msg', item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=30, normalizaton_value='s1'
        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(
            sent_item_pdf=similarities_fuzzy, text_col_nm='item_name_in_msg', item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=30, normalizaton_value='s2'
        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias']).groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')
        
        return cand_entities_sim

    def extract_entities_by_llm(self, msg_text, rank_limit=5):
        """
        LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
        
        Args:
            msg_text: ë¶„ì„í•  ë©”ì‹œì§€ í…ìŠ¤íŠ¸
            rank_limit: ë°˜í™˜í•  ìµœëŒ€ ì—”í‹°í‹° ìˆ˜
        
        Returns:
            DataFrame: LLMì´ ì¶”ì¶œí•œ ì—”í‹°í‹°ì™€ ìœ ì‚¬ë„ ì •ë³´
        """
        from langchain.prompts import PromptTemplate
        
        # ë¡œì§ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ í›„ë³´ ì—”í‹°í‹° ë¨¼ì € ì¶”ì¶œ
        cand_entities_by_sim = self.extract_entities_by_logic([msg_text], threshold_for_fuzzy=PROCESSING_CONFIG.similarity_threshold)['item_nm_alias'].unique()
        
        # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        zero_shot_prompt = PromptTemplate(
            input_variables=["msg","cand_entities"],
            template="""
            Extract all product names, including tangible products, services, promotional events, programs, loyalty initiatives, and named campaigns or event identifiers, from the provided advertisement text.
            Reference the provided candidate entities list as a guide for potential matches. Extract only those terms from the candidate list that appear in the advertisement text and qualify as distinct product names based on the following criteria.
            Consider any named offerings, such as apps, membership programs, events, specific branded items, or campaign names like 'T day' or '0 day', as products if presented as distinct products, services, or promotional entities.
            For terms that may be platforms or brand elements, include them only if they are presented as standalone offerings.
            Avoid extracting base or parent brand names (e.g., 'FLO' or 'POOQ') if they are components of more specific offerings (e.g., 'FLO ì•¤ ë°ì´í„°' or 'POOQ ì•¤ ë°ì´í„°') presented in the text; focus on the full, distinct product or service names as they appear.
            Exclude customer support services, such as customer centers or helplines, even if named in the text.
            Exclude descriptive modifiers or attributes (e.g., terms like "ë””ì§€í„¸ ì „ìš©" that describe a product but are not distinct offerings).
            Exclude sales agency names such as '###ëŒ€ë¦¬ì '.
            If multiple terms refer to closely related promotional events (e.g., a general campaign and its specific instances or dates), include the most prominent or overarching campaign name (e.g., '0 day' as a named event) in addition to specific offerings tied to it, unless they are clearly identical.
            Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering from the candidate list that matches the text.
            Ensure that extracted names are presented exactly as they appear in the original text, without translation into English or any other language.
            Just return a list with matched entities where the entities are separated by commas without any other text.

            ## message:                
            {msg}

            ## Candidate entities:
            {cand_entities}
            """
        )
        
        # LLM ì²´ì¸ ì‹¤í–‰
        chain = zero_shot_prompt | self.llm_model
        cand_entities = chain.invoke({"msg": msg_text, "cand_entities": cand_entities_by_sim}).content

        # LLM ì‘ë‹µ íŒŒì‹± ë° ì •ë¦¬
        cand_entity_list = [e.strip() for e in cand_entities.split(',') if e.strip()]
        cand_entity_list = [e for e in cand_entity_list if e not in self.stop_item_names and len(e)>=2]

        if not cand_entity_list:
            return pd.DataFrame()

        # í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­
        similarities_fuzzy = parallel_fuzzy_similarity(
            cand_entity_list, 
            self.item_pdf_all['item_nm_alias'].unique(), 
            threshold=0.6,
            text_col_nm='item_name_in_msg',
            item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs,
            batch_size=30
        )
        
        if similarities_fuzzy.empty:
            return pd.DataFrame()
        
        # ì •ì§€ì–´ í•„í„°ë§
        similarities_fuzzy = similarities_fuzzy[~similarities_fuzzy['item_nm_alias'].isin(self.stop_item_names)]

        # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ë§¤ì¹­
        cand_entities_sim = parallel_seq_similarity(
            sent_item_pdf=similarities_fuzzy,
            text_col_nm='item_name_in_msg',
            item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs,
            batch_size=30,
            normalizaton_value='s1'
        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(
            sent_item_pdf=similarities_fuzzy,
            text_col_nm='item_name_in_msg',
            item_col_nm='item_nm_alias',
            n_jobs=PROCESSING_CONFIG.n_jobs,
            batch_size=30,
            normalizaton_value='s2'
        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias'])
        
        # ìœ ì‚¬ë„ ì ìˆ˜ í•©ì‚°
        cand_entities_sim = cand_entities_sim.groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')
        cand_entities_sim = cand_entities_sim.query("sim>=1.5").copy()

        # ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ
        cand_entities_sim["rank"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(method='first',ascending=False)
        cand_entities_sim = cand_entities_sim.query(f"rank<={rank_limit}").sort_values(['item_name_in_msg','rank'], ascending=[True,True])

        return cand_entities_sim

    def process_message(self, mms_msg):
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜)
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
                - title: ê´‘ê³  ì œëª©
                - purpose: ê´‘ê³  ëª©ì 
                - product: ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                - channel: ì±„ë„ ì •ë³´ ë¦¬ìŠ¤íŠ¸
                - pgm: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´
        """
        print(f"Processing message: {mms_msg[:100]}...")
        msg = mms_msg.strip()
        
        # Kiwië¥¼ í†µí•œ ì´ˆê¸° ì—”í‹°í‹° ì¶”ì¶œ
        cand_item_list, extra_item_pdf = self.extract_entities_from_kiwi(msg)
        
        # NLP ëª¨ë“œìš© ìƒí’ˆ ìš”ì†Œ ì¤€ë¹„
        product_df = extra_item_pdf.rename(columns={'item_nm':'name'}).query("not name in @self.stop_item_names")[['name']]
        product_df['action'] = 'ê³ ê°ì—ê²Œ ê¸°ëŒ€í•˜ëŠ” í–‰ë™: [êµ¬ë§¤, ê°€ì…, ì‚¬ìš©, ë°©ë¬¸, ì°¸ì—¬, ì½”ë“œì…ë ¥, ì¿ í°ë‹¤ìš´ë¡œë“œ, ê¸°íƒ€] ì¤‘ì—ì„œ ì„ íƒ'
        product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
        
        # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
        mms_embedding = self.emb_model.encode([msg.lower()], convert_to_tensor=True, show_progress_bar=False)
        similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
        
        # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
        pgm_pdf_tmp = self.pgm_pdf.copy()
        pgm_pdf_tmp['sim'] = similarities
        pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
        pgm_cand_info = "\n\t".join(pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm'])+" : "+x['clue_tag'], axis=1).to_list())
        rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_cand_info}" if self.num_cand_pgms > 0 else ""

        # LLM ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‚¬ê³  ê³¼ì • ì •ì˜
        chain_of_thought = """
1. Identify the advertisement's purpose first, using expressions as they appear in the original text.
2. Extract product names based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.
3. Provide channel information considering the extracted product information, preserving original text expressions.
"""

        # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜ (LLM ì‘ë‹µ êµ¬ì¡°í™”ìš©)
        schema_prd = {
            "title": {
                "type": "string",
                "description": "Advertisement title, using the exact expressions as they appear in the original text. Clearly describe the core theme and value proposition of the advertisement."
            },
            "purpose": {
                "type": "array",
                "items": {
                    "type": "string",
                    "enum": ["ìƒí’ˆ ê°€ì… ìœ ë„", "ëŒ€ë¦¬ì /ë§¤ì¥ ë°©ë¬¸ ìœ ë„", "ì›¹/ì•± ì ‘ì† ìœ ë„", "ì´ë²¤íŠ¸ ì‘ëª¨ ìœ ë„", "í˜œíƒ ì•ˆë‚´", "ì¿ í° ì œê³µ ì•ˆë‚´", "ê²½í’ˆ ì œê³µ ì•ˆë‚´", "ìˆ˜ì‹  ê±°ë¶€ ì•ˆë‚´", "ê¸°íƒ€ ì •ë³´ ì œê³µ"]
                },
                "description": "Primary purpose(s) of the advertisement, expressed using the exact terms from the original text where applicable."
            },
            "product": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "Name of the advertised product or service, as it appears in the original text without translation."
                        },
                        "action": {
                            "type": "string",
                            "enum": ["êµ¬ë§¤", "ê°€ì…", "ì‚¬ìš©", "ë°©ë¬¸", "ì°¸ì—¬", "ì½”ë“œì…ë ¥", "ì¿ í°ë‹¤ìš´ë¡œë“œ", "ê¸°íƒ€"],
                            "description": "Expected customer action for the product, derived from the original text context."
                        }
                    }
                },
            "description": "Extract all product names, including tangible products, services, promotional events, programs, loyalty initiatives, and named campaigns or event identifiers, using the exact expressions as they appear in the original text without translation. Consider only named offerings (e.g., apps, membership programs, events, specific branded items, or campaign names like 'T day' or '0 day') presented as distinct products, services, or promotional entities. Include platform or brand elements only if explicitly presented as standalone offerings. Avoid extracting base or parent brand names (e.g., 'FLO' or 'POOQ') if they are components of more specific offerings (e.g., 'FLO ì•¤ ë°ì´í„°' or 'POOQ ì•¤ ë°ì´í„°') presented in the text; focus on the full, distinct product or service names as they appear. Exclude customer support services (e.g., customer centers, helplines). Exclude descriptive modifiers, attributes, or qualifiers (e.g., 'ë””ì§€í„¸ ì „ìš©'). Exclude sales agency names such as '###ëŒ€ë¦¬ì '. If multiple terms refer to closely related promotional events (e.g., a general campaign and its specific instances or dates), include the most prominent or overarching campaign name (e.g., '0 day' as a named event) in addition to specific offerings tied to it, unless they are clearly identical. Prioritize recall over precision, but verify each term is a distinct offering."            },
            "channel": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "type": {
                            "type": "string",
                            "enum": ["URL", "ì „í™”ë²ˆí˜¸", "ì•±", "ëŒ€ë¦¬ì "],
                            "description": "Channel type, as derived from the original text."
                        },
                        "value": {
                            "type": "string",
                            "description": "Specific information for the channel (e.g., URL, phone number, app name, agency name), as it appears in the original text."
                        },
                        "action": {
                            "type": "string",
                            "enum": ["ê°€ì…", "ì¶”ê°€ ì •ë³´", "ë¬¸ì˜", "ìˆ˜ì‹ ", "ìˆ˜ì‹  ê±°ë¶€"],
                            "description": "Purpose of the channel, derived from the original text context."
                        }
                    }
                },
                "description": "Channels provided in the advertisement, including URLs, phone numbers, apps, or agencies, using the exact expressions from the original text where applicable, based on the purpose and products."
            },
            "pgm": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "Select the two most relevant pgm_nm from the advertising classification criteria, using the exact expressions from the criteria, ordered by relevance, based on the message content."
            }
        }

        # ì¶”ì¶œ ê°€ì´ë“œë¼ì¸ ì„¤ì •
        prd_ext_guide = """
* Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering.
* Extract all information (title, purpose, product, channel, pgm) using the exact expressions as they appear in the original text without translation, as specified in the schema.
* If the advertisement purpose includes encouraging agency/store visits, provide agency channel information.
"""

        # ì¶”ì¶œ ëª¨ë“œë³„ ì²˜ë¦¬
        if len(cand_item_list) > 0:
            if self.product_info_extraction_mode == 'rag':
                # RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
                rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                prd_ext_guide += """
* Use the provided candidate product names as a reference to guide product extraction, ensuring alignment with the advertisement content and using exact expressions from the original text.
"""
            elif self.product_info_extraction_mode == 'nlp' and product_element:
                # NLP ëª¨ë“œ: ë¯¸ë¦¬ ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ ì‚¬ìš©
                schema_prd['product'] = product_element
                chain_of_thought = """
1. Identify the advertisement's purpose first, using expressions as they appear in the original text.
2. Extract product information based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.
3. Extract the action field for each product based on the provided name information, derived from the original text context.
4. Provide channel information considering the extracted product information, preserving original text expressions.
"""
                prd_ext_guide += """
* Extract the action field for each product based on the identified product names, using the original text context.
"""

        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        schema_prompt = f"""
Provide the results in the following schema:

{json.dumps(schema_prd, indent=4, ensure_ascii=False)}
"""

        prompt = f"""
Extract the advertisement purpose and product names from the provided advertisement text.

### Advertisement Message ###
{msg}

### Extraction Steps ###
{chain_of_thought}

### Extraction Guidelines ###
{prd_ext_guide}

{schema_prompt}

{rag_context}
"""

        # LLM ì‹¤í–‰ ë° JSON íŒŒì‹±
        result_json_text = self.llm_model.invoke(prompt).content
        json_objects_list = extract_json_objects(result_json_text)
        if not json_objects_list:
            print("LLM did not return a valid JSON object.")
            return {}
        
        json_objects = json_objects_list[0]
        
        # LLM ì‘ë‹µì—ì„œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        product_items = json_objects.get('product', [])
        if isinstance(product_items, dict):
            product_items = product_items.get('items', [])
        
        # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if self.entity_extraction_mode == 'logic':
            # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
            cand_entities = [item['name'] for item in product_items]
            similarities_fuzzy = self.extract_entities_by_logic(cand_entities)
        else:
            # LLM ê¸°ë°˜: LLMì„ í†µí•œ ì—”í‹°í‹° ì¶”ì¶œ
            similarities_fuzzy = self.extract_entities_by_llm(msg)

        final_result = json_objects.copy()
        
        print("Entity from LLM:", [x['name'] for x in product_items])

        # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ìƒí’ˆ ì •ë³´ ë§¤í•‘
        if not similarities_fuzzy.empty:
            # ë†’ì€ ìœ ì‚¬ë„ ì•„ì´í…œë“¤ í•„í„°ë§
            high_sim_items = similarities_fuzzy.query('sim >= 1.5')['item_nm_alias'].unique()
            filtered_similarities = similarities_fuzzy[
                (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &
                (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &
                (~similarities_fuzzy['item_name_in_msg'].isin(self.stop_item_names))
            ]
            # ìƒí’ˆ ì •ë³´ì™€ ë§¤í•‘í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
            product_tag = convert_df_to_json_list(self.item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])) # ëŒ€ë¦¬ì  ì œì™¸
            final_result['product'] = product_tag
        else:
            # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            product_items = json_objects.get('product', [])
            if isinstance(product_items, dict):
                product_items = product_items.get('items', [])
            final_result['product'] = [
                {'item_name_in_msg':d['name'], 'item_in_voca':[{'item_name_in_voca':d['name'], 'item_id': ['#']}]} 
                for d in product_items 
                if d.get('name') and d['name'] not in self.stop_item_names
            ]

        # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
        if self.num_cand_pgms > 0 and 'pgm' in json_objects and isinstance(json_objects['pgm'], list):
            pgm_json = self.pgm_pdf[self.pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')
            final_result['pgm'] = pgm_json

        # ì±„ë„ ì •ë³´ ì²˜ë¦¬ (íŠ¹íˆ ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­)
        channel_tag = []
        channel_items = json_objects.get('channel', [])
        if isinstance(channel_items, dict):
            channel_items = channel_items.get('items', [])

        for d in channel_items:
            if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                org_pdf_cand = parallel_fuzzy_similarity(
                    [preprocess_text(d['value'].lower())], self.org_pdf['org_abbr_nm'].unique(), 
                    threshold=0.5, text_col_nm='org_nm_in_msg', item_col_nm='org_abbr_nm', 
                    n_jobs=PROCESSING_CONFIG.n_jobs, batch_size=PROCESSING_CONFIG.batch_size
                ).drop('org_nm_in_msg', axis=1)

                if not org_pdf_cand.empty:
                    # ì¡°ì§ ì •ë³´ì™€ ë§¤ì¹­
                    org_pdf_cand = self.org_pdf.merge(org_pdf_cand, on=['org_abbr_nm'])
                    org_pdf_cand['sim'] = org_pdf_cand.apply(lambda x: combined_sequence_similarity(d['value'], x['org_nm'])[0], axis=1).round(5)
                    
                    # ëŒ€ë¦¬ì  ì½”ë“œ('D'ë¡œ ì‹œì‘) ìš°ì„  ê²€ìƒ‰
                    org_pdf_tmp = org_pdf_cand.query("org_cd.str.startswith('D') & sim >= @PROCESSING_CONFIG.similarity_threshold", engine='python').sort_values('sim', ascending=False)
                    if org_pdf_tmp.empty:
                        # ëŒ€ë¦¬ì ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰
                        org_pdf_tmp = org_pdf_cand.query("sim>=@PROCESSING_CONFIG.similarity_threshold").sort_values('sim', ascending=False)
                    
                    if not org_pdf_tmp.empty:
                        # ìµœê³  ìˆœìœ„ ì¡°ì§ë“¤ì˜ ì •ë³´ ì¶”ì¶œ
                        org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense',ascending=False)
                        org_pdf_tmp['org_cd_full'] = org_pdf_tmp.apply(lambda x: x['org_cd']+x['sub_org_cd'], axis=1)
                        org_info = org_pdf_tmp.query("rank==1").groupby('org_nm')['org_cd_full'].apply(list).reset_index(name='org_cd').to_dict('records')
                        d['store_info'] = org_info
                    else:
                        d['store_info'] = []
                else:
                    d['store_info'] = []
            else:
                d['store_info'] = []
            channel_tag.append(d)

        final_result['channel'] = channel_tag
        return final_result

if __name__ == '__main__':
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ì¶”ì¶œê¸° ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°')
    parser.add_argument('--message', type=str, help='í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€')
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='local',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm' ,'rag'], default='nlp',
                       help='ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ (nlp: í˜•íƒœì†Œë¶„ì„, llm: LLM ê¸°ë°˜, rag: ê²€ìƒ‰ì¦ê°•ìƒì„±)')
    parser.add_argument('--entity-matching-mode', choices=['logic', 'llm'], default='llm',
                       help='ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œ (logic: ë¡œì§ ê¸°ë°˜, llm: LLM ê¸°ë°˜)')
    parser.add_argument('--llm-model', choices=['gemma', 'gpt', 'claude'], default='gemma',
                       help='ì‚¬ìš©í•  LLM ëª¨ë¸ (gemma: Gemma, gpt: GPT, claude: Claude)')
    
    args = parser.parse_args()
    
    # íŒŒì‹±ëœ ì¸ìë“¤ ì‚¬ìš©
    offer_info_data_src = args.offer_data_source
    product_info_extraction_mode = args.product_info_extraction_mode
    entity_extraction_mode = args.entity_matching_mode
    llm_model = args.llm_model
    
    # ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = MMSExtractor(
        offer_info_data_src=offer_info_data_src, 
        product_info_extraction_mode=product_info_extraction_mode, 
        entity_extraction_mode=entity_extraction_mode, 
        llm_model=llm_model
    )
    
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
    test_text = """
    [SKí…”ë ˆì½¤] ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3 ì•ˆë‚´
    (ê´‘ê³ )[SKT] ìš°ë¦¬ ì•„ì´ ì²« ë²ˆì§¸ ìŠ¤ë§ˆíŠ¸í°, ZEM í‚¤ì¦ˆí°__#04 ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”!
    ìš°ë¦¬ ì•„ì´ ìŠ¤ë§ˆíŠ¸í° ê³ ë¯¼ ì¤‘ì´ì…¨ë‹¤ë©´, ìë…€ ìŠ¤ë§ˆíŠ¸í° ê´€ë¦¬ ì•± ZEMì´ ì„¤ì¹˜ëœ SKTë§Œì˜ ì•ˆì „í•œ í‚¤ì¦ˆí°,
    ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3ìœ¼ë¡œ ìš°ë¦¬ ì•„ì´ ì·¨í–¥ì„ ì €ê²©í•´ ë³´ì„¸ìš”!
    ì‹ í•™ê¸°ë¥¼ ë§ì´í•˜ì—¬ SKí…”ë ˆì½¤ ê³µì‹ ì¸ì¦ ëŒ€ë¦¬ì ì—ì„œ í’ì„±í•œ í˜œíƒì„ ì œê³µí•´ ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤!
    â–  ì£¼ìš” ê¸°ëŠ¥
    1. ì‹¤ì‹œê°„ ìœ„ì¹˜ ì¡°íšŒ
    2. ëª¨ë¥´ëŠ” íšŒì„  ìë™ ì°¨ë‹¨
    3. ìŠ¤ë§ˆíŠ¸í° ì‚¬ìš© ì‹œê°„ ì œí•œ
    4. IP68 ë°©ìˆ˜ ë°©ì§„
    5. ìˆ˜ì—… ì‹œê°„ ìë™ ë¬´ìŒëª¨ë“œ
    6. ìœ í•´ ì½˜í…ì¸  ì°¨ë‹¨
    â–  ê°€ê¹Œìš´ SKí…”ë ˆì½¤ ê³µì‹ ì¸ì¦ ëŒ€ë¦¬ì  ì°¾ê¸°
    http://t-mms.kr/t.do?m=#61&s=30684&a=&u=https://bit.ly/3yQF2hx
    â–  ë¬¸ì˜ : SKT ê³ ê°ì„¼í„°(1558, ë¬´ë£Œ)
    ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ 1504
    """

    # ë©”ì‹œì§€ ì²˜ë¦¬ ë° ê²°ê³¼ ì¶œë ¥
    result = extractor.process_message(test_text)
    
    print("\n" + "="*40)
    print("ìµœì¢… ì¶”ì¶œëœ ì •ë³´")
    print("="*40)
    print(json.dumps(result, indent=4, ensure_ascii=False)) 