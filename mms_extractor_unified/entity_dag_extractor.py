from concurrent.futures import ThreadPoolExecutor
import time
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
import json
import re
# from pygments import highlight
# from pygments.lexers import JsonLexer
# from pygments.formatters import HtmlFormatter
# from IPython.display import HTML
import pandas as pd
# from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from openai import OpenAI
from typing import List, Tuple, Union, Dict, Any
import ast
from rapidfuzz import fuzz, process
import re
import json
import glob
import os
from config import settings

pd.set_option('display.max_colwidth', 500)

llm_api_key = settings.API_CONFIG.llm_api_key
llm_api_url = settings.API_CONFIG.llm_api_url
client = OpenAI(
    api_key = llm_api_key,
    base_url = llm_api_url
)
# from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.schema import AIMessage, HumanMessage, SystemMessage

llm_gem = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gemma_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_ax = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.ax_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_cld = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.claude_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_gen = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gemini_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_gpt = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gpt_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

stop_item_names = pd.read_csv(settings.METADATA_CONFIG.stop_items_path)['stop_words'].to_list()
mms_pdf = pd.read_csv(settings.METADATA_CONFIG.mms_msg_path)
mms_pdf = mms_pdf.astype('str')

import re, networkx as nx
from typing import List, Tuple, Set, Dict

###############################################################################
# 1) ì •ê·œì‹ : ( node ) -[ relation ]-> ( node )
###############################################################################
PAT = re.compile(r"\((.*?)\)\s*-\[(.*?)\]->\s*\((.*?)\)")
NODE_ONLY = re.compile(r"\((.*?)\)\s*$")

###############################################################################
# 2) íŒŒì„œ
###############################################################################

def parse_block(text: str):
    nodes: Set[str] = set()
    edges: List[Tuple[str,str,str]] = []
    for line in filter(None, map(str.strip, text.splitlines())):
        m = PAT.match(line)
        if m:                            # (A) -[rel]-> (B)
            src, rel, dst = map(str.strip, m.groups())
            nodes.update([src, dst])
            edges.append((src, dst, rel))
            continue

        m2 = NODE_ONLY.match(line)       # (A:B:C)
        if m2:                           # â† ë…ë¦½ ë…¸ë“œ
            nodes.add(m2.group(1).strip())
            continue

        raise ValueError(f"ëª»ì½ì€ ë¼ì¸:\n  {line}")
    return list(nodes), edges

###############################################################################
# 3) ë…¸ë“œ ìŠ¤í”Œë¦¬í„° â€“ 3ì¹¸Â·2ì¹¸Â·1ì¹¸ í—ˆìš©
###############################################################################
def split_node(raw: str) -> Dict[str,str]:
    parts = [p.strip() for p in raw.split(":")]
    
    if len(parts) == 1:                       # entity only
        ent, act, kpi = parts[0], "", ""
    elif len(parts) == 2:                     # entity:metric
        ent, act = parts
        kpi = ""
    elif len(parts) == 3:                     # entity:action:metric
        ent, act, kpi = parts
    elif len(parts) == 4:                     # entity:action:behavior:metric
        ent, act, behavior, kpi = parts
        # Combine action and behavior, or handle separately
        act = f"{act}:{behavior}"  # Option 1: combine them
        # Or you could add a new field:
        # return {"entity": ent, "action": act, "behavior": behavior, "metric": kpi}
    elif len(parts) >= 5:                     # Handle even more parts flexibly
        ent = parts[0]
        kpi = parts[-1]  # Last part is usually the metric
        act = ":".join(parts[1:-1])  # Everything in between becomes action
    else:
        # This shouldn't happen with len(parts) >= 1, but just in case
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë…¸ë“œ í˜•ì‹: {raw}")
    
    return {"entity": ent, "action": act, "metric": kpi}


###############################################################################
# 4) DAG ë¹Œë”
###############################################################################
def build_dag(nodes: List[str], edges: List[Tuple[str,str,str]]) -> nx.DiGraph:
    g = nx.DiGraph()
    
    # Add nodes with error handling
    for n in nodes:
        try:
            node_attrs = split_node(n)
            g.add_node(n, **node_attrs)
        except ValueError as e:
            print(f"Warning: {e}")
            # Add node with minimal attributes if parsing fails
            g.add_node(n, entity=n, action="", metric="")
    
    # Add edges
    for src, dst, rel in edges:
        g.add_edge(src, dst, relation=rel)
    
    # Optional: Check if it's a DAG
    # if not nx.is_directed_acyclic_graph(g):
    #     raise nx.NetworkXUnfeasible("ì‚¬ì´í´ì´ ìˆìŠµë‹ˆë‹¤ â€• DAG ì•„ë‹˜!")
    
    return g


###############################################################################
# 5) Path Finder
###############################################################################
def get_root_to_leaf_paths(dag):
    """Generate all paths from root nodes (no predecessors) to leaf nodes (no successors)"""
    # Find root nodes (no incoming edges)
    root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]
    
    # Find leaf nodes (no outgoing edges)
    leaf_nodes = [node for node in dag.nodes() if dag.out_degree(node) == 0]
    
    all_paths = []
    for root in root_nodes:
        for leaf in leaf_nodes:
            try:
                paths = list(nx.all_simple_paths(dag, root, leaf))
                all_paths.extend(paths)
            except nx.NetworkXNoPath:
                continue
    
    return all_paths, root_nodes, leaf_nodes

import random

def extract_dag(num_msgs=50, llm_model_nm='ax'):

    if llm_model_nm == 'ax':
        llm_model = llm_ax
    elif llm_model_nm == 'gem':
        llm_model = llm_gem
    elif llm_model_nm == 'cld':
        llm_model = llm_cld
    elif llm_model_nm == 'gen':
        llm_model = llm_gen
    elif llm_model_nm == 'gpt':
        llm_model = llm_gpt

    # ì¶œë ¥ì„ íŒŒì¼ì— ì €ì¥í•˜ê¸° ìœ„í•œ ì„¤ì •
    output_file = "/Users/1110566/workspace/AgenticWorkflow/mms_extractor_unified/dag_extraction_output.txt"

    line_break_patterns = {"__":"\n", "â– ":"\nâ– ", "â–¶":"\nâ–¶", "_":"\n"}
    
    with open(output_file, 'a', encoding='utf-8') as f:
        # ì‹¤í–‰ ì‹œì‘ ì‹œì  ê¸°ë¡
        from datetime import datetime
        start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        f.write(f"\n{'='*80}\n")
        f.write(f"DAG ì¶”ì¶œ ì‹¤í–‰ ì‹œì‘: {start_time}\n")
        f.write(f"{'='*80}\n\n")
        
        for msg in random.sample(mms_pdf.query("msg.str.contains('')")['msg'].unique().tolist(), num_msgs):
            try:
    #             msg = """
    # [SKí…”ë ˆì½¤] ê°•ë‚¨í„°ë¯¸ë„ëŒ€ë¦¬ì  ë³¸ì  ê°¤ëŸ­ì‹œ S25 ì‚¬ì „ì˜ˆì•½ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.
    # (ê´‘ê³ )[SKT] ê°•ë‚¨í„°ë¯¸ë„ëŒ€ë¦¬ì  ë³¸ì  ê°¤ëŸ­ì‹œ S25 ì‚¬ì „ì˜ˆì•½ ì•ˆë‚´__ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”. _ìƒˆë¡œìš´ ì‹œì‘, ì„¤ë ˆì´ëŠ” 1ì›”! SKí…”ë ˆì½¤ ê°•ë‚¨í„°ë¯¸ë„ ëŒ€ë¦¬ì ì´ ê³ ê°ë‹˜ì˜ íŠ¹ë³„í•œ ìƒˆí•´ë¥¼ ì‘ì›í•©ë‹ˆë‹¤._ê³§ ì¶œì‹œí•˜ëŠ” ì‚¼ì„±ì˜ ìµœì‹  í”Œë˜ê·¸ì‹­ ìŠ¤ë§ˆíŠ¸í° ê°¤ëŸ­ì‹œ S25 ì‚¬ì „ì˜ˆì•½ í˜œíƒ ë°›ì•„ ê°€ì„¸ìš”.__â–  ìƒˆ í•™ê¸° ë§ì´ í‚¤ì¦ˆí° íŠ¹ë³„ í–‰ì‚¬_- ì›”ì •ì•¡ ìš”ê¸ˆ ë° ê¸°ê¸° í• ì¸ ìµœëŒ€ ì„¤ê³„_- 12ê°œì›” ì•½ì •__â–  ê°¤ëŸ­ì‹œ S25 ì‚¬ì „ì˜ˆì•½ ì¤‘!_- ê°œí†µì¼ : 2ì›”4ì¼_- ë”ë¸” ìŠ¤í† ë¦¬ì§€, ì›Œì¹˜7 ë“± í‘¸ì§í•œ ì‚¬ì€ í˜œíƒì€ ì•„ë˜ ë§¤ì¥ ì—°ë½ì²˜ë¡œ ë¬¸ì˜ì£¼ì„¸ìš”._- ì˜ˆì•½ ì„ ë¬¼ë„ ì±™ê¸°ì‹œê³ , ì¢‹ì€ ì¡°ê±´ìœ¼ë¡œ êµ¬ë§¤ ìƒë‹´ë„ ë°›ì•„ ë³´ì„¸ìš”.__â–  ê°¤ëŸ­ì‹œ S24 ë§ˆì§€ë§‰ ì°¬ìŠ¤_- ìš”ê¸ˆ ë° ê¸°ê¸° í• ì¸ ìµœëŒ€ ì„¤ê³„_- ì›Œì¹˜7 ë¬´ë£Œ ì¦ì • (â€»í”„ë¼ì„ ìš”ê¸ˆì œ ì‚¬ìš© ê¸°ì¤€)__â–  ì¸í„°ë„·+TVê²°í•© í˜œíƒ_- 60ë§Œ ì› ìƒë‹¹ì˜ ìµœëŒ€ ì‚¬ì€í’ˆ ì¦ì •_- ì›” ìµœì € ìš”ê¸ˆ ì„¤ê³„__â–  ê°•ë‚¨í„°ë¯¸ë„ëŒ€ë¦¬ì  ë³¸ì _- ì£¼ì†Œ : ì„œìš¸ì‹œ ì„œì´ˆêµ¬ ì‹ ë°˜í¬ë¡œ 176, 1ì¸µ 130í˜¸ (ì‹ ì„¸ê³„ë°±í™”ì  ì˜†, ì„¼íŠ¸ëŸ´ì‹œí‹°ë‚´ í˜¸ë‚¨ì„  í•˜ì°¨ì¥ ì•„ì›ƒë°± ì•„ë˜ 1ì¸µ)_- ì—°ë½ì²˜ : 02-6282-1011_â–¶ ë§¤ì¥ í™ˆí˜ì´ì§€/ì˜ˆì•½/ìƒë‹´ : http://t-mms.kr/t.do?m=#61&s=30251&a=&u=http://tworldfriends.co.kr/D145410000__â–  ë¬¸ì˜: SKT ê³ ê°ì„¼í„°(1558, ë¬´ë£Œ)_SKTì™€ í•¨ê»˜ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.__ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ 1504
    #             """

                for pattern, replacement in line_break_patterns.items():
                    msg = msg.replace(pattern, replacement)

                prompt_1 = f"""
## ì‘ì—…
í†µì‹ ì‚¬ ê´‘ê³  ë©”ì‹œì§€ì—ì„œ ê°œì²´ëª…ê³¼ ê¸°ëŒ€ í–‰ë™ì„ ì¶”ì¶œí•˜ê³  DAG í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹
- **ë…ë¦½ ë…¸ë“œ**: `(ê°œì²´ëª…:ê¸°ëŒ€í–‰ë™)`
- **ê´€ê³„ ë…¸ë“œ**: `(ê°œì²´ëª…:ê¸°ëŒ€í–‰ë™) -[ê´€ê³„ë™ì‚¬]-> (ê°œì²´ëª…:ê¸°ëŒ€í–‰ë™)`

## ê°œì²´ëª… ìœ í˜• ë° ì˜ˆì‹œ
### ğŸ“± ì œí’ˆ/ë‹¨ë§ê¸°
- ìŠ¤ë§ˆíŠ¸í°: ê°¤ëŸ­ì‹œS24, ì•„ì´í°15, ê°¤ëŸ­ì‹œí´ë”ë¸”6, ê°¤ëŸ­ì‹œì›Œì¹˜, ê°¤ëŸ­ì‹œë²„ì¦ˆ
- ê¸°íƒ€ ê¸°ê¸°: ZEMê¾¸ëŸ¬ë¯¸í°, í‚¤ì¦ˆí°, ì‹¤ë²„í°, íƒœë¸”ë¦¿

### ğŸ“ ì„œë¹„ìŠ¤/ìš”ê¸ˆì œ
- ìš”ê¸ˆì œ: 5Gìš”ê¸ˆì œ, í”„ë¼ì„ìš”ê¸ˆì œ, Tí”Œëœ, ZEMìš”ê¸ˆì œ
- í†µì‹ ì„œë¹„ìŠ¤: ì¸í„°ë„·, IPTV, ADTìº¡ìŠ¤, ìš°ì£¼íŒ¨ìŠ¤, ì—ì´ë‹·
- ë¶€ê°€ì„œë¹„ìŠ¤: Vì»¬ëŸ¬ë§, ì½œí‚¤í¼, í†µí™”ê°€ëŠ¥í†µë³´í”ŒëŸ¬ìŠ¤

### ğŸ í˜œíƒ/í• ì¸
- í• ì¸: 50%í• ì¸, 10ë§Œì›í• ì¸, ìš”ê¸ˆí• ì¸, ê¸°ê¸°ê°’í• ì¸
- í˜œíƒ: ì‚¬ì€í’ˆ, ì¿ í°, í¬ì¸íŠ¸ì ë¦½, ë¬´ë£Œì²´í—˜
- êµ¬ì²´ì  í˜œíƒ: ê°¤ëŸ­ì‹œì›Œì¹˜ì¦ì •, ì—ì–´íŒŸì¦ì •, ì¶©ì „ê¸°ì„¸íŠ¸

### ğŸ¢ ì¥ì†Œ/ë§¤ì¥
- ì˜¨ë¼ì¸: Të‹¤ì´ë ‰íŠ¸ìƒµ, ì˜¨ë¼ì¸ëª°, í™ˆí˜ì´ì§€, ì•±
- ì˜¤í”„ë¼ì¸: SKTëŒ€ë¦¬ì , Tì›”ë“œë§¤ì¥, êµ¬ì²´ì ë§¤ì¥ëª…(ì˜ˆ: ê°•ë‚¨ì )

### ğŸ‰ ì´ë²¤íŠ¸/í”„ë¡œëª¨ì…˜
- ê¸°ê°„ ì´ë²¤íŠ¸: ë´„ë§ì´í–‰ì‚¬, ì‹ ê·œê°€ì…ì´ë²¤íŠ¸, ì‚¬ì „ì˜ˆì•½ì´ë²¤íŠ¸
- ë©¤ë²„ì‹­: Të©¤ë²„ì‹­, ë‹¨ê³¨ë“±ë¡, ì¹œêµ¬ì¶”ê°€

### ì£¼ì˜ ì‚¬í•­
- ê´‘ê³  íƒ€ê²Ÿì€ ê°œì²´ëª…ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
- ì¼ì •/ê¸°ê°„ì€ ê°œì²´ëª…ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”.

## ê¸°ëŒ€ í–‰ë™ (í‘œì¤€í™”ëœ 10ê°œ ë™ì‚¬)
**[êµ¬ë§¤, ê°€ì…, ì‚¬ìš©, ë°©ë¬¸, ì°¸ì—¬, ë“±ë¡, ë‹¤ìš´ë¡œë“œ, í™•ì¸, ìˆ˜ë ¹, ì ë¦½]**

## ê´€ê³„ ë™ì‚¬ ê°€ì´ë“œë¼ì¸

### ğŸ”¥ ì¡°ê±´ë¶€ ê´€ê³„ (ìµœìš°ì„  ì‚¬ìš©)
**ì¡°ê±´ ì¶©ì¡± ì‹œ í˜œíƒ ì œê³µì„ ëª…í™•íˆ í‘œí˜„**
- `ê°€ì…í•˜ë©´`, `êµ¬ë§¤í•˜ë©´`, `ë°©ë¬¸í•˜ë©´`, `ì‹ ì²­í•˜ë©´`, `ë“±ë¡í•˜ë©´`
- `ê°€ì…ì‹œ`, `êµ¬ë§¤ì‹œ`, `ë“±ë¡ì‹œ`, `ì‚¬ìš©ì‹œ`, `ë°©ë¬¸ì‹œ`
- `ê°€ì…í›„`, `êµ¬ë§¤í›„`, `ì™„ë£Œí›„`, `ì„¤ì¹˜í›„`

### ğŸ’ í˜œíƒ ì œê³µ ê´€ê³„
**í˜œíƒ/ë³´ìƒ ìˆ˜ë ¹ì„ í‘œí˜„**
- `ì¦ì •ë°›ë‹¤`, `í• ì¸ë°›ë‹¤`, `ì œê³µë°›ë‹¤`, `ì§€ì›ë°›ë‹¤`
- `ìˆ˜ë ¹í•˜ë‹¤`, `ì ë¦½í•˜ë‹¤`, `ë°›ë‹¤`

### ğŸ”— ì—°ê²°/ê²½ë¡œ ê´€ê³„
**ì„œë¹„ìŠ¤ ê°„ ì—°ê²°ì´ë‚˜ ê²½ë¡œë¥¼ í‘œí˜„**
- `í†µí•´`, `í†µí•˜ì—¬`, `ì´ìš©í•˜ì—¬`, `í™œìš©í•˜ì—¬`
- `í•¨ê»˜`, `ê²°í•©í•˜ì—¬`, `ì—°ê²°í•˜ì—¬`, `ë™ì‹œê°€ì…`

### âš¡ í–‰ë™ ìœ ë„ ê´€ê³„
**íŠ¹ì • í–‰ë™ì„ ìœ ë„í•˜ëŠ” ê´€ê³„**
- `ì°¸ì—¬í•˜ì—¬`, `ì²´í—˜í•˜ì—¬`, `ì‹ ì²­í•˜ì—¬`
- `ë¬¸ì˜í•˜ì—¬`, `í™•ì¸í•˜ì—¬`, `ì•ˆë‚´ë°›ì•„`

### ğŸ“± í”Œë«í¼/ì±„ë„ ê´€ê³„
**íŠ¹ì • í”Œë«í¼ì´ë‚˜ ì±„ë„ì„ í†µí•œ ì ‘ê·¼**
- `ì ‘ì†í•˜ì—¬`, `ë‹¤ìš´ë¡œë“œí•˜ì—¬`, `ì„¤ì¹˜í•˜ì—¬`
- `ë¡œê·¸ì¸í•˜ì—¬`, `ì¸ì¦í•˜ì—¬`

## ê³ ê¸‰ ì¶”ì¶œ ê·œì¹™

### âœ… ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  ìš”ì†Œ
1. **Root Node**: ì‚¬ìš©ìê°€ ì‹œì‘í•  ìˆ˜ ìˆëŠ” í–‰ë™ (ë°©ë¬¸, ê°€ì…, ë‹¤ìš´ë¡œë“œ ë“±)
2. **ì¡°ê±´ë¶€ í˜œíƒ**: "~í•˜ë©´ ~ë°›ì„ ìˆ˜ ìˆë‹¤" êµ¬ì¡°
3. **ì—°ì‡„ í˜œíƒ**: A â†’ B â†’ C í˜•íƒœì˜ ë‹¤ë‹¨ê³„ í˜œíƒ
4. **ì„ íƒì  ì˜µì…˜**: ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ íƒ1 ìƒí™©

### âŒ ì œì™¸í•´ì•¼ í•  ìš”ì†Œ
1. ì¼ë°˜ì ì¸ ì •ë³´ì„± ë©˜íŠ¸ ("ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤")
2. ì—°ë½ì²˜, ì£¼ì†Œ ë“± ë©”íƒ€ì •ë³´
3. ë²•ì  ê³ ì§€ì‚¬í•­ ("ìˆ˜ì‹ ê±°ë¶€", "ìœ ì˜ì‚¬í•­")
4. ì¤‘ë³µë˜ëŠ” ìœ ì‚¬í•œ í˜œíƒ

### ğŸ¯ ê°œì²´ëª… ì •ê·œí™” ê·œì¹™
1. **êµ¬ì²´ì  ëª…ì¹­ ì‚¬ìš©**: "ìŠ¤ë§ˆíŠ¸í°" â†’ "ê°¤ëŸ­ì‹œS24"
2. **ì¼ê´€ëœ í‘œê¸°**: "ê°¤ëŸ­ì‹œ S24" â†’ "ê°¤ëŸ­ì‹œS24" (ë„ì–´ì“°ê¸° ì œê±°)
3. **ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€**: "5ë§Œì›í• ì¸ì¿ í°" (ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ)
4. **ë¸Œëœë“œëª… í¬í•¨**: "ì‚¼ì„±ì¼€ì–´í”ŒëŸ¬ìŠ¤", "Të©¤ë²„ì‹­"

### ğŸ”„ ê´€ê³„ ë°©í–¥ì„± ì›ì¹™
1. **ì‹œê°„ ìˆœì„œ**: ë¨¼ì € ì¼ì–´ë‚˜ëŠ” í–‰ë™ â†’ ë‚˜ì¤‘ í–‰ë™
2. **ì¡°ê±´ê³¼ ê²°ê³¼**: ì¡°ê±´ í–‰ë™ â†’ ê²°ê³¼ í˜œíƒ
3. **ì˜ì¡´ì„±**: ì „ì œ ì¡°ê±´ â†’ ìˆ˜í–‰ ê°€ëŠ¥í•œ í–‰ë™

## ì¶œë ¥ ì˜ˆì‹œ

### ë‹¨ìˆœí•œ ì¡°ê±´ë¶€ í˜œíƒ
```
(ê°¤ëŸ­ì‹œS24:êµ¬ë§¤) -[êµ¬ë§¤ì‹œ]-> (50%í• ì¸:ìˆ˜ë ¹)
(Të©¤ë²„ì‹­:ê°€ì…) -[ê°€ì…í•˜ë©´]-> (ë§¤ì›”í• ì¸:ìˆ˜ë ¹)
```

### ë³µí•©ì  ì—°ì‡„ ê´€ê³„
```
(SKTëŒ€ë¦¬ì :ë°©ë¬¸) -[ë°©ë¬¸í•˜ì—¬]-> (ìƒë‹´:í™•ì¸)
(ìƒë‹´:í™•ì¸) -[ì™„ë£Œí›„]-> (ê°¤ëŸ­ì‹œS24:êµ¬ë§¤)
(ê°¤ëŸ­ì‹œS24:êµ¬ë§¤) -[êµ¬ë§¤ì‹œ]-> (ê°¤ëŸ­ì‹œì›Œì¹˜:ìˆ˜ë ¹)
```

### ë‹¤ì¤‘ ì„ íƒ ê´€ê³„
```
(ìš°ì£¼íŒ¨ìŠ¤:ê°€ì…) -[ê°€ì…ì‹œ]-> (Netflix:ì‚¬ìš©)
(ìš°ì£¼íŒ¨ìŠ¤:ê°€ì…) -[ê°€ì…ì‹œ]-> (Wavve:ì‚¬ìš©)
(ìš°ì£¼íŒ¨ìŠ¤:ê°€ì…) -[ê°€ì…ì‹œ]-> (YouTube Premium:ì‚¬ìš©)
```

### í”Œë«í¼ ì—°ê³„
```
(Tì›”ë“œì•±:ë‹¤ìš´ë¡œë“œ) -[ë‹¤ìš´ë¡œë“œí•˜ì—¬]-> (ì¿ í°:ìˆ˜ë ¹)
(ì¿ í°:ìˆ˜ë ¹) -[ì‚¬ìš©í•˜ì—¬]-> (30%í• ì¸:ìˆ˜ë ¹)
```

## ë¶„ì„ ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸
â–  Root Node ì‹ë³„: ì‚¬ìš©ìê°€ ì‹œì‘í•  ìˆ˜ ìˆëŠ” í–‰ë™ì´ ìˆëŠ”ê°€?
â–  ì¡°ê±´ë¶€ ê´€ê³„: "~í•˜ë©´", "~ì‹œ" êµ¬ì¡°ê°€ ëª…í™•í•œê°€?
â–  í˜œíƒ ì—°ì‡„: ì—¬ëŸ¬ ë‹¨ê³„ì˜ í˜œíƒì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ê°€?
â–  ê°œì²´ëª… êµ¬ì²´ì„±: ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì  ëª…ì¹­ì„ ì‚¬ìš©í–ˆëŠ”ê°€?
â–  ê´€ê³„ ë°©í–¥ì„±: ì‹œê°„ìˆœì„œì™€ ì˜ì¡´ì„±ì´ ì˜¬ë°”ë¥¸ê°€?
â–  ì¤‘ë³µ ì œê±°: ê°™ì€ ì˜ë¯¸ì˜ ë…¸ë“œê°€ ì¤‘ë³µë˜ì§€ ì•Šì•˜ëŠ”ê°€?

**ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìœ„ í˜•ì‹ìœ¼ë¡œ DAGë§Œì„ ì¶œë ¥í•˜ì„¸ìš”. mermaid í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

## message:
{msg}
"""

                # ë©”ì‹œì§€ ì¶œë ¥
                msg_header = "==="*15+" Message "+"==="*15
                print(msg_header)
                f.write(msg_header + "\n")
                print(msg)
                f.write(msg + "\n")
                
                # DAG ì¶œë ¥
                dag_header = "==="*15+f" DAG ({llm_model_nm.upper()}) "+"==="*15
                print(dag_header)
                f.write(dag_header + "\n")
                dag_raw = llm_model.invoke(prompt_1).content
                print(dag_raw)
                f.write(dag_raw + "\n")

                nodes, edges = parse_block(re.sub(r'^```|```$', '', dag_raw.strip()))
                dag = build_dag(nodes, edges)

                # Root Nodes ì¶œë ¥
                root_header = "==="*15+" Root Nodes "+"==="*15
                print(root_header)
                f.write(root_header + "\n")
                root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]
                for root in root_nodes:
                    node_data = dag.nodes[root]
                    root_info = f"  {root} | {node_data}"
                    print(root_info)
                    f.write(root_info + "\n")

                # Paths ì¶œë ¥
                paths_header = "==="*15+" Paths "+"==="*15
                print(paths_header)
                f.write(paths_header + "\n")
                paths, roots, leaves = get_root_to_leaf_paths(dag)

                for i, path in enumerate(paths):
                    path_info = f"\nPath {i+1}:"
                    print(path_info)
                    f.write(path_info + "\n")
                    for j, node in enumerate(path):
                        if j < len(path) - 1:
                            edge_data = dag.get_edge_data(node, path[j+1])
                            relation = edge_data['relation'] if edge_data else ''
                            node_info = f"  {node}"
                            relation_info = f"    --[{relation}]-->"
                            print(node_info)
                            print(relation_info)
                            f.write(node_info + "\n")
                            f.write(relation_info + "\n")
                        else:
                            final_node = f"  {node}"
                            print(final_node)
                            f.write(final_node + "\n")

                separator = "\n" + "#"*100 + "\n"
                print(separator)
                f.write(separator)

            except Exception as e:
                print(f"Error: {e}")
                f.write(f"Error: {e}\n")
                continue
    
    print(f"ì¶œë ¥ì´ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='DAG ì¶”ì¶œê¸°')
    parser.add_argument('--num_msgs', type=int, default=50, help='ì¶”ì¶œí•  ë©”ì‹œì§€ ìˆ˜')
    parser.add_argument('--llm_model', type=str, default='ax', help='ì‚¬ìš©í•  LLM ëª¨ë¸')
    args = parser.parse_args()
    extract_dag(num_msgs=args.num_msgs, llm_model_nm=args.llm_model)