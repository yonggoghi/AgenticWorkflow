{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "# from pygments import highlight\n",
        "# from pygments.lexers import JsonLexer\n",
        "# from pygments.formatters import HtmlFormatter\n",
        "# from IPython.display import HTML\n",
        "import pandas as pd\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from config import settings\n",
        "\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "llm_api_key = settings.API_CONFIG.llm_api_key\n",
        "llm_api_url = settings.API_CONFIG.llm_api_url\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "llm_gem3 = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.gemma_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens\n",
        "        )\n",
        "\n",
        "llm_chat = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"gpt-4.1\",\n",
        "        openai_api_key=settings.API_CONFIG.openai_api_key,\n",
        "        max_tokens=2000,\n",
        ")\n",
        "llm_cld40 = ChatAnthropic(\n",
        "    api_key=settings.API_CONFIG.anthropic_api_key,\n",
        "    model=settings.ModelConfig.claude_model,\n",
        "    max_tokens=3000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \"\"\"\n",
        "    More advanced JSON repair that handles edge cases better\n",
        "    \"\"\"\n",
        "    json_str = broken_json\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+)\\s*:', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix unquoted values more carefully\n",
        "    # Split on quotes to avoid modifying content inside strings\n",
        "    parts = json_str.split('\"')\n",
        "    \n",
        "    for i in range(0, len(parts), 2):  # Only process parts outside quotes (even indices)\n",
        "        # Fix unquoted values in this part\n",
        "        parts[i] = re.sub(r':\\s*([a-zA-Z0-9_]+)(?=\\s*[,\\]\\}])', r': \"\\1\"', parts[i])\n",
        "    \n",
        "    json_str = '\"'.join(parts)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/gemma3-12b-it\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n",
        "\n",
        "def convert_to_custom_format(json_items):\n",
        "    custom_format = []\n",
        "    \n",
        "    for item in json_items:\n",
        "        item_name = item.get(\"item_name_in_message\", \"\")\n",
        "        item_id = item.get(\"item_id\", \"\")\n",
        "        category = item.get(\"category\", \"\")\n",
        "        \n",
        "        # Create custom format for each item\n",
        "        custom_line = f\"[Item Name] {item_name} [Item ID] {item_id} [Item Category] {category}\"\n",
        "        custom_format.append(custom_line)\n",
        "    \n",
        "    return \"\\n\".join(custom_format)\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "    # Replace URLs with an empty string\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_custom_pattern(text, keyword=\"바로가기\"):\n",
        "    # Create a pattern that matches any text followed by the specified keyword\n",
        "    # We escape the keyword to handle any special regex characters it might contain\n",
        "    escaped_keyword = re.escape(keyword)\n",
        "    pattern = re.compile(r'.*? ' + escaped_keyword)\n",
        "    \n",
        "    # Replace the matched pattern with an empty string\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "def select_most_comprehensive(strings):\n",
        "    \"\"\"\n",
        "    Select the most comprehensive string from a list of overlapping strings.\n",
        "    Returns the longest string that contains other strings as substrings.\n",
        "    \n",
        "    Args:\n",
        "        strings: List of strings to filter\n",
        "        \n",
        "    Returns:\n",
        "        List of most comprehensive strings (usually one, but could be multiple if no containment)\n",
        "    \"\"\"\n",
        "    if not strings:\n",
        "        return []\n",
        "    \n",
        "    # Remove duplicates and sort by length (longest first)\n",
        "    unique_strings = list(set(strings))\n",
        "    unique_strings.sort(key=len, reverse=True)\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for current in unique_strings:\n",
        "        # Check if current string contains any of the strings already in result\n",
        "        is_contained = any(current in existing for existing in result)\n",
        "        \n",
        "        # Check if current string contains other strings not yet in result\n",
        "        contains_others = any(other in current for other in unique_strings if other != current and other not in result)\n",
        "        \n",
        "        # If current is not contained by existing results and either:\n",
        "        # 1. It contains other strings, or \n",
        "        # 2. No strings contain each other (keep all unique)\n",
        "        if not is_contained:\n",
        "            # Remove any strings from result that are contained in current\n",
        "            result = [r for r in result if r not in current]\n",
        "            result.append(current)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def replace_special_chars_comprehensive(text):\n",
        "    \"\"\"\n",
        "    More comprehensive: Handle various types of special characters.\n",
        "    \"\"\"\n",
        "    # Replace common punctuation with space\n",
        "    punctuation_pattern = r'[!@#$%^&*()_+\\-=\\[\\]{};\\':\"\\\\|,.<>?/~`]'\n",
        "    text = re.sub(punctuation_pattern, ' ', text)\n",
        "    \n",
        "    # Replace other special symbols\n",
        "    symbol_pattern = r'[₩＄￦※◆▲▼◀▶★☆♪♫♬♩♭♯]'\n",
        "    text = re.sub(symbol_pattern, ' ', text)\n",
        "    \n",
        "    # Replace various dashes and quotes\n",
        "    dash_quote_pattern = r'[—–‒―\"\"''‚„‹›«»]'\n",
        "    text = re.sub(dash_quote_pattern, ' ', text)\n",
        "    \n",
        "    # Clean up multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수문자를 공백으로 변환\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # 여러 공백을 하나로 통일\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 앞뒤 공백 제거\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "def advanced_sequential_similarity(str1, str2, metrics=None, visualize=False):\n",
        "    \"\"\"\n",
        "    Calculate multiple character-level similarity metrics between two strings.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    str1 : str\n",
        "        First string\n",
        "    str2 : str\n",
        "        Second string\n",
        "    metrics : list\n",
        "        List of metrics to compute. Options:\n",
        "        ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "        If None, all metrics will be computed\n",
        "    visualize : bool\n",
        "        If True, visualize the differences between strings\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing similarity scores for each metric\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "    results = {}\n",
        "    # Handle empty strings\n",
        "    if not str1 or not str2:\n",
        "        return {metric: 0.0 for metric in metrics+['overall']}\n",
        "    # Prepare strings\n",
        "    s1, s2 = str1.lower(), str2.lower()\n",
        "    # 1. N-gram similarity (with multiple window sizes)\n",
        "    if 'ngram' in metrics:\n",
        "        ngram_scores = {}\n",
        "        for window in range(min([len(s1),len(s2),2]), min([5,max([len(s1),len(s2)])+1])):\n",
        "            # Skip if strings are shorter than window\n",
        "            if len(s1) < window or len(s2) < window:\n",
        "                ngram_scores[f'window_{window}'] = 0.0\n",
        "                continue\n",
        "            # Generate character n-grams\n",
        "            ngrams1 = [s1[i:i+window] for i in range(len(s1) - window + 1)]\n",
        "            ngrams2 = [s2[i:i+window] for i in range(len(s2) - window + 1)]\n",
        "            # Count matches\n",
        "            matches = sum(1 for ng in ngrams1 if ng in ngrams2)\n",
        "            max_possible = max(len(ngrams1), len(ngrams2))\n",
        "            # Normalize\n",
        "            score = matches / max_possible if max_possible > 0 else 0.0\n",
        "            ngram_scores[f'window_{window}'] = score\n",
        "        # Average of all n-gram scores\n",
        "        results['ngram'] = max(ngram_scores.values())#sum(ngram_scores.values()) / len(ngram_scores)\n",
        "        results['ngram_details'] = ngram_scores\n",
        "    # 2. Longest Common Substring (LCS)\n",
        "    if 'lcs' in metrics:\n",
        "        def longest_common_substring(s1, s2):\n",
        "            # Dynamic programming approach\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            max_length = 0\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                        max_length = max(max_length, dp[i][j])\n",
        "            return max_length\n",
        "        lcs_length = longest_common_substring(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['lcs'] = lcs_length / max_length if max_length > 0 else 0.0\n",
        "    # 3. Longest Common Subsequence\n",
        "    if 'subsequence' in metrics:\n",
        "        def longest_common_subsequence(s1, s2):\n",
        "            # Dynamic programming approach for subsequence\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "            return dp[m][n]\n",
        "        subseq_length = longest_common_subsequence(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['subsequence'] = subseq_length / max_length if max_length > 0 else 0.0\n",
        "    # 4. SequenceMatcher from difflib\n",
        "    if 'difflib' in metrics:\n",
        "        sm = SequenceMatcher(None, s1, s2)\n",
        "        results['difflib'] = sm.ratio()\n",
        "    # Visualization of differences\n",
        "    if visualize:\n",
        "        try:\n",
        "            # Only works in notebooks or environments that support plotting\n",
        "            sm = SequenceMatcher(None, s1, s2)\n",
        "            matches = sm.get_matching_blocks()\n",
        "            # Prepare for visualization\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            # Draw strings as horizontal bars\n",
        "            ax.barh(0, len(s1), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            ax.barh(1, len(s2), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            # Draw matching parts\n",
        "            for match in matches:\n",
        "                i, j, size = match\n",
        "                if size > 0:  # Ignore zero-length matches\n",
        "                    ax.barh(0, size, height=0.4, left=i, color='green', alpha=0.5)\n",
        "                    ax.barh(1, size, height=0.4, left=j, color='green', alpha=0.5)\n",
        "                    # Draw connection lines between matches\n",
        "                    ax.plot([i + size/2, j + size/2], [0.2, 0.8], 'k-', alpha=0.3)\n",
        "            # Add string texts\n",
        "            for i, c in enumerate(s1):\n",
        "                ax.text(i + 0.5, 0, c, ha='center', va='center')\n",
        "            for i, c in enumerate(s2):\n",
        "                ax.text(i + 0.5, 1, c, ha='center', va='center')\n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_yticklabels(['String 1', 'String 2'])\n",
        "            ax.set_xlabel('Character Position')\n",
        "            ax.set_title('Character-Level String Comparison')\n",
        "            ax.grid(False)\n",
        "            plt.tight_layout()\n",
        "            # plt.show()  # Uncomment to display\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "    # Calculate overall similarity score (average of all metrics)\n",
        "    metrics_to_average = [m for m in results.keys() if not m.endswith('_details')]\n",
        "    results['overall'] = sum(results[m] for m in metrics_to_average) / len(metrics_to_average)\n",
        "    return results\n",
        "# advanced_sequential_similarity('시크릿', '시크릿', metrics='ngram')\n",
        "# advanced_sequential_similarity('에이닷_자사', '에이닷')['overall']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import difflib\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "def longest_common_subsequence_ratio(s1, s2, normalizaton_value):\n",
        "    \"\"\"\n",
        "    Calculate similarity based on longest common subsequence (LCS).\n",
        "    Preserves order and gives high scores for substring relationships.\n",
        "    \"\"\"\n",
        "    def lcs_length(x, y):\n",
        "        m, n = len(x), len(y)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if x[i-1] == y[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    lcs_len = lcs_length(s1, s2)\n",
        "    if normalizaton_value == 'max':\n",
        "        max_len = max(len(s1), len(s2))\n",
        "        return lcs_len / max_len if max_len > 0 else 1.0\n",
        "    elif normalizaton_value == 'min':\n",
        "        min_len = min(len(s1), len(s2))\n",
        "        return lcs_len / min_len if min_len > 0 else 1.0\n",
        "    elif normalizaton_value == 's1':\n",
        "        return lcs_len / len(s1) if len(s1) > 0 else 1.0\n",
        "    elif normalizaton_value == 's2':\n",
        "        return lcs_len / len(s2) if len(s2) > 0 else 1.0\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid normalization value: {normalizaton_value}\")\n",
        "\n",
        "# def sequence_matcher_similarity(s1, s2):\n",
        "#     \"\"\"\n",
        "#     Use Python's built-in SequenceMatcher which considers sequence order.\n",
        "#     \"\"\"\n",
        "#     return SequenceMatcher(None, s1, s2).ratio()\n",
        "\n",
        "def sequence_matcher_similarity(s1, s2, normalizaton_value):\n",
        "    \"\"\"Normalize by minimum length (favors shorter strings)\"\"\"\n",
        "    matcher = difflib.SequenceMatcher(None, s1, s2)\n",
        "    matches = sum(triple.size for triple in matcher.get_matching_blocks())\n",
        "\n",
        "    normalization_length = min(len(s1), len(s2))\n",
        "    if normalizaton_value == 'max':\n",
        "        normalization_length = max(len(s1), len(s2))\n",
        "    elif normalizaton_value == 's1':\n",
        "        normalization_length = len(s1)\n",
        "    elif normalizaton_value == 's2':\n",
        "        normalization_length = len(s2)\n",
        "        \n",
        "    if normalization_length == 0: \n",
        "        return 0.0\n",
        "    \n",
        "    return matches / normalization_length\n",
        "\n",
        "def substring_aware_similarity(s1, s2, normalizaton_value):\n",
        "    \"\"\"\n",
        "    Custom similarity that heavily weights substring relationships\n",
        "    while considering sequence order.\n",
        "    \"\"\"\n",
        "    # Check if one is a substring of the other\n",
        "    if s1 in s2 or s2 in s1:\n",
        "        shorter = min(s1, s2, key=len)\n",
        "        longer = max(s1, s2, key=len)\n",
        "        # High base score for substring relationship\n",
        "        base_score = len(shorter) / len(longer)\n",
        "        # Bonus for exact substring match\n",
        "        return min(0.95 + base_score * 0.05, 1.0)\n",
        "    \n",
        "    # Use LCS ratio for non-substring cases\n",
        "    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)\n",
        "\n",
        "def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\\s_\\-]+'):\n",
        "    \"\"\"\n",
        "    Tokenize strings and calculate similarity based on token sequence overlap.\n",
        "    Good for product names with separators.\n",
        "    \"\"\"\n",
        "    tokens1 = re.split(separator_pattern, s1.strip())\n",
        "    tokens2 = re.split(separator_pattern, s2.strip())\n",
        "    \n",
        "    # Remove empty tokens\n",
        "    tokens1 = [t for t in tokens1 if t]\n",
        "    tokens2 = [t for t in tokens2 if t]\n",
        "    \n",
        "    if not tokens1 or not tokens2:\n",
        "        return 0.0\n",
        "    \n",
        "    # Find longest common subsequence of tokens\n",
        "    def token_lcs_length(t1, t2):\n",
        "        m, n = len(t1), len(t2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if t1[i-1] == t2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    lcs_tokens = token_lcs_length(tokens1, tokens2)\n",
        "    normalization_tokens = max(len(tokens1), len(tokens2))\n",
        "    if normalizaton_value == 'min':\n",
        "        normalization_tokens = min(len(tokens1), len(tokens2))\n",
        "    elif normalizaton_value == 's1':\n",
        "        normalization_tokens = len(tokens1)\n",
        "    elif normalizaton_value == 's2':\n",
        "        normalization_tokens = len(tokens2)\n",
        "    \n",
        "    # print(normalizaton_value, normalization_tokens, lcs_tokens)\n",
        "        \n",
        "    return lcs_tokens / normalization_tokens  \n",
        "\n",
        "def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):\n",
        "    \"\"\"\n",
        "    Combine multiple sequence-aware similarity measures.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            'substring': 0.4,\n",
        "            'sequence_matcher': 0.4,\n",
        "            'token_sequence': 0.2\n",
        "        }\n",
        "    \n",
        "    similarities = {\n",
        "        'substring': substring_aware_similarity(s1, s2, normalizaton_value),\n",
        "        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),\n",
        "        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)\n",
        "    }\n",
        "    \n",
        "    combined = sum(similarities[key] * weights[key] for key in weights)\n",
        "    return combined, similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def fuzzy_similarities(text, entities):\n",
        "    results = []\n",
        "    for entity in entities:\n",
        "        scores = {\n",
        "            'ratio': fuzz.ratio(text, entity) / 100,\n",
        "            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,\n",
        "            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,\n",
        "            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100\n",
        "        }\n",
        "        max_score = max(scores.values())\n",
        "        results.append((entity, max_score))\n",
        "    return results\n",
        "\n",
        "def get_fuzzy_similarities(args_dict):\n",
        "    text = args_dict['text']\n",
        "    entities = args_dict['entities']\n",
        "    threshold = args_dict['threshold']\n",
        "    text_col_nm = args_dict['text_col_nm']\n",
        "    item_col_nm = args_dict['item_col_nm']\n",
        "\n",
        "    # Get similarities using auto method selection\n",
        "    text_processed = preprocess_text(text.lower())\n",
        "    similarities = fuzzy_similarities(text_processed, entities)\n",
        "    \n",
        "    # Filter by threshold and create DataFrame\n",
        "    filtered_results = [\n",
        "        {\n",
        "            text_col_nm: text,\n",
        "            item_col_nm: entity, \n",
        "            \"sim\": score\n",
        "        } \n",
        "        for entity, score in similarities \n",
        "        if score >= threshold\n",
        "    ]\n",
        "    \n",
        "    return filtered_results\n",
        "\n",
        "def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):\n",
        "    \"\"\"\n",
        "    Batched version for better performance with large datasets.\n",
        "    \"\"\"\n",
        "    if n_jobs is None:\n",
        "        n_jobs = min(os.cpu_count()-1, 8)  # Limit to 8 jobs max\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = max(1, len(entities) // (n_jobs * 2))\n",
        "        \n",
        "    # Create batches\n",
        "    batches = []\n",
        "    for text in texts:\n",
        "        for i in range(0, len(entities), batch_size):\n",
        "            batch = entities[i:i + batch_size]\n",
        "            batches.append({\"text\": text, \"entities\": batch, \"threshold\": threshold, \"text_col_nm\": text_col_nm, \"item_col_nm\": item_col_nm})\n",
        "    \n",
        "    # print(f\"Processing {len(item_list)} items in {len(batches)|} batches with {n_jobs} jobs...\")\n",
        "    \n",
        "    # Run parallel jobs\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)\n",
        "    \n",
        "    # # Flatten results\n",
        "    # similarities = []\n",
        "    # for batch_result in batch_results:\n",
        "    #     similarities.extend(batch_result)\n",
        "    \n",
        "    return pd.DataFrame(sum(batch_results, []))\n",
        "\n",
        "def calculate_seq_similarity(args_dict):\n",
        "    \"\"\"\n",
        "    Process a batch of items in one job for better efficiency.\n",
        "    \"\"\"\n",
        "    sent_item_batch = args_dict['sent_item_batch']\n",
        "    text_col_nm = args_dict['text_col_nm']\n",
        "    item_col_nm = args_dict['item_col_nm']\n",
        "    normalizaton_value = args_dict['normalizaton_value']\n",
        "    \n",
        "    results = []\n",
        "    for sent_item in sent_item_batch:\n",
        "        sent = sent_item[text_col_nm]\n",
        "        item = sent_item[item_col_nm]\n",
        "        try:\n",
        "            sent_processed = preprocess_text(sent.lower())\n",
        "            item_processed = preprocess_text(item.lower())\n",
        "            similarity = combined_sequence_similarity(sent_processed, item_processed, normalizaton_value=normalizaton_value)[0]\n",
        "            results.append({text_col_nm:sent, item_col_nm:item, \"sim\":similarity})\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item}: {e}\")\n",
        "            results.append({text_col_nm:sent, item_col_nm:item, \"sim\":0.0})\n",
        "    \n",
        "    return results\n",
        "\n",
        "def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, normalizaton_value='s2'):\n",
        "    \"\"\"\n",
        "    Batched version for better performance with large datasets.\n",
        "    \"\"\"\n",
        "    if n_jobs is None:\n",
        "        n_jobs = min(os.cpu_count()-1, 8)  # Limit to 8 jobs max\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))\n",
        "        \n",
        "    # Create batches\n",
        "    batches = []\n",
        "    for i in range(0, sent_item_pdf.shape[0], batch_size):\n",
        "        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')\n",
        "        batches.append({\"sent_item_batch\": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'normalizaton_value': normalizaton_value})\n",
        "    \n",
        "    # print(f\"Processing {len(item_list)} items in {len(batches)|} batches with {n_jobs} jobs...\")\n",
        "    \n",
        "    # Run parallel jobs\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)\n",
        "    \n",
        "    # Flatten results\n",
        "    # similarities = []\n",
        "    # for batch_result in batch_results:\n",
        "    #     similarities.extend(batch_result)\n",
        "    \n",
        "    return pd.DataFrame(sum(batch_results, []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "def save_embeddings_numpy(embeddings, texts, filename):\n",
        "    \"\"\"\n",
        "    Save embeddings as NumPy arrays (.npz format).\n",
        "    Most common and efficient method.\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(embeddings):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    np.savez_compressed(\n",
        "        filename,\n",
        "        embeddings=embeddings,\n",
        "        texts=texts,\n",
        "        timestamp=str(datetime.now())\n",
        "    )\n",
        "    print(f\"✅ Saved embeddings to {filename}\")\n",
        "def load_embeddings_numpy(filename):\n",
        "    \"\"\"Load embeddings from NumPy .npz file.\"\"\"\n",
        "    data = np.load(filename, allow_pickle=True)\n",
        "    embeddings = data['embeddings']\n",
        "    texts = data['texts']\n",
        "    timestamp = data['timestamp'] if 'timestamp' in data else None\n",
        "    print(f\"✅ Loaded {len(embeddings)} embeddings from {filename}\")\n",
        "    if timestamp:\n",
        "        print(f\"   Created: {timestamp}\")\n",
        "    return embeddings, texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "def save_sentence_transformer(model_name, save_path):\n",
        "    \"\"\"Download and save SentenceTransformer model locally\"\"\"\n",
        "    print(f\"Downloading {model_name}...\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    # Save the model\n",
        "    model.save(save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "    return model\n",
        "def load_sentence_transformer(model_path, device=None):\n",
        "    \"\"\"Load SentenceTransformer model from local path\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = SentenceTransformer(model_path).to(device)\n",
        "    print(f\"Model loaded on {device}\")\n",
        "    return model\n",
        "# Usage\n",
        "# Save model (do this once)\n",
        "# model = save_sentence_transformer('jhgan/ko-sbert-nli', './models/ko-sbert-nli')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1\n",
            "MPS available: True\n",
            "MPS built: True\n",
            "✅ MPS is available and ready to use!\n",
            "Loading model from ./models/ko-sbert-nli...\n",
            "Model loaded on mps\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def check_mps_availability():\n",
        "    \"\"\"Check if MPS is available on this Mac.\"\"\"\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
        "    \n",
        "    if torch.backends.mps.is_available():\n",
        "        print(\"✅ MPS is available and ready to use!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"❌ MPS is not available. Using CPU instead.\")\n",
        "        return False\n",
        "\n",
        "mps_available = check_mps_availability()\n",
        "    \n",
        "# Determine device\n",
        "if mps_available:\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# emb_model = SentenceTransformer('jhgan/ko-sbert-nli').to(device)\n",
        "emb_model = load_sentence_transformer('./models/ko-sbert-nli', device)\n",
        "# emb_model = SentenceTransformer('jhgan/ko-sroberta-multitask').to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_pdf_raw = pd.read_csv(settings.METADATA_CONFIG.offer_data_path)\n",
        "\n",
        "item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','item_dmn']].copy()\n",
        "item_pdf_all['item_ctg'] = None\n",
        "item_pdf_all['item_emb_vec'] = None\n",
        "item_pdf_all['ofer_cd'] = item_pdf_all['item_id']\n",
        "item_pdf_all['oper_dt_hms'] = '20250101000000'\n",
        "\n",
        "item_pdf_all = item_pdf_all.rename(columns={c:c.lower() for c in item_pdf_all.columns})\n",
        "\n",
        "if settings.PROCESSING_CONFIG.excluded_domain_codes_for_items:\n",
        "    item_pdf_all = item_pdf_all.query(\"item_dmn not in @settings.PROCESSING_CONFIG.excluded_domain_codes_for_items\")\n",
        "\n",
        "# item_pdf_all.query(\"rank<1000\")[['item_nm']].drop_duplicates().to_csv(\"./data/item_nm_1000.csv\", index=False)\n",
        "alias_pdf = pd.read_csv(settings.METADATA_CONFIG.alias_rules_path)\n",
        "alia_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))\n",
        "\n",
        "def apply_alias_rule(item_nm):\n",
        "    item_nm_list = [item_nm]\n",
        "\n",
        "    for r in alia_rule_set:\n",
        "        if r[0] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[0], r[1]))\n",
        "        if r[1] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[1], r[0]))\n",
        "    return item_nm_list\n",
        "\n",
        "item_pdf_all['item_nm_alias'] = item_pdf_all['item_nm'].apply(apply_alias_rule)\n",
        "\n",
        "item_pdf_all = item_pdf_all.explode('item_nm_alias')\n",
        "\n",
        "user_defined_entity = ['AIA Vitality' , '부스트 파크 건대입구' , 'Boost Park 건대입구']\n",
        "item_pdf_ext = pd.DataFrame([{'item_nm':e,'item_id':e,'item_desc':e, 'item_dmn':'user_defined', 'start_dt':20250101, 'end_dt':99991231, 'rank':1, 'item_nm_alias':e} for e in user_defined_entity])\n",
        "item_pdf_all = pd.concat([item_pdf_all,item_pdf_ext])\n",
        "\n",
        "stop_item_names = pd.read_csv(settings.METADATA_CONFIG.stop_items_path)['stop_words'].to_list()\n",
        "\n",
        "entity_vocab = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    if row['item_nm_alias'] in stop_item_names:\n",
        "        continue\n",
        "    entity_vocab.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'item_dmn':row['item_dmn'],'item_nm_alias':row['item_nm_alias']}))\n",
        "\n",
        "entity_list_for_fuzzy = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    entity_list_for_fuzzy.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'item_dmn':row['item_dmn'], 'start_dt':row['start_dt'], 'end_dt':row['end_dt'], 'rank':1, 'item_nm_alias':row['item_nm_alias']}))\n",
        "\n",
        "# text_list_item = [preprocess_text(x).lower() for x in item_pdf_all['item_nm_alias'].tolist()]\n",
        "# item_embeddings = emb_model.encode(text_list_item\n",
        "#                             # ,batch_size=64  # Optimal for MPS\n",
        "#                             ,convert_to_tensor=True\n",
        "#                             ,show_progress_bar=True)\n",
        "\n",
        "# save_embeddings_numpy(item_embeddings, text_list_item, './data/item_embeddings_250527.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "mms_pdf = pd.read_csv(settings.METADATA_CONFIG.mms_msg_path)\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "mms_pdf = mms_pdf.reset_index()\n",
        "mms_pdf = mms_pdf.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "num_cand_pgms = 5\n",
        "pgm_pdf = pd.read_csv(settings.METADATA_CONFIG.pgm_info_path)\n",
        "clue_embeddings = emb_model.encode(pgm_pdf[[\"pgm_nm\",\"clue_tag\"]].apply(lambda x: preprocess_text(x['pgm_nm'].lower())+\" \"+x['clue_tag'].lower(), axis=1).tolist()\n",
        "                            # ,batch_size=64  # Optimal for MPS\n",
        "                            ,convert_to_tensor=True\n",
        "                            ,show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "org_pdf = pd.read_csv(settings.METADATA_CONFIG.org_info_path, encoding='cp949')\n",
        "org_pdf['sub_org_cd'] = org_pdf['sub_org_cd'].apply(lambda x: x.zfill(4))\n",
        "# text_list_org_all = org_pdf[[\"org_abbr_nm\",\"bas_addr\",\"dtl_addr\"]].apply(lambda x: preprocess_text(x['org_abbr_nm'].lower())+\" \"+x['bas_addr'].lower()+\" \"+x['dtl_addr'].lower(), axis=1).tolist()\n",
        "# org_all_embeddings = emb_model.encode(text_list_org_all\n",
        "#                     # ,batch_size=32  # Optimal for MPS\n",
        "#                     ,convert_to_tensor=True\n",
        "#                     ,show_progress_bar=True)\n",
        "# save_embeddings_numpy(org_all_embeddings, text_list_org_all, './data/org_all_embeddings_250605.npz')\n",
        "# text_list_org_nm = org_pdf[[\"org_abbr_nm\"]].apply(lambda x: preprocess_text(x['org_abbr_nm'].lower()), axis=1).tolist()\n",
        "# org_nm_embeddings = emb_model.encode(text_list_org_nm\n",
        "#                     # ,batch_size=32  # Optimal for MPS\n",
        "#                     ,convert_to_tensor=True\n",
        "#                     ,show_progress_bar=True)\n",
        "# save_embeddings_numpy(org_nm_embeddings, text_list_org_nm, './data/org_nm_embeddings_250605.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# item_embeddings, text_list_item = load_embeddings_numpy('./data/item_embeddings_250527.npz')\n",
        "# org_all_embeddings, text_list_org_all = load_embeddings_numpy('./data/org_all_embeddings_250605.npz')\n",
        "# org_nm_embeddings, text_list_org_nm = load_embeddings_numpy('./data/org_nm_embeddings_250605.npz')\n",
        "# item_embeddings = torch.from_numpy(item_embeddings).to(device)\n",
        "# org_all_embeddings = torch.from_numpy(org_all_embeddings).to(device)\n",
        "# org_nm_embeddings = torch.from_numpy(org_nm_embeddings).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_df_to_json_list(df):\n",
        "    \"\"\"\n",
        "    Convert DataFrame to the specific JSON structure you want\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    # Group by 'item_name_in_msg' to create the main structure\n",
        "    grouped = df.groupby('item_name_in_msg')\n",
        "    for item_name_in_msg, group in grouped:\n",
        "        # Create the main item dictionary\n",
        "        item_dict = {\n",
        "            'item_name_in_msg': item_name_in_msg,\n",
        "            'item_in_voca': []\n",
        "        }\n",
        "        # Group by item_nm within each item_name_in_msg to collect item_ids\n",
        "        item_nm_groups = group.groupby('item_nm')\n",
        "        for item_nm, item_group in item_nm_groups:\n",
        "            # Collect all item_ids for this item_nm\n",
        "            item_ids = list(item_group['item_id'].unique())\n",
        "            voca_item = {\n",
        "                'item_nm': item_nm,\n",
        "                'item_id': item_ids\n",
        "            }\n",
        "            item_dict['item_in_voca'].append(voca_item)\n",
        "        result.append(item_dict)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">개채명 추출기 (Kiwi)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "# Simple approach\n",
        "kiwi = Kiwi()\n",
        "entity_list_for_kiwi = list(item_pdf_all['item_nm_alias'].unique())\n",
        "for w in entity_list_for_kiwi:\n",
        "    kiwi.add_user_word(w, \"NNP\")\n",
        "# for w in stop_item_names:\n",
        "#     kiwi.add_user_word(w, \"NNG\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_text_by_exc_patterns(sentence, exc_tag_patterns):\n",
        "    \"\"\"\n",
        "    Create a new text by replacing tokens that match exclusion tag patterns with whitespace.\n",
        "    Handles both individual tags and consecutive tag sequences.\n",
        "    Preserves original whitespace from the source text.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Separate individual tags from sequences\n",
        "    individual_tags = set()\n",
        "    sequences = []\n",
        "    \n",
        "    for pattern in exc_tag_patterns:\n",
        "        if isinstance(pattern, list):\n",
        "            if len(pattern) == 1:\n",
        "                individual_tags.add(pattern[0])\n",
        "            else:\n",
        "                sequences.append(pattern)\n",
        "        else:\n",
        "            individual_tags.add(pattern)\n",
        "    \n",
        "    # Track which tokens to exclude\n",
        "    tokens_to_exclude = set()\n",
        "    \n",
        "    # Check for individual tag matches\n",
        "    for i, token in enumerate(sentence.tokens):\n",
        "        if token.tag in individual_tags:\n",
        "            tokens_to_exclude.add(i)\n",
        "    \n",
        "    # Check for sequence matches\n",
        "    for sequence in sequences:\n",
        "        seq_len = len(sequence)\n",
        "        for i in range(len(sentence.tokens) - seq_len + 1):\n",
        "            # Check if consecutive tokens match the sequence\n",
        "            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):\n",
        "                # Mark all tokens in this sequence for exclusion\n",
        "                for j in range(seq_len):\n",
        "                    tokens_to_exclude.add(i + j)\n",
        "    \n",
        "    # Create a character array from the original text\n",
        "    result_chars = list(sentence.text)\n",
        "    \n",
        "    # Replace excluded tokens with whitespace while preserving original whitespace\n",
        "    for i, token in enumerate(sentence.tokens):\n",
        "        if i in tokens_to_exclude:\n",
        "            # Replace token characters with spaces, but keep original whitespace intact\n",
        "            start_pos = token.start - sentence.start  # Adjust for sentence start offset\n",
        "            end_pos = start_pos + token.len\n",
        "            for j in range(start_pos, end_pos):\n",
        "                if j < len(result_chars) and result_chars[j] != ' ':\n",
        "                    result_chars[j] = ' '\n",
        "    \n",
        "    # Join the character array to create filtered text\n",
        "    filtered_text = ''.join(result_chars)\n",
        "\n",
        "    #Replace consecutive whitespaces with a single whitespace\n",
        "    filtered_text = re.sub(r'\\s+', ' ', filtered_text)\n",
        "    \n",
        "    return filtered_text\n",
        "\n",
        "# Define Token and Sentence classes\n",
        "class Token:\n",
        "    def __init__(self, form, tag, start, length):\n",
        "        self.form = form\n",
        "        self.tag = tag\n",
        "        self.start = start\n",
        "        self.len = length\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, text, start, end, tokens, subs=None):\n",
        "        self.text = text\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.tokens = tokens\n",
        "        self.subs = subs or []\n",
        "\n",
        "exc_tag_patterns = [['SN', 'NNB'],\n",
        " ['W_SERIAL'],\n",
        " ['JKO'],\n",
        " ['W_URL'],\n",
        " ['W_EMAIL'],\n",
        " ['XSV', 'EC'],\n",
        " ['VV', 'EC'],\n",
        " ['VCP', 'ETM'],\n",
        " ['XSA', 'ETM'],\n",
        " ['VV', 'ETN'],\n",
        " ['W_SERIAL'],\n",
        " ['W_URL'],\n",
        " ['JKO'],\n",
        " ['SSO'],\n",
        " ['SSC'],\n",
        " ['SW'],\n",
        " ['SF'],\n",
        " ['SP'],\n",
        " ['SS'],\n",
        " ['SE'],\n",
        " ['SO'],\n",
        " ['SB'],\n",
        " ['SH'],\n",
        " ['W_HASHTAG']\n",
        " ]\n",
        "\n",
        "# sentence = sentences[1]\n",
        "\n",
        "# Apply the filtering\n",
        "# filtered_text = filter_text_by_exc_patterns(sentence, exc_tag_patterns)\n",
        "\n",
        "# print(\"Original text:\", repr(sentence.text))\n",
        "# print(\"Filtered text:\", repr(filtered_text))\n",
        "\n",
        "# # Show which tokens were excluded\n",
        "# print(\"\\nToken analysis:\")\n",
        "# individual_tags = set()\n",
        "# sequences = []\n",
        "\n",
        "# for pattern in exc_tag_patterns:\n",
        "#     if isinstance(pattern, list):\n",
        "#         if len(pattern) == 1:\n",
        "#             individual_tags.add(pattern[0])\n",
        "#         else:\n",
        "#             sequences.append(pattern)\n",
        "#     else:\n",
        "#         individual_tags.add(pattern)\n",
        "\n",
        "# print(\"Individual exclusion tags:\", individual_tags)\n",
        "# print(\"Sequence exclusion patterns:\", sequences)\n",
        "# print()\n",
        "\n",
        "# for i, token in enumerate(sentence.tokens):\n",
        "#     status = \"EXCLUDED\" if token.tag in individual_tags else \"KEPT\"\n",
        "#     print(f\"Token {i}: '{token.form}' ({token.tag}) at pos {token.start}-{token.start + token.len - 1} - {status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "msg_text_list = [\"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:통화 부가서비스를 패키지로 저렴하게!\n",
        "광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ',\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "(광고)[SKT] 1월 0 day 혜택 안내_ _[1월 20일(토) 혜택]_만 13~34세 고객이라면 _CU에서 핫바 1,000원에 구매 하세요!_(선착순 1만 명 증정)_▶ 자세히 보기 : http://t-mms.kr/t.do?m=#61&s=24264&a=&u=https://bit.ly/3H2OHSs__■ 에이닷 X T 멤버십 구독캘린더 이벤트_0 day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요! _알림 설정하면 추첨을 통해 [스타벅스 카페 라떼tall 모바일쿠폰]을 드립니다. _▶ 이벤트 참여하기 : https://bit.ly/3RVSojv_ _■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "'[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:[SK텔레콤] T건강습관 X AIA Vitality, 우리 가족의 든든한 보험!\n",
        "광고 내용:(광고)[SKT] 가족의 든든한 보험 (무배당)AIA Vitality 베스트핏 보장보험 안내  고객님, 안녕하세요. 4인 가족 표준생계비, 준비하고 계시나요? (무배당)AIA Vitality 베스트핏 보장보험(디지털 전용)으로 최대 20% 보험료 할인과 가족의 든든한 보험 보장까지 누려 보세요.   ▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/36oWjgX  ■ AIA Vitality  혜택 - 매달 리워드 최대 12,000원 - 등급 업그레이드 시 특별 리워드 - T건강습관 제휴 할인 최대 40% ※ 제휴사별 할인 조건과 주간 미션 달성 혜택 등 자세한 내용은 AIA Vitality 사이트에서 확인하세요. ※ 이 광고는 AIA생명의 광고이며 SK텔레콤은 모집 행위를 하지 않습니다.  - 보험료 납입 기간 중 피보험자가 장해분류표 중 동일한 재해 또는 재해 이외의 동일한 원인으로 여러 신체 부위의 장해지급률을 더하여 50% 이상인 장해 상태가 된 경우 차회 이후의 보험료 납입 면제 - 사망보험금은 계약일(부활일/효력회복일)로부터 2년 안에 자살한 경우 보장하지 않음 - 일부 특약 갱신 시 보험료 인상 가능 - 기존 계약 해지 후 신계약 체결 시 보험인수 거절, 보험료 인상, 보장 내용 변경 가능 - 해약 환급금(또는 만기 시 보험금이나 사고보험금)에 기타 지급금을 합해 5천만 원까지(본 보험 회사 모든 상품 합산) 예금자 보호 - 계약 체결 전 상품 설명서 및 약관 참조 - 월 보험료 5,500원(부가세 포함)  * 생명보험협회 심의필 제2020-03026호(2020-09-22) COM-2020-09-32426  ■문의: 청약 관련(1600-0880)  무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "[SK텔레콤]추석맞이 추가할인 쿠폰 증정\n",
        "(광고)[SKT]공식인증매장 고촌점 추석맞이 행사__안녕하세요 고객님!_고촌역 1번 출구 고촌파출소 방향 100m SK텔레콤 대리점 입니다._스마트폰 개통, 인터넷/TV 설치 시 조건 없이 추가 할인 행사를 진행합니다.__■삼성 갤럭시 Z플립5/Z폴드5는_  9월 내내 즉시개통 가능!!_1.갤럭시 워치6 개통 시 추가 할인_2.삼성케어+ 파손보장 1년권_3.삼성 정품 악세사리 30% 할인 쿠폰_4.정품 보호필름 1회 무료 부착__■새로운 아이폰15 출시 전_  아이폰14 재고 대방출!!_1.투명 범퍼 케이스 증정_2.방탄 유리 필름 부착_3.25W C타입 PD 충전기__여기에 5만원 추가 할인 적용!!__■기가인터넷+IPTV 가입 시_1.최대 36만원 상당 상품권 지급_2.스마트폰 개통 시 10만원 할인_3.매장 특별 사은품 지급_(특별 사은품은 매장 상황에 따라 변경될 수 있습니다)__■SKT 공식인증매장 고촌점_- 주소: 경기 김포시 고촌읍 장차로 3, SK텔레콤_- 연락처: 0507-1480-7833_- 네이버 예약하기: http://t-mms.kr/bSo/#74_- 매장 홈페이지: http://t-mms.kr/bSt/#74__■ 문의 : SKT 고객센터(1558, 무료)_무료 수신거부 1504_\n",
        "\"\"\"\n",
        "]\n",
        "message_idx = 0\n",
        "mms_msg = msg_text_list[message_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_specific_terms(strings: List[str]) -> List[str]:\n",
        "    unique_strings = list(set(strings))  # 중복 제거\n",
        "    unique_strings.sort(key=len, reverse=True)  # 길이 기준 내림차순 정렬\n",
        "\n",
        "    filtered = []\n",
        "    for s in unique_strings:\n",
        "        if not any(s in other for other in filtered):\n",
        "            filtered.append(s)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def extract_entities_from_kiwi(mms_msg, item_pdf_all, stop_item_names):\n",
        "    sentences = sum(kiwi.split_into_sents(re.split(r\"_+\",mms_msg), return_tokens=True, return_sub_sents=True), [])\n",
        "    # sentence_list = [sent.text.strip() for sent in sentences if sent.text.strip()]\n",
        "\n",
        "    sentences_all = []\n",
        "    for sent in sentences:\n",
        "        # print(sent.text.strip())\n",
        "        # print(\"-\"*100)\n",
        "        if sent.subs:\n",
        "            for sub_sent in sent.subs:\n",
        "                sentences_all.append(sub_sent)\n",
        "        else:\n",
        "            sentences_all.append(sent)\n",
        "\n",
        "    sentence_list = []\n",
        "    for sent in sentences_all:\n",
        "        # print(sent.text, \" --> \", filter_text_by_exc_patterns(sent, exc_tag_patterns))\n",
        "        sentence_list.append(filter_text_by_exc_patterns(sent, exc_tag_patterns))\n",
        "\n",
        "    result_msg = kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "    entities_from_kiwi = []\n",
        "    for token in result_msg:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "        if token.tag == 'NNP' and token.form not in stop_item_names+['-'] and len(token.form)>=2 and not token.form.lower() in stop_item_names:  # 고유명사인 경우\n",
        "        # if token.tag == 'NNG' and token.form in stop_item_names_ext:  # 고유명사인 경우\n",
        "            entities_from_kiwi.append(token.form)\n",
        "\n",
        "    from typing import List\n",
        "\n",
        "    entities_from_kiwi = filter_specific_terms(entities_from_kiwi)\n",
        "\n",
        "    print(\"추출된 개체명:\", list(set(entities_from_kiwi)))\n",
        "\n",
        "    similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "        sentence_list, \n",
        "        item_pdf_all['item_nm_alias'].unique(), \n",
        "        threshold=0.4,\n",
        "        text_col_nm='sent',\n",
        "        item_col_nm='item_nm_alias',\n",
        "        n_jobs=6,\n",
        "        batch_size=30\n",
        "    )\n",
        "\n",
        "    similarities_seq = parallel_seq_similarity(\n",
        "        sent_item_pdf=similarities_fuzzy,\n",
        "        text_col_nm='sent',\n",
        "        item_col_nm='item_nm_alias',\n",
        "        n_jobs=6,\n",
        "        batch_size=100\n",
        "    )\n",
        "\n",
        "    cand_items = similarities_seq.query(\"sim>=0.7 and item_nm_alias.str.contains('', case=False) and item_nm_alias not in @stop_item_names\")\n",
        "\n",
        "    entities_from_kiwi_pdf = item_pdf_all.query(\"item_nm_alias in @entities_from_kiwi\")[['item_nm','item_nm_alias']]\n",
        "    entities_from_kiwi_pdf['sim'] = 1.0\n",
        "\n",
        "    cand_item_pdf = pd.concat([cand_items,entities_from_kiwi_pdf])\n",
        "    cand_item_list = cand_item_pdf.sort_values('sim', ascending=False).groupby([\"item_nm_alias\"])['sim'].max().reset_index(name='final_sim').sort_values('final_sim', ascending=False).query(\"final_sim>=0.2\")['item_nm_alias'].unique()\n",
        "\n",
        "    # product_tag = [{\"item_name_in_msg\":d['item_nm'], \"item_in_voca\":[{\"item_name_in_voca\":d['item_nm'], \"item_id\":d['item_id']}]} for d in item_pdf_all.query(\"item_nm_alias in @cand_item_list\")[['item_nm','item_nm_alias','item_id']].groupby([\"item_nm\"])['item_id'].apply(list).reset_index().to_dict(orient='records')]\n",
        "\n",
        "    # product_tag    \n",
        "\n",
        "    extra_item_pdf = item_pdf_all.query(\"item_nm_alias in @cand_item_list\")[['item_nm','item_nm_alias','item_id']].groupby([\"item_nm\"])['item_id'].apply(list).reset_index()\n",
        "\n",
        "    # extra_item_pdf\n",
        "\n",
        "    return cand_item_list, extra_item_pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_by_logic(cand_entities, threshold_for_fuzzy=0.8):\n",
        "    similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "    cand_entities, \n",
        "    item_pdf_all['item_nm_alias'].unique(), \n",
        "    threshold=threshold_for_fuzzy,\n",
        "    text_col_nm='item_name_in_msg',\n",
        "    item_col_nm='item_nm_alias',\n",
        "    n_jobs=6,\n",
        "    batch_size=30\n",
        "    )\n",
        "\n",
        "    if similarities_fuzzy.shape[0]>0:\n",
        "\n",
        "        similarities_fuzzy = parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s1'\n",
        "        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s2'\n",
        "        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias']).groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')\n",
        "\n",
        "    return similarities_fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "def extract_entities_by_llm(llm_model, msg_text, rank_limit=5):\n",
        "        \"\"\"\n",
        "        Extract entities using LLM-based approach.\n",
        "        \"\"\"\n",
        "        from langchain.prompts import PromptTemplate\n",
        "\n",
        "        cand_entities_by_sim = extract_entities_by_logic([msg_text], threshold_for_fuzzy=0.7)['item_nm_alias'].unique()\n",
        "        \n",
        "        zero_shot_prompt = PromptTemplate(\n",
        "            input_variables=[\"msg\",\"cand_entities\"],\n",
        "            template=\"\"\"\n",
        "            Extract all product names, including tangible products, services, promotional events, programs, loyalty initiatives, and named campaigns or event identifiers, from the provided advertisement text.\n",
        "            Reference the provided candidate entities list as a guide for potential matches. Extract only those terms from the candidate list that appear in the advertisement text and qualify as distinct product names based on the following criteria.\n",
        "            Consider any named offerings, such as apps, membership programs, events, specific branded items, or campaign names like 'T day' or '0 day', as products if presented as distinct products, services, or promotional entities.\n",
        "            For terms that may be platforms or brand elements, include them only if they are presented as standalone offerings.\n",
        "            Avoid extracting base or parent brand names (e.g., 'FLO' or 'POOQ') if they are components of more specific offerings (e.g., 'FLO 앤 데이터' or 'POOQ 앤 데이터') presented in the text; focus on the full, distinct product or service names as they appear.\n",
        "            Exclude customer support services, such as customer centers or helplines, even if named in the text.\n",
        "            Exclude descriptive modifiers or attributes (e.g., terms like \"디지털 전용\" that describe a product but are not distinct offerings).\n",
        "            Exclude sales agency names such as '###대리점'.\n",
        "            If multiple terms refer to closely related promotional events (e.g., a general campaign and its specific instances or dates), include the most prominent or overarching campaign name (e.g., '0 day' as a named event) in addition to specific offerings tied to it, unless they are clearly identical.\n",
        "            Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering from the candidate list that matches the text.\n",
        "            Ensure that extracted names are presented exactly as they appear in the original text, without translation into English or any other language.\n",
        "            Just return a list with matched entities where the entities are separated by commas without any other text.\n",
        "\n",
        "            ## message:                \n",
        "            {msg}\n",
        "\n",
        "            ## Candidate entities:\n",
        "            {cand_entities}\n",
        "            \"\"\"\n",
        "        )\n",
        "        # Use the new LangChain pattern instead of deprecated LLMChain\n",
        "        chain = zero_shot_prompt | llm_model\n",
        "        cand_entities = chain.invoke({\"msg\": msg_text, \"cand_entities\": cand_entities_by_sim}).content\n",
        "\n",
        "        # Filter out stop words\n",
        "        cand_entity_list = [e.strip() for e in cand_entities.split(',') if e.strip()]\n",
        "        cand_entity_list = [e for e in cand_entity_list if e not in stop_item_names and len(e)>=2]\n",
        "\n",
        "        if not cand_entity_list:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Fuzzy similarity matching\n",
        "        similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "            cand_entity_list, \n",
        "            item_pdf_all['item_nm_alias'].unique(), \n",
        "            threshold=0.6,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30\n",
        "        )\n",
        "        \n",
        "        if similarities_fuzzy.empty:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Filter out stop words from results\n",
        "        similarities_fuzzy = similarities_fuzzy[~similarities_fuzzy['item_nm_alias'].isin(stop_item_names)]\n",
        "\n",
        "        # Sequence similarity matching\n",
        "        cand_entities_sim = parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s1'\n",
        "        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s2'\n",
        "        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias'])\n",
        "        \n",
        "        # Combine similarity scores\n",
        "        cand_entities_sim = cand_entities_sim.groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')\n",
        "        cand_entities_sim = cand_entities_sim.query(\"sim>=1.5\")\n",
        "\n",
        "        # Rank and limit results\n",
        "        cand_entities_sim[\"rank\"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(method='first',ascending=False)\n",
        "        cand_entities_sim = cand_entities_sim.query(f\"rank<={rank_limit}\").sort_values(['item_name_in_msg','rank'], ascending=[True,True])\n",
        "\n",
        "        return cand_entities_sim\n",
        "\n",
        "# extract_entities_by_llm(llm_gem3, msg_text_list[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test text: 요금제 무료혜택 안내\n",
            "(광고)[SKT] #04 고객님, 현재 놓치고 계신 POOQ & FLO 무료 혜택을 안내해드립니다.   #91 요금제 가입 고객님은 아래 이용권 모두 무료로 이용하실 수 있어요. 다양한 방송 콘텐츠를 즐길 수 있는 POOQ과 음악을 무제한 감상할 수 있는 FLO를 무료로 즐겨보세요.  ■ POOQ 앤 데이터 (월 9,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/Dj8L4 - 지상파, 종편 실시간 TV + VOD 무제한 시청 가능 - POOQ 전용 데이터 매일 1GB 제공(전용 데이터를 다 쓰면 최대 3Mbps 속도로 계속 사용)  ■ FLO 앤 데이터 (월 7,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/l98dC - FLO 음악 무제한 듣기(모바일 기기 전용) - FLO 전용 데이터 월 3GB 제공 (음원 다운로드를 제외한 스트리밍 서비스에 한해 이용 가능)  ※ 5GX 플래티넘 요금제 가입 고객님은 POOQ 앤 데이터 플러스, FLO 앤 데이터 플러스 무료 이용 가능 (POOQ 앤 데이터/FLO 앤 데이터와 중복으로 가입할 수 없습니다.) ※ 서비스 가입 후 이용권 발급 필요 - 이용권 발급 방법: FLO 앱 > 이용권 > T 혜택 > 5GX 요금제 혜택 > 발급받기  SKT와 함께해주셔서 감사합니다.  ※ 이 메시지는 2019년 8월 19일 기준으로 작성되었습니다.  무료 수신거부 1504\n",
            "추출된 개체명: ['FLO 앤 데이터 플러스', '플래티넘', '스트리밍']\n",
            "\n",
            "Entity from extractor: ['ⓜ시아 모바일', 'G5', '5GX 플래티넘(T 우주)', '벅스 앤 데이터 플러스_인증', 'FLO 앤 Data 플러스', 'FLO 앤 데이터 plus', '(N)FLO 앤 데이터④ⓑ', 'LTE 혜택 안내', '5GX플래티넘', 'T다이렉트 플러스', '사내 데이터전용 서비스', 'PPS플러스', '데이터 테스트 서비스', '눋 데이터 전환 1GB', 'FLO 앤 데이터 플러스 구독', '데이터 특별지원', '플로 앤 데이터 플러스 구독', '웨이브 앤 데이터 플러스 구독', '원스토리 앤 데이터', '팅데이터프리 플러스', 'Biz용 전용 매일 1GB', 'Btv 모바일', 'UCC 무제한 요금제', 'FLO 앤 데이터④ⓑ', '와이파이 15 요금제', '모바일 육아 서비스', '모바일 무제한 듣기', 'FLO 앤 데이터①ⓐ', '부가-PDS 서비스', '웨이브 앤 데이터 플러스', 'FLO 앤 데이터', '뮤 모바일', '바로 요금제', 'N 바로 요금제', 'TVN VM 서비스', 'b tv mobile(무료)', '데이터추가 1G', 'FLO 앤 데이터 플러스', 'G3', '(N)FLO 앤 데이터④ⓐ', '웨이브 앤 데이터', '플로 앤 데이터', 'FLO 앤 데이터 PLUS', 'FLO 앤 데이터_할인2', 'LTE 52 요금제', '(N)FLO 앤 데이터④ⓓ', '데이터 Logger', '일상플러스(무료)', '플로 앤 데이터 플러스', 'FLO 앤 데이터③ⓓ', '요금제 test', '데이터세이프 플러스', 'ⓜ바이러스', 'FLO 앤 data 플러스', '홍콩 데이터로밍 무제한 요금제', 'FLO 이용권', '벅스 앤 데이터 플러스', 'FLO 앤 데이터 Plus', '포인트캠 플러스', '데이터1G', 'TV 1mm플러스', 'FLO 앤 데이터_할인3', '부가-IVR 플러스', '밴드 데이터 3.5G', 'FLO 앤 데이터_인증', '5GX 플래티넘(넷플릭스)', 'FLO 앤 데이터③ⓐ', 'T클래스 플러스', '모바일 심전도 서비스', '이모티콘 플러스', 'FLO 앤 데이터 플러스_할인2', '5GX 플래티넘(스마트기기)', 'FLO 앤 DATA 플러스', '(N)FLO 앤 데이터④ⓒ', '세종', 'FLO 앤 데이터③ⓑ', 'FLO 앤 데이터 플러스_할인1', '부가 테스트 요금제', '1원요금제', 'Page 플러스', 'QoS 3Mbps', '로밍요금제가입자', '모스트', '무료음악', '5GX 플래티넘(디즈니+)', 'FLO 앤 데이터_할인1', 'FLO 앤 데이터④ⓒ', 'FLO 무제한', '모바일 트위터 서비스', '플로 앤 데이터 플러스_할인1', '기업전용 P-5GX', 'T 링플러스(무료)', '멜론 모바일 스트리밍', '모바일 안전결제', 'FLO 앤 데이터 플러스 구독_T', '데이터 속도 레귤러', '베이직 데이터 충전 3GB', 'FLO 앤 데이터④ⓐ', 'T콜 플러스', '(N)FLO 앤 데이터①ⓐ', 'T 서비스', '모바일 싸이월드', 'FLO 앤 데이터③ⓒ', 'FLO 앤 데이터④ⓓ', 'T1 요금제', '플로 앤 데이터 플러스_인증', 'FLO팩 플러스', '플로 무제한', 'FLO 앤 데이터 플러스_인증', 'PooQ 팩', '한한', 'FLO 앤 데이터 구독', '플로 앤 데이터 플러스_할인2', 'Wavve 앤 데이터 플러스', 'PPS스마트폰 플러스', '5GX 레귤러플러스', '레터링플러스(무료)']\n",
            "Entity from LLM: ['POOQ 앤 데이터', 'FLO 앤 데이터', 'POOQ 앤 데이터 플러스', 'FLO 앤 데이터 플러스', 'FLO', 'POOQ']\n",
            "{\n",
            "    \"title\": \"요금제 무료혜택 안내\",\n",
            "    \"purpose\": [\n",
            "        \"혜택 안내\"\n",
            "    ],\n",
            "    \"product\": [\n",
            "        {\n",
            "            \"item_name_in_msg\": \"FLO\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"flo_프로모션종료\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00007052\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"FLO 앤 데이터\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터\",\n",
            "                    \"item_id\": [\n",
            "                        \"PR00000205\",\n",
            "                        \"NA00006520\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 구독\",\n",
            "                    \"item_id\": [\n",
            "                        \"NM00000030\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터_인증\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00006521\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터①ⓐ\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00007792\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터③ⓐ\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00008011\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"FLO 앤 데이터 플러스\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스\",\n",
            "                    \"item_id\": [\n",
            "                        \"PR00000206\",\n",
            "                        \"NA00006599\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스 구독\",\n",
            "                    \"item_id\": [\n",
            "                        \"NM00000031\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스_인증\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00006600\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스_할인1\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00006601\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스_할인2\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00006602\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"POOQ\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"B tv 모바일_pooq\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00004269\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"POOQ 앤 데이터 플러스\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"FLO 앤 데이터 플러스\",\n",
            "                    \"item_id\": [\n",
            "                        \"PR00000206\",\n",
            "                        \"NA00006599\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"벅스 앤 데이터 플러스\",\n",
            "                    \"item_id\": [\n",
            "                        \"NA00006767\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"channel\": [\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"http://t-mms.kr/t.do?m=#61&u=https://skt.sh/Dj8L4\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"store_info\": []\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"http://t-mms.kr/t.do?m=#61&u=https://skt.sh/l98dC\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"store_info\": []\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"앱\",\n",
            "            \"value\": \"FLO 앱\",\n",
            "            \"action\": \"가입\",\n",
            "            \"store_info\": []\n",
            "        }\n",
            "    ],\n",
            "    \"pgm\": [\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Sales]상품및부가서비스가입유도_요금제\",\n",
            "            \"pgm_id\": \"2019SCEFE01\"\n",
            "        },\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Sales]상품및부가서비스가입유도_SKT\",\n",
            "            \"pgm_id\": \"2020SCCSV05\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0h/3c1x50w901n85pq5kbbn9mn020dh7m/T/ipykernel_88103/2556002711.py:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cand_entities_sim[\"rank\"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(method='first',ascending=False)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "product_info_extraction_mode = 'llm' # options: 'rag', 'llm', 'nlp'\n",
        "entity_matching_mode = 'llm' # options: 'llm', 'logic'\n",
        "\n",
        "# for test_text in mms_pdf.query(\"msg.str.contains('대리점')\").sample(10)['msg'].tolist():\n",
        "\n",
        "test_text = \"\"\"\n",
        "[SK텔레콤] ZEM폰 포켓몬에디션3 안내\\n(광고)[SKT] 우리 아이 첫 번째 스마트폰, ZEM 키즈폰__#04 고객님, 안녕하세요!_우리 아이 스마트폰 고민 중이셨다면, _자녀 스마트폰 관리 앱 ZEM이 설치된 SKT만의 안전한 키즈폰,_ZEM폰 포켓몬에디션3으로 우리 아이 취향을 저격해 보세요!__신학기를 맞이하여 SK텔레콤 공식 인증 대리점에서 풍성한 혜택을 제공해 드리고 있습니다!__■ 주요 기능_1. 실시간 위치 조회_2. 모르는 회선 자동 차단_3. 스마트폰 사용 시간 제한_4. IP68 방수 방진_5. 수업 시간 자동 무음모드_6. 유해 콘텐츠 차단__■ 가까운 SK텔레콤 공식 인증 대리점 찾기_http://t-mms.kr/t.do?m=#61&s=30684&a=&u=https://bit.ly/3yQF2hx__■ 문의 : SKT 고객센터(1558, 무료)__무료 수신거부 1504\n",
        "\"\"\"\n",
        "\n",
        "test_text = \"\"\"\n",
        "요금제 무료혜택 안내\n",
        "(광고)[SKT] #04 고객님, 현재 놓치고 계신 POOQ & FLO 무료 혜택을 안내해드립니다.   #91 요금제 가입 고객님은 아래 이용권 모두 무료로 이용하실 수 있어요. 다양한 방송 콘텐츠를 즐길 수 있는 POOQ과 음악을 무제한 감상할 수 있는 FLO를 무료로 즐겨보세요.  ■ POOQ 앤 데이터 (월 9,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/Dj8L4 - 지상파, 종편 실시간 TV + VOD 무제한 시청 가능 - POOQ 전용 데이터 매일 1GB 제공(전용 데이터를 다 쓰면 최대 3Mbps 속도로 계속 사용)  ■ FLO 앤 데이터 (월 7,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/l98dC - FLO 음악 무제한 듣기(모바일 기기 전용) - FLO 전용 데이터 월 3GB 제공 (음원 다운로드를 제외한 스트리밍 서비스에 한해 이용 가능)  ※ 5GX 플래티넘 요금제 가입 고객님은 POOQ 앤 데이터 플러스, FLO 앤 데이터 플러스 무료 이용 가능 (POOQ 앤 데이터/FLO 앤 데이터와 중복으로 가입할 수 없습니다.) ※ 서비스 가입 후 이용권 발급 필요 - 이용권 발급 방법: FLO 앱 > 이용권 > T 혜택 > 5GX 요금제 혜택 > 발급받기  SKT와 함께해주셔서 감사합니다.  ※ 이 메시지는 2019년 8월 19일 기준으로 작성되었습니다.  무료 수신거부 1504\n",
        "\"\"\"\n",
        "\n",
        "# test_text = msg_text_list[0]\n",
        "\n",
        "print(f\"Test text: {test_text.strip()}\")\n",
        "msg = test_text.strip()\n",
        "\n",
        "cand_item_list, extra_item_pdf = extract_entities_from_kiwi(msg, item_pdf_all, stop_item_names)\n",
        "\n",
        "product_df = extra_item_pdf.rename(columns={'item_nm':'name'}).query(\"not name in @stop_item_names\")[['name']]\n",
        "product_df['action'] = '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'\n",
        "# product_df['position'] = '광고 상품의 분류. [main, sub, etc] 중에서 선택'\n",
        "product_element = product_df.to_dict(orient='records') if product_df.shape[0]>0 else schema_prd['product']\n",
        "\n",
        "# print(cand_item_list)\n",
        "\n",
        "mms_embedding = emb_model.encode([msg.lower()], convert_to_tensor=True)\n",
        "\n",
        "similarities = torch.nn.functional.cosine_similarity(\n",
        "    mms_embedding,  \n",
        "    clue_embeddings,  \n",
        "    dim=1 \n",
        ").cpu().numpy()\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf.copy()\n",
        "pgm_pdf_tmp['sim'] = similarities\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)\n",
        "\n",
        "pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "rag_context = f\"\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms > 0 else \"\"\n",
        "\n",
        "chain_of_thought = \"\"\"\n",
        "1. Identify the advertisement’s purpose first, using expressions as they appear in the original text.\n",
        "2. Extract product names based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.\n",
        "3. Provide channel information considering the extracted product information, preserving original text expressions.\n",
        "\"\"\"\n",
        "\n",
        "# Revised schema with updated guidelines for preserving original text\n",
        "schema_prd = \"\"\"\n",
        "{\n",
        "  \"title\": {\n",
        "    \"type\": \"string\",\n",
        "    \"description\": \"Advertisement title, using the exact expressions as they appear in the original text. Clearly describe the core theme and value proposition of the advertisement.\"\n",
        "  },\n",
        "  \"purpose\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"상품 가입 유도\", \"대리점/매장 방문 유도\", \"웹/앱 접속 유도\", \"이벤트 응모 유도\", \"혜택 안내\", \"쿠폰 제공 안내\", \"경품 제공 안내\", \"수신 거부 안내\", \"기타 정보 제공\"]\n",
        "    },\n",
        "    \"description\": \"Primary purpose(s) of the advertisement, expressed using the exact terms from the original text where applicable.\"\n",
        "  },\n",
        "  \"product\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"name\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Name of the advertised product or service, as it appears in the original text without translation.\"\n",
        "        },\n",
        "        \"action\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"구매\", \"가입\", \"사용\", \"방문\", \"참여\", \"코드입력\", \"쿠폰다운로드\", \"기타\"],\n",
        "          \"description\": \"Expected customer action for the product, derived from the original text context.\"\n",
        "        }\n",
        "      },\n",
        "    \"description\": \"Extract all product names, including tangible products, services, promotional events, programs, loyalty initiatives, and named campaigns or event identifiers, using the exact expressions as they appear in the original text without translation. Consider only named offerings (e.g., apps, membership programs, events, specific branded items, or campaign names like 'T day' or '0 day') presented as distinct products, services, or promotional entities. Include platform or brand elements only if explicitly presented as standalone offerings. Avoid extracting base or parent brand names (e.g., 'FLO' or 'POOQ') if they are components of more specific offerings (e.g., 'FLO 앤 데이터' or 'POOQ 앤 데이터') presented in the text; focus on the full, distinct product or service names as they appear. Exclude customer support services (e.g., customer centers, helplines). Exclude descriptive modifiers, attributes, or qualifiers (e.g., '디지털 전용'). Exclude sales agency names such as '###대리점'. If multiple terms refer to closely related promotional events (e.g., a general campaign and its specific instances or dates), include the most prominent or overarching campaign name (e.g., '0 day' as a named event) in addition to specific offerings tied to it, unless they are clearly identical. Prioritize recall over precision, but verify each term is a distinct offering.\"            },\n",
        "    }\n",
        "  },\n",
        "  \"channel\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"type\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"URL\", \"전화번호\", \"앱\", \"대리점\"],\n",
        "          \"description\": \"Channel type, as derived from the original text.\"\n",
        "        },\n",
        "        \"value\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Specific information for the channel (e.g., URL, phone number, app name, agency name), as it appears in the original text.\"\n",
        "        },\n",
        "        \"action\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"가입\", \"추가 정보\", \"문의\", \"수신\", \"수신 거부\"],\n",
        "          \"description\": \"Purpose of the channel, derived from the original text context.\"\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"description\": \"Channels provided in the advertisement, including URLs, phone numbers, apps, or agencies, using the exact expressions from the original text where applicable, based on the purpose and products.\"\n",
        "  },\n",
        "  \"pgm\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"string\"\n",
        "    },\n",
        "    \"description\": \"Select the two most relevant pgm_nm from the advertising classification criteria, using the exact expressions from the criteria, ordered by relevance, based on the message content.\"\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "prd_ext_guide = \"\"\"\n",
        "* Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering.\n",
        "* Extract all information (title, purpose, product, channel, pgm) using the exact expressions as they appear in the original text without translation, as specified in the schema.\n",
        "* If the advertisement purpose includes encouraging agency/store visits, provide agency channel information.\n",
        "\"\"\"\n",
        "\n",
        "if len(cand_item_list) > 0: \n",
        "    if product_info_extraction_mode == 'rag':\n",
        "        rag_context += f\"\\n\\n### 후보 상품 이름 목록 ###\\n\\t{cand_item_list}\"\n",
        "        prd_ext_guide += f\"\"\"\n",
        "* Use the provided candidate product names as a reference to guide product extraction, ensuring alignment with the advertisement content and using exact expressions from the original text.\n",
        "        \"\"\"\n",
        "    elif product_info_extraction_mode == 'nlp':\n",
        "        schema_prd['product'] = product_element  # Assuming product_element is defined elsewhere\n",
        "        chain_of_thought = \"\"\"\n",
        "1. Identify the advertisement’s purpose first, using expressions as they appear in the original text.\n",
        "2. Extract product information based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.\n",
        "3. Extract the action field for each product based on the provided name information, derived from the original text context.\n",
        "4. Provide channel information considering the extracted product information, preserving original text expressions.\n",
        "        \"\"\"\n",
        "        prd_ext_guide += f\"\"\"\n",
        "* Extract the action field for each product based on the identified product names, using the original text context.\n",
        "        \"\"\"\n",
        "\n",
        "schema_prompt = f\"\"\"\n",
        "Provide the results in the following schema:\n",
        "\n",
        "{schema_prd}\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the advertisement purpose and product names from the provided advertisement text.\n",
        "\n",
        "### Advertisement Message ###\n",
        "{msg}\n",
        "\n",
        "### Extraction Steps ###\n",
        "{chain_of_thought}\n",
        "\n",
        "### Extraction Guidelines ###\n",
        "{prd_ext_guide}\n",
        "\n",
        "{schema_prompt}\n",
        "\n",
        "{rag_context}\n",
        "\"\"\"\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "# result_json_text = llm_cld40.invoke(prompt).content\n",
        "result_json_text = llm_gem3.invoke(prompt).content\n",
        "\n",
        "json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "# print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "\n",
        "\n",
        "if entity_matching_mode == 'logic':\n",
        "    cand_entities = [item['name'] for item in json_objects['product']['items']] if isinstance(json_objects['product'], dict) else [item['name'] for item in json_objects['product']]\n",
        "    similarities_fuzzy = extract_entities_by_logic(cand_entities)\n",
        "elif entity_matching_mode == 'llm':\n",
        "    similarities_fuzzy = extract_entities_by_llm(llm_gem3, msg)\n",
        "\n",
        "final_result = json_objects.copy()\n",
        "\n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "# print(\"===\"*15+\"claude sonnet (emb)\"+\"===\"*15+\"\\n\")\n",
        "# print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "print(\"Entity from extractor:\", list(set(cand_item_list)))\n",
        "print(\"Entity from LLM:\", [x['name'] for x in ([item for item in json_objects['product']['items']] if isinstance(json_objects['product'], dict) else json_objects['product']) ])\n",
        "\n",
        "if similarities_fuzzy.shape[0]>0:\n",
        "        # Break down the complex query into simpler steps to avoid pandas/numexpr evaluation error\n",
        "    # Step 1: Get high similarity items\n",
        "    high_sim_items = similarities_fuzzy.query('sim >= 1.5')['item_nm_alias'].unique()\n",
        "    \n",
        "    # Step 2: Filter similarities_fuzzy for conditions\n",
        "    filtered_similarities = similarities_fuzzy[\n",
        "        (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &\n",
        "        (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &\n",
        "        (~similarities_fuzzy['item_name_in_msg'].isin(stop_item_names))\n",
        "    ]\n",
        "    \n",
        "    # Step 3: Merge with item_pdf_all\n",
        "    product_tag = convert_df_to_json_list(\n",
        "        item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])\n",
        "    )\n",
        "\n",
        "    final_result = {\n",
        "        \"title\":json_objects['title'],\n",
        "        \"purpose\":json_objects['purpose'],\n",
        "        \"product\":product_tag,\n",
        "        \"channel\":json_objects['channel'],\n",
        "        \"pgm\":json_objects['pgm']\n",
        "    }\n",
        "\n",
        "else:\n",
        "    final_result = json_objects.copy()\n",
        "    product_tag = [item for item in json_objects['product']['items']] if isinstance(json_objects['product'], dict) else json_objects['product']\n",
        "    final_result['product'] = [{'item_name_in_msg':d['name'], 'item_in_voca':[{'item_name_in_voca':d['name'], 'item_id': ['#']}]} for d in product_tag if d['name'] not in stop_item_names]\n",
        "\n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "channel_tag = []\n",
        "for d in [item for item in json_objects['channel']['items']] if isinstance(json_objects['channel'], dict) else json_objects['channel']:\n",
        "    if d['type']=='대리점':\n",
        "\n",
        "        # _embedding = emb_model.encode([preprocess_text(d['value'].lower())], convert_to_tensor=True)\n",
        "\n",
        "        # similarities = torch.nn.functional.cosine_similarity(\n",
        "        #     _embedding,  \n",
        "        #     org_all_embeddings,  \n",
        "        #     dim=1 \n",
        "        # ).cpu().numpy()\n",
        "\n",
        "        # org_pdf_tmp = org_pdf.copy()\n",
        "        # org_pdf_tmp['sim'] = similarities.round(5)\n",
        "\n",
        "        org_pdf_cand = parallel_fuzzy_similarity(\n",
        "            [preprocess_text(d['value'].lower())], \n",
        "            org_pdf['org_abbr_nm'].unique(), \n",
        "            threshold=0.5,\n",
        "            text_col_nm='org_nm_in_msg',\n",
        "            item_col_nm='org_abbr_nm',\n",
        "            n_jobs=6,\n",
        "            batch_size=100\n",
        "        ).drop('org_nm_in_msg', axis=1)\n",
        "\n",
        "        org_pdf_cand = org_pdf.merge(org_pdf_cand, on=['org_abbr_nm'])\n",
        "\n",
        "        org_pdf_cand['sim'] = org_pdf_cand['sim'].round(5)\n",
        "        \n",
        "        org_pdf_tmp = org_pdf_cand.query(\"org_cd.str.startswith('D')\").sort_values('sim', ascending=False).query(\"sim>=0.7\")\n",
        "        if org_pdf_tmp.shape[0]<1:\n",
        "            org_pdf_tmp = org_pdf_cand.sort_values('sim', ascending=False).query(\"sim>=0.7\")\n",
        "\n",
        "        org_pdf_tmp['sim'] = org_pdf_tmp.apply(lambda x: combined_sequence_similarity(d['value'], x['org_nm'])[0], axis=1)\n",
        "        org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense',ascending=False)\n",
        "        org_pdf_tmp['org_cd'] = org_pdf_tmp.apply(lambda x: x['org_cd']+x['sub_org_cd'], axis=1)\n",
        "\n",
        "        org_pdf_tmp = org_pdf_tmp.query(\"rank==1\").groupby('org_nm')['org_cd'].apply(list).reset_index(name='org_cd').to_dict('records')\n",
        "\n",
        "        # org_nm_id_list =  list(zip(org_pdf_tmp['org_nm'], org_pdf_tmp['org_id']))\n",
        "\n",
        "        d['store_info'] = org_pdf_tmp\n",
        "    else:\n",
        "        d['store_info'] = []\n",
        "\n",
        "    channel_tag.append(d)\n",
        "\n",
        "final_result['channel'] = channel_tag\n",
        "\n",
        "# print(\"===\"*15+\"claude sonnet (fuzzy)\"+\"===\"*15+\"\\n\")\n",
        "print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "# Extract information\n",
        "response = requests.post('http://127.0.0.1:8080/extract', json={\n",
        "    \"message\": \"\"\"광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\"\"\",\n",
        "    \"llm_model\": \"gemma\",\n",
        "    \"product_info_extraction_mode\": \"nlp\",\n",
        "    \"entity_matching_mode\": \"logic\"\n",
        "})\n",
        "result = response.json()\n",
        "print(json.dumps(result['result'], indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SK텔레콤] 평내대리점 본점 폴더블6 출시 이벤트 안내드립니다\n",
            "(광고)[SKT] 평내대리점 본점 폴더블6 출시 이벤트__고객님 안녕하세요_SK텔레콤 공식인증대리점 평내대리점 본점입니다__■ 폴더블6 _전작 대비 더욱 강력해진 성능으로 19일 첫 출시!_지금 바로 예약(7월 18일 공식예약 마감)하시고 예약고객만의 다양한 혜택을 누려보시기 바랍니다__■ 신형 휴대폰 외 _효도폰, 키즈폰 등 다양한 할인 이벤트도 진행중입니다__■ 추가할인, 사은품 등 다양한 혜택과_요금할인, 숨어있는 포인트 활용 등 꿀Tip상담을 약속드립니다__■ SK공식인증대리점 평내대리점 본점_- 주소 : 경기 남양주시 평내로29번길 49, 1층 106호_- 연락처 : 031-594-2022_- 구술약도 : 평내동 금호아파트 맞은편 신한은행 건물 1층_▶ 홈페이지 : http://t-mms.kr/t.do?m=#61&s=27260&a=&u=http://tworldfriends.co.kr/D146140000__■ 문의 : SKT 고객센터(1558, 무료)_SK텔레콤과 함께해 주셔서 감사합니다._무료 수신거부 1504\n",
            "entities from gem3:  ['폴더블6']\n",
            "entities from claude:  ['포인트']\n",
            "\n",
            "[SK텔레콤] ACE 대리점 신내동점 3월 신학기맞이 이벤트\n",
            "(광고)[SKT]ACE대리점 신내동점 3월 신학기맞이 이벤트__안녕하세요. 고객님~ _저희 SKT신내동점에서 3월 신학기 시즌을 맞이하여~ _진행중인 행사 안내드립니다.__■ SKT전용 포켓몬 에디션 신규가입 이벤트_ - 대상: 12세 이하 _ - 혜택: 포켓몬 에디션 본품 + 매장 자체 준비 추가 사은품 _ - 기타내용: 1년약정도 가능, 포켓몬 에디션 상품 한정 판매 중__■ 약정 종료된 고객님 대상! 삼성 갤럭시 스마트폰 행사!_ - 대상 : 2년약정 종료고객님 대상_ - 모델 : 갤럭시Wide6 , 갤럭시 퀀텀3 _ - 기타내용 : 최강 가성비폰 행사 진행 중!!__■ SKT 인터넷+TV 가입시 푸짐한 사은품 증정  _ - 휴대폰과 함께 신청 시 결합 할인 & 최대 사은품 증정_ - 핸드폰 구매없이 신청하셔도 SKT 핸드폰과 결합 설계 & 사은품 증정__■ ACE대리점 신내동점 _▶연락처: 070-4788-7570_▶ 주소: 서울 중랑구 봉화산로 209, 1층 티월드 _▶매장 위치 보기: http://t-mms.kr/t.do?m=#61&s=18759&a=&u=https://naver.me/xGidlD2G_▶T월드프렌즈H.P : http://t-mms.kr/t.do?m=#61&s=18760&a=&u=https://tworldfriends.co.kr/D151760008_▶네이버플레이스 : http://t-mms.kr/t.do?m=#61&s=18761&a=&u=https://naver.me/xkxp6tT6_▶홈페이지 방문 후 네이버 톡톡 으로도 문의 가능합니다.__■ SKT와 함께 해 주셔서 감사합니다_■ 문의 : SKT고객센터(1588,무료)_무료수신거부 1504_\n",
            "entities from gem3:  ['SKT 인터넷+TV', 'SKT전용 포켓몬 에디션', 'T월드프렌즈', '갤럭시 퀀텀3', '갤럭시Wide6', '네이버플레이스']\n",
            "entities from claude:  ['갤럭시 Wide6', '갤럭시 퀀텀3', '프렌즈']\n",
            "\n",
            "[SK텔레콤] 광장대리점 중곡점 가정의달 맞이 혜택 받아가세요~\n",
            "(광고)[SKT]SK텔레콤 중곡점 가정의 달 맞이 혜택 안내  고객님, 안녕하세요. 광장대리점 중곡점에서 가정의달을 맞아 그동안 성원해 주신 고객님께 감사의 마음을 담아 다양한 혜택을 준비했습니다.  ■삼성 갤럭시S21시리즈, 노트시리즈 구매시! - 삼성 태블릿 1+1 행사  ■가정의달 맞이! - 우리 아이 키즈폰 신규가입시 특가 할인+추가 사은품 증정  - 휴대폰 가족 단위 가입시 추가 할인 제공!   ■SK매직 렌탈시작 - 정수기, 공기청정기, 비데, 안마의자 이제는 렌탈하세요! - 월요금 15% 추가할인 (계약기간만료시 상품은 고객님께 드립니다.)  ■ 광장대리점 중곡점 - 주소: 광진구 중곡동 171-1 1층 - 연락처: 02-581-5777 - 영업 시간: 오전 10시~오후 8시 00분(월~금), 오전 11시~오후 8시(토) 일요일(미오픈)  ▶매장 행사 보기: http://t-mms.kr/t.do?m=#61&s=1744&a=&u=https://tworldfriends.co.kr/d132250212  ■ 문의: SKT 고객센터(1558, 무료) ※ 코로나19 확산으로 고객센터에 문의가 증가하고 있습니다. 고객센터와 전화 연결이 원활하지 않을 수 있으니 양해 바랍니다.  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\n",
            "entities from gem3:  ['SK매직', '삼성 갤럭시S21', '삼성 태블릿', '우리아이', '휴대폰가족보호']\n",
            "entities from claude:  ['갤럭시S21', '우리아이', '휴대폰가족보호']\n",
            "\n",
            "[SK텔레콤] 갤럭시S23 출시기념 ACE대리점 상도점 행사 안내\n",
            "(광고)[SKT] SKT 상도점에서 갤럭시S23 출시 행사 안내 __안녕하세요. 고객님~ _저희 SKT상도점에서 갤럭시S23 출시!! 2월 이벤트 안내드립니다.__■ 갤럭시S23 공식출시_ - 갤럭시S23 구매 시 프리미엄 사은품 증정_ - 당일개통! 갤럭시 워치5 초특가 증정_ - 제휴카드 이용시 48만~60만원 추가 할인_ - 삼성케어 1년 지원 or 사은품10만원 쿠폰 증정__■ SKT전용 포켓몬 에디션 신규가입 이벤트_ - 대상: 12세 이하 _ - 혜택: 포켓몬 에디션 본품 + 매장 자체 준비 추가 사은품 _ - 기타내용: 1년약정도 가능, 포켓몬 에디션상품 한정 판매 __■ SKT 인터넷+TV 가입 시 푸짐한 사은품 증정  _ - 휴대폰과 함께 신청 시 결합 할인 & 최대 사은품 증정_ - 핸드폰 구매없이 신청하셔도  SKT 핸드폰과 결합 설계 & 최대 사은품 증정__■ ACE대리점 상도점 _▶연락처 : 070-4788-7707_▶주 소 : 서울 동작구 상도로 207, 상가동 1층 티월드_▶매장 위치 보기: http://t-mms.kr/t.do?m=#61&s=18377&a=&u=https://naver.me/xmP7z7a3_▶매장 홈페이지_T월드프렌즈H.P : http://t-mms.kr/t.do?m=#61&s=18378&a=&u=https://tworldfriends.co.kr/D151760014_네이버플레이스 : http://t-mms.kr/t.do?m=#61&s=18379&a=&u=https://m.place.naver.com/place/1014197983/home_▶홈페이지 방문 후 네이버 톡톡으로 문의 가능합니다__■ 문의 : SKT고객센터(1588,무료)_무료수신거부 1504 \n",
            "entities from gem3:  ['SKT 인터넷+TV', 'T월드프렌즈', '갤럭시 워치5', '갤럭시S23', '포켓몬 에디션']\n",
            "entities from claude:  ['갤럭시 워치5', '갤럭시S', '프렌즈']\n",
            "\n",
            "[SK텔레콤] 에스알대리점 거여직영점 새해 1월 새소식 안내\n",
            "(광고)[SKT] 에스알대리점 거여직영점 새해 1월 새소식 안내__고객님, 안녕하세요. _거여역 7번 출구 앞, SK텔레콤 1등 거여직영점에서 1월 새소식을 알려드립니다.__■ 삼성 갤럭시 S24 사전예약 혜택_① 정품 충전기 세트 증정_② 필름+케이스 무제한 제공_③ 갤럭시 워치 최대 할인_④ 기존폰 최대 보상_⑤ 스타벅스 텀블러 증정_⑥ 하나카드 사용 시 60만 원 할인 (24개월, 월 40만 원 사용 조건)_※ 1월 25일 까지 예약 접수 중!_- 02-6015-8777 상담 문의 주세요!__■신학기 우리아이 첫안심 폰_- 포켓몬 에디션2 출시기념, 초저가_- 1년 약정시 월요금 14,850원 (ZEM플랜 스마트 + 선택약정)__■ 인터넷+TV 가입 시 최대 사은품 지급__■ 에스알대리점 거여직영점_ - 주소 : 서울시 송파구 오금로 500, 거여역 7번 출구_ - 연락처 : 02-6015-8777_▶매장 홈페이지 : http://t-mms.kr/t.do?m=#61&s=24278&a=&u=http://tworldfriends.co.kr/D150040042__■ 문의: SKT 고객센터(1558, 무료)_SKT와 함께 해주셔서 감사합니다._무료 수신거부 1504\n",
            "entities from gem3:  ['ZEM폰 포켓몬에디션2', '갤럭시 버디2', '갤럭시 워치', '삼성 갤럭시 S24', '스타벅스']\n",
            "entities from claude:  ['ZEM폰 포켓몬에디션2', '스타벅스']\n",
            "\n",
            "(0814) 큰사랑대리점 배곧직영점 오픈행사 안내\n",
            "(광고)[SKT] 큰사랑대리점 배곧직영점 오픈행사 안내   고객님, 안녕하세요. SK텔레콤 공식인증 큰사랑대리점 배곧직영점이 새롭게 오픈하여 인사드립니다. 고객님께 늘 사랑받는 매장이 되도록 노력하겠습니다. 편안하게 방문하시어 친절한 상담 받으세요.  ■ 오픈 기념 고객 사은행사 안내 - 모든 구매 고객님께 드리는 푸짐한 사은품    : 제주도여행권, 바베큐그릴, 서큘레이터, 에어프라이어 등 10종 이상의 사은품 - 갤럭시 노트20 개통시 특별 혜택 - 단골고객 등록시, 5만원권 요금할인 쿠폰 제공 - 휴대폰 악세서리 최저가 판매 - 학생폰, 어르신폰 최저 요금제 개통 가능 (선택약정시 14,850원 적용)  ■ 큰사랑대리점 배곧직영점만의 특징 - 바쁘신 주변상가 고객님들을 위한 배달서비스 - 신규가입,기기변경,요금수납,명의변경,가족결합 등 필요하신 모든 업무 가능 - 1년 365일 연중무휴, 신속한 업무처리  ■ 큰사랑대리점 배곧직영점 - 주소 : 경기 시흥시 서울대학로278번길 61, A동 1호(정왕동, 서영베니스스퀘어) - 연락처 : 031-503-9933~4 ▶ 배곧직영점 매장 :http://t-mms.kr/t.do?m=#61&u=http://tworldfriends.co.kr/d134220144  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\n",
            "entities from gem3:  ['갤럭시 노트20']\n",
            "entities from claude:  ['갤럭시 노트20']\n",
            "\n",
            "[SK텔레콤] 광장대리점 솔샘점 고객사은품 혜택 안내\n",
            "(광고)[SKT]광장대리점 솔샘점 고객사은품 혜택 안내__고객님, 안녕하세요._SK텔레콤 공식인증대리점 광장대리점 솔샘점이 갤럭시 퀀텀 출시 기념/할인)혜택을 준비했습니다.__■ 갤럭시 퀀텀 출시 기념 혜택 안내_- 퀀텀3 사전예약하시고 푸짐한 사은품과 파격적인 할인 받아가세요~!_- 사전예약 : 4월19일(화)~4월25일(월)까지__■ 4~5월 사은품 이벤트_① 단골 등록 시: 필름 증정_② 휴대폰 구매시: 사은품 증정_ _■ 광장대리점 솔샘점_- 주소: 서울특별시 강북구 솔샘로 213 1층_- 연락처: 02-983-8400__▶ 매장 위치 보기: http://t-mms.kr/t.do?m=#61&s=11483&a=&u=http://tworldfriends.co.kr/D132250188__■ 문의 : SK텔레콤 고객센터 1558__※ 코로나19 확산으로 고객센터에 문의가 증가하고 있습니다. 고객센터와 전화 연결이 원활하지 않을 수 있으니 양해 바랍니다__무료 수신거부 1504\n",
            "entities from gem3:  ['갤럭시 퀀텀3', '사은품']\n",
            "entities from claude:  ['갤럭시 퀀텀3']\n",
            "\n",
            "[SK텔레콤] 6월 정원대리점 광주경안동점 SK텔레콤 이벤트 안내드립니다\n",
            "(광고)[SKT] 정원대리점 광주 경안동직영점 6월 이벤트__고객님 안녕하세요._SK텔레콤 공식인증대리점 정원대리점 광주 경안동직영점입니다__■ 전문성, 친절성, 신속성_구매도 상담도 이젠 믿을 수 있는 SK텔레콤 공식인증매장에서_추가할인, 사은품 등 다양한 혜택과_요금할인, 숨어있는포인트 활용 등 꿀Tip상담을 받아보세요__■ SK공식인증대리점 정원대리점 광주 경안동직영점_- 주소 : 경기 광주시 중앙로 118, 1층 (경안동)_- 연락처 : 031-766-4745_- 구술약도 : 광주시외버스터미널 도로 건너 240M 직진 후 사거리에서 우측으로 50M 직진하여 베스킨라빈스 옆건물_▶ 홈페이지 : http://t-mms.kr/t.do?m=#61&s=26625&a=&u=http://tworldfriends.co.kr/D151730027__■ 문의 : SKT 고객센터(1558, 무료)_SK텔레콤과 함께해 주셔서 감사합니다._무료 수신거부 1504\n",
            "entities from gem3:  ['포인트 사용']\n",
            "entities from claude:  []\n",
            "\n",
            "[SK텔레콤] 한유대리점 화곡역점 10월 혜택 안내\n",
            "(광고)[SKT] 한유대리점 화곡역점 10월 혜택 안내__고객님, 안녕하세요. SK텔레콤 최우수 화곡역점에서 10월 특별 혜택을 안내드립니다.__■ 아이폰14 즉시 개통 혜택 (물량 최대 확보)_- 정품 충전기, 도킹 배터리, 강화 유리, 울트라씬 케이스 등 푸짐한 사은품 증정_- 매장 추가 사은품 준비__■ 10월 강력추천 폰, 삼성 와이드6 출시_- 대화면 디스플레이, 오래가는 대용량 배터리, 5천만 고화소, 빠른 5G 성능_- 고객님 사용 패턴에 맞는 실속형 요금제 추천__■ 인터넷+TV 가입 혜택_- 신규 가입 시 최대 45만 원 상당의 푸짐한 사은품 증정_(※기가 라이트 + TV 신청 고객 한정)__■ 모든 구매 고객님께 사은품 100% 무료 증정_- 휴대폰, 인터넷,TV, 캡스, 정수기 등 구매 시 추첨 없이 생활용품 선물셋트 100% 증정__■ 한유대리점 화곡역점_- 주소 : 서울특별시 강서구 화곡로 160 (화곡동, 청산학원), 화곡역 6번 출구_- 연락처 : 02-2690-4050_▶ 매장 홈페이지 가기 : http://t-mms.kr/t.do?m=#61&s=16645&a=&u=http://tworldfriends.co.kr/D137420165__■ 문의: SKT 고객센터(1558, 무료)_SKT와 함께해주셔서 감사합니다.__무료 수신거부 1504\n",
            "entities from gem3:  ['아이폰14', '정수기', '캡스']\n",
            "entities from claude:  ['라이트', '아이폰 14']\n",
            "\n",
            "[SK텔레콤] HK대리점 명지대역점 신규오픈 안내\n",
            "(광고)[SKT] HK대리점 명지대역점 신규 오픈 안내__고객님, 안녕하세요_공식인증 대리점 HK대리점 명지대역점이 6월 1일(목) 신규 오픈합니다.__■ 오픈 이벤트 안내_- 기간: ~ 6월 30일(금)_① 방문하시는 모든 고객님들께 충전케이블 & 휴대폰 그립톡 무료 증정_② 구매 고객님들께는 제조사 정품 충전기와 오픈기념 사은품 추가 증정__■ 상담 안내_- 요금(선택약정)할인 25% 연장_- 가족 결합할인 추가 및 변경_- 불필요한 부가서비스 점검 등 맞춤 상담__언제나 고객님 가까운 곳에서 고객님을 위한 서비스를 제공하겠습니다.__■ HK대리점 명지대역점_- 주소: 경기 용인시 처인구 금학로 237, 1층_- 연락처: 070-4617-0890_▶매장 홈페이지/예약/상담: http://t-mms.kr/t.do?m=#61&s=19935&a=&u=https://tworldfriends.co.kr/stores/D153160003_▶매장 위치: http://t-mms.kr/t.do?m=#61&s=19936&a=&u=http://kko.to/SejN1c99cB__■ 문의: SKT 고객센터(1558, 무료)__SKT와 함께해 주셔서 감사합니다.__무료 수신거부 1504\n",
            "entities from gem3:  []\n",
            "entities from claude:  ['서비스 점검 테스트', '선택약정할인제도']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "stop_item_names = pd.read_csv(\"./data/stop_words.csv\")['stop_words'].to_list()\n",
        "\n",
        "for msg in random.sample(mms_pdf.query(\"msg.str.contains('대리점')\")['msg'].unique().tolist(), 10):\n",
        "\n",
        "    cand_entities = extract_entities_by_logic([msg], threshold_for_fuzzy=0.7)['item_nm_alias'].unique()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extract all product names, including tangible products, services, promotional events, programs, loyalty initiatives, and named campaigns or event identifiers, from the provided advertisement text.\n",
        "    Reference the provided candidate entities list as a guide for potential matches. Extract only those terms from the candidate list that appear in the advertisement text and qualify as distinct product names based on the following criteria.\n",
        "    Consider any named offerings, such as apps, membership programs, events, specific branded items, or campaign names like 'T day' or '0 day', as products if presented as distinct products, services, or promotional entities.\n",
        "    For terms that may be platforms or brand elements, include them only if they are presented as standalone offerings.\n",
        "    Avoid extracting base or parent brand names (e.g., 'FLO' or 'POOQ') if they are components of more specific offerings (e.g., 'FLO 앤 데이터' or 'POOQ 앤 데이터') presented in the text; focus on the full, distinct product or service names as they appear.\n",
        "    Exclude customer support services, such as customer centers or helplines, even if named in the text.\n",
        "    Exclude descriptive modifiers or attributes (e.g., terms like \"디지털 전용\" that describe a product but are not distinct offerings).\n",
        "    Exclude sales agency names such as '###대리점'.\n",
        "    If multiple terms refer to closely related promotional events (e.g., a general campaign and its specific instances or dates), include the most prominent or overarching campaign name (e.g., '0 day' as a named event) in addition to specific offerings tied to it, unless they are clearly identical.\n",
        "    Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering from the candidate list that matches the text.\n",
        "    Ensure that extracted names are presented exactly as they appear in the original text, without translation into English or any other language.\n",
        "    Just return a list with matched entities where the entities are separated by commas without any other text.\n",
        "\n",
        "    ## message:                \n",
        "    {msg}\n",
        "\n",
        "    ## Candidate entities:\n",
        "    {cand_entities}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    entities_gem3 = sorted([e.strip() for e in llm_gem3.invoke(prompt).content.split(\",\") if e.strip() not in stop_item_names and len(e.strip())>=2])\n",
        "    entities_claude = sorted([e.strip() for e in llm_cld40.invoke(prompt).content.split(\",\") if e.strip() not in stop_item_names and len(e.strip())>=2])\n",
        "\n",
        "    print(msg)\n",
        "    print(\"entities from gem3: \", entities_gem3)\n",
        "    print(\"entities from claude: \", entities_claude)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
