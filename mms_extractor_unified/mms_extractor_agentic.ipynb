{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "# from pygments import highlight\n",
        "# from pygments.lexers import JsonLexer\n",
        "# from pygments.formatters import HtmlFormatter\n",
        "# from IPython.display import HTML\n",
        "import pandas as pd\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from config import settings\n",
        "\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "llm_api_key = settings.API_CONFIG.llm_api_key\n",
        "llm_api_url = settings.API_CONFIG.llm_api_url\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "llm_gem = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.gemma_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens,\n",
        "        seed=42  \n",
        "        )\n",
        "\n",
        "llm_ax = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.ax_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens,\n",
        "         seed=42  \n",
        "        )\n",
        "\n",
        "llm_cld = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.claude_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens,\n",
        "        seed=42  \n",
        "        )\n",
        "\n",
        "llm_gen = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.gemini_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens,\n",
        "        seed=42  \n",
        "        )\n",
        "\n",
        "llm_gpt = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=llm_api_key,\n",
        "        openai_api_base=llm_api_url,\n",
        "        model=settings.ModelConfig.gpt_model,\n",
        "        max_tokens=settings.ModelConfig.llm_max_tokens,\n",
        "        seed=42  \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \"\"\"\n",
        "    More advanced JSON repair that handles edge cases better\n",
        "    \"\"\"\n",
        "    json_str = broken_json\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+)\\s*:', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix unquoted values more carefully\n",
        "    # Split on quotes to avoid modifying content inside strings\n",
        "    parts = json_str.split('\"')\n",
        "    \n",
        "    for i in range(0, len(parts), 2):  # Only process parts outside quotes (even indices)\n",
        "        # Fix unquoted values in this part\n",
        "        parts[i] = re.sub(r':\\s*([a-zA-Z0-9_]+)(?=\\s*[,\\]\\}])', r': \"\\1\"', parts[i])\n",
        "    \n",
        "    json_str = '\"'.join(parts)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/gemma3-12b-it\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.0\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n",
        "\n",
        "def convert_to_custom_format(json_items):\n",
        "    custom_format = []\n",
        "    \n",
        "    for item in json_items:\n",
        "        item_name = item.get(\"item_name_in_message\", \"\")\n",
        "        item_id = item.get(\"item_id\", \"\")\n",
        "        category = item.get(\"category\", \"\")\n",
        "        \n",
        "        # Create custom format for each item\n",
        "        custom_line = f\"[Item Name] {item_name} [Item ID] {item_id} [Item Category] {category}\"\n",
        "        custom_format.append(custom_line)\n",
        "    \n",
        "    return \"\\n\".join(custom_format)\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "    # Replace URLs with an empty string\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_custom_pattern(text, keyword=\"바로가기\"):\n",
        "    # Create a pattern that matches any text followed by the specified keyword\n",
        "    # We escape the keyword to handle any special regex characters it might contain\n",
        "    escaped_keyword = re.escape(keyword)\n",
        "    pattern = re.compile(r'.*? ' + escaped_keyword)\n",
        "    \n",
        "    # Replace the matched pattern with an empty string\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "def select_most_comprehensive(strings):\n",
        "    \"\"\"\n",
        "    Select the most comprehensive string from a list of overlapping strings.\n",
        "    Returns the longest string that contains other strings as substrings.\n",
        "    \n",
        "    Args:\n",
        "        strings: List of strings to filter\n",
        "        \n",
        "    Returns:\n",
        "        List of most comprehensive strings (usually one, but could be multiple if no containment)\n",
        "    \"\"\"\n",
        "    if not strings:\n",
        "        return []\n",
        "    \n",
        "    # Remove duplicates and sort by length (longest first)\n",
        "    unique_strings = list(set(strings))\n",
        "    unique_strings.sort(key=len, reverse=True)\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for current in unique_strings:\n",
        "        # Check if current string contains any of the strings already in result\n",
        "        is_contained = any(current in existing for existing in result)\n",
        "        \n",
        "        # Check if current string contains other strings not yet in result\n",
        "        contains_others = any(other in current for other in unique_strings if other != current and other not in result)\n",
        "        \n",
        "        # If current is not contained by existing results and either:\n",
        "        # 1. It contains other strings, or \n",
        "        # 2. No strings contain each other (keep all unique)\n",
        "        if not is_contained:\n",
        "            # Remove any strings from result that are contained in current\n",
        "            result = [r for r in result if r not in current]\n",
        "            result.append(current)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def replace_special_chars_comprehensive(text):\n",
        "    \"\"\"\n",
        "    More comprehensive: Handle various types of special characters.\n",
        "    \"\"\"\n",
        "    # Replace common punctuation with space\n",
        "    punctuation_pattern = r'[!@#$%^&*()_+\\-=\\[\\]{};\\':\"\\\\|,.<>?/~`]'\n",
        "    text = re.sub(punctuation_pattern, ' ', text)\n",
        "    \n",
        "    # Replace other special symbols\n",
        "    symbol_pattern = r'[₩＄￦※◆▲▼◀▶★☆♪♫♬♩♭♯]'\n",
        "    text = re.sub(symbol_pattern, ' ', text)\n",
        "    \n",
        "    # Replace various dashes and quotes\n",
        "    dash_quote_pattern = r'[—–‒―\"\"''‚„‹›«»]'\n",
        "    text = re.sub(dash_quote_pattern, ' ', text)\n",
        "    \n",
        "    # Clean up multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수문자를 공백으로 변환\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # 여러 공백을 하나로 통일\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 앞뒤 공백 제거\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "def advanced_sequential_similarity(str1, str2, metrics=None, visualize=False):\n",
        "    \"\"\"\n",
        "    Calculate multiple character-level similarity metrics between two strings.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    str1 : str\n",
        "        First string\n",
        "    str2 : str\n",
        "        Second string\n",
        "    metrics : list\n",
        "        List of metrics to compute. Options:\n",
        "        ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "        If None, all metrics will be computed\n",
        "    visualize : bool\n",
        "        If True, visualize the differences between strings\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing similarity scores for each metric\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "    results = {}\n",
        "    # Handle empty strings\n",
        "    if not str1 or not str2:\n",
        "        return {metric: 0.0 for metric in metrics+['overall']}\n",
        "    # Prepare strings\n",
        "    s1, s2 = str1.lower(), str2.lower()\n",
        "    # 1. N-gram similarity (with multiple window sizes)\n",
        "    if 'ngram' in metrics:\n",
        "        ngram_scores = {}\n",
        "        for window in range(min([len(s1),len(s2),2]), min([5,max([len(s1),len(s2)])+1])):\n",
        "            # Skip if strings are shorter than window\n",
        "            if len(s1) < window or len(s2) < window:\n",
        "                ngram_scores[f'window_{window}'] = 0.0\n",
        "                continue\n",
        "            # Generate character n-grams\n",
        "            ngrams1 = [s1[i:i+window] for i in range(len(s1) - window + 1)]\n",
        "            ngrams2 = [s2[i:i+window] for i in range(len(s2) - window + 1)]\n",
        "            # Count matches\n",
        "            matches = sum(1 for ng in ngrams1 if ng in ngrams2)\n",
        "            max_possible = max(len(ngrams1), len(ngrams2))\n",
        "            # Normalize\n",
        "            score = matches / max_possible if max_possible > 0 else 0.0\n",
        "            ngram_scores[f'window_{window}'] = score\n",
        "        # Average of all n-gram scores\n",
        "        results['ngram'] = max(ngram_scores.values())#sum(ngram_scores.values()) / len(ngram_scores)\n",
        "        results['ngram_details'] = ngram_scores\n",
        "    # 2. Longest Common Substring (LCS)\n",
        "    if 'lcs' in metrics:\n",
        "        def longest_common_substring(s1, s2):\n",
        "            # Dynamic programming approach\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            max_length = 0\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                        max_length = max(max_length, dp[i][j])\n",
        "            return max_length\n",
        "        lcs_length = longest_common_substring(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['lcs'] = lcs_length / max_length if max_length > 0 else 0.0\n",
        "    # 3. Longest Common Subsequence\n",
        "    if 'subsequence' in metrics:\n",
        "        def longest_common_subsequence(s1, s2):\n",
        "            # Dynamic programming approach for subsequence\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "            return dp[m][n]\n",
        "        subseq_length = longest_common_subsequence(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['subsequence'] = subseq_length / max_length if max_length > 0 else 0.0\n",
        "    # 4. SequenceMatcher from difflib\n",
        "    if 'difflib' in metrics:\n",
        "        sm = SequenceMatcher(None, s1, s2)\n",
        "        results['difflib'] = sm.ratio()\n",
        "    # Visualization of differences\n",
        "    if visualize:\n",
        "        try:\n",
        "            # Only works in notebooks or environments that support plotting\n",
        "            sm = SequenceMatcher(None, s1, s2)\n",
        "            matches = sm.get_matching_blocks()\n",
        "            # Prepare for visualization\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            # Draw strings as horizontal bars\n",
        "            ax.barh(0, len(s1), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            ax.barh(1, len(s2), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            # Draw matching parts\n",
        "            for match in matches:\n",
        "                i, j, size = match\n",
        "                if size > 0:  # Ignore zero-length matches\n",
        "                    ax.barh(0, size, height=0.4, left=i, color='green', alpha=0.5)\n",
        "                    ax.barh(1, size, height=0.4, left=j, color='green', alpha=0.5)\n",
        "                    # Draw connection lines between matches\n",
        "                    ax.plot([i + size/2, j + size/2], [0.2, 0.8], 'k-', alpha=0.3)\n",
        "            # Add string texts\n",
        "            for i, c in enumerate(s1):\n",
        "                ax.text(i + 0.5, 0, c, ha='center', va='center')\n",
        "            for i, c in enumerate(s2):\n",
        "                ax.text(i + 0.5, 1, c, ha='center', va='center')\n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_yticklabels(['String 1', 'String 2'])\n",
        "            ax.set_xlabel('Character Position')\n",
        "            ax.set_title('Character-Level String Comparison')\n",
        "            ax.grid(False)\n",
        "            plt.tight_layout()\n",
        "            # plt.show()  # Uncomment to display\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "    # Calculate overall similarity score (average of all metrics)\n",
        "    metrics_to_average = [m for m in results.keys() if not m.endswith('_details')]\n",
        "    results['overall'] = sum(results[m] for m in metrics_to_average) / len(metrics_to_average)\n",
        "    return results\n",
        "# advanced_sequential_similarity('시크릿', '시크릿', metrics='ngram')\n",
        "# advanced_sequential_similarity('에이닷_자사', '에이닷')['overall']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import difflib\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "def longest_common_subsequence_ratio(s1, s2, normalizaton_value):\n",
        "    \"\"\"\n",
        "    Calculate similarity based on longest common subsequence (LCS).\n",
        "    Preserves order and gives high scores for substring relationships.\n",
        "    \"\"\"\n",
        "    def lcs_length(x, y):\n",
        "        m, n = len(x), len(y)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if x[i-1] == y[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    lcs_len = lcs_length(s1, s2)\n",
        "    if normalizaton_value == 'max':\n",
        "        max_len = max(len(s1), len(s2))\n",
        "        return lcs_len / max_len if max_len > 0 else 1.0\n",
        "    elif normalizaton_value == 'min':\n",
        "        min_len = min(len(s1), len(s2))\n",
        "        return lcs_len / min_len if min_len > 0 else 1.0\n",
        "    elif normalizaton_value == 's1':\n",
        "        return lcs_len / len(s1) if len(s1) > 0 else 1.0\n",
        "    elif normalizaton_value == 's2':\n",
        "        return lcs_len / len(s2) if len(s2) > 0 else 1.0\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid normalization value: {normalizaton_value}\")\n",
        "\n",
        "# def sequence_matcher_similarity(s1, s2):\n",
        "#     \"\"\"\n",
        "#     Use Python's built-in SequenceMatcher which considers sequence order.\n",
        "#     \"\"\"\n",
        "#     return SequenceMatcher(None, s1, s2).ratio()\n",
        "\n",
        "def sequence_matcher_similarity(s1, s2, normalizaton_value):\n",
        "    \"\"\"Normalize by minimum length (favors shorter strings)\"\"\"\n",
        "    matcher = difflib.SequenceMatcher(None, s1, s2)\n",
        "    matches = sum(triple.size for triple in matcher.get_matching_blocks())\n",
        "\n",
        "    normalization_length = min(len(s1), len(s2))\n",
        "    if normalizaton_value == 'max':\n",
        "        normalization_length = max(len(s1), len(s2))\n",
        "    elif normalizaton_value == 's1':\n",
        "        normalization_length = len(s1)\n",
        "    elif normalizaton_value == 's2':\n",
        "        normalization_length = len(s2)\n",
        "        \n",
        "    if normalization_length == 0: \n",
        "        return 0.0\n",
        "    \n",
        "    return matches / normalization_length\n",
        "\n",
        "def substring_aware_similarity(s1, s2, normalizaton_value):\n",
        "    \"\"\"\n",
        "    Custom similarity that heavily weights substring relationships\n",
        "    while considering sequence order.\n",
        "    \"\"\"\n",
        "    # Check if one is a substring of the other\n",
        "    if s1 in s2 or s2 in s1:\n",
        "        shorter = min(s1, s2, key=len)\n",
        "        longer = max(s1, s2, key=len)\n",
        "        # High base score for substring relationship\n",
        "        base_score = len(shorter) / len(longer)\n",
        "        # Bonus for exact substring match\n",
        "        return min(0.95 + base_score * 0.05, 1.0)\n",
        "    \n",
        "    # Use LCS ratio for non-substring cases\n",
        "    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)\n",
        "\n",
        "def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\\s_\\-]+'):\n",
        "    \"\"\"\n",
        "    Tokenize strings and calculate similarity based on token sequence overlap.\n",
        "    Good for product names with separators.\n",
        "    \"\"\"\n",
        "    tokens1 = re.split(separator_pattern, s1.strip())\n",
        "    tokens2 = re.split(separator_pattern, s2.strip())\n",
        "    \n",
        "    # Remove empty tokens\n",
        "    tokens1 = [t for t in tokens1 if t]\n",
        "    tokens2 = [t for t in tokens2 if t]\n",
        "    \n",
        "    if not tokens1 or not tokens2:\n",
        "        return 0.0\n",
        "    \n",
        "    # Find longest common subsequence of tokens\n",
        "    def token_lcs_length(t1, t2):\n",
        "        m, n = len(t1), len(t2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if t1[i-1] == t2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    lcs_tokens = token_lcs_length(tokens1, tokens2)\n",
        "    normalization_tokens = max(len(tokens1), len(tokens2))\n",
        "    if normalizaton_value == 'min':\n",
        "        normalization_tokens = min(len(tokens1), len(tokens2))\n",
        "    elif normalizaton_value == 's1':\n",
        "        normalization_tokens = len(tokens1)\n",
        "    elif normalizaton_value == 's2':\n",
        "        normalization_tokens = len(tokens2)\n",
        "    \n",
        "    # print(normalizaton_value, normalization_tokens, lcs_tokens)\n",
        "        \n",
        "    return lcs_tokens / normalization_tokens  \n",
        "\n",
        "def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):\n",
        "    \"\"\"\n",
        "    Combine multiple sequence-aware similarity measures.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            'substring': 0.4,\n",
        "            'sequence_matcher': 0.4,\n",
        "            'token_sequence': 0.2\n",
        "        }\n",
        "    \n",
        "    similarities = {\n",
        "        'substring': substring_aware_similarity(s1, s2, normalizaton_value),\n",
        "        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),\n",
        "        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)\n",
        "    }\n",
        "    \n",
        "    combined = sum(similarities[key] * weights[key] for key in weights)\n",
        "    return combined, similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def fuzzy_similarities(text, entities):\n",
        "    results = []\n",
        "    for entity in entities:\n",
        "        scores = {\n",
        "            'ratio': fuzz.ratio(text, entity) / 100,\n",
        "            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,\n",
        "            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,\n",
        "            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100\n",
        "        }\n",
        "        max_score = max(scores.values())\n",
        "        results.append((entity, max_score))\n",
        "    return results\n",
        "\n",
        "def get_fuzzy_similarities(args_dict):\n",
        "    text = args_dict['text']\n",
        "    entities = args_dict['entities']\n",
        "    threshold = args_dict['threshold']\n",
        "    text_col_nm = args_dict['text_col_nm']\n",
        "    item_col_nm = args_dict['item_col_nm']\n",
        "\n",
        "    # Get similarities using auto method selection\n",
        "    text_processed = preprocess_text(text.lower())\n",
        "    similarities = fuzzy_similarities(text_processed, entities)\n",
        "    \n",
        "    # Filter by threshold and create DataFrame\n",
        "    filtered_results = [\n",
        "        {\n",
        "            text_col_nm: text,\n",
        "            item_col_nm: entity, \n",
        "            \"sim\": score\n",
        "        } \n",
        "        for entity, score in similarities \n",
        "        if score >= threshold\n",
        "    ]\n",
        "    \n",
        "    return filtered_results\n",
        "\n",
        "def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):\n",
        "    \"\"\"\n",
        "    Batched version for better performance with large datasets.\n",
        "    \"\"\"\n",
        "    if n_jobs is None:\n",
        "        n_jobs = min(os.cpu_count()-1, 8)  # Limit to 8 jobs max\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = max(1, len(entities) // (n_jobs * 2))\n",
        "        \n",
        "    # Create batches\n",
        "    batches = []\n",
        "    for text in texts:\n",
        "        for i in range(0, len(entities), batch_size):\n",
        "            batch = entities[i:i + batch_size]\n",
        "            batches.append({\"text\": text, \"entities\": batch, \"threshold\": threshold, \"text_col_nm\": text_col_nm, \"item_col_nm\": item_col_nm})\n",
        "    \n",
        "    # print(f\"Processing {len(item_list)} items in {len(batches)|} batches with {n_jobs} jobs...\")\n",
        "    \n",
        "    # Run parallel jobs\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)\n",
        "    \n",
        "    # # Flatten results\n",
        "    # similarities = []\n",
        "    # for batch_result in batch_results:\n",
        "    #     similarities.extend(batch_result)\n",
        "    \n",
        "    return pd.DataFrame(sum(batch_results, []))\n",
        "\n",
        "def calculate_seq_similarity(args_dict):\n",
        "    \"\"\"\n",
        "    Process a batch of items in one job for better efficiency.\n",
        "    \"\"\"\n",
        "    sent_item_batch = args_dict['sent_item_batch']\n",
        "    text_col_nm = args_dict['text_col_nm']\n",
        "    item_col_nm = args_dict['item_col_nm']\n",
        "    normalizaton_value = args_dict['normalizaton_value']\n",
        "    \n",
        "    results = []\n",
        "    for sent_item in sent_item_batch:\n",
        "        sent = sent_item[text_col_nm]\n",
        "        item = sent_item[item_col_nm]\n",
        "        try:\n",
        "            sent_processed = preprocess_text(sent.lower())\n",
        "            item_processed = preprocess_text(item.lower())\n",
        "            similarity = combined_sequence_similarity(sent_processed, item_processed, normalizaton_value=normalizaton_value)[0]\n",
        "            results.append({text_col_nm:sent, item_col_nm:item, \"sim\":similarity})\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item}: {e}\")\n",
        "            results.append({text_col_nm:sent, item_col_nm:item, \"sim\":0.0})\n",
        "    \n",
        "    return results\n",
        "\n",
        "def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, normalizaton_value='s2'):\n",
        "    \"\"\"\n",
        "    Batched version for better performance with large datasets.\n",
        "    \"\"\"\n",
        "    if n_jobs is None:\n",
        "        n_jobs = min(os.cpu_count()-1, 8)  # Limit to 8 jobs max\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))\n",
        "        \n",
        "    # Create batches\n",
        "    batches = []\n",
        "    for i in range(0, sent_item_pdf.shape[0], batch_size):\n",
        "        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')\n",
        "        batches.append({\"sent_item_batch\": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'normalizaton_value': normalizaton_value})\n",
        "    \n",
        "    # print(f\"Processing {len(item_list)} items in {len(batches)|} batches with {n_jobs} jobs...\")\n",
        "    \n",
        "    # Run parallel jobs\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)\n",
        "    \n",
        "    # Flatten results\n",
        "    # similarities = []\n",
        "    # for batch_result in batch_results:\n",
        "    #     similarities.extend(batch_result)\n",
        "    \n",
        "    return pd.DataFrame(sum(batch_results, []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "def save_embeddings_numpy(embeddings, texts, filename):\n",
        "    \"\"\"\n",
        "    Save embeddings as NumPy arrays (.npz format).\n",
        "    Most common and efficient method.\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(embeddings):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    np.savez_compressed(\n",
        "        filename,\n",
        "        embeddings=embeddings,\n",
        "        texts=texts,\n",
        "        timestamp=str(datetime.now())\n",
        "    )\n",
        "    print(f\"✅ Saved embeddings to {filename}\")\n",
        "def load_embeddings_numpy(filename):\n",
        "    \"\"\"Load embeddings from NumPy .npz file.\"\"\"\n",
        "    data = np.load(filename, allow_pickle=True)\n",
        "    embeddings = data['embeddings']\n",
        "    texts = data['texts']\n",
        "    timestamp = data['timestamp'] if 'timestamp' in data else None\n",
        "    print(f\"✅ Loaded {len(embeddings)} embeddings from {filename}\")\n",
        "    if timestamp:\n",
        "        print(f\"   Created: {timestamp}\")\n",
        "    return embeddings, texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "def save_sentence_transformer(model_name, save_path):\n",
        "    \"\"\"Download and save SentenceTransformer model locally\"\"\"\n",
        "    print(f\"Downloading {model_name}...\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    # Save the model\n",
        "    model.save(save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "    return model\n",
        "def load_sentence_transformer(model_path, device=None):\n",
        "    \"\"\"Load SentenceTransformer model from local path\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = SentenceTransformer(model_path).to(device)\n",
        "    print(f\"Model loaded on {device}\")\n",
        "    return model\n",
        "# Usage\n",
        "# Save model (do this once)\n",
        "# model = save_sentence_transformer('jhgan/ko-sbert-nli', './models/ko-sbert-nli')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1\n",
            "MPS available: True\n",
            "MPS built: True\n",
            "✅ MPS is available and ready to use!\n",
            "Loading model from ./models/ko-sbert-nli...\n",
            "Model loaded on mps\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def check_mps_availability():\n",
        "    \"\"\"Check if MPS is available on this Mac.\"\"\"\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "    print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
        "    \n",
        "    if torch.backends.mps.is_available():\n",
        "        print(\"✅ MPS is available and ready to use!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"❌ MPS is not available. Using CPU instead.\")\n",
        "        return False\n",
        "\n",
        "mps_available = check_mps_availability()\n",
        "    \n",
        "# Determine device\n",
        "if mps_available:\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# emb_model = SentenceTransformer('jhgan/ko-sbert-nli').to(device)\n",
        "emb_model = load_sentence_transformer('./models/ko-sbert-nli', device)\n",
        "# emb_model = SentenceTransformer('jhgan/ko-sroberta-multitask').to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_pdf_raw = pd.read_csv(settings.METADATA_CONFIG.offer_data_path)\n",
        "\n",
        "item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','item_dmn']].copy()\n",
        "item_pdf_all['item_ctg'] = None\n",
        "item_pdf_all['item_emb_vec'] = None\n",
        "item_pdf_all['ofer_cd'] = item_pdf_all['item_id']\n",
        "item_pdf_all['oper_dt_hms'] = '20250101000000'\n",
        "\n",
        "item_pdf_all = item_pdf_all.rename(columns={c:c.lower() for c in item_pdf_all.columns})\n",
        "\n",
        "if settings.PROCESSING_CONFIG.excluded_domain_codes_for_items:\n",
        "    item_pdf_all = item_pdf_all.query(\"item_dmn not in @settings.PROCESSING_CONFIG.excluded_domain_codes_for_items\")\n",
        "\n",
        "# item_pdf_all.query(\"rank<1000\")[['item_nm']].drop_duplicates().to_csv(\"./data/item_nm_1000.csv\", index=False)\n",
        "alias_pdf = pd.read_csv(settings.METADATA_CONFIG.alias_rules_path)\n",
        "alia_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))\n",
        "\n",
        "def apply_alias_rule(item_nm):\n",
        "    item_nm_list = [item_nm]\n",
        "\n",
        "    for r in alia_rule_set:\n",
        "        if r[0] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[0], r[1]))\n",
        "        if r[1] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[1], r[0]))\n",
        "    return item_nm_list\n",
        "\n",
        "item_pdf_all['item_nm_alias'] = item_pdf_all['item_nm'].apply(apply_alias_rule)\n",
        "\n",
        "item_pdf_all = item_pdf_all.explode('item_nm_alias')\n",
        "\n",
        "user_defined_entity = ['AIA Vitality' , '부스트 파크 건대입구' , 'Boost Park 건대입구']\n",
        "item_pdf_ext = pd.DataFrame([{'item_nm':e,'item_id':e,'item_desc':e, 'item_dmn':'user_defined', 'start_dt':20250101, 'end_dt':99991231, 'rank':1, 'item_nm_alias':e} for e in user_defined_entity])\n",
        "item_pdf_all = pd.concat([item_pdf_all,item_pdf_ext])\n",
        "\n",
        "stop_item_names = pd.read_csv(settings.METADATA_CONFIG.stop_items_path)['stop_words'].to_list()\n",
        "\n",
        "entity_vocab = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    if row['item_nm_alias'] in stop_item_names:\n",
        "        continue\n",
        "    entity_vocab.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'item_dmn':row['item_dmn'],'item_nm_alias':row['item_nm_alias']}))\n",
        "\n",
        "entity_list_for_fuzzy = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    entity_list_for_fuzzy.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'item_dmn':row['item_dmn'], 'start_dt':row['start_dt'], 'end_dt':row['end_dt'], 'rank':1, 'item_nm_alias':row['item_nm_alias']}))\n",
        "\n",
        "# text_list_item = [preprocess_text(x).lower() for x in item_pdf_all['item_nm_alias'].tolist()]\n",
        "# item_embeddings = emb_model.encode(text_list_item\n",
        "#                             # ,batch_size=64  # Optimal for MPS\n",
        "#                             ,convert_to_tensor=True\n",
        "#                             ,show_progress_bar=True)\n",
        "\n",
        "# save_embeddings_numpy(item_embeddings, text_list_item, './data/item_embeddings_250527.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "mms_pdf = pd.read_csv(settings.METADATA_CONFIG.mms_msg_path)\n",
        "# mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "# mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "# mms_pdf = mms_pdf.reset_index()\n",
        "mms_pdf = mms_pdf.astype('str')\n",
        "# mms_pdf['msg_id'] = mms_pdf.index\n",
        "# mms_pdf['ext_yn'] = 'N'\n",
        "\n",
        "# mms_pdf.drop(columns=['ext_yn'], inplace=True)\n",
        "# mms_pdf.to_csv(settings.METADATA_CONFIG.mms_msg_path, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "num_cand_pgms = 5\n",
        "pgm_pdf = pd.read_csv(settings.METADATA_CONFIG.pgm_info_path)\n",
        "\n",
        "clue_embeddings = emb_model.encode(pgm_pdf[[\"pgm_nm\",\"clue_tag\"]].apply(lambda x: preprocess_text(x['pgm_nm'].lower())+\" \"+x['clue_tag'].lower(), axis=1).tolist()\n",
        "                            # ,batch_size=64  # Optimal for MPS\n",
        "                            ,convert_to_tensor=True\n",
        "                            ,show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "org_pdf = pd.read_csv(settings.METADATA_CONFIG.org_info_path, encoding='cp949')\n",
        "org_pdf['sub_org_cd'] = org_pdf['sub_org_cd'].apply(lambda x: x.zfill(4))\n",
        "# text_list_org_all = org_pdf[[\"org_abbr_nm\",\"bas_addr\",\"dtl_addr\"]].apply(lambda x: preprocess_text(x['org_abbr_nm'].lower())+\" \"+x['bas_addr'].lower()+\" \"+x['dtl_addr'].lower(), axis=1).tolist()\n",
        "# org_all_embeddings = emb_model.encode(text_list_org_all\n",
        "#                     # ,batch_size=32  # Optimal for MPS\n",
        "#                     ,convert_to_tensor=True\n",
        "#                     ,show_progress_bar=True)\n",
        "# save_embeddings_numpy(org_all_embeddings, text_list_org_all, './data/org_all_embeddings_250605.npz')\n",
        "# text_list_org_nm = org_pdf[[\"org_abbr_nm\"]].apply(lambda x: preprocess_text(x['org_abbr_nm'].lower()), axis=1).tolist()\n",
        "# org_nm_embeddings = emb_model.encode(text_list_org_nm\n",
        "#                     # ,batch_size=32  # Optimal for MPS\n",
        "#                     ,convert_to_tensor=True\n",
        "#                     ,show_progress_bar=True)\n",
        "# save_embeddings_numpy(org_nm_embeddings, text_list_org_nm, './data/org_nm_embeddings_250605.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# item_embeddings, text_list_item = load_embeddings_numpy('./data/item_embeddings_250527.npz')\n",
        "# org_all_embeddings, text_list_org_all = load_embeddings_numpy('./data/org_all_embeddings_250605.npz')\n",
        "# org_nm_embeddings, text_list_org_nm = load_embeddings_numpy('./data/org_nm_embeddings_250605.npz')\n",
        "# item_embeddings = torch.from_numpy(item_embeddings).to(device)\n",
        "# org_all_embeddings = torch.from_numpy(org_all_embeddings).to(device)\n",
        "# org_nm_embeddings = torch.from_numpy(org_nm_embeddings).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_df_to_json_list(df):\n",
        "    \"\"\"\n",
        "    Convert DataFrame to the specific JSON structure you want\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    # Group by 'item_name_in_msg' to create the main structure\n",
        "    grouped = df.groupby('item_name_in_msg')\n",
        "    for item_name_in_msg, group in grouped:\n",
        "        # Create the main item dictionary\n",
        "        item_dict = {\n",
        "            'item_name_in_msg': item_name_in_msg,\n",
        "            'item_in_voca': []\n",
        "        }\n",
        "        # Group by item_nm within each item_name_in_msg to collect item_ids\n",
        "        item_nm_groups = group.groupby('item_nm')\n",
        "        for item_nm, item_group in item_nm_groups:\n",
        "            # Collect all item_ids for this item_nm\n",
        "            item_ids = list(item_group['item_id'].unique())\n",
        "            voca_item = {\n",
        "                'item_nm': item_nm,\n",
        "                'item_id': item_ids\n",
        "            }\n",
        "            item_dict['item_in_voca'].append(voca_item)\n",
        "        result.append(item_dict)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">개채명 추출기 (Kiwi)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "# Simple approach\n",
        "kiwi = Kiwi()\n",
        "entity_list_for_kiwi = list(item_pdf_all['item_nm_alias'].unique())\n",
        "for w in entity_list_for_kiwi:\n",
        "    kiwi.add_user_word(w, \"NNP\")\n",
        "# for w in stop_item_names:\n",
        "#     kiwi.add_user_word(w, \"NNG\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_text_by_exc_patterns(sentence, exc_tag_patterns):\n",
        "    \"\"\"\n",
        "    Create a new text by replacing tokens that match exclusion tag patterns with whitespace.\n",
        "    Handles both individual tags and consecutive tag sequences.\n",
        "    Preserves original whitespace from the source text.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Separate individual tags from sequences\n",
        "    individual_tags = set()\n",
        "    sequences = []\n",
        "    \n",
        "    for pattern in exc_tag_patterns:\n",
        "        if isinstance(pattern, list):\n",
        "            if len(pattern) == 1:\n",
        "                individual_tags.add(pattern[0])\n",
        "            else:\n",
        "                sequences.append(pattern)\n",
        "        else:\n",
        "            individual_tags.add(pattern)\n",
        "    \n",
        "    # Track which tokens to exclude\n",
        "    tokens_to_exclude = set()\n",
        "    \n",
        "    # Check for individual tag matches\n",
        "    for i, token in enumerate(sentence.tokens):\n",
        "        if token.tag in individual_tags:\n",
        "            tokens_to_exclude.add(i)\n",
        "    \n",
        "    # Check for sequence matches\n",
        "    for sequence in sequences:\n",
        "        seq_len = len(sequence)\n",
        "        for i in range(len(sentence.tokens) - seq_len + 1):\n",
        "            # Check if consecutive tokens match the sequence\n",
        "            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):\n",
        "                # Mark all tokens in this sequence for exclusion\n",
        "                for j in range(seq_len):\n",
        "                    tokens_to_exclude.add(i + j)\n",
        "    \n",
        "    # Create a character array from the original text\n",
        "    result_chars = list(sentence.text)\n",
        "    \n",
        "    # Replace excluded tokens with whitespace while preserving original whitespace\n",
        "    for i, token in enumerate(sentence.tokens):\n",
        "        if i in tokens_to_exclude:\n",
        "            # Replace token characters with spaces, but keep original whitespace intact\n",
        "            start_pos = token.start - sentence.start  # Adjust for sentence start offset\n",
        "            end_pos = start_pos + token.len\n",
        "            for j in range(start_pos, end_pos):\n",
        "                if j < len(result_chars) and result_chars[j] != ' ':\n",
        "                    result_chars[j] = ' '\n",
        "    \n",
        "    # Join the character array to create filtered text\n",
        "    filtered_text = ''.join(result_chars)\n",
        "\n",
        "    #Replace consecutive whitespaces with a single whitespace\n",
        "    filtered_text = re.sub(r'\\s+', ' ', filtered_text)\n",
        "    \n",
        "    return filtered_text\n",
        "\n",
        "# Define Token and Sentence classes\n",
        "class Token:\n",
        "    def __init__(self, form, tag, start, length):\n",
        "        self.form = form\n",
        "        self.tag = tag\n",
        "        self.start = start\n",
        "        self.len = length\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, text, start, end, tokens, subs=None):\n",
        "        self.text = text\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.tokens = tokens\n",
        "        self.subs = subs or []\n",
        "\n",
        "exc_tag_patterns = [['SN', 'NNB'],\n",
        " ['W_SERIAL'],\n",
        " ['JKO'],\n",
        " ['W_URL'],\n",
        " ['W_EMAIL'],\n",
        " ['XSV', 'EC'],\n",
        " ['VV', 'EC'],\n",
        " ['VCP', 'ETM'],\n",
        " ['XSA', 'ETM'],\n",
        " ['VV', 'ETN'],\n",
        " ['W_SERIAL'],\n",
        " ['W_URL'],\n",
        " ['JKO'],\n",
        " ['SSO'],\n",
        " ['SSC'],\n",
        " ['SW'],\n",
        " ['SF'],\n",
        " ['SP'],\n",
        " ['SS'],\n",
        " ['SE'],\n",
        " ['SO'],\n",
        " ['SB'],\n",
        " ['SH'],\n",
        " ['W_HASHTAG']\n",
        " ]\n",
        "\n",
        "# sentence = sentences[1]\n",
        "\n",
        "# Apply the filtering\n",
        "# filtered_text = filter_text_by_exc_patterns(sentence, exc_tag_patterns)\n",
        "\n",
        "# print(\"Original text:\", repr(sentence.text))\n",
        "# print(\"Filtered text:\", repr(filtered_text))\n",
        "\n",
        "# # Show which tokens were excluded\n",
        "# print(\"\\nToken analysis:\")\n",
        "# individual_tags = set()\n",
        "# sequences = []\n",
        "\n",
        "# for pattern in exc_tag_patterns:\n",
        "#     if isinstance(pattern, list):\n",
        "#         if len(pattern) == 1:\n",
        "#             individual_tags.add(pattern[0])\n",
        "#         else:\n",
        "#             sequences.append(pattern)\n",
        "#     else:\n",
        "#         individual_tags.add(pattern)\n",
        "\n",
        "# print(\"Individual exclusion tags:\", individual_tags)\n",
        "# print(\"Sequence exclusion patterns:\", sequences)\n",
        "# print()\n",
        "\n",
        "# for i, token in enumerate(sentence.tokens):\n",
        "#     status = \"EXCLUDED\" if token.tag in individual_tags else \"KEPT\"\n",
        "#     print(f\"Token {i}: '{token.form}' ({token.tag}) at pos {token.start}-{token.start + token.len - 1} - {status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "msg_text_list = [\"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:통화 부가서비스를 패키지로 저렴하게!\n",
        "광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ',\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "(광고)[SKT] 1월 0 day 혜택 안내_ _[1월 20일(토) 혜택]_만 13~34세 고객이라면 _CU에서 핫바 1,000원에 구매 하세요!_(선착순 1만 명 증정)_▶ 자세히 보기 : http://t-mms.kr/t.do?m=#61&s=24264&a=&u=https://bit.ly/3H2OHSs__■ 에이닷 X T 멤버십 구독캘린더 이벤트_0 day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요! _알림 설정하면 추첨을 통해 [스타벅스 카페 라떼tall 모바일쿠폰]을 드립니다. _▶ 이벤트 참여하기 : https://bit.ly/3RVSojv_ _■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "'[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:[SK텔레콤] T건강습관 X AIA Vitality, 우리 가족의 든든한 보험!\n",
        "광고 내용:(광고)[SKT] 가족의 든든한 보험 (무배당)AIA Vitality 베스트핏 보장보험 안내  고객님, 안녕하세요. 4인 가족 표준생계비, 준비하고 계시나요? (무배당)AIA Vitality 베스트핏 보장보험(디지털 전용)으로 최대 20% 보험료 할인과 가족의 든든한 보험 보장까지 누려 보세요.   ▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/36oWjgX  ■ AIA Vitality  혜택 - 매달 리워드 최대 12,000원 - 등급 업그레이드 시 특별 리워드 - T건강습관 제휴 할인 최대 40% ※ 제휴사별 할인 조건과 주간 미션 달성 혜택 등 자세한 내용은 AIA Vitality 사이트에서 확인하세요. ※ 이 광고는 AIA생명의 광고이며 SK텔레콤은 모집 행위를 하지 않습니다.  - 보험료 납입 기간 중 피보험자가 장해분류표 중 동일한 재해 또는 재해 이외의 동일한 원인으로 여러 신체 부위의 장해지급률을 더하여 50% 이상인 장해 상태가 된 경우 차회 이후의 보험료 납입 면제 - 사망보험금은 계약일(부활일/효력회복일)로부터 2년 안에 자살한 경우 보장하지 않음 - 일부 특약 갱신 시 보험료 인상 가능 - 기존 계약 해지 후 신계약 체결 시 보험인수 거절, 보험료 인상, 보장 내용 변경 가능 - 해약 환급금(또는 만기 시 보험금이나 사고보험금)에 기타 지급금을 합해 5천만 원까지(본 보험 회사 모든 상품 합산) 예금자 보호 - 계약 체결 전 상품 설명서 및 약관 참조 - 월 보험료 5,500원(부가세 포함)  * 생명보험협회 심의필 제2020-03026호(2020-09-22) COM-2020-09-32426  ■문의: 청약 관련(1600-0880)  무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "[SK텔레콤]추석맞이 추가할인 쿠폰 증정\n",
        "(광고)[SKT]공식인증매장 고촌점 추석맞이 행사__안녕하세요 고객님!_고촌역 1번 출구 고촌파출소 방향 100m SK텔레콤 대리점 입니다._스마트폰 개통, 인터넷/TV 설치 시 조건 없이 추가 할인 행사를 진행합니다.__■삼성 갤럭시 Z플립5/Z폴드5는_  9월 내내 즉시개통 가능!!_1.갤럭시 워치6 개통 시 추가 할인_2.삼성케어+ 파손보장 1년권_3.삼성 정품 악세사리 30% 할인 쿠폰_4.정품 보호필름 1회 무료 부착__■새로운 아이폰15 출시 전_  아이폰14 재고 대방출!!_1.투명 범퍼 케이스 증정_2.방탄 유리 필름 부착_3.25W C타입 PD 충전기__여기에 5만원 추가 할인 적용!!__■기가인터넷+IPTV 가입 시_1.최대 36만원 상당 상품권 지급_2.스마트폰 개통 시 10만원 할인_3.매장 특별 사은품 지급_(특별 사은품은 매장 상황에 따라 변경될 수 있습니다)__■SKT 공식인증매장 고촌점_- 주소: 경기 김포시 고촌읍 장차로 3, SK텔레콤_- 연락처: 0507-1480-7833_- 네이버 예약하기: http://t-mms.kr/bSo/#74_- 매장 홈페이지: http://t-mms.kr/bSt/#74__■ 문의 : SKT 고객센터(1558, 무료)_무료 수신거부 1504_\n",
        "\"\"\"\n",
        "]\n",
        "message_idx = 0\n",
        "mms_msg = msg_text_list[message_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_specific_terms(strings: List[str]) -> List[str]:\n",
        "    unique_strings = list(set(strings))  # 중복 제거\n",
        "    unique_strings.sort(key=len, reverse=True)  # 길이 기준 내림차순 정렬\n",
        "\n",
        "    filtered = []\n",
        "    for s in unique_strings:\n",
        "        if not any(s in other for other in filtered):\n",
        "            filtered.append(s)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def extract_entities_from_kiwi(mms_msg, item_pdf_all, stop_item_names):\n",
        "    sentences = sum(kiwi.split_into_sents(re.split(r\"_+\",mms_msg), return_tokens=True, return_sub_sents=True), [])\n",
        "    # sentence_list = [sent.text.strip() for sent in sentences if sent.text.strip()]\n",
        "\n",
        "    sentences_all = []\n",
        "    for sent in sentences:\n",
        "        # print(sent.text.strip())\n",
        "        # print(\"-\"*100)\n",
        "        if sent.subs:\n",
        "            for sub_sent in sent.subs:\n",
        "                sentences_all.append(sub_sent)\n",
        "        else:\n",
        "            sentences_all.append(sent)\n",
        "\n",
        "    sentence_list = []\n",
        "    for sent in sentences_all:\n",
        "        # print(sent.text, \" --> \", filter_text_by_exc_patterns(sent, exc_tag_patterns))\n",
        "        sentence_list.append(filter_text_by_exc_patterns(sent, exc_tag_patterns))\n",
        "\n",
        "    result_msg = kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "    entities_from_kiwi = []\n",
        "    for token in result_msg:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "        if token.tag == 'NNP' and token.form not in stop_item_names+['-'] and len(token.form)>=2 and not token.form.lower() in stop_item_names:  # 고유명사인 경우\n",
        "        # if token.tag == 'NNG' and token.form in stop_item_names_ext:  # 고유명사인 경우\n",
        "            entities_from_kiwi.append(token.form)\n",
        "\n",
        "    from typing import List\n",
        "\n",
        "    entities_from_kiwi = filter_specific_terms(entities_from_kiwi)\n",
        "\n",
        "    print(\"추출된 개체명:\", list(set(entities_from_kiwi)))\n",
        "\n",
        "    similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "        sentence_list, \n",
        "        item_pdf_all['item_nm_alias'].unique(), \n",
        "        threshold=0.4,\n",
        "        text_col_nm='sent',\n",
        "        item_col_nm='item_nm_alias',\n",
        "        n_jobs=6,\n",
        "        batch_size=30\n",
        "    )\n",
        "\n",
        "    similarities_seq = parallel_seq_similarity(\n",
        "        sent_item_pdf=similarities_fuzzy,\n",
        "        text_col_nm='sent',\n",
        "        item_col_nm='item_nm_alias',\n",
        "        n_jobs=6,\n",
        "        batch_size=100\n",
        "    )\n",
        "\n",
        "    cand_items = similarities_seq.query(\"sim>=0.7 and item_nm_alias.str.contains('', case=False) and item_nm_alias not in @stop_item_names\")\n",
        "\n",
        "    entities_from_kiwi_pdf = item_pdf_all.query(\"item_nm_alias in @entities_from_kiwi\")[['item_nm','item_nm_alias']]\n",
        "    entities_from_kiwi_pdf['sim'] = 1.0\n",
        "\n",
        "    cand_item_pdf = pd.concat([cand_items,entities_from_kiwi_pdf])\n",
        "    cand_item_list = cand_item_pdf.sort_values('sim', ascending=False).groupby([\"item_nm_alias\"])['sim'].max().reset_index(name='final_sim').sort_values('final_sim', ascending=False).query(\"final_sim>=0.2\")['item_nm_alias'].unique()\n",
        "\n",
        "    # product_tag = [{\"item_name_in_msg\":d['item_nm'], \"item_in_voca\":[{\"item_name_in_voca\":d['item_nm'], \"item_id\":d['item_id']}]} for d in item_pdf_all.query(\"item_nm_alias in @cand_item_list\")[['item_nm','item_nm_alias','item_id']].groupby([\"item_nm\"])['item_id'].apply(list).reset_index().to_dict(orient='records')]\n",
        "\n",
        "    # product_tag    \n",
        "\n",
        "    extra_item_pdf = item_pdf_all.query(\"item_nm_alias in @cand_item_list\")[['item_nm','item_nm_alias','item_id']].groupby([\"item_nm\"])['item_id'].apply(list).reset_index()\n",
        "\n",
        "    # extra_item_pdf\n",
        "\n",
        "    return cand_item_list, extra_item_pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_by_logic(cand_entities, threshold_for_fuzzy=0.8):\n",
        "    similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "    cand_entities, \n",
        "    item_pdf_all['item_nm_alias'].unique(), \n",
        "    threshold=threshold_for_fuzzy,\n",
        "    text_col_nm='item_name_in_msg',\n",
        "    item_col_nm='item_nm_alias',\n",
        "    n_jobs=6,\n",
        "    batch_size=30\n",
        "    )\n",
        "\n",
        "    if similarities_fuzzy.shape[0]>0:\n",
        "\n",
        "        similarities_fuzzy = parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s1'\n",
        "        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s2'\n",
        "        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias']).groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')\n",
        "\n",
        "    return similarities_fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "def extract_entities_by_llm(llm_model, msg_text, rank_limit=5):\n",
        "        \"\"\"\n",
        "        Extract entities using LLM-based approach.\n",
        "        \"\"\"\n",
        "        from langchain.prompts import PromptTemplate\n",
        "\n",
        "        cand_entities_by_sim = sorted([e.strip() for e in extract_entities_by_logic([msg_text], threshold_for_fuzzy=0.7)['item_nm_alias'].unique() if e.strip() not in stop_item_names and len(e.strip())>=2])\n",
        "        \n",
        "        zero_shot_prompt = PromptTemplate(\n",
        "            input_variables=[\"msg\",\"cand_entities\"],\n",
        "            template=\"\"\"\n",
        "            {entity_extraction_prompt}\n",
        "            \n",
        "            ## message:                \n",
        "            {msg}\n",
        "\n",
        "            ## Candidate entities:\n",
        "            {cand_entities}\n",
        "            \"\"\"\n",
        "        )\n",
        "        # Use the new LangChain pattern instead of deprecated LLMChain\n",
        "        chain = zero_shot_prompt | llm_model\n",
        "        cand_entities = chain.invoke({\"entity_extraction_prompt\": settings.PROCESSING_CONFIG.entity_extraction_prompt, \"msg\": msg_text, \"cand_entities\": cand_entities_by_sim}).content\n",
        "\n",
        "        # Filter out stop words\n",
        "        cand_entity_list = [e.strip() for e in cand_entities.split(',') if e.strip()]\n",
        "        cand_entity_list = [e for e in cand_entity_list if e not in stop_item_names and len(e)>=2]\n",
        "\n",
        "        if not cand_entity_list:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Fuzzy similarity matching\n",
        "        similarities_fuzzy = parallel_fuzzy_similarity(\n",
        "            cand_entity_list, \n",
        "            item_pdf_all['item_nm_alias'].unique(), \n",
        "            threshold=0.6,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30\n",
        "        )\n",
        "        \n",
        "        if similarities_fuzzy.empty:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Filter out stop words from results\n",
        "        similarities_fuzzy = similarities_fuzzy[~similarities_fuzzy['item_nm_alias'].isin(stop_item_names)]\n",
        "\n",
        "        # Sequence similarity matching\n",
        "        cand_entities_sim = parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s1'\n",
        "        ).rename(columns={'sim':'sim_s1'}).merge(parallel_seq_similarity(\n",
        "            sent_item_pdf=similarities_fuzzy,\n",
        "            text_col_nm='item_name_in_msg',\n",
        "            item_col_nm='item_nm_alias',\n",
        "            n_jobs=6,\n",
        "            batch_size=30,\n",
        "            normalizaton_value='s2'\n",
        "        ).rename(columns={'sim':'sim_s2'}), on=['item_name_in_msg','item_nm_alias'])\n",
        "        \n",
        "        # Combine similarity scores\n",
        "        cand_entities_sim = cand_entities_sim.groupby(['item_name_in_msg','item_nm_alias'])[['sim_s1','sim_s2']].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')\n",
        "        cand_entities_sim = cand_entities_sim.query(\"sim>=1.5\").copy()\n",
        "\n",
        "        # Rank and limit results\n",
        "        cand_entities_sim[\"rank\"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(method='first',ascending=False)\n",
        "        cand_entities_sim = cand_entities_sim.query(f\"rank<={rank_limit}\").sort_values(['item_name_in_msg','rank'], ascending=[True,True])\n",
        "\n",
        "        return cand_entities_sim\n",
        "\n",
        "# extract_entities_by_llm(llm_gem3, msg_text_list[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test text: 광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
            "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
            "추출된 개체명: ['뚜레쥬르', '에이닷이빵쏜닷', '데일리', '시크릿', '베어유', '0 day']\n",
            "🔍 LLM 모드 프롬프트 길이: 6228 문자\n",
            "🔍 후보 상품 목록 포함 여부: True\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "extract_dag() takes 2 positional arguments but 3 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    234\u001b[39m extract_entity_dag = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extract_entity_dag:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     extract_dag_result = \u001b[43mextract_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     dag_raw = extract_dag_result[\u001b[33m'\u001b[39m\u001b[33mdag_raw\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    239\u001b[39m     dag_section = extract_dag_result[\u001b[33m'\u001b[39m\u001b[33mdag_section\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[31mTypeError\u001b[39m: extract_dag() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "\n",
        "llm_model_nm = 'ax'\n",
        "\n",
        "if llm_model_nm == 'ax':\n",
        "    llm_model = llm_ax\n",
        "elif llm_model_nm == 'gem':\n",
        "    llm_model = llm_gem\n",
        "elif llm_model_nm == 'cld':\n",
        "    llm_model = llm_cld\n",
        "elif llm_model_nm == 'gen':\n",
        "    llm_model = llm_gen\n",
        "elif llm_model_nm == 'gpt':\n",
        "    llm_model = llm_gpt\n",
        "\n",
        "product_info_extraction_mode = 'llm' # options: 'rag', 'llm', 'nlp'\n",
        "entity_matching_mode = 'llm' # options: 'llm', 'logic'\n",
        "\n",
        "# for test_text in mms_pdf.query(\"msg.str.contains('대리점')\").sample(10)['msg'].tolist():\n",
        "\n",
        "test_text = \"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\"\"\"\n",
        "\n",
        "# test_text = \"\"\"\n",
        "# 요금제 무료혜택 안내\n",
        "# (광고)[SKT] #04 고객님, 현재 놓치고 계신 POOQ & FLO 무료 혜택을 안내해드립니다.   #91 요금제 가입 고객님은 아래 이용권 모두 무료로 이용하실 수 있어요. 다양한 방송 콘텐츠를 즐길 수 있는 POOQ과 음악을 무제한 감상할 수 있는 FLO를 무료로 즐겨보세요.  ■ POOQ 앤 데이터 (월 9,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/Dj8L4 - 지상파, 종편 실시간 TV + VOD 무제한 시청 가능 - POOQ 전용 데이터 매일 1GB 제공(전용 데이터를 다 쓰면 최대 3Mbps 속도로 계속 사용)  ■ FLO 앤 데이터 (월 7,900원, 부가세 포함 → 무료) - 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/l98dC - FLO 음악 무제한 듣기(모바일 기기 전용) - FLO 전용 데이터 월 3GB 제공 (음원 다운로드를 제외한 스트리밍 서비스에 한해 이용 가능)  ※ 5GX 플래티넘 요금제 가입 고객님은 POOQ 앤 데이터 플러스, FLO 앤 데이터 플러스 무료 이용 가능 (POOQ 앤 데이터/FLO 앤 데이터와 중복으로 가입할 수 없습니다.) ※ 서비스 가입 후 이용권 발급 필요 - 이용권 발급 방법: FLO 앱 > 이용권 > T 혜택 > 5GX 요금제 혜택 > 발급받기  SKT와 함께해주셔서 감사합니다.  ※ 이 메시지는 2019년 8월 19일 기준으로 작성되었습니다.  무료 수신거부 1504\n",
        "# \"\"\"\n",
        "\n",
        "# test_text = msg_text_list[0]\n",
        "\n",
        "# Revised schema with updated guidelines for preserving original text\n",
        "schema_prd = {\n",
        "            \"title\": \"Advertisement title, using the exact expressions as they appear in the original text.\",\n",
        "            \"purpose\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"상품 가입 유도\", \"대리점/매장 방문 유도\", \"웹/앱 접속 유도\", \"이벤트 응모 유도\", \n",
        "                           \"혜택 안내\", \"쿠폰 제공 안내\", \"경품 제공 안내\", \"수신 거부 안내\", \"기타 정보 제공\"]\n",
        "                },\n",
        "                \"description\": \"Primary purpose(s) of the advertisement.\"\n",
        "            },\n",
        "            \"product\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"name\": {\"type\": \"string\", \"description\": \"Name of the advertised product or service.\"},\n",
        "                        \"action\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"enum\": [\"구매\", \"가입\", \"사용\", \"방문\", \"참여\", \"코드입력\", \"쿠폰다운로드\", \"기타\"],\n",
        "                            \"description\": \"Expected customer action for the product.\"\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                \"description\": \"Extract all product or service names from the advertisement.\"\n",
        "            },\n",
        "            \"channel\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"type\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"enum\": [\"URL\", \"전화번호\", \"앱\", \"대리점\"],\n",
        "                            \"description\": \"Channel type.\"\n",
        "                        },\n",
        "                        \"value\": {\"type\": \"string\", \"description\": \"Specific information for the channel.\"},\n",
        "                        \"action\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"enum\": [\"가입\", \"추가 정보\", \"문의\", \"수신\", \"수신 거부\"],\n",
        "                            \"description\": \"Purpose of the channel.\"\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                \"description\": \"Channels provided in the advertisement.\"\n",
        "            },\n",
        "            \"pgm\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"Select the two most relevant pgm_nm from the advertising classification criteria.\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(f\"Test text: {test_text.strip()}\")\n",
        "msg = test_text.strip()\n",
        "\n",
        "cand_item_list, extra_item_pdf = extract_entities_from_kiwi(msg, item_pdf_all, stop_item_names)\n",
        "\n",
        "product_df = extra_item_pdf.rename(columns={'item_nm':'name'}).query(\"not name in @stop_item_names\")[['name']]\n",
        "product_df['action'] = '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'\n",
        "# product_df['position'] = '광고 상품의 분류. [main, sub, etc] 중에서 선택'\n",
        "product_element = product_df.to_dict(orient='records') if product_df.shape[0]>0 else schema_prd['product']\n",
        "\n",
        "# print(cand_item_list)\n",
        "\n",
        "mms_embedding = emb_model.encode([msg.lower()], convert_to_tensor=True)\n",
        "\n",
        "similarities = torch.nn.functional.cosine_similarity(\n",
        "    mms_embedding,  \n",
        "    clue_embeddings,  \n",
        "    dim=1 \n",
        ").cpu().numpy()\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf.copy()\n",
        "pgm_pdf_tmp['sim'] = similarities\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)\n",
        "\n",
        "pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "rag_context = f\"\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms > 0 else \"\"\n",
        "\n",
        "# 기본 chain of thought (LLM 모드 일관성 강화)\n",
        "if product_info_extraction_mode == 'llm':\n",
        "    chain_of_thought = \"\"\"\n",
        "1. Identify the advertisement's purpose first, using expressions as they appear in the original text.\n",
        "2. Extract ONLY explicitly mentioned product/service names from the text, using exact original expressions.\n",
        "3. For each product, assign a standardized action from: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타].\n",
        "4. Avoid inferring or adding products not directly mentioned in the text.\n",
        "5. Provide channel information considering the extracted product information, preserving original text expressions.\n",
        "\"\"\"\n",
        "else:\n",
        "    chain_of_thought = \"\"\"\n",
        "1. Identify the advertisement's purpose first, using expressions as they appear in the original text.\n",
        "2. Extract product names based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.\n",
        "3. Provide channel information considering the extracted product information, preserving original text expressions.\n",
        "\"\"\"\n",
        "\n",
        "prd_ext_guide = \"\"\"\n",
        "* Prioritize recall over precision to ensure all relevant products are captured, but verify that each extracted term is a distinct offering.\n",
        "* Extract all information (title, purpose, product, channel, pgm) using the exact expressions as they appear in the original text without translation, as specified in the schema.\n",
        "* If the advertisement purpose includes encouraging agency/store visits, provide agency channel information.\n",
        "\"\"\"\n",
        "\n",
        "if len(cand_item_list) > 0: \n",
        "    if product_info_extraction_mode == 'rag':\n",
        "        rag_context += f\"\\n\\n### 후보 상품 이름 목록 ###\\n\\t{cand_item_list}\"\n",
        "        prd_ext_guide += f\"\"\"\n",
        "* Use the provided candidate product names as a reference to guide product extraction, ensuring alignment with the advertisement content and using exact expressions from the original text.\n",
        "        \"\"\"\n",
        "    elif product_info_extraction_mode == 'llm':\n",
        "        # LLM 모드에도 후보 목록 제공하여 일관성 향상\n",
        "        rag_context += f\"\\n\\n### 참고용 후보 상품 이름 목록 ###\\n\\t{cand_item_list}\"\n",
        "        prd_ext_guide += f\"\"\"\n",
        "* Refer to the candidate product names list as guidance, but extract products based on your understanding of the advertisement content.\n",
        "* Maintain consistency by using standardized product naming conventions.\n",
        "* If multiple similar products exist, choose the most specific and relevant one to reduce variability.\n",
        "        \"\"\"\n",
        "    elif product_info_extraction_mode == 'nlp':\n",
        "        schema_prd['product'] = product_element  # Assuming product_element is defined elsewhere\n",
        "        chain_of_thought = \"\"\"\n",
        "1. Identify the advertisement’s purpose first, using expressions as they appear in the original text.\n",
        "2. Extract product information based on the identified purpose, ensuring only distinct offerings are included and using original text expressions.\n",
        "3. Extract the action field for each product based on the provided name information, derived from the original text context.\n",
        "4. Provide channel information considering the extracted product information, preserving original text expressions.\n",
        "        \"\"\"\n",
        "        prd_ext_guide += f\"\"\"\n",
        "* Extract the action field for each product based on the identified product names, using the original text context.\n",
        "        \"\"\"\n",
        "\n",
        "schema_prompt = f\"\"\"\n",
        "Return your response as a JSON object that follows this exact structure:\n",
        "\n",
        "{json.dumps(schema_prd, indent=4, ensure_ascii=False)}\n",
        "\n",
        "IMPORTANT: \n",
        "- Do NOT return the schema definition itself\n",
        "- Return actual extracted data in the specified format\n",
        "- For \"purpose\": return an array of strings from the enum values\n",
        "- For \"product\": return an array of objects with \"name\" and \"action\" fields\n",
        "- For \"channel\": return an array of objects with \"type\", \"value\", and \"action\" fields\n",
        "- For \"pgm\": return an array of strings\n",
        "\n",
        "Example response format:\n",
        "{{\n",
        "    \"title\": \"실제 광고 제목\",\n",
        "    \"purpose\": [\"상품 가입 유도\", \"혜택 안내\"],\n",
        "    \"product\": [\n",
        "        {{\"name\": \"실제 상품명\", \"action\": \"가입\"}},\n",
        "        {{\"name\": \"다른 상품명\", \"action\": \"구매\"}}\n",
        "    ],\n",
        "    \"channel\": [\n",
        "        {{\"type\": \"URL\", \"value\": \"실제 URL\", \"action\": \"가입\"}}\n",
        "    ],\n",
        "    \"pgm\": [\"실제 프로그램명\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# LLM 모드에서 일관성 강화를 위한 추가 지시사항\n",
        "consistency_note = \"\"\n",
        "if product_info_extraction_mode == 'llm':\n",
        "    consistency_note = \"\"\"\n",
        "\n",
        "### 일관성 유지 지침 ###\n",
        "* 동일한 광고 메시지에 대해서는 항상 동일한 결과를 생성해야 합니다.\n",
        "* 애매한 표현이 있을 때는 가장 명확하고 구체적인 해석을 선택하세요.\n",
        "* 상품명은 원문에서 정확히 언급된 표현만 사용하세요.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Extract the advertisement purpose and product names from the provided advertisement text.\n",
        "\n",
        "### Advertisement Message ###\n",
        "{msg}\n",
        "\n",
        "### Extraction Steps ###\n",
        "{chain_of_thought}\n",
        "\n",
        "### Extraction Guidelines ###\n",
        "{prd_ext_guide}{consistency_note}\n",
        "\n",
        "{schema_prompt}\n",
        "\n",
        "### OUTPUT FORMAT REQUIREMENT ###\n",
        "You MUST respond with a valid JSON object containing actual extracted data.\n",
        "Do NOT include schema definitions, type specifications, or template structures.\n",
        "Return only the concrete extracted information in the specified JSON format.\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "### FINAL REMINDER ###\n",
        "Return a JSON object with actual data, not schema definitions!\n",
        "\"\"\"\n",
        "\n",
        "# 디버깅을 위한 프롬프트 로깅 (LLM 모드에서만)\n",
        "if product_info_extraction_mode == 'llm':\n",
        "    print(f\"🔍 LLM 모드 프롬프트 길이: {len(prompt)} 문자\")\n",
        "    print(f\"🔍 후보 상품 목록 포함 여부: {'참고용 후보 상품 이름 목록' in rag_context}\")\n",
        "\n",
        "print()\n",
        "\n",
        "from entity_dag_extractor import DAGParser, extract_dag\n",
        "parser = DAGParser()\n",
        "\n",
        "extract_entity_dag = True\n",
        "if extract_entity_dag:\n",
        "    extract_dag_result = extract_dag(parser, msg, llm_model)\n",
        "\n",
        "    dag_raw = extract_dag_result['dag_raw']\n",
        "    dag_section = extract_dag_result['dag_section']\n",
        "    dag = extract_dag_result['dag']\n",
        "else:\n",
        "    dag_section = \"\"\n",
        "\n",
        "# result_json_text = llm_cld40.invoke(prompt).content\n",
        "result_json_text = llm_model.invoke(prompt).content\n",
        "\n",
        "# print(result_json_text)\n",
        "\n",
        "json_objects_list = extract_json_objects(result_json_text)\n",
        "if not json_objects_list:\n",
        "    print(\"⚠️ LLM이 유효한 JSON 객체를 반환하지 않았습니다\")\n",
        "    print(f\"LLM 응답: {result_json_text}\")\n",
        "    json_objects = {\n",
        "        \"title\": \"광고 메시지\",\n",
        "        \"purpose\": [\"정보 제공\"],\n",
        "        \"product\": [],\n",
        "        \"channel\": [],\n",
        "        \"pgm\": []\n",
        "    }\n",
        "else:\n",
        "    json_objects = json_objects_list[-1]\n",
        "    \n",
        "    # 스키마 응답 감지\n",
        "    def is_schema_response(obj):\n",
        "        \"\"\"LLM이 스키마 정의를 반환했는지 감지\"\"\"\n",
        "        for field in ['purpose', 'product', 'channel']:\n",
        "            field_value = obj.get(field, {})\n",
        "            if isinstance(field_value, dict) and 'type' in field_value and field_value.get('type') == 'array':\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    if is_schema_response(json_objects):\n",
        "        print(\"🚨 LLM이 스키마 정의를 반환했습니다! 재시도가 필요합니다.\")\n",
        "        print(\"현재 응답:\", json_objects)\n",
        "        \n",
        "        # 강화된 프롬프트로 재시도\n",
        "        enhanced_prompt = \"\"\"\n",
        "🚨 CRITICAL: Return actual extracted data, NOT schema definitions!\n",
        "\n",
        "DO NOT return: {\"purpose\": {\"type\": \"array\", ...}}\n",
        "DO return: {\"purpose\": [\"상품 가입 유도\"]}\n",
        "\n",
        "\"\"\" + prompt\n",
        "        \n",
        "        result_json_text = llm_model.invoke(enhanced_prompt).content\n",
        "        json_objects_retry = extract_json_objects(result_json_text)\n",
        "        if json_objects_retry and not is_schema_response(json_objects_retry[-1]):\n",
        "            json_objects = json_objects_retry[-1]\n",
        "            print(\"✅ 재시도 성공: 올바른 데이터 형식 반환\")\n",
        "        else:\n",
        "            print(\"❌ 재시도 실패: fallback 결과 사용\")\n",
        "\n",
        "# print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "\n",
        "if entity_matching_mode == 'logic':\n",
        "    # 제품 정보 추출 시 스키마 응답 처리\n",
        "    product_data = json_objects.get('product', [])\n",
        "    if isinstance(product_data, dict) and 'items' in product_data:\n",
        "        # 스키마 구조인 경우 빈 리스트 사용\n",
        "        cand_entities = []\n",
        "        print(\"⚠️ product 필드가 스키마 구조입니다. 빈 엔티티 리스트 사용\")\n",
        "    elif isinstance(product_data, list):\n",
        "        # 올바른 배열 구조\n",
        "        cand_entities = [item.get('name', '') for item in product_data if isinstance(item, dict) and item.get('name')]\n",
        "    else:\n",
        "        cand_entities = []\n",
        "        print(f\"⚠️ 예상하지 못한 product 구조: {type(product_data)}\")\n",
        "    \n",
        "    similarities_fuzzy = extract_entities_by_logic(cand_entities)\n",
        "elif entity_matching_mode == 'llm':\n",
        "    similarities_fuzzy = extract_entities_by_llm(llm_model, msg)\n",
        "\n",
        "final_result = json_objects.copy()\n",
        "\n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "# print(\"===\"*15+\"claude sonnet (emb)\"+\"===\"*15+\"\\n\")\n",
        "# print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "print(\"Entity from extractor:\", list(set(cand_item_list)))\n",
        "print(\"Entity from LLM:\", [x['name'] for x in ([item for item in json_objects['product']['items']] if isinstance(json_objects['product'], dict) else json_objects['product']) ])\n",
        "\n",
        "if similarities_fuzzy.shape[0]>0:\n",
        "        # Break down the complex query into simpler steps to avoid pandas/numexpr evaluation error\n",
        "    # Step 1: Get high similarity items\n",
        "    high_sim_items = similarities_fuzzy.query('sim >= 1.5')['item_nm_alias'].unique()\n",
        "    \n",
        "    # Step 2: Filter similarities_fuzzy for conditions\n",
        "    filtered_similarities = similarities_fuzzy[\n",
        "        (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &\n",
        "        (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &\n",
        "        (~similarities_fuzzy['item_name_in_msg'].isin(stop_item_names))\n",
        "    ]\n",
        "    \n",
        "    # Step 3: Merge with item_pdf_all\n",
        "    product_tag = convert_df_to_json_list(\n",
        "        item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])\n",
        "    )\n",
        "    \n",
        "    # Add action information from original json_objects\n",
        "    # Create a mapping from item_name_in_msg to action\n",
        "    action_mapping = {}\n",
        "    product_data = json_objects.get('product', [])\n",
        "    if isinstance(product_data, list):\n",
        "        for item in product_data:\n",
        "            if isinstance(item, dict) and 'name' in item and 'action' in item:\n",
        "                action_mapping[item['name']] = item['action']\n",
        "    \n",
        "    # Add action to product_tag\n",
        "    for product in product_tag:\n",
        "        item_name = product.get('item_name_in_msg', '')\n",
        "        product['expected_action'] = action_mapping.get(item_name, '기타')\n",
        "\n",
        "    final_result = {\n",
        "        \"title\":json_objects['title'],\n",
        "        \"purpose\":json_objects['purpose'],\n",
        "        \"product\":product_tag,\n",
        "        \"channel\":json_objects['channel'],\n",
        "        \"pgm\":json_objects['pgm']\n",
        "    }\n",
        "\n",
        "else:\n",
        "    final_result = json_objects.copy()\n",
        "    product_tag = [item for item in json_objects['product']['items']] if isinstance(json_objects['product'], dict) else json_objects['product']\n",
        "    final_result['product'] = [{'item_name_in_msg':d['name'], 'expected_action':d['action'], 'item_in_voca':[{'item_name_in_voca':d['name'], 'item_id': ['#']}]} for d in product_tag if d['name'] not in stop_item_names]\n",
        "    \n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "channel_tag = []\n",
        "for d in [item for item in json_objects['channel']['items']] if isinstance(json_objects['channel'], dict) else json_objects['channel']:\n",
        "    if d['type']=='대리점':\n",
        "\n",
        "        # _embedding = emb_model.encode([preprocess_text(d['value'].lower())], convert_to_tensor=True)\n",
        "\n",
        "        # similarities = torch.nn.functional.cosine_similarity(\n",
        "        #     _embedding,  \n",
        "        #     org_all_embeddings,  \n",
        "        #     dim=1 \n",
        "        # ).cpu().numpy()\n",
        "\n",
        "        # org_pdf_tmp = org_pdf.copy()\n",
        "        # org_pdf_tmp['sim'] = similarities.round(5)\n",
        "\n",
        "        org_pdf_cand = parallel_fuzzy_similarity(\n",
        "            [preprocess_text(d['value'].lower())], \n",
        "            org_pdf['org_abbr_nm'].unique(), \n",
        "            threshold=0.5,\n",
        "            text_col_nm='org_nm_in_msg',\n",
        "            item_col_nm='org_abbr_nm',\n",
        "            n_jobs=6,\n",
        "            batch_size=100\n",
        "        ).drop('org_nm_in_msg', axis=1)\n",
        "\n",
        "        org_pdf_cand = org_pdf.merge(org_pdf_cand, on=['org_abbr_nm'])\n",
        "\n",
        "        org_pdf_cand['sim'] = org_pdf_cand['sim'].round(5)\n",
        "        \n",
        "        org_pdf_tmp = org_pdf_cand.query(\"org_cd.str.startswith('D')\").sort_values('sim', ascending=False).query(\"sim>=0.7\")\n",
        "        if org_pdf_tmp.shape[0]<1:\n",
        "            org_pdf_tmp = org_pdf_cand.sort_values('sim', ascending=False).query(\"sim>=0.7\")\n",
        "\n",
        "        org_pdf_tmp['sim'] = org_pdf_tmp.apply(lambda x: combined_sequence_similarity(d['value'], x['org_nm'])[0], axis=1)\n",
        "        org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense',ascending=False)\n",
        "        org_pdf_tmp['org_cd'] = org_pdf_tmp.apply(lambda x: x['org_cd']+x['sub_org_cd'], axis=1)\n",
        "\n",
        "        org_pdf_tmp = org_pdf_tmp.query(\"rank==1\").groupby('org_nm')['org_cd'].apply(list).reset_index(name='org_cd').to_dict('records')\n",
        "\n",
        "        # org_nm_id_list =  list(zip(org_pdf_tmp['org_nm'], org_pdf_tmp['org_id']))\n",
        "\n",
        "        d['store_info'] = org_pdf_tmp\n",
        "    else:\n",
        "        d['store_info'] = []\n",
        "\n",
        "    channel_tag.append(d)\n",
        "\n",
        "final_result['channel'] = channel_tag\n",
        "final_result['entity_dag'] = entity_dag\n",
        "\n",
        "# print(\"===\"*15+\"claude sonnet (fuzzy)\"+\"===\"*15+\"\\n\")\n",
        "print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# # Extract information\n",
        "# response = requests.post('http://127.0.0.1:8080/extract', json={\n",
        "#     \"message\": \"\"\"광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "# 광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\"\"\",\n",
        "#     \"llm_model\": \"ax\",\n",
        "#     \"product_info_extraction_mode\": \"llm\",\n",
        "#     \"entity_matching_mode\": \"llm\"\n",
        "# })\n",
        "\n",
        "# result = response.json()\n",
        "# print(json.dumps(result['result'], indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, networkx as nx\n",
        "from typing import List, Tuple, Set, Dict\n",
        "\n",
        "###############################################################################\n",
        "# 1) 정규식 : ( node ) -[ relation ]-> ( node )\n",
        "###############################################################################\n",
        "PAT = re.compile(r\"\\((.*?)\\)\\s*-\\[(.*?)\\]->\\s*\\((.*?)\\)\")\n",
        "NODE_ONLY = re.compile(r\"\\((.*?)\\)\\s*$\")\n",
        "\n",
        "###############################################################################\n",
        "# 2) 파서\n",
        "###############################################################################\n",
        "\n",
        "def parse_block(text: str):\n",
        "    nodes: Set[str] = set()\n",
        "    edges: List[Tuple[str,str,str]] = []\n",
        "    for line in filter(None, map(str.strip, text.splitlines())):\n",
        "        m = PAT.match(line)\n",
        "        if m:                            # (A) -[rel]-> (B)\n",
        "            src, rel, dst = map(str.strip, m.groups())\n",
        "            nodes.update([src, dst])\n",
        "            edges.append((src, dst, rel))\n",
        "            continue\n",
        "\n",
        "        m2 = NODE_ONLY.match(line)       # (A:B:C)\n",
        "        if m2:                           # ← 독립 노드\n",
        "            nodes.add(m2.group(1).strip())\n",
        "            continue\n",
        "\n",
        "        raise ValueError(f\"못읽은 라인:\\n  {line}\")\n",
        "    return list(nodes), edges\n",
        "\n",
        "###############################################################################\n",
        "# 3) 노드 스플리터 – 3칸·2칸·1칸 허용\n",
        "###############################################################################\n",
        "def split_node(raw: str) -> Dict[str,str]:\n",
        "    parts = [p.strip() for p in raw.split(\":\")]\n",
        "    \n",
        "    if len(parts) == 1:                       # entity only\n",
        "        ent, act, kpi = parts[0], \"\", \"\"\n",
        "    elif len(parts) == 2:                     # entity:metric\n",
        "        ent, act = parts\n",
        "        kpi = \"\"\n",
        "    elif len(parts) == 3:                     # entity:action:metric\n",
        "        ent, act, kpi = parts\n",
        "    elif len(parts) == 4:                     # entity:action:behavior:metric\n",
        "        ent, act, behavior, kpi = parts\n",
        "        # Combine action and behavior, or handle separately\n",
        "        act = f\"{act}:{behavior}\"  # Option 1: combine them\n",
        "        # Or you could add a new field:\n",
        "        # return {\"entity\": ent, \"action\": act, \"behavior\": behavior, \"metric\": kpi}\n",
        "    elif len(parts) >= 5:                     # Handle even more parts flexibly\n",
        "        ent = parts[0]\n",
        "        kpi = parts[-1]  # Last part is usually the metric\n",
        "        act = \":\".join(parts[1:-1])  # Everything in between becomes action\n",
        "    else:\n",
        "        # This shouldn't happen with len(parts) >= 1, but just in case\n",
        "        raise ValueError(f\"알 수 없는 노드 형식: {raw}\")\n",
        "    \n",
        "    return {\"entity\": ent, \"action\": act, \"metric\": kpi}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 4) DAG 빌더\n",
        "###############################################################################\n",
        "def build_dag(nodes: List[str], edges: List[Tuple[str,str,str]]) -> nx.DiGraph:\n",
        "    g = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes with error handling\n",
        "    for n in nodes:\n",
        "        try:\n",
        "            node_attrs = split_node(n)\n",
        "            g.add_node(n, **node_attrs)\n",
        "        except ValueError as e:\n",
        "            print(f\"Warning: {e}\")\n",
        "            # Add node with minimal attributes if parsing fails\n",
        "            g.add_node(n, entity=n, action=\"\", metric=\"\")\n",
        "    \n",
        "    # Add edges\n",
        "    for src, dst, rel in edges:\n",
        "        g.add_edge(src, dst, relation=rel)\n",
        "    \n",
        "    # Optional: Check if it's a DAG\n",
        "    # if not nx.is_directed_acyclic_graph(g):\n",
        "    #     raise nx.NetworkXUnfeasible(\"사이클이 있습니다 ― DAG 아님!\")\n",
        "    \n",
        "    return g\n",
        "\n",
        "###############################################################################\n",
        "# 5) Path Finder\n",
        "###############################################################################\n",
        "def get_root_to_leaf_paths(dag):\n",
        "    \"\"\"Generate all paths from root nodes (no predecessors) to leaf nodes (no successors)\"\"\"\n",
        "    # Find root nodes (no incoming edges)\n",
        "    root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]\n",
        "    \n",
        "    # Find leaf nodes (no outgoing edges)\n",
        "    leaf_nodes = [node for node in dag.nodes() if dag.out_degree(node) == 0]\n",
        "    \n",
        "    all_paths = []\n",
        "    for root in root_nodes:\n",
        "        for leaf in leaf_nodes:\n",
        "            try:\n",
        "                paths = list(nx.all_simple_paths(dag, root, leaf))\n",
        "                all_paths.extend(paths)\n",
        "            except nx.NetworkXNoPath:\n",
        "                continue\n",
        "    \n",
        "    return all_paths, root_nodes, leaf_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import networkx as nx\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "\n",
        "class DAGParser:\n",
        "    \"\"\"통신사 광고 메시지에서 추출된 DAG를 NetworkX 그래프로 변환하는 파서\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 개선된 정규표현식 패턴 - 관계 부분에 쉼표와 공백 허용\n",
        "        # 관계 부분([...])에 모든 문자 허용 (]를 제외하고)\n",
        "        self.edge_pattern = r'\\(([^:)]+):([^)]+)\\)\\s*-\\[([^\\]]+)\\]->\\s*\\(([^:)]+):([^)]+)\\)'\n",
        "        # 독립형 노드 패턴 추가\n",
        "        self.standalone_node_pattern = r'\\(([^:)]+):([^)]+)\\)\\s*$'\n",
        "        # 섹션 패턴 수정: ## 또는 ###로 시작하는 2. 수정된 DAG 섹션\n",
        "        self.section_pattern = r'#{2,3}\\s*2\\.\\s*수정된\\s*DAG'\n",
        "        \n",
        "    def parse_dag_line(self, line: str) -> Optional[Union[Tuple[str, str, str, str, str], Tuple[str, str]]]:\n",
        "        \"\"\"단일 DAG 라인을 파싱하여 구성 요소 반환\"\"\"\n",
        "        # 먼저 엣지 패턴 확인\n",
        "        edge_match = re.match(self.edge_pattern, line)\n",
        "        if edge_match:\n",
        "            return (\n",
        "                edge_match.group(1).strip(),  # src_entity\n",
        "                edge_match.group(2).strip(),  # src_action\n",
        "                edge_match.group(3).strip(),  # relation (쉼표, 조건 포함 가능)\n",
        "                edge_match.group(4).strip(),  # dst_entity\n",
        "                edge_match.group(5).strip()   # dst_action\n",
        "            )\n",
        "        \n",
        "        # 독립형 노드 패턴 확인\n",
        "        standalone_match = re.match(self.standalone_node_pattern, line)\n",
        "        if standalone_match:\n",
        "            return (\n",
        "                standalone_match.group(1).strip(),  # entity\n",
        "                standalone_match.group(2).strip()   # action\n",
        "            )\n",
        "        \n",
        "        return None\n",
        "        \n",
        "    def extract_dag_section(self, full_text: str) -> str:\n",
        "        \"\"\"전체 텍스트에서 DAG 섹션만 추출\"\"\"\n",
        "        lines = full_text.split('\\n')\n",
        "        \n",
        "        # 더 유연한 DAG 섹션 찾기\n",
        "        dag_section_patterns = [\n",
        "            r'#{2,3}\\s*최종\\s*DAG',            # 최종 DAG\n",
        "            r'#{2,3}\\s*4\\.\\s*수정된\\s*DAG',    # 4. 수정된 DAG 최우선\n",
        "            r'#{2,3}\\s*수정된\\s*DAG',          # 수정된 DAG\n",
        "            r'#{2,3}\\s*2\\.\\s*추출된\\s*DAG',    # 2. 추출된 DAG (기본)\n",
        "            r'#{2,3}\\s*DAG',\n",
        "            r'추출된\\s*DAG',\n",
        "            r'2\\.\\s*추출된\\s*DAG'\n",
        "        ]\n",
        "        \n",
        "        # DAG 섹션 찾기\n",
        "        start_idx = -1\n",
        "        end_idx = len(lines)\n",
        "        in_dag_section = False\n",
        "        in_code_block = False\n",
        "        \n",
        "        # 패턴들을 순차적으로 시도\n",
        "        for pattern in dag_section_patterns:\n",
        "            for i, line in enumerate(lines):\n",
        "                if re.search(pattern, line, re.IGNORECASE):\n",
        "                    in_dag_section = True\n",
        "                    # 다음 라인이 ```인지 확인\n",
        "                    if i + 1 < len(lines) and lines[i + 1].strip() == '```':\n",
        "                        start_idx = i + 2\n",
        "                        in_code_block = True\n",
        "                    else:\n",
        "                        start_idx = i + 1\n",
        "                    break\n",
        "            if start_idx != -1:\n",
        "                break\n",
        "        \n",
        "        # DAG 섹션 종료 조건 찾기\n",
        "        if start_idx != -1:\n",
        "            for i in range(start_idx, len(lines)):\n",
        "                line = lines[i]\n",
        "                if in_code_block and line.strip() == '```':\n",
        "                    end_idx = i\n",
        "                    break\n",
        "                elif not in_code_block and re.match(r'#{2,3}\\s*3\\.', line):\n",
        "                    end_idx = i\n",
        "                    break\n",
        "        \n",
        "        if start_idx == -1:\n",
        "            # 섹션 헤더가 없는 경우, DAG 패턴을 직접 찾기\n",
        "            dag_lines = []\n",
        "            for line in lines:\n",
        "                if (re.match(self.edge_pattern, line) or \n",
        "                    re.match(self.standalone_node_pattern, line) or \n",
        "                    line.strip().startswith('#')):\n",
        "                    dag_lines.append(line)\n",
        "            \n",
        "            if dag_lines:\n",
        "                result = '\\n'.join(dag_lines)\n",
        "                return result\n",
        "            else:\n",
        "                raise ValueError(\"DAG 섹션을 찾을 수 없습니다.\")\n",
        "        \n",
        "        result = '\\n'.join(lines[start_idx:end_idx])\n",
        "        return result\n",
        "    \n",
        "    def parse_dag(self, dag_text: str) -> nx.DiGraph:\n",
        "        \"\"\"DAG 텍스트를 NetworkX DiGraph로 변환\"\"\"\n",
        "        G = nx.DiGraph()\n",
        "        \n",
        "        # 통계 정보 저장\n",
        "        stats = {\n",
        "            'total_edges': 0,\n",
        "            'comment_lines': 0,\n",
        "            'empty_lines': 0,\n",
        "            'paths': [],\n",
        "            'parse_errors': [],\n",
        "            'parsed_lines': []\n",
        "        }\n",
        "        \n",
        "        current_path = None\n",
        "        \n",
        "        for line_num, line in enumerate(dag_text.strip().split('\\n'), 1):\n",
        "            line = line.strip()\n",
        "            \n",
        "            # 빈 라인 처리\n",
        "            if not line:\n",
        "                stats['empty_lines'] += 1\n",
        "                continue\n",
        "            \n",
        "            # 주석 라인 처리 (경로 정보 추출)\n",
        "            if line.startswith('#'):\n",
        "                stats['comment_lines'] += 1\n",
        "                current_path = line[1:].strip()\n",
        "                if current_path:\n",
        "                    stats['paths'].append(current_path)\n",
        "                continue\n",
        "            \n",
        "            # DAG 엣지 또는 독립형 노드 파싱\n",
        "            parsed = self.parse_dag_line(line)\n",
        "            if parsed:\n",
        "                try:\n",
        "                    if len(parsed) == 5:  # 엣지 (src_entity, src_action, relation, dst_entity, dst_action)\n",
        "                        src_entity, src_action, relation, dst_entity, dst_action = parsed\n",
        "                        \n",
        "                        # 노드 ID 생성\n",
        "                        src_node = f\"{src_entity}:{src_action}\"\n",
        "                        dst_node = f\"{dst_entity}:{dst_action}\"\n",
        "                        \n",
        "                        # 노드 추가 (속성 포함)\n",
        "                        G.add_node(src_node, \n",
        "                                  entity=src_entity, \n",
        "                                  action=src_action,\n",
        "                                  path=current_path)\n",
        "                        G.add_node(dst_node, \n",
        "                                  entity=dst_entity, \n",
        "                                  action=dst_action,\n",
        "                                  path=current_path)\n",
        "                        \n",
        "                        # 엣지 추가 (관계에 쉼표나 조건이 포함될 수 있음)\n",
        "                        G.add_edge(src_node, dst_node, \n",
        "                                  relation=relation,\n",
        "                                  path=current_path)\n",
        "                        \n",
        "                        stats['total_edges'] += 1\n",
        "                        stats['parsed_lines'].append(f\"Line {line_num}: {src_node} -[{relation}]-> {dst_node}\")\n",
        "                        \n",
        "                    elif len(parsed) == 2:  # 독립형 노드 (entity, action)\n",
        "                        entity, action = parsed\n",
        "                        \n",
        "                        # 노드 ID 생성\n",
        "                        node_id = f\"{entity}:{action}\"\n",
        "                        \n",
        "                        # 독립형 노드 추가\n",
        "                        G.add_node(node_id, \n",
        "                                  entity=entity, \n",
        "                                  action=action,\n",
        "                                  path=current_path)\n",
        "                        \n",
        "                        stats['parsed_lines'].append(f\"Line {line_num}: Standalone node {node_id}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    stats['parse_errors'].append(f\"Line {line_num}: {str(e)}\")\n",
        "            else:\n",
        "                # 파싱 실패한 라인 기록 (주석이 아닌 경우만)\n",
        "                if not line.startswith('#') and line.strip():\n",
        "                    stats['parse_errors'].append(f\"Line {line_num}: 패턴 매칭 실패 - {line[:80]}...\")\n",
        "        \n",
        "        # 그래프에 통계 정보 저장\n",
        "        G.graph['stats'] = stats\n",
        "        \n",
        "        return G\n",
        "    \n",
        "    def get_root_nodes(self, G: nx.DiGraph) -> List[str]:\n",
        "        \"\"\"Root 노드(들어오는 엣지가 없는 노드) 찾기\"\"\"\n",
        "        return [node for node in G.nodes() if G.in_degree(node) == 0]\n",
        "    \n",
        "    def get_leaf_nodes(self, G: nx.DiGraph) -> List[str]:\n",
        "        \"\"\"Leaf 노드(나가는 엣지가 없는 노드) 찾기\"\"\"\n",
        "        return [node for node in G.nodes() if G.out_degree(node) == 0]\n",
        "    \n",
        "    def get_paths_from_root_to_leaf(self, G: nx.DiGraph) -> List[List[str]]:\n",
        "        \"\"\"Root에서 Leaf까지의 모든 경로 찾기\"\"\"\n",
        "        roots = self.get_root_nodes(G)\n",
        "        leaves = self.get_leaf_nodes(G)\n",
        "        \n",
        "        all_paths = []\n",
        "        for root in roots:\n",
        "            for leaf in leaves:\n",
        "                try:\n",
        "                    paths = list(nx.all_simple_paths(G, root, leaf))\n",
        "                    all_paths.extend(paths)\n",
        "                except nx.NetworkXNoPath:\n",
        "                    continue\n",
        "        \n",
        "        return all_paths\n",
        "    \n",
        "    def analyze_graph(self, G: nx.DiGraph) -> Dict:\n",
        "        \"\"\"그래프 분석 정보 생성\"\"\"\n",
        "        analysis = {\n",
        "            'num_nodes': G.number_of_nodes(),\n",
        "            'num_edges': G.number_of_edges(),\n",
        "            'root_nodes': self.get_root_nodes(G),\n",
        "            'leaf_nodes': self.get_leaf_nodes(G),\n",
        "            'is_dag': nx.is_directed_acyclic_graph(G),\n",
        "            'num_components': nx.number_weakly_connected_components(G),\n",
        "            'paths_info': G.graph.get('stats', {}).get('paths', []),\n",
        "            'longest_path_length': 0\n",
        "        }\n",
        "        \n",
        "        # 최장 경로 찾기\n",
        "        if analysis['is_dag'] and G.number_of_nodes() > 0:\n",
        "            try:\n",
        "                longest = nx.dag_longest_path(G)\n",
        "                analysis['longest_path_length'] = len(longest) - 1 if longest else 0\n",
        "            except:\n",
        "                analysis['longest_path_length'] = 0\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def to_json(self, G: nx.DiGraph) -> str:\n",
        "        \"\"\"그래프를 JSON 형식으로 변환\"\"\"\n",
        "        data = {\n",
        "            'nodes': [\n",
        "                {\n",
        "                    'id': node,\n",
        "                    'entity': G.nodes[node].get('entity', ''),\n",
        "                    'action': G.nodes[node].get('action', ''),\n",
        "                    'path': G.nodes[node].get('path', '')\n",
        "                }\n",
        "                for node in G.nodes()\n",
        "            ],\n",
        "            'edges': [\n",
        "                {\n",
        "                    'source': edge[0],\n",
        "                    'target': edge[1],\n",
        "                    'relation': G.edges[edge].get('relation', ''),\n",
        "                    'path': G.edges[edge].get('path', '')\n",
        "                }\n",
        "                for edge in G.edges()\n",
        "            ],\n",
        "            'analysis': self.analyze_graph(G)\n",
        "        }\n",
        "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    def visualize_paths(self, G: nx.DiGraph) -> str:\n",
        "        \"\"\"경로별로 구조화된 텍스트 출력\"\"\"\n",
        "        output = []\n",
        "        paths_dict = {}\n",
        "        \n",
        "        # 경로별로 엣지 그룹화\n",
        "        for edge in G.edges():\n",
        "            path = G.edges[edge].get('path', 'Unknown')\n",
        "            if path not in paths_dict:\n",
        "                paths_dict[path] = []\n",
        "            paths_dict[path].append(edge)\n",
        "        \n",
        "        # 경로별 출력\n",
        "        for path, edges in paths_dict.items():\n",
        "            if path and path != 'Unknown':\n",
        "                output.append(f\"\\n[{path}]\")\n",
        "            for edge in edges:\n",
        "                relation = G.edges[edge].get('relation', '')\n",
        "                output.append(f\"  {edge[0]} -{relation}-> {edge[1]}\")\n",
        "        \n",
        "        return '\\n'.join(output)\n",
        "\n",
        "def extract_dag(parser, sample_text, i=3):\n",
        "    \"\"\"개선된 DAG 추출 함수\"\"\"\n",
        "    try:\n",
        "        # DAG 섹션 추출\n",
        "        dag_section = parser.extract_dag_section(sample_text)\n",
        "        G = parser.parse_dag(dag_section)\n",
        "\n",
        "        print(\"추출된 DAG 섹션 (처음 200자):\")\n",
        "        print(dag_section[:200] + \"...\" if len(dag_section) > 200 else dag_section)\n",
        "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "                \n",
        "        # 파싱 성공 라인 출력 (디버깅용)\n",
        "        if G.graph['stats'].get('parsed_lines') and i == 3:  # 예시 3만 상세 출력\n",
        "            print(\"파싱 성공한 라인:\")\n",
        "            for parsed_line in G.graph['stats']['parsed_lines'][:3]:\n",
        "                print(f\"  ✓ {parsed_line}\")\n",
        "            if len(G.graph['stats']['parsed_lines']) > 3:\n",
        "                print(f\"  ... 외 {len(G.graph['stats']['parsed_lines']) - 3}개\")\n",
        "            print()\n",
        "        \n",
        "        # 파싱 에러 확인\n",
        "        if G.graph['stats'].get('parse_errors'):\n",
        "            print(\"파싱 에러:\")\n",
        "            for error in G.graph['stats']['parse_errors']:\n",
        "                print(f\"  ✗ {error}\")\n",
        "            print()\n",
        "        \n",
        "        # 분석 결과 출력\n",
        "        analysis = parser.analyze_graph(G)\n",
        "        print(\"그래프 분석:\")\n",
        "        print(f\"- 노드 수: {analysis['num_nodes']}\")\n",
        "        print(f\"- 엣지 수: {analysis['num_edges']}\")\n",
        "        print(f\"- Root 노드: {analysis['root_nodes'][:3]}{'...' if len(analysis['root_nodes']) > 3 else ''}\")\n",
        "        print(f\"- Leaf 노드: {analysis['leaf_nodes'][:3]}{'...' if len(analysis['leaf_nodes']) > 3 else ''}\")\n",
        "        print(f\"- DAG 여부: {analysis['is_dag']}\")\n",
        "        print(f\"- 최장 경로 길이: {analysis['longest_path_length']}\")\n",
        "        \n",
        "        if G.number_of_nodes() > 0 and i == 3:  # 예시 3의 특수 관계 확인\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "            print(\"특수 관계가 포함된 엣지 (쉼표 포함):\")\n",
        "            for edge in G.edges(data=True):\n",
        "                if ',' in edge[2]['relation']:\n",
        "                    print(f\"  - {edge[0]} --[{edge[2]['relation']}]--> {edge[1]}\")\n",
        "        \n",
        "        return G\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================= Message =============================================\n",
            "[SK텔레콤] 9월 0 day 혜택 안내\n",
            "(광고)[SKT] 9월 0 day 혜택 안내\n",
            "<9월 20일(금) 혜택>\n",
            "만 13~34세 고객이라면\n",
            "SKT 0 day\n",
            "[배달의민족(페리카나 전용) 8,000원 할인]\n",
            "이게 되네!\n",
            "(16,000원 이상 주문 시 할인 적용)\n",
            "\n",
            " 자세히 보기 :  http://t-mms.kr/t.do?m=#61&s=28335&a=&u=https://bit.ly/3XiX8lJ\n",
            "\n",
            " 에이닷 X T 멤버십 구독캘린더 이벤트\n",
            "T day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요!\n",
            "알림 설정하면 추첨을 통해 [맥도날드 1만원권]을 드립니다.\n",
            "\n",
            " 이벤트 참여하기 : https://bit.ly/3T95wC2\n",
            "\n",
            " 문의: SKT 고객센터(1558, 무료)\n",
            "무료 수신거부 1504\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤은 만 13~34세 고객을 대상으로 9월 20일(금)에 배달의민족(페리카나 전용)에서 16,000원 이상 주문 시 8,000원 할인 혜택을 제공하며, 에이닷 캘린더에 T day 일정을 등록하고 알림 설정을 하면 추첨을 통해 맥도날드 1만원권을 증정하는 이벤트를 안내하고 있다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 배달의민족(페리카나 전용) 8,000원 할인 (16,000원 이상 주문 시)\n",
            "- 에이닷 캘린더에 T day 일정 등록 및 알림 설정 시 맥도날드 1만원권 추첨 기회\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(만 13~34세 고객:확인) -[확인후]-> (SKT 0 day:참여)\n",
            "(SKT 0 day:참여) -[참여하면]-> (배달의민족:주문)\n",
            "(배달의민족:주문) -[주문하면]-> (8000원 할인:수령)\n",
            "\n",
            "(에이닷 캘린더:접속) -[접속하여]-> (T day 일정:등록)\n",
            "(T day 일정:등록) -[등록후]-> (알림:설정)\n",
            "(알림:설정) -[설정하면]-> (맥도날드 1만원권:추첨)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가?\n",
            "  → 일부 불명확함. \"만 13~34세 고객:확인\"은 실제 행동이라기보다는 자격 확인에 가까움.\n",
            "  → \"SKT 0 day:참여\"는 구체적인 행동이라기보다는 이벤트 자체를 지칭함.\n",
            "- 모든 독립적 가치 제안이 포함되었는가?\n",
            "  → 포함됨. 배달의민족 할인과 맥도날드 1만원권 추첨 모두 표현됨.\n",
            "- 주요 기능들이 적절히 그룹화되었는가?\n",
            "  → 그룹화 필요 없음. 각 경로는 독립적인 혜택임.\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 대체로 명확함.\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 다소 복잡함. \"SKT 0 day:참여\"와 \"배달의민족:주문\" 사이의 관계가 명확하지 않음.\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가?\n",
            "  → 조건부 관계(참여하면, 주문하면)와 결과 관계(수령, 추첨)가 적절히 사용됨.\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- Root Node가 실제 사용자 행동으로 명확히 정의되지 않음.\n",
            "  → \"만 13~34세 고객:확인\"은 실제 행동이라기보다는 자격 요건에 가까움.\n",
            "  → \"SKT 0 day:참여\"는 구체적인 행동(예: 접속, 주문)이 아닌 이벤트 자체를 지칭함.\n",
            "- \"SKT 0 day:참여\"와 \"배달의민족:주문\" 사이의 관계가 모호함.\n",
            "  → \"참여\"가 무엇을 의미하는지 명확하지 않음. 단순히 이벤트 페이지를 보는 것인지, 실제로 주문하는 것인지 불분명함.\n",
            "- \"에이닷 캘린더:접속\"이 Root Node로 사용되었으나, 실제 첫 행동은 이벤트 페이지 접속일 가능성이 높음.\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(SKT 0 day 페이지:접속) -[접속하여]-> (배달의민족:주문)\n",
            "(배달의민족:주문) -[주문하면, 16000원 이상]-> (8000원 할인:수령)\n",
            "\n",
            "(이벤트 페이지:접속) -[접속하여]-> (T day 일정:등록)\n",
            "(T day 일정:등록) -[등록후]-> (알림:설정)\n",
            "(알림:설정) -[설정하면]-> (맥도날드 1만원권:추첨)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node가 명확히 식별됨:\n",
            "  → \"SKT 0 day 페이지:접속\"과 \"이벤트 페이지:접속\"으로 실제 첫 행동을 명확히 표현함.\n",
            "- 모든 독립적 가치 제안이 포함됨:\n",
            "  → 배달의민족 8,000원 할인과 맥도날드 1만원권 추첨 기회 모두 표현됨.\n",
            "- 주요 기능들이 적절히 그룹화됨:\n",
            "  → 각 경로는 독립적인 혜택으로, 그룹화가 필요하지 않음.\n",
            "- 각 경로가 명확한 가치를 전달함:\n",
            "  → 첫 번째 경로는 \"접속 → 주문 → 할인 수령\"의 명확한 흐름\n",
            "  → 두 번째 경로는 \"접속 → 등록 → 알림 설정 → 추첨\"의 명확한 흐름\n",
            "- 전체 구조가 간결하고 이해하기 쉬움:\n",
            "  → 불필요한 노드(예: \"만 13~34세 고객:확인\", \"SKT 0 day:참여\")를 제거하고 실제 행동 중심으로 구성함.\n",
            "- 관계 동사가 우선순위에 맞게 사용됨:\n",
            "  → 조건부 관계(주문하면, 설정하면)와 결과 관계(수령, 추첨)가 적절히 사용됨.\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- \"SKT 0 day 페이지:접속\"과 \"이벤트 페이지:접속\"을 Root Node로 선택한 이유:\n",
            "  → 실제 사용자가 혜택을 받기 위해 가장 먼저 해야 할 행동은 해당 이벤트 페이지에 접속하는 것임.\n",
            "  → \"만 13~34세 고객:확인\"은 실제 행동이라기보다는 자격 요건에 해당하므로 제외함.\n",
            "- \"SKT 0 day:참여\" 대신 \"SKT 0 day 페이지:접속\"으로 수정한 이유:\n",
            "  → \"참여\"라는 모호한 표현 대신, 구체적인 행동인 \"페이지 접속\"으로 명확히 함.\n",
            "  → 페이지 접속 후 배달의민족에서 주문하는 것이 실제 혜택 수령의 흐름임.\n",
            "- \"배달의민족:주문\"과 \"8000원 할인:수령\" 사이에 \"16000원 이상\" 조건을 추가한 이유:\n",
            "  → 광고 문구에서 \"16,000원 이상 주문 시 할인 적용\"이라는 조건이 명시되어 있음.\n",
            "  → 이 조건은 할인 수령을 위한 필수 조건이므로 조건부 관계로 표현함.\n",
            "- \"에이닷 캘린더:접속\" 대신 \"이벤트 페이지:접속\"으로 수정한 이유:\n",
            "  → 실제 사용자가 T day 일정을 등록하기 위해서는 먼저 이벤트 페이지에 접속해야 함.\n",
            "  → \"에이닷 캘린더:접속\"은 이벤트 참여 후 이루어지는 행동이므로, 전체 흐름의 시작점으로 적절하지 않음.\n",
            "- \"알림:설정\"을 \"T day 일정:등록\"의 후속 행동으로 배치한 이유:\n",
            "  → 광고 문구에서 \"T day 일정을 에이닷 캘린더에 등록하고 알림 설정을 하면\"이라고 명시되어 있음.\n",
            "  → 일정 등록 후 알림 설정이 이루어지는 순차적 흐름을 반영함.\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  SKT 0 day 페이지:접속 | {'entity': 'SKT 0 day 페이지', 'action': '접속', 'path': None}\n",
            "  이벤트 페이지:접속 | {'entity': '이벤트 페이지', 'action': '접속', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  SKT 0 day 페이지:접속\n",
            "    --[접속하여]-->\n",
            "  배달의민족:주문\n",
            "    --[주문하면, 16000원 이상]-->\n",
            "  8000원 할인:수령\n",
            "\n",
            "Path 2:\n",
            "  이벤트 페이지:접속\n",
            "    --[접속하여]-->\n",
            "  T day 일정:등록\n",
            "    --[등록후]-->\n",
            "  알림:설정\n",
            "    --[설정하면]-->\n",
            "  맥도날드 1만원권:추첨\n",
            "\n",
            "============================================= Message =============================================\n",
            "\n",
            "[SK텔레콤] 아이폰 고객님만을 위한 특별 혜택, 네이버페이 5,000원!\n",
            "(광고)[SKT]아이폰 고객님만을 위한 특별 혜택, 네이버페이 5,000원!\n",
            "아이폰을 쓰시는 #04고객님, 안녕하세요?\n",
            "지금 에이닷에 신규 가입 후 AI 전화 서비스를 이용하시면 네이버페이 포인트 5,000원을 100% 드려요! 선착순 1만명 한정이니 지금 바로 참여해 보세요!\n",
            "\n",
            "이벤트 참여하기 : http://t-mms.kr/jP6/#74\n",
            "에이닷 전화 서비스를 이용하시면, 아이폰에서도 AI 통화 녹음/요약 뿐만 아니라 더 강력한 AI 스팸 필터링으로 더 편리하게 통화하실 수 있습니다. 에이닷에서는 모든 AI 서비스가 무료! 지금 바로 경험해 보세요!\n",
            "\n",
            " 이벤트 관련문의: 더브리즈 02-6101-2000 \n",
            "SKT와 함께해 주셔서 감사합니다\n",
            "무료 수신거부 1504\n",
            "\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤은 아이폰 사용자를 대상으로 에이닷(A.) 서비스에 신규 가입하고 AI 전화 서비스를 이용할 경우 네이버페이 포인트 5,000원을 제공하는 이벤트를 홍보하고 있다. 이 이벤트는 선착순 1만 명에게 한정되며, 에이닷의 다양한 AI 통화 기능(녹음, 요약, 스팸 필터링)을 무료로 이용할 수 있다는 점도 강조하고 있다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 네이버페이 포인트 5,000원 제공 (즉각적 혜택)\n",
            "- 에이닷의 AI 전화 서비스 무료 이용 (서비스 가치)\n",
            "  - AI 통화 녹음/요약 기능\n",
            "  - AI 스팸 필터링 기능\n",
            "- 선착순 1만 명 한정 (참여 동기 부여)\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(에이닷:가입) -[가입후]-> (AI전화서비스:사용)\n",
            "(AI전화서비스:사용) -[사용하면]-> (네이버페이5000원:수령)\n",
            "(에이닷:가입) -[가입후]-> (AI통화기능:사용)\n",
            "(AI통화기능:사용) -[사용하면]-> (AI통화녹음/요약:사용)\n",
            "(AI통화기능:사용) -[사용하면]-> (AI스팸필터링:사용)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가?\n",
            "  → 아니오. 사용자의 첫 번째 행동(이벤트 참여)이 누락됨\n",
            "- 모든 독립적 가치 제안이 포함되었는가?\n",
            "  → 예. 네이버페이 포인트와 AI 통화 서비스(녹음/요약, 스팸 필터링) 모두 포함됨\n",
            "- 주요 기능들이 적절히 그룹화되었는가?\n",
            "  → 예. AI 통화 기능을 상위 노드로 두고 하위 기능들로 분기함\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 예. 각 경로는 네이버페이 포인트 수령 또는 AI 통화 기능 체험이라는 명확한 가치를 전달함\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 보통. 'AI통화기능'과 'AI전화서비스'가 유사한 의미를 가지므로 통합 가능\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가?\n",
            "  → 예. '가입후', '사용하면' 등 조건부/결과 관계 동사가 적절히 사용됨\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- Root Node가 명확하지 않음: 사용자가 가장 먼저 해야 할 행동(이벤트 참여)이 DAG에 반영되지 않음. 이벤트 참여는 에이닷 가입의 선행 조건이므로 포함되어야 함.\n",
            "- 'AI통화기능'과 'AI전화서비스'의 중복: 두 노드가 사실상 동일한 기능을 지칭하므로 하나로 통합하는 것이 더 간결함.\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(이벤트:참여) -[참여하여]-> (에이닷:가입)\n",
            "(에이닷:가입) -[가입후]-> (AI전화서비스:사용)\n",
            "(AI전화서비스:사용) -[사용하면]-> (네이버페이5000원:수령)\n",
            "(AI전화서비스:사용) -[사용하면]-> (AI통화녹음/요약:사용)\n",
            "(AI전화서비스:사용) -[사용하면]-> (AI스팸필터링:사용)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node가 명확히 식별됨: '이벤트:참여'가 DAG의 시작점으로 설정되어 사용자가 가장 먼저 해야 할 행동을 명확히 제시함.\n",
            "- 모든 독립적 가치 제안이 포함됨: 네이버페이 포인트 5,000원 수령과 AI 전화 서비스(녹음/요약, 스팸 필터링) 무료 이용이라는 두 가지 주요 가치 제안이 모두 반영됨.\n",
            "- 주요 기능들이 적절히 그룹화됨: 'AI전화서비스' 노드를 중심으로 하위 기능들(AI통화녹음/요약, AI스팸필터링)이 분기되어 있어 기능 간 관계가 명확함.\n",
            "- 각 경로가 명확한 가치를 전달함: 첫 번째 경로는 네이버페이 포인트 수령, 나머지 경로는 AI 통화 기능 체험이라는 명확한 가치를 전달함.\n",
            "- 전체 구조가 간결하고 이해하기 쉬움: 중복되는 노드를 통합하여 구조가 간결해졌으며, 각 노드와 엣지의 의미가 명확함.\n",
            "- 관계 동사가 우선순위에 맞게 사용됨: '참여하여', '가입후', '사용하면' 등 조건부/결과 관계 동사가 적절히 사용됨.\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- '이벤트:참여'를 Root Node로 설정한 이유: 메시지에는 \"이벤트 참여하기\" 링크가 명시되어 있으며, 이는 사용자가 가장 먼저 취해야 할 행동임. 이벤트 참여가 에이닷 가입의 선행 조건이므로 DAG의 시작점으로 설정함.\n",
            "- '에이닷:가입'과 'AI전화서비스:사용'의 연결: 메시지에서는 에이닷에 신규 가입한 후 AI 전화 서비스를 이용해야 네이버페이 포인트를 받을 수 있다고 명시하고 있어, '가입후'라는 조건부 관계로 연결함.\n",
            "- 'AI전화서비스:사용'에서 '네이버페이5000원:수령'으로의 연결: AI 전화 서비스 사용 시 네이버페이 포인트를 100% 제공한다고 명시되어 있어, '사용하면'이라는 조건부 관계로 연결함.\n",
            "- 'AI전화서비스:사용'에서 'AI통화녹음/요약:사용' 및 'AI스팸필터링:사용'으로의 연결: AI 전화 서비스에는 여러 기능이 포함되어 있으며, 각 기능은 독립적으로 사용 가능하므로 분기 구조로 표현함.\n",
            "- 'AI통화기능' 대신 'AI전화서비스' 사용: 메시지에서는 'AI 전화 서비스'라는 용어를 사용하고 있으며, 이는 통화 관련 모든 AI 기능을 포괄하는 상위 개념이므로 이를 사용함.\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  이벤트:참여 | {'entity': '이벤트', 'action': '참여', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  이벤트:참여\n",
            "    --[참여하여]-->\n",
            "  에이닷:가입\n",
            "    --[가입후]-->\n",
            "  AI전화서비스:사용\n",
            "    --[사용하면]-->\n",
            "  네이버페이5000원:수령\n",
            "\n",
            "Path 2:\n",
            "  이벤트:참여\n",
            "    --[참여하여]-->\n",
            "  에이닷:가입\n",
            "    --[가입후]-->\n",
            "  AI전화서비스:사용\n",
            "    --[사용하면]-->\n",
            "  AI통화녹음/요약:사용\n",
            "\n",
            "Path 3:\n",
            "  이벤트:참여\n",
            "    --[참여하여]-->\n",
            "  에이닷:가입\n",
            "    --[가입후]-->\n",
            "  AI전화서비스:사용\n",
            "    --[사용하면]-->\n",
            "  AI스팸필터링:사용\n",
            "\n",
            "============================================= Message =============================================\n",
            "[SK텔레콤] 제이스대리점 미사강변점 고객 혜택 안내\n",
            "(광고)[SKT] 제이스대리점 미사강변점 고객 혜택 안내  고객님, 안녕하세요. SKT 제이스대리점 미사강변점에서 고객님을 위한 혜택을 준비했습니다.   \n",
            " 갤럭시 S21 구매 고객 혜택(3월 한정)  - 5GX 프라임 요금제 지원금 45만원 100% + 15% 추가 지원금 적용  - 사용하던 기기 반납시 \"2배보상\"  - 제휴 삼성카드 가입 시 추가 4% 할인  - 제휴 롯데카드 가입 시 5% 할인  - 버즈라이브 50% 할인쿠폰 제공   ※ 모든 혜택조건은 중복 적용가능  \n",
            " 초등학생 신학기 이벤트  - 가장 많이 찾는 \"ZEM폰, 미니폰\" 매장 입고되어 즉시개통 가능  - 원격으로 아이 휴대폰 관리부터 실시간 위치 조회 가능  - 기기값 부담은 확 낮추고 결합할인을 통해 더블할인까지~   \n",
            " 제이스대리점 미사강변점 - 주소: 경기 하남시 미사강변대로64 제일아이조움 1층 T월드 - 연락처: 031-795-2423 ※ 건물 내 무료 주차  \n",
            " 매장 홈페이지/예약/상담: http://t-mms.kr/t.do?m=#61&s=556&a=&u=https://tworldfriends.co.kr/D145710102 \n",
            " 매장 위치 보기: http://t-mms.kr/t.do?m=#61&s=557&a=&u=http://kko.to/vtT5qpJDp  \n",
            " 문의: SKT 고객센터(1558, 무료) ※ 코로나19 확산으로 고객센터에 문의가 증가하고 있습니다. 고객센터와 전화 연결이 원활하지 않을 수 있으니 양해 바랍니다.  SKT와 함께해주셔서 감사합니다. 무료 수신거부 1504\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤 제이스대리점 미사강변점에서 갤럭시 S21 구매 고객과 초등학생을 대상으로 다양한 혜택을 제공하는 광고 메시지입니다. 갤럭시 S21 구매 시 지원금, 할인, 쿠폰 등 금전적 혜택을 제공하며, 초등학생 대상 ZEM폰/미니폰의 즉시 개통과 관리 기능, 할인 혜택을 강조합니다. 매장 방문과 온라인 예약/상담 경로를 안내하고 있습니다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 갤럭시 S21 구매 시:\n",
            "  - 5GX 프라임 요금제 지원금 45만원 100% + 15% 추가 지원금\n",
            "  - 사용하던 기기 반납 시 \"2배보상\"\n",
            "  - 제휴 삼성카드 가입 시 추가 4% 할인\n",
            "  - 제휴 롯데카드 가입 시 5% 할인\n",
            "  - 버즈라이브 50% 할인쿠폰 제공\n",
            "  - 모든 혜택 중복 적용 가능\n",
            "- 초등학생 신학기 이벤트:\n",
            "  - ZEM폰, 미니폰 즉시 개통 가능\n",
            "  - 원격 관리 및 실시간 위치 조회 기능\n",
            "  - 기기값 부담 감소\n",
            "  - 결합할인을 통한 더블할인\n",
            "- 매장 방문/예약/상담 경로 안내\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(제이스대리점:방문) -[방문하여]-> (갤럭시 S21:구매)\n",
            "(갤럭시 S21:구매) -[구매하면]-> (5GX프라임요금제:가입)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (지원금45만원+15%:수령)\n",
            "(갤럭시 S21:구매) -[구매하면]-> (기기반납:참여)\n",
            "(기기반납:참여) -[참여하면]-> (2배보상:수령)\n",
            "(갤럭시 S21:구매) -[구매후]-> (제휴삼성카드:가입)\n",
            "(제휴삼성카드:가입) -[가입하면]-> (4%할인:수령)\n",
            "(갤럭시 S21:구매) -[구매후]-> (제휴롯데카드:가입)\n",
            "(제휴롯데카드:가입) -[가입하면]-> (5%할인:수령)\n",
            "(갤럭시 S21:구매) -[구매하면]-> (버즈라이브쿠폰:수령)\n",
            "(제이스대리점:방문) -[방문하여]-> (ZEM폰/미니폰:구매)\n",
            "(ZEM폰/미니폰:구매) -[구매하면]-> (즉시개통:참여)\n",
            "(ZEM폰/미니폰:구매) -[구매후]-> (원격관리기능:사용)\n",
            "(ZEM폰/미니폰:구매) -[구매후]-> (위치조회기능:사용)\n",
            "(ZEM폰/미니폰:구매) -[구매하면]-> (기기값부담감소:수령)\n",
            "(ZEM폰/미니폰:구매) -[구매후]-> (결합할인:참여)\n",
            "(결합할인:참여) -[참여하면]-> (더블할인:수령)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가?\n",
            "  → 네, \"제이스대리점:방문\"이 모든 경로의 시작점으로 명확히 식별됨\n",
            "- 모든 독립적 가치 제안이 포함되었는가?\n",
            "  → 네, 갤럭시 S21 구매 혜택(지원금, 할인, 쿠폰)과 ZEM폰/미니폰 혜택(즉시개통, 관리기능, 할인) 모두 포함됨\n",
            "- 주요 기능들이 적절히 그룹화되었는가?\n",
            "  → 아니오, ZEM폰/미니폰의 \"원격관리기능\"과 \"위치조회기능\"이 별도로 표현됨\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 네, 각 경로는 특정 혜택이나 기능 획득을 명확히 표현함\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 아니오, 중복된 \"구매후\" 관계와 \"참여\" 노드가 많아 복잡함\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가?\n",
            "  → 네, \"구매하면\", \"가입하면\", \"참여하면\" 등 조건부 관계 동사가 적절히 사용됨\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- ZEM폰/미니폰의 관리 기능이 \"원격관리기능\"과 \"위치조회기능\"으로 분리되어 있어 중복 표현됨\n",
            "- \"구매후\" 관계가 반복적으로 사용되어 경로가 불필요하게 길어짐\n",
            "- \"기기값부담감소\"와 \"더블할인\"이 \"수령\"으로 표현되어 실제 행동(할인 적용)과 차이가 있음\n",
            "- \"2배보상\"이 \"참여\"로 표현되어 실제 행동(기기 반납)과 혜택(보상)이 명확히 구분되지 않음\n",
            "- \"제휴카드 가입\"이 \"구매후\"로 표현되어, 구매와 카드 가입의 선후 관계가 명확하지 않음\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(제이스대리점:방문) -[방문하여]-> (갤럭시 S21:구매)\n",
            "(갤럭시 S21:구매) -[구매하면]-> (5GX프라임요금제:가입)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (지원금45만원+15%:수령)\n",
            "(갤럭시 S21:구매) -[구매후]-> (기기반납:참여)\n",
            "(기기반납:참여) -[참여하면]-> (2배보상:수령)\n",
            "(갤럭시 S21:구매) -[구매후]-> (제휴삼성카드:가입)\n",
            "(제휴삼성카드:가입) -[가입하면]-> (4%할인:적용)\n",
            "(갤럭시 S21:구매) -[구매후]-> (제휴롯데카드:가입)\n",
            "(제휴롯데카드:가입) -[가입하면]-> (5%할인:적용)\n",
            "(갤럭시 S21:구매) -[구매하면]-> (버즈라이브쿠폰:수령)\n",
            "(제이스대리점:방문) -[방문하여]-> (ZEM폰/미니폰:구매)\n",
            "(ZEM폰/미니폰:구매) -[구매하면]-> (즉시개통:참여)\n",
            "(ZEM폰/미니폰:구매) -[구매후]-> (아이관리기능:사용)\n",
            "(ZEM폰/미니폰:구매) -[구매후]-> (결합할인:참여)\n",
            "(결합할인:참여) -[참여하면]-> (더블할인:적용)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node가 명확히 식별됨: \"제이스대리점:방문\"이 모든 경로의 시작점\n",
            "- 모든 독립적 가치 제안이 포함됨: 갤럭시 S21 구매 혜택(지원금, 할인, 쿠폰)과 ZEM폰/미니폰 혜택(즉시개통, 관리기능, 할인) 모두 표현\n",
            "- 주요 기능들이 적절히 그룹화됨: \"원격관리\"와 \"위치조회\"를 \"아이관리기능\"으로 통합\n",
            "- 각 경로가 명확한 가치를 전달함: 지원금 수령, 할인 적용, 쿠폰 수령, 즉시 개통, 기능 사용 등\n",
            "- 전체 구조가 간결하고 이해하기 쉬움: 중복된 \"구매후\" 관계와 \"참여\" 노드 최소화\n",
            "- 관계 동사가 우선순위에 맞게 사용됨: \"구매하면\", \"가입하면\", \"참여하면\" 등 조건부 관계 동사 사용\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- \"제이스대리점:방문\"을 Root Node로 선택한 이유: 매장 주소와 연락처가 명시되어 있어 매장 방문이 모든 혜택의 시작점으로 판단됨\n",
            "- \"기기반납:참여\"와 \"2배보상:수령\"을 분리한 이유: 기기 반납은 실제 행동이고, 2배보상은 그 결과로 받는 혜택이므로 명확히 구분\n",
            "- \"4%할인:적용\"과 \"5%할인:적용\"으로 수정한 이유: \"수령\"은 실제 금전적 보상을 받는 것을 의미하므로, 할인이 적용되는 상황을 더 정확히 표현\n",
            "- \"아이관리기능:사용\"으로 통합한 이유: 원격 관리와 위치 조회는 모두 부모/보호자가 아이의 휴대폰을 관리하는 주요 기능들로, \"아이관리기능\"이라는 상위 개념으로 그룹화하여 간결성 확보\n",
            "- \"더블할인:적용\"으로 수정한 이유: \"더블할인\"은 결합할인을 통해 기기값 부담이 줄어드는 실제 효과를 의미하므로, \"적용\"이 더 적합함\n",
            "- \"구매후\" 관계를 최소화한 이유: \"구매후\"는 시간적 순서만 나타내며, 실제 혜택이나 기능 획득의 인과관계를 명확히 하기 위해 \"구매하면\"과 \"참여하면\" 등 조건부 관계를 주로 사용\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  제이스대리점:방문 | {'entity': '제이스대리점', 'action': '방문', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시 S21:구매\n",
            "    --[구매하면]-->\n",
            "  5GX프라임요금제:가입\n",
            "    --[가입하면]-->\n",
            "  지원금45만원+15%:수령\n",
            "\n",
            "Path 2:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시 S21:구매\n",
            "    --[구매후]-->\n",
            "  기기반납:참여\n",
            "    --[참여하면]-->\n",
            "  2배보상:수령\n",
            "\n",
            "Path 3:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시 S21:구매\n",
            "    --[구매후]-->\n",
            "  제휴삼성카드:가입\n",
            "    --[가입하면]-->\n",
            "  4%할인:적용\n",
            "\n",
            "Path 4:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시 S21:구매\n",
            "    --[구매후]-->\n",
            "  제휴롯데카드:가입\n",
            "    --[가입하면]-->\n",
            "  5%할인:적용\n",
            "\n",
            "Path 5:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시 S21:구매\n",
            "    --[구매하면]-->\n",
            "  버즈라이브쿠폰:수령\n",
            "\n",
            "Path 6:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  ZEM폰/미니폰:구매\n",
            "    --[구매하면]-->\n",
            "  즉시개통:참여\n",
            "\n",
            "Path 7:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  ZEM폰/미니폰:구매\n",
            "    --[구매후]-->\n",
            "  아이관리기능:사용\n",
            "\n",
            "Path 8:\n",
            "  제이스대리점:방문\n",
            "    --[방문하여]-->\n",
            "  ZEM폰/미니폰:구매\n",
            "    --[구매후]-->\n",
            "  결합할인:참여\n",
            "    --[참여하면]-->\n",
            "  더블할인:적용\n",
            "\n",
            "============================================= Message =============================================\n",
            "수도권 부스트 파크 이벤트를 안내드립니다. \n",
            "(광고)[SKT] 부스트 파크 추첨 이벤트 안내  고객님, 안녕하세요. 수도권 부스트 파크에서 진행되는 특별한 이벤트를 안내해드립니다!  \n",
            " 이벤트1. 갤럭시 노트20 사전예약 이벤트  - 기간: 2020년 8월 7일(금)~8월 13일(목) - 장소: 부스트 파크 강남역/가로수길, 잠실, 광화문, 대학로, 인천 구월, 안양 범계, 일산 라페/웨돔, 수원 광교에 있는 T월드 매장 - 대상: 부스트 파크 T월드 매장에서 5G 체험한 뒤 갤럭시 노트20 사전예약하시는 고객님 - 사은품: 부스트 파크 상생 제휴처 사은품 증정 ※ 사은품은 매장마다 다를 수 있으며, 일찍 소진될 수 있습니다.  \n",
            " 부스트 파크 위치 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/3iduqLN  \n",
            " 이벤트2. 갤럭시 노트20 개통 고객 대상 추첨 이벤트  - 기간: 2020년 8월 14일(금)~8월 20일(목) - 대상: 이벤트1에 참여하고 갤럭시 노트20 개통하신 고객님 중 25명 추첨 - 사은품: 인사동 나인트리 프리미어 호텔 1박2일 숙박권 \n",
            " 이벤트 세부 내용 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/2XS94fl  ※ 갤럭시 노트20 제조사 예약/개통 프로모션과 별개이며, 중복 혜택을 받으실 수 있습니다.  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SKT는 수도권 부스트 파크에서 갤럭시 노트20 사전예약 및 개통 고객을 대상으로 두 가지 이벤트를 진행합니다. 첫 번째 이벤트는 부스트 파크 T월드 매장에서 5G를 체험하고 갤럭시 노트20을 사전 예약하는 고객에게 매장별 제휴처 사은품을 제공합니다. 두 번째 이벤트는 첫 번째 이벤트에 참여한 후 갤럭시 노트20을 개통한 고객 중 25명을 추첨하여 호텔 숙박권을 제공합니다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 갤럭시 노트20 사전예약 시 매장별 제휴처 사은품 제공\n",
            "- 갤럭시 노트20 개통 고객 중 추첨을 통해 호텔 숙박권 제공\n",
            "- 5G 체험 기회 제공\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(부스트파크T월드매장:방문) -[방문하여]-> (5G:체험)\n",
            "(5G:체험) -[체험후]-> (갤럭시노트20:사전예약)\n",
            "(갤럭시노트20:사전예약) -[사전예약하면]-> (제휴처사은품:수령)\n",
            "(갤럭시노트20:사전예약) -[사전예약후]-> (갤럭시노트20:개통)\n",
            "(갤럭시노트20:개통) -[개통후]-> (추첨참여)\n",
            "(추첨참여) -[추첨되면]-> (호텔숙박권:수령)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가? (방문/접속/다운로드 등)\n",
            "  → 예, \"부스트파크T월드매장:방문\"이 첫 번째 행동으로 명확히 식별됨\n",
            "- 모든 독립적 가치 제안이 포함되었는가? (즉각적 혜택과 서비스 가치)\n",
            "  → 예, 제휴처 사은품, 호텔 숙박권, 5G 체험 등 모든 가치 제안 포함\n",
            "- 주요 기능들이 적절히 그룹화되었는가? (중복 제거 여부)\n",
            "  → 예, 5G 체험은 단일 노드로 표현됨\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 예, 각 경로는 명확한 혜택(사은품, 숙박권)으로 연결됨\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 예, 주요 행동 흐름이 직관적으로 표현됨\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가? (조건부 > 결과 > 경로)\n",
            "  → 예, \"체험후\", \"사전예약하면\", \"사전예약후\", \"개통후\", \"추첨되면\" 등 우선순위에 맞게 사용됨\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- \"5G:체험\" 노드가 독립적인 가치 제안으로 보이지만, 실제 핵심 행동은 \"갤럭시노트20:사전예약\"과 \"갤럭시노트20:개통\"임. 5G 체험은 사전예약의 전제 조건이므로 별도의 노드로 분리하는 것이 흐름상 불필요함.\n",
            "- \"추첨참여\" 노드가 명시적으로 표현되어 있으나, 이는 \"갤럭시노트20:개통\"의 자동적 결과이므로 별도의 노드로 분리할 필요가 없음.\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(부스트파크T월드매장:방문) -[방문하여]-> (갤럭시노트20:사전예약)\n",
            "(갤럭시노트20:사전예약) -[사전예약하면]-> (제휴처사은품:수령)\n",
            "(갤럭시노트20:사전예약) -[사전예약후]-> (갤럭시노트20:개통)\n",
            "(갤럭시노트20:개통) -[개통하면(추첨대상)]-> (호텔숙박권:수령)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node가 명확히 식별됨: \"부스트파크T월드매장:방문\"\n",
            "- 모든 독립적 가치 제안이 포함됨: 제휴처 사은품, 호텔 숙박권\n",
            "- 주요 기능(5G 체험)은 핵심 행동(사전예약)의 전제 조건으로 통합되어 중복 제거됨\n",
            "- 각 경로가 명확한 가치를 전달함: 사전예약→사은품, 개통→숙박권(추첨)\n",
            "- 전체 구조가 간결하고 이해하기 쉬움\n",
            "- 관계 동사가 우선순위에 맞게 사용됨: \"사전예약하면\", \"사전예약후\", \"개통하면(추첨대상)\"\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- \"부스트파크T월드매장:방문\"은 모든 이벤트 참여의 시작점으로, 매장 방문이 필수적임\n",
            "- \"5G:체험\"은 \"갤럭시노트20:사전예약\"의 전제 조건이므로 별도의 노드로 분리하지 않고, \"방문\"과 \"사전예약\" 사이에 암시적으로 포함됨\n",
            "- \"제휴처사은품:수령\"은 \"갤럭시노트20:사전예약\"의 직접적 혜택이므로 \"사전예약하면\"으로 연결\n",
            "- \"갤럭시노트20:개통\"은 \"사전예약후\"의 자연스러운 다음 단계로, 두 번째 이벤트의 시작점이 됨\n",
            "- \"호텔숙박권:수령\"은 \"갤럭시노트20:개통\"한 고객 중 추첨을 통해 제공되므로, \"개통하면(추첨대상)\"으로 표현하여 조건부 관계를 명확히 함\n",
            "- \"추첨참여\"는 별도의 노드로 표현하지 않고 \"개통하면(추첨대상)\"에 포함시켜 흐름을 간결하게 유지함\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  부스트파크T월드매장:방문 | {'entity': '부스트파크T월드매장', 'action': '방문', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  부스트파크T월드매장:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시노트20:사전예약\n",
            "    --[사전예약하면]-->\n",
            "  제휴처사은품:수령\n",
            "\n",
            "Path 2:\n",
            "  부스트파크T월드매장:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시노트20:사전예약\n",
            "    --[사전예약후]-->\n",
            "  갤럭시노트20:개통\n",
            "    --[개통하면(추첨대상)]-->\n",
            "  호텔숙박권:수령\n",
            "\n",
            "============================================= Message =============================================\n",
            "[SK텔레콤] 엄마손대리점 본점 폴더블6 출시 이벤트 안내드립니다\n",
            "(광고)[SKT] 엄마손대리점 본점 폴더블6 출시 이벤트\n",
            "고객님 안녕하세요\n",
            "SK텔레콤 공식인증대리점 엄마손대리점 본점입니다\n",
            "\n",
            "\n",
            "■ 폴더블6 \n",
            "전작 대비 더욱 강력해진 성능으로 19일 첫 출시!\n",
            "지금 바로 예약(7월 18일 공식예약 마감)하시고 예약고객만의 다양한 혜택을 누려보시기 바랍니다\n",
            "\n",
            "\n",
            "■ 신형 휴대폰 외 \n",
            "효도폰, 키즈폰 등 다양한 할인 이벤트도 진행중입니다\n",
            "\n",
            "\n",
            "■ 추가할인, 사은품 등 다양한 혜택과\n",
            "요금할인, 숨어있는 포인트 활용 등 꿀Tip상담을 약속드립니다\n",
            "\n",
            "\n",
            "■ SK공식인증대리점 엄마손대리점 본점\n",
            "- 주소 : 서울 송파구 가락로 110 \n",
            "- 연락처 : 02-420-9011\n",
            "- 구술약도 : 송파역 석촌시장에서 대로변에 위치\n",
            "\n",
            "\n",
            "▶ 홈페이지 : http://t-mms.kr/t.do?m=#61&s=27256&a=&u=https://naver.me/FyeuZoSp\n",
            "\n",
            "\n",
            "■ 문의 : SKT 고객센터(1558, 무료)\n",
            "SK텔레콤과 함께해 주셔서 감사합니다.\n",
            "무료 수신거부 1504\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤 공식인증대리점 엄마손대리점 본점에서 폴더블6 출시와 관련하여 예약 및 구매 시 다양한 혜택을 제공하는 이벤트를 안내하고 있습니다. 또한 효도폰, 키즈폰 등 다른 휴대폰에 대한 할인 이벤트와 추가 할인, 사은품, 요금할인, 포인트 활용 등 다양한 혜택을 상담받을 수 있음을 강조하고 있습니다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 폴더블6 예약 및 구매 시 다양한 혜택\n",
            "- 효도폰, 키즈폰 등 다양한 휴대폰 할인 이벤트\n",
            "- 추가 할인, 사은품 제공\n",
            "- 요금할인 상담\n",
            "- 숨어있는 포인트 활용 상담\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (폴더블6:예약)\n",
            "(폴더블6:예약) -[예약하면]-> (다양한 혜택:수령)\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (효도폰/키즈폰:구매)\n",
            "(효도폰/키즈폰:구매) -[구매하면]-> (할인:수령)\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (상담:참여)\n",
            "(상담:참여) -[참여하면]-> (추가 할인:수령)\n",
            "(상담:참여) -[참여하면]-> (사은품:수령)\n",
            "(상담:참여) -[참여하면]-> (요금할인:수령)\n",
            "(상담:참여) -[참여하면]-> (포인트 활용:수령)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가? (방문/접속/다운로드 등)\n",
            "  → 네, \"엄마손대리점 본점:방문\"이 모든 경로의 시작점으로 명확히 식별됨\n",
            "- 모든 독립적 가치 제안이 포함되었는가? (즉각적 혜택과 서비스 가치)\n",
            "  → 네, 폴더블6 예약 혜택, 효도폰/키즈폰 할인, 추가 할인, 사은품, 요금할인, 포인트 활용 등 모든 가치 제안 포함\n",
            "- 주요 기능들이 적절히 그룹화되었는가? (중복 제거 여부)\n",
            "  → 네, 효도폰과 키즈폰을 \"효도폰/키즈폰\"으로 그룹화함\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 네, 각 경로는 특정 행동(예약, 구매, 상담)에 따른 혜택을 명확히 전달함\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 네, 각 가치 제안이 별도의 경로로 표현되어 간결함\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가? (조건부 > 결과 > 경로)\n",
            "  → 네, \"방문하여\", \"예약하면\", \"구매하면\", \"참여하면\" 등 조건부 관계 동사가 적절히 사용됨\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- 없음. 초기 DAG가 모든 기준을 충족하는 것으로 판단됨\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (폴더블6:예약)\n",
            "(폴더블6:예약) -[예약하면]-> (다양한 혜택:수령)\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (효도폰/키즈폰:구매)\n",
            "(효도폰/키즈폰:구매) -[구매하면]-> (할인:수령)\n",
            "(엄마손대리점 본점:방문) -[방문하여]-> (상담:참여)\n",
            "(상담:참여) -[참여하면]-> (추가 할인:수령)\n",
            "(상담:참여) -[참여하면]-> (사은품:수령)\n",
            "(상담:참여) -[참여하면]-> (요금할인:수령)\n",
            "(상담:참여) -[참여하면]-> (포인트 활용:수령)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node로 \"엄마손대리점 본점:방문\"이 명확히 설정되어 있음\n",
            "- 모든 가치 제안(폴더블6 예약 혜택, 효도폰/키즈폰 할인, 추가 할인, 사은품, 요금할인, 포인트 활용)이 빠짐없이 포함됨\n",
            "- 효도폰과 키즈폰 등 유사한 기능은 \"효도폰/키즈폰\"으로 적절히 그룹화됨\n",
            "- 각 경로는 특정 행동(예약, 구매, 상담)에 따른 혜택을 명확히 전달함\n",
            "- 전체 구조가 간결하고 이해하기 쉬움\n",
            "- 관계 동사가 우선순위에 맞게 사용됨(조건부 관계 동사 사용)\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- \"엄마손대리점 본점:방문\"은 모든 혜택의 시작점으로, 매장 주소와 연락처가 명시되어 있어 방문의 필요성이 강조됨\n",
            "- \"폴더블6:예약\"과 \"효도폰/키즈폰:구매\"는 각각 별도의 혜택 경로로 분리함. 예약과 구매는 서로 다른 행동이므로 구분함\n",
            "- \"상담:참여\" 노드는 추가 할인, 사은품, 요금할인, 포인트 활용 등 다양한 혜택을 한 번에 받을 수 있는 경로로 설정함\n",
            "- \"다양한 혜택:수령\"은 폴더블6 예약 시 받을 수 있는 구체적인 혜택들이 명시되지 않았으므로 포괄적으로 표현함\n",
            "- \"할인:수령\", \"추가 할인:수령\", \"사은품:수령\", \"요금할인:수령\", \"포인트 활용:수령\"은 각각 상담 참여의 결과로 받을 수 있는 구체적인 혜택들을 표현함\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  엄마손대리점 본점:방문 | {'entity': '엄마손대리점 본점', 'action': '방문', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  폴더블6:예약\n",
            "    --[예약하면]-->\n",
            "  다양한 혜택:수령\n",
            "\n",
            "Path 2:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  효도폰/키즈폰:구매\n",
            "    --[구매하면]-->\n",
            "  할인:수령\n",
            "\n",
            "Path 3:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  상담:참여\n",
            "    --[참여하면]-->\n",
            "  추가 할인:수령\n",
            "\n",
            "Path 4:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  상담:참여\n",
            "    --[참여하면]-->\n",
            "  사은품:수령\n",
            "\n",
            "Path 5:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  상담:참여\n",
            "    --[참여하면]-->\n",
            "  요금할인:수령\n",
            "\n",
            "Path 6:\n",
            "  엄마손대리점 본점:방문\n",
            "    --[방문하여]-->\n",
            "  상담:참여\n",
            "    --[참여하면]-->\n",
            "  포인트 활용:수령\n",
            "\n",
            "============================================= Message =============================================\n",
            "[SK텔레콤] 동진대리점 가양역 본점 이벤트 안내\n",
            "(광고)[SKT] 동진대리점 가양역 본점 이벤트 안내\n",
            "안녕하세요 고객님\n",
            "SK텔레콤 가양역 본점에서 검은 토끼의 해 계묘년 특별행사 안내드립니다. \n",
            "\n",
            "\n",
            "■ 이 달의 특가 휴대폰!\n",
            "- 준프리미엄 모델 갤럭시 퀀텀3 최대지원! + 갤럭시 워치까지 무료!(5GX프라임 요금제 이용 조건)\n",
            "- 자녀분들을 위한 ZEM포켓몬 에디션 이벤트!\n",
            "\n",
            "\n",
            "■ 초고속 인터넷 + TV 신규가입 또는 약정 만료고객 재약정시 사은품 최대 지급\n",
            "\n",
            "\n",
            "■ ADT 캡스 홈보안 6개월 무료 체험 (체험 만료시 위약금 면제)\n",
            "\n",
            "\n",
            "■ 새롭게 출시되는 갤럭시 S23 사전예약 시작\n",
            "- 2월13일까지 사전예약하면 2월14일에 바로 받아보실 수 있습니다!\n",
            "매장으로 방문하시면 자세히 설명드리겠습니다.\n",
            "\n",
            "\n",
            "■ 동진대리점 가양역본점 \n",
            "- 위치: 서울 강서구 화곡로68길 3 영스퀘어 1층 SK텔레콤\n",
            "- 연락처: 02-2659-7577\n",
            "\n",
            "\n",
            "▶카톡:http://pf.kakao.com/\n",
            "xjwmxaxj\n",
            "\n",
            "\n",
            "▶바로가기 http://t-mms.kr/t.do?m=#61&s=17996&a=&u=http://t-mms.kr/t.do?m=945711&s=17634&a=&u=http://tworldfriends.co.kr/D151510021\n",
            "SKT와 함께해주셔서 감사합니다. \n",
            "무료 수신거부 1504\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤 동진대리점 가양역 본점에서 계묘년 특별행사를 안내하며, 다양한 휴대폰 및 서비스 가입 혜택을 홍보하고 매장 방문을 유도함.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 갤럭시 퀀텀3 최대 지원 + 갤럭시 워치 무료 (5GX프라임 요금제 이용 조건)\n",
            "- ZEM포켓몬 에디션 이벤트 (자녀용)\n",
            "- 초고속 인터넷 + TV 신규가입 또는 재약정시 사은품 최대 지급\n",
            "- ADT 캡스 홈보안 6개월 무료 체험 (체험 만료시 위약금 면제)\n",
            "- 갤럭시 S23 사전예약 혜택 (2월13일까지 예약시 2월14일 수령)\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(동진대리점:방문) -[방문하여]-> (갤럭시퀀텀3:구매)\n",
            "(갤럭시퀀텀3:구매) -[구매하면]-> (5GX프라임요금제:가입)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (지원금:수령)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (갤럭시워치:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (ZEM포켓몬에디션:구매)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (초고속인터넷+TV:가입)\n",
            "(초고속인터넷+TV:가입) -[가입하면]-> (사은품:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (ADT캡스홈보안:체험)\n",
            "(ADT캡스홈보안:체험) -[체험후]-> (위약금면제:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (갤럭시S23:사전예약)\n",
            "(갤럭시S23:사전예약) -[사전예약하면]-> (갤럭시S23:수령)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가? (방문) → 예, 매장 방문이 모든 경로의 시작점임\n",
            "- 모든 독립적 가치 제안이 포함되었는가? → 예, 5가지 가치 제안이 모두 포함됨\n",
            "- 주요 기능들이 적절히 그룹화되었는가? → 예, 중복되는 노드는 없음\n",
            "- 각 경로가 명확한 가치를 전달하는가? → 예, 각 경로는 특정 혜택이나 서비스 체험으로 연결됨\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가? → 예, 각 경로가 독립적으로 표현되어 있음\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가? (조건부 > 결과 > 경로) → 예, \"구매하면\", \"가입하면\", \"체험후\" 등 적절히 사용됨\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- 갤럭시 퀀텀3와 갤럭시 워치 경로가 분리되어 있음: 갤럭시 워치는 갤럭시 퀀텀3 구매와 5GX프라임 요금제 가입의 조건부 혜택이므로 통합 필요\n",
            "- ZEM포켓몬 에디션의 구체적 혜택이 명시되지 않음: 구매 후 어떤 혜택을 받는지 불분명함\n",
            "- ADT 캡스 홈보안 체험의 시작점이 명확하지 않음: \"체험\"이 \"가입\"인지 \"신청\"인지 불분명함\n",
            "- 갤럭시 S23 사전예약 경로의 시간적 조건이 명시되지 않음: 2월13일까지라는 조건이 누락됨\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(동진대리점:방문) -[방문하여]-> (갤럭시퀀텀3:구매)\n",
            "(갤럭시퀀텀3:구매) -[구매하면]-> (5GX프라임요금제:가입)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (지원금:수령)\n",
            "(5GX프라임요금제:가입) -[가입하면]-> (갤럭시워치:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (ZEM포켓몬에디션:구매)\n",
            "(ZEM포켓몬에디션:구매) -[구매하면]-> (자녀용혜택:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (초고속인터넷+TV:가입)\n",
            "(초고속인터넷+TV:가입) -[가입하면]-> (사은품:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (ADT캡스홈보안:신청)\n",
            "(ADT캡스홈보안:신청) -[신청후]-> (6개월무료체험:사용)\n",
            "(6개월무료체험:사용) -[체험만료시]-> (위약금면제:수령)\n",
            "\n",
            "(동진대리점:방문) -[방문하여]-> (갤럭시S23:사전예약)\n",
            "(갤럭시S23:사전예약) -[2월13일까지사전예약하면]-> (갤럭시S23:수령)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- **Root Node**: \"동진대리점:방문\"이 모든 경로의 시작점으로 명확히 설정됨\n",
            "- **모든 가치 제안 포함**: 5가지 주요 가치 제안이 모두 DAG에 반영됨\n",
            "- **기능 그룹화**: 중복되는 노드가 없으며, 각 혜택이 적절히 그룹화됨\n",
            "- **명확한 가치 전달**: 각 경로가 특정 혜택(지원금, 갤럭시 워치, 자녀용 혜택, 사은품, 위약금 면제, 갤럭시 S23 수령)으로 연결됨\n",
            "- **간결성**: 각 경로가 독립적으로 표현되어 전체 구조가 간결함\n",
            "- **관계 동사 우선순위**: \"구매하면\", \"가입하면\", \"신청후\", \"체험만료시\" 등 조건부/결과 관계가 명확히 표현됨\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- **갤럭시 퀀텀3 + 갤럭시 워치**: 갤럭시 워치는 갤럭시 퀀텀3 구매와 5GX프라임 요금제 가입의 조건부 혜택이므로 하나의 경로로 통합\n",
            "- **ZEM포켓몬 에디션**: 구체적 혜택이 명시되지 않았으나, \"자녀용혜택\"으로 포괄적 표현 사용\n",
            "- **ADT 캡스 홈보안**: \"신청\"과 \"체험\"을 구분하여, 신청 후 체험, 체험 만료 후 위약금 면제라는 순차적 흐름 반영\n",
            "- **갤럭시 S23 사전예약**: 시간적 조건(2월13일까지)을 관계 동사에 명시하여 조건부 혜택임을 명확히 함\n",
            "- **매장 방문(Root Node)**: 모든 혜택과 서비스 체험이 매장 방문을 통해 시작되므로, DAG의 시작점으로 설정\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  동진대리점:방문 | {'entity': '동진대리점', 'action': '방문', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시퀀텀3:구매\n",
            "    --[구매하면]-->\n",
            "  5GX프라임요금제:가입\n",
            "    --[가입하면]-->\n",
            "  지원금:수령\n",
            "\n",
            "Path 2:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시퀀텀3:구매\n",
            "    --[구매하면]-->\n",
            "  5GX프라임요금제:가입\n",
            "    --[가입하면]-->\n",
            "  갤럭시워치:수령\n",
            "\n",
            "Path 3:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  ZEM포켓몬에디션:구매\n",
            "    --[구매하면]-->\n",
            "  자녀용혜택:수령\n",
            "\n",
            "Path 4:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  초고속인터넷+TV:가입\n",
            "    --[가입하면]-->\n",
            "  사은품:수령\n",
            "\n",
            "Path 5:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  ADT캡스홈보안:신청\n",
            "    --[신청후]-->\n",
            "  6개월무료체험:사용\n",
            "    --[체험만료시]-->\n",
            "  위약금면제:수령\n",
            "\n",
            "Path 6:\n",
            "  동진대리점:방문\n",
            "    --[방문하여]-->\n",
            "  갤럭시S23:사전예약\n",
            "    --[2월13일까지사전예약하면]-->\n",
            "  갤럭시S23:수령\n",
            "\n",
            "============================================= Message =============================================\n",
            "\n",
            "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
            "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내\n",
            "[2월 10일(토) 혜택]\n",
            "만 13~34세 고객이라면\n",
            "베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!\n",
            "(선착순 3만 명 증정)\n",
            "\n",
            "▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc\n",
            "\n",
            "■ 에이닷 X T 멤버십 시크릿코드 이벤트\n",
            "에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!\n",
            "뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다.\n",
            "\n",
            "▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM\n",
            "\n",
            "■ 문의: SKT 고객센터(1558, 무료)\n",
            "무료 수신거부 1504\n",
            "\n",
            "\n",
            "============================================= DAG (AX) =============================================\n",
            "### 1. 메시지 분석\n",
            "```\n",
            "[메시지 요약 및 핵심 의도]\n",
            "SK텔레콤은 2월 10일 만 13~34세 고객을 대상으로 베어유 강의 14일 무료 수강 쿠폰을 선착순 3만 명에게 제공하며, 에이닷 T 멤버십 쿠폰함에 '에이닷이빵쏜닷' 시크릿코드를 입력하면 뚜레쥬르 데일리우유식빵 무료 쿠폰을 제공하는 이벤트를 안내하고 있다.\n",
            "\n",
            "[식별된 가치 제안 목록]\n",
            "- 베어유 강의 14일 무료 수강 쿠폰 (선착순 3만 명, 만 13~34세 대상)\n",
            "- 뚜레쥬르 데일리우유식빵 무료 쿠폰 (시크릿코드 입력 시)\n",
            "```\n",
            "\n",
            "### 2. 초기 DAG\n",
            "```\n",
            "(만13~34세고객:확인) -[확인후]-> (SKT홈페이지:접속) -[접속하여]-> (베어유쿠폰:수령)\n",
            "(에이닷T멤버십:접속) -[접속하여]-> (시크릿코드:입력) -[입력하면]-> (식빵쿠폰:수령)\n",
            "```\n",
            "\n",
            "### 3. 자기 검증 결과\n",
            "```\n",
            "[평가 기준별 검토 결과]\n",
            "- Root Node가 명확히 식별되었는가?\n",
            "  → 부분적으로 충족. '만13~34세고객:확인'은 실제 행동이라기보다는 자격 확인에 가까움.\n",
            "- 모든 독립적 가치 제안이 포함되었는가?\n",
            "  → 충족. 두 가지 주요 혜택(베어유 쿠폰, 식빵 쿠폰) 모두 포함됨.\n",
            "- 주요 기능들이 적절히 그룹화되었는가?\n",
            "  → 충족. 각 혜택별로 독립적인 경로로 표현됨.\n",
            "- 각 경로가 명확한 가치를 전달하는가?\n",
            "  → 충족. 각 경로는 특정 쿠폰 수령이라는 명확한 가치를 전달함.\n",
            "- 전체 구조가 간결하고 이해하기 쉬운가?\n",
            "  → 충족. 두 개의 독립적인 경로로 간결하게 표현됨.\n",
            "- 관계 동사가 우선순위에 맞게 사용되었는가?\n",
            "  → 충족. '입력하면', '접속하여' 등 우선순위에 맞게 사용됨.\n",
            "\n",
            "[식별된 문제점 및 이유]\n",
            "- Root Node가 실제 행동보다는 자격 확인에 초점이 맞춰져 있음.\n",
            "  → '만13~34세고객:확인'은 실제 행동이라기보다는 자격 요건에 가까움.\n",
            "  → 실제 첫 행동은 'SKT홈페이지:접속' 또는 '에이닷T멤버십:접속'이 되어야 함.\n",
            "- '만13~34세고객:확인' 노드는 실제 행동 흐름에서 필수적이지 않음.\n",
            "  → 이 노드는 혜택 수령의 조건일 뿐, 행동 흐름의 시작점으로 보기 어려움.\n",
            "```\n",
            "\n",
            "### 4. 수정된 DAG\n",
            "```\n",
            "(SKT홈페이지:접속) -[접속하여]-> (베어유쿠폰:수령)\n",
            "(에이닷T멤버십:접속) -[접속하여]-> (시크릿코드:입력) -[입력하면]-> (식빵쿠폰:수령)\n",
            "```\n",
            "\n",
            "### 5. 최종 검증 및 추출 근거\n",
            "```\n",
            "[최종 DAG가 모든 기준을 충족하는 이유]\n",
            "- Root Node가 명확히 식별됨:\n",
            "  → 'SKT홈페이지:접속'과 '에이닷T멤버십:접속'이 실제 행동 흐름의 시작점으로 설정됨.\n",
            "- 모든 독립적 가치 제안이 포함됨:\n",
            "  → 베어유 강의 14일 무료 수강 쿠폰과 뚜레쥬르 데일리우유식빵 무료 쿠폰 모두 표현됨.\n",
            "- 주요 기능들이 적절히 그룹화됨:\n",
            "  → 각 혜택별로 독립적인 경로로 표현되어 중복 없이 간결함.\n",
            "- 각 경로가 명확한 가치를 전달함:\n",
            "  → 첫 번째 경로는 베어유 쿠폰 수령, 두 번째 경로는 시크릿코드 입력 후 식빵 쿠폰 수령이라는 명확한 가치를 전달함.\n",
            "- 전체 구조가 간결하고 이해하기 쉬움:\n",
            "  → 두 개의 독립적인 경로로 구성되어 있어 복잡하지 않음.\n",
            "- 관계 동사가 우선순위에 맞게 사용됨:\n",
            "  → '접속하여', '입력하면' 등 우선순위에 맞게 사용됨.\n",
            "\n",
            "[노드/엣지 선택의 논리적 근거]\n",
            "- (SKT홈페이지:접속):\n",
            "  → 베어유 쿠폰은 SKT 홈페이지나 이벤트 페이지에서 수령할 수 있으므로 접속이 첫 행동임.\n",
            "  → '만 13~34세' 자격 확인은 접속 후 이루어지는 것으로 간주함.\n",
            "- (에이닷T멤버십:접속):\n",
            "  → 시크릿코드 입력은 에이닷 T 멤버십 쿠폰함 내에서 이루어지므로 접속이 첫 행동임.\n",
            "- (베어유쿠폰:수령):\n",
            "  → SKT 홈페이지 접속 후 베어유 강의 14일 무료 수강 쿠폰을 수령하는 것이 주요 혜택임.\n",
            "- (시크릿코드:입력):\n",
            "  → 접속 후 시크릿코드를 입력하는 행동이 쿠폰 수령의 조건임.\n",
            "- (식빵쿠폰:수령):\n",
            "  → 시크릿코드 입력 후 뚜레쥬르 데일리우유식빵 무료 쿠폰을 수령하는 것이 주요 혜택임.\n",
            "```\n",
            "============================================= Root Nodes =============================================\n",
            "  SKT홈페이지:접속 | {'entity': 'SKT홈페이지', 'action': '접속', 'path': None}\n",
            "  에이닷T멤버십:접속 | {'entity': '에이닷T멤버십', 'action': '접속', 'path': None}\n",
            "============================================= Paths =============================================\n",
            "\n",
            "Path 1:\n",
            "  SKT홈페이지:접속\n",
            "    --[접속하여]-->\n",
            "  베어유쿠폰:수령\n",
            "\n",
            "Path 2:\n",
            "  에이닷T멤버십:접속\n",
            "    --[접속하여]-->\n",
            "  시크릿코드:입력\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from utils import create_dag_diagram, sha256_hash\n",
        "\n",
        "llm_model_nm = 'ax'\n",
        "\n",
        "if llm_model_nm == 'ax':\n",
        "    llm_model = llm_ax\n",
        "elif llm_model_nm == 'gem':\n",
        "    llm_model = llm_gem\n",
        "elif llm_model_nm == 'cld':\n",
        "    llm_model = llm_cld\n",
        "elif llm_model_nm == 'gen':\n",
        "    llm_model = llm_gen\n",
        "elif llm_model_nm == 'gpt':\n",
        "    llm_model = llm_gpt\n",
        "\n",
        "stop_item_names = pd.read_csv(\"./data/stop_words.csv\")['stop_words'].to_list()\n",
        "\n",
        "line_break_patterns = {\"__\":\"\\n\", \"■\":\"\\n■\", \"▶\":\"\\n▶\", \"_\":\"\\n\"}\n",
        "\n",
        "parser = DAGParser()\n",
        "\n",
        "mms_list = [\n",
        "\"\"\"[SK텔레콤] 9월 0 day 혜택 안내\n",
        "(광고)[SKT] 9월 0 day 혜택 안내\n",
        "<9월 20일(금) 혜택>\n",
        "만 13~34세 고객이라면\n",
        "SKT 0 day\n",
        "[배달의민족(페리카나 전용) 8,000원 할인]\n",
        "이게 되네!\n",
        "(16,000원 이상 주문 시 할인 적용)\n",
        "\n",
        " 자세히 보기 :  http://t-mms.kr/t.do?m=#61&s=28335&a=&u=https://bit.ly/3XiX8lJ\n",
        "\n",
        " 에이닷 X T 멤버십 구독캘린더 이벤트\n",
        "T day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요!\n",
        "알림 설정하면 추첨을 통해 [맥도날드 1만원권]을 드립니다.\n",
        "\n",
        " 이벤트 참여하기 : https://bit.ly/3T95wC2\n",
        "\n",
        " 문의: SKT 고객센터(1558, 무료)\n",
        "무료 수신거부 1504\"\"\"\n",
        ",\n",
        "\"\"\"\n",
        "[SK텔레콤] 아이폰 고객님만을 위한 특별 혜택, 네이버페이 5,000원!\n",
        "(광고)[SKT]아이폰 고객님만을 위한 특별 혜택, 네이버페이 5,000원!\n",
        "아이폰을 쓰시는 #04고객님, 안녕하세요?\n",
        "지금 에이닷에 신규 가입 후 AI 전화 서비스를 이용하시면 네이버페이 포인트 5,000원을 100% 드려요! 선착순 1만명 한정이니 지금 바로 참여해 보세요!\n",
        "\n",
        "이벤트 참여하기 : http://t-mms.kr/jP6/#74\n",
        "에이닷 전화 서비스를 이용하시면, 아이폰에서도 AI 통화 녹음/요약 뿐만 아니라 더 강력한 AI 스팸 필터링으로 더 편리하게 통화하실 수 있습니다. 에이닷에서는 모든 AI 서비스가 무료! 지금 바로 경험해 보세요!\n",
        "\n",
        " 이벤트 관련문의: 더브리즈 02-6101-2000 \n",
        "SKT와 함께해 주셔서 감사합니다\n",
        "무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"[SK텔레콤] 제이스대리점 미사강변점 고객 혜택 안내\n",
        "(광고)[SKT] 제이스대리점 미사강변점 고객 혜택 안내  고객님, 안녕하세요. SKT 제이스대리점 미사강변점에서 고객님을 위한 혜택을 준비했습니다.   \n",
        " 갤럭시 S21 구매 고객 혜택(3월 한정)  - 5GX 프라임 요금제 지원금 45만원 100% + 15% 추가 지원금 적용  - 사용하던 기기 반납시 \"2배보상\"  - 제휴 삼성카드 가입 시 추가 4% 할인  - 제휴 롯데카드 가입 시 5% 할인  - 버즈라이브 50% 할인쿠폰 제공   ※ 모든 혜택조건은 중복 적용가능  \n",
        " 초등학생 신학기 이벤트  - 가장 많이 찾는 \"ZEM폰, 미니폰\" 매장 입고되어 즉시개통 가능  - 원격으로 아이 휴대폰 관리부터 실시간 위치 조회 가능  - 기기값 부담은 확 낮추고 결합할인을 통해 더블할인까지~   \n",
        " 제이스대리점 미사강변점 - 주소: 경기 하남시 미사강변대로64 제일아이조움 1층 T월드 - 연락처: 031-795-2423 ※ 건물 내 무료 주차  \n",
        " 매장 홈페이지/예약/상담: http://t-mms.kr/t.do?m=#61&s=556&a=&u=https://tworldfriends.co.kr/D145710102 \n",
        " 매장 위치 보기: http://t-mms.kr/t.do?m=#61&s=557&a=&u=http://kko.to/vtT5qpJDp  \n",
        " 문의: SKT 고객센터(1558, 무료) ※ 코로나19 확산으로 고객센터에 문의가 증가하고 있습니다. 고객센터와 전화 연결이 원활하지 않을 수 있으니 양해 바랍니다.  SKT와 함께해주셔서 감사합니다. 무료 수신거부 1504\"\"\"\n",
        " ,\n",
        " \"\"\"수도권 부스트 파크 이벤트를 안내드립니다. \n",
        "(광고)[SKT] 부스트 파크 추첨 이벤트 안내  고객님, 안녕하세요. 수도권 부스트 파크에서 진행되는 특별한 이벤트를 안내해드립니다!  \n",
        " 이벤트1. 갤럭시 노트20 사전예약 이벤트  - 기간: 2020년 8월 7일(금)~8월 13일(목) - 장소: 부스트 파크 강남역/가로수길, 잠실, 광화문, 대학로, 인천 구월, 안양 범계, 일산 라페/웨돔, 수원 광교에 있는 T월드 매장 - 대상: 부스트 파크 T월드 매장에서 5G 체험한 뒤 갤럭시 노트20 사전예약하시는 고객님 - 사은품: 부스트 파크 상생 제휴처 사은품 증정 ※ 사은품은 매장마다 다를 수 있으며, 일찍 소진될 수 있습니다.  \n",
        " 부스트 파크 위치 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/3iduqLN  \n",
        " 이벤트2. 갤럭시 노트20 개통 고객 대상 추첨 이벤트  - 기간: 2020년 8월 14일(금)~8월 20일(목) - 대상: 이벤트1에 참여하고 갤럭시 노트20 개통하신 고객님 중 25명 추첨 - 사은품: 인사동 나인트리 프리미어 호텔 1박2일 숙박권 \n",
        " 이벤트 세부 내용 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/2XS94fl  ※ 갤럭시 노트20 제조사 예약/개통 프로모션과 별개이며, 중복 혜택을 받으실 수 있습니다.  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\"\"\"\n",
        ",\n",
        "\"\"\"[SK텔레콤] 엄마손대리점 본점 폴더블6 출시 이벤트 안내드립니다\n",
        "(광고)[SKT] 엄마손대리점 본점 폴더블6 출시 이벤트\n",
        "고객님 안녕하세요\n",
        "SK텔레콤 공식인증대리점 엄마손대리점 본점입니다\n",
        "\n",
        "■ 폴더블6 \n",
        "전작 대비 더욱 강력해진 성능으로 19일 첫 출시!\n",
        "지금 바로 예약(7월 18일 공식예약 마감)하시고 예약고객만의 다양한 혜택을 누려보시기 바랍니다\n",
        "\n",
        "■ 신형 휴대폰 외 \n",
        "효도폰, 키즈폰 등 다양한 할인 이벤트도 진행중입니다\n",
        "\n",
        "■ 추가할인, 사은품 등 다양한 혜택과\n",
        "요금할인, 숨어있는 포인트 활용 등 꿀Tip상담을 약속드립니다\n",
        "\n",
        "■ SK공식인증대리점 엄마손대리점 본점\n",
        "- 주소 : 서울 송파구 가락로 110 \n",
        "- 연락처 : 02-420-9011\n",
        "- 구술약도 : 송파역 석촌시장에서 대로변에 위치\n",
        "\n",
        "▶ 홈페이지 : http://t-mms.kr/t.do?m=#61&s=27256&a=&u=https://naver.me/FyeuZoSp\n",
        "\n",
        "■ 문의 : SKT 고객센터(1558, 무료)\n",
        "SK텔레콤과 함께해 주셔서 감사합니다.\n",
        "무료 수신거부 1504\"\"\",\n",
        "\n",
        "\"\"\"[SK텔레콤] 동진대리점 가양역 본점 이벤트 안내\n",
        "(광고)[SKT] 동진대리점 가양역 본점 이벤트 안내\n",
        "안녕하세요 고객님\n",
        "SK텔레콤 가양역 본점에서 검은 토끼의 해 계묘년 특별행사 안내드립니다. \n",
        "\n",
        "■ 이 달의 특가 휴대폰!\n",
        "- 준프리미엄 모델 갤럭시 퀀텀3 최대지원! + 갤럭시 워치까지 무료!(5GX프라임 요금제 이용 조건)\n",
        "- 자녀분들을 위한 ZEM포켓몬 에디션 이벤트!\n",
        "\n",
        "■ 초고속 인터넷 + TV 신규가입 또는 약정 만료고객 재약정시 사은품 최대 지급\n",
        "\n",
        "■ ADT 캡스 홈보안 6개월 무료 체험 (체험 만료시 위약금 면제)\n",
        "\n",
        "■ 새롭게 출시되는 갤럭시 S23 사전예약 시작\n",
        "- 2월13일까지 사전예약하면 2월14일에 바로 받아보실 수 있습니다!\n",
        "매장으로 방문하시면 자세히 설명드리겠습니다.\n",
        "\n",
        "■ 동진대리점 가양역본점 \n",
        "- 위치: 서울 강서구 화곡로68길 3 영스퀘어 1층 SK텔레콤\n",
        "- 연락처: 02-2659-7577\n",
        "\n",
        "▶카톡:http://pf.kakao.com/\n",
        "xjwmxaxj\n",
        "\n",
        "▶바로가기 http://t-mms.kr/t.do?m=#61&s=17996&a=&u=http://t-mms.kr/t.do?m=945711&s=17634&a=&u=http://tworldfriends.co.kr/D151510021\n",
        "SKT와 함께해주셔서 감사합니다. \n",
        "무료 수신거부 1504\"\"\",\n",
        "\n",
        "\"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "]\n",
        "\n",
        "# for msg in random.sample(mms_pdf.query(\"msg.str.contains('')\")['msg'].unique().tolist(), 1):\n",
        "for msg in mms_list[:]:\n",
        "\n",
        "    for pattern, replacement in line_break_patterns.items():\n",
        "        msg = msg.replace(pattern, replacement)\n",
        "\n",
        "    cand_entities = sorted([e.strip() for e in extract_entities_by_logic([msg], threshold_for_fuzzy=0.7)['item_nm_alias'].unique() if e.strip() not in stop_item_names and len(e.strip())>=2])\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "## 작업 목표\n",
        "통신사 광고 메시지에서 **핵심 행동 흐름**을 추출하여 간결한 DAG(Directed Acyclic Graph) 형식으로 표현\n",
        "\n",
        "## 핵심 원칙\n",
        "1. **최소 경로 원칙**: 동일한 결과를 얻는 가장 짧은 경로만 표현\n",
        "2. **핵심 흐름 우선**: 부가적 설명보다 주요 행동 연쇄에 집중\n",
        "3. **중복 제거**: 의미가 겹치는 노드는 하나로 통합\n",
        "\n",
        "## 출력 형식\n",
        "```\n",
        "(개체명:기대행동) -[관계동사]-> (개체명:기대행동)\n",
        "또는\n",
        "(개체명:기대행동)\n",
        "```\n",
        "\n",
        "## 개체명 카테고리\n",
        "### 필수 추출 대상\n",
        "- **제품/서비스**: 구체적 제품명, 서비스명, 요금제명\n",
        "- **핵심 혜택**: 금전적 혜택, 사은품, 할인\n",
        "- **행동 장소**: 온/오프라인 채널 (필요 시만)\n",
        "\n",
        "### 추출 제외 대상\n",
        "- 광고 대상자 (예: \"아이폰 고객님\")\n",
        "- 일정/기간 정보\n",
        "- 부가 설명 기능 (핵심 흐름과 무관한 경우)\n",
        "- 법적 고지사항\n",
        "\n",
        "## 기대 행동 (10개 표준 동사)\n",
        "`구매, 가입, 사용, 방문, 참여, 등록, 다운로드, 확인, 수령, 적립`\n",
        "\n",
        "## 관계 동사 우선순위\n",
        "### 1순위: 조건부 관계\n",
        "- `가입하면`, `구매하면`, `사용하면`\n",
        "- `가입후`, `구매후`, `사용후`\n",
        "\n",
        "### 2순위: 결과 관계\n",
        "- `받다`, `수령하다`, `적립하다`\n",
        "\n",
        "### 3순위: 경로 관계 (필요 시만)\n",
        "- `통해`, `이용하여`\n",
        "\n",
        "## DAG 구성 전략\n",
        "### Step 1: 모든 가치 제안 식별\n",
        "광고에서 제시하는 **모든 독립적 가치**를 파악\n",
        "- **즉각적 혜택**: 금전적 보상, 사은품 등\n",
        "- **서비스 가치**: 제품/서비스 자체의 기능과 혜택\n",
        "예: \"네이버페이 5000원\" + \"AI 통화 기능 무료 이용\"\n",
        "\n",
        "### Step 2: 독립 경로 구성\n",
        "각 가치 제안별로 별도 경로 생성:\n",
        "1. **혜택 획득 경로**: 가입 → 사용 → 보상\n",
        "2. **서비스 체험 경로**: 가입 → 경험 → 기능 활용\n",
        "\n",
        "### Step 3: 세부 기능 표현\n",
        "주요 기능들이 명시된 경우 분기 구조로 표현:\n",
        "- 통합 가능한 기능은 하나로 (예: AI통화녹음/요약 → \"AI통화기능\")\n",
        "- 독립적 기능은 별도로 (예: AI스팸필터링)\n",
        "\n",
        "## 분석 프로세스 (필수 단계)\n",
        "### Step 1: 메시지 이해\n",
        "- 전체 메시지를 한 문단으로 요약\n",
        "- 광고주의 의도 파악\n",
        "- **암시된 행동 식별**: 명시되지 않았지만 필수적인 행동 (예: 매장 방문)\n",
        "\n",
        "### Step 2: 가치 제안 식별\n",
        "- 즉각적 혜택 (금전, 사은품 등)\n",
        "- 서비스 가치 (기능, 편의성 등)\n",
        "- 부가 혜택 (있다면)\n",
        "\n",
        "### Step 3: Root Node 결정\n",
        "- **사용자의 첫 번째 행동은 무엇인가?**\n",
        "- 매장 주소/연락처가 있다면 → 방문이 시작점\n",
        "- 온라인 링크가 있다면 → 접속이 시작점\n",
        "- 앱 관련 내용이라면 → 다운로드가 시작점\n",
        "\n",
        "### Step 4: 관계 분석\n",
        "- Root Node부터 시작하는 전체 흐름\n",
        "- 각 행동 간 인과관계 검증\n",
        "- 조건부 관계 명확화\n",
        "- 시간적 순서 확인\n",
        "\n",
        "### Step 5: DAG 구성\n",
        "- 위 분석을 바탕으로 노드와 엣지 결정\n",
        "- 중복 제거 및 통합\n",
        "\n",
        "### Step 6: 자기 검증 및 수정\n",
        "- **평가**: 초기 DAG를 아래 기준에 따라 검토\n",
        "  - Root Node가 명확히 식별되었는가? (방문/접속/다운로드 등)\n",
        "  - 모든 독립적 가치 제안이 포함되었는가? (즉각적 혜택과 서비스 가치)\n",
        "  - 주요 기능들이 적절히 그룹화되었는가? (중복 제거 여부)\n",
        "  - 각 경로가 명확한 가치를 전달하는가?\n",
        "  - 전체 구조가 간결하고 이해하기 쉬운가?\n",
        "  - 관계 동사가 우선순위에 맞게 사용되었는가? (조건부 > 결과 > 경로)\n",
        "- **문제 식별**: 위 기준 중 충족되지 않은 항목을 명시하고, 그 이유를 설명\n",
        "- **수정**: 식별된 문제를 해결한 수정된 DAG를 생성\n",
        "\n",
        "### Step 7: 최종 검증\n",
        "- 수정된 DAG가 모든 기준을 충족하는지 재확인\n",
        "- 만약 문제가 남아있다면, 추가 수정 수행 (최대 2회 반복)\n",
        "- 최종적으로 모든 기준이 충족되었음을 확인\n",
        "\n",
        "## 출력 형식 (반드시 모든 섹션 포함)\n",
        "### 1. 메시지 분석\n",
        "```\n",
        "[메시지 요약 및 핵심 의도]\n",
        "[식별된 가치 제안 목록]\n",
        "```\n",
        "\n",
        "### 2. 초기 DAG\n",
        "```\n",
        "[초기 DAG 구조]\n",
        "```\n",
        "\n",
        "### 3. 자기 검증 결과\n",
        "```\n",
        "[평가 기준별 검토 결과]\n",
        "[식별된 문제점 및 이유]\n",
        "```\n",
        "\n",
        "### 4. 수정된 DAG\n",
        "```\n",
        "[수정된 DAG 구조]\n",
        "```\n",
        "\n",
        "### 5. 최종 검증 및 추출 근거\n",
        "```\n",
        "[최종 DAG가 모든 기준을 충족하는 이유]\n",
        "[노드/엣지 선택의 논리적 근거]\n",
        "```\n",
        "\n",
        "## 실행 지침\n",
        "1. 위 7단계 분석 프로세스를 **순서대로** 수행\n",
        "2. 각 단계에서 발견한 내용을 **명시적으로** 기록\n",
        "3. DAG 구성 전 **충분한 분석** 수행\n",
        "4. 초기 DAG 생성 후 **반드시 자기 검증 및 수정** 수행\n",
        "5. 최종 출력에 **모든 섹션** 포함\n",
        "6. **중요**: 분석 과정을 생략하지 말고, 사고 과정과 수정 이유를 투명하게 보여주세요\n",
        "\n",
        "## 예시 분석\n",
        "### 잘못된 예시 (핵심 흐름만 추출)\n",
        "```\n",
        "(에이닷:가입) -[가입후]-> (AI전화서비스:사용)\n",
        "(AI전화서비스:사용) -[사용하면]-> (네이버페이5000원:수령)\n",
        "```\n",
        "→ 문제: 서비스 자체의 가치(AI 기능들)가 누락됨\n",
        "\n",
        "### 올바른 예시 (완전한 가치 표현 - Root Node 포함)\n",
        "```\n",
        "# 매장 방문부터 시작하는 경로\n",
        "(제이스대리점:방문) -[방문하여]-> (갤럭시S21:구매)\n",
        "(갤럭시S21:구매) -[구매시]-> (5GX프라임요금제:가입)\n",
        "(5GX프라임요금제:가입) -[가입하면]-> (지원금45만원+15%:수령)\n",
        "\n",
        "# 온라인 시작 경로 예시\n",
        "(T다이렉트샵:접속) -[접속하여]-> (갤럭시S24:구매)\n",
        "(갤럭시S24:구매) -[구매시]-> (사은품:수령)\n",
        "```\n",
        "→ 장점: 사용자의 첫 행동(Root Node)부터 명확히 표현\n",
        "\n",
        "## message:\n",
        "{msg}\n",
        "\"\"\"\n",
        "    \n",
        "    print(\"===\"*15+\" Message \"+\"===\"*15)\n",
        "    print(msg)\n",
        "    # print(\"===\"*15+\" DAG (1) \"+\"===\"*15)\n",
        "    # dag = llm_cld.invoke(prompt_1).content\n",
        "    # print(dag)\n",
        "\n",
        "    print(\"===\"*15+f\" DAG ({llm_model_nm.upper()}) \"+\"===\"*15)\n",
        "    dag_raw = llm_model.invoke(prompt).content\n",
        "    print(dag_raw)\n",
        "\n",
        "    # NetworkX 그래프로 활용\n",
        "    dag_section = parser.extract_dag_section(dag_raw)\n",
        "    dag = parser.parse_dag(dag_section)\n",
        "\n",
        "    # 기존 방식 주석 처리\n",
        "    # nodes, edges = parse_block(re.sub(r'^```|```$', '', dag_raw.strip()))\n",
        "    # dag = build_dag(nodes, edges)\n",
        "\n",
        "    print(\"===\"*15+\" Root Nodes \"+\"===\"*15)\n",
        "    root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]\n",
        "    for root in root_nodes:\n",
        "        node_data = dag.nodes[root]\n",
        "        print(f\"  {root} | {node_data}\")\n",
        "\n",
        "    print(\"===\"*15+\" Paths \"+\"===\"*15)\n",
        "    paths, roots, leaves = get_root_to_leaf_paths(dag)\n",
        "\n",
        "    for i, path in enumerate(paths):\n",
        "        print(f\"\\nPath {i+1}:\")\n",
        "        for j, node in enumerate(path):\n",
        "            if j < len(path) - 1:\n",
        "                edge_data = dag.get_edge_data(node, path[j+1])\n",
        "                relation = edge_data['relation'] if edge_data else ''\n",
        "                print(f\"  {node}\")\n",
        "                print(f\"    --[{relation}]-->\")\n",
        "            else:\n",
        "                print(f\"  {node}\")\n",
        "\n",
        "    create_dag_diagram(dag, filename=f'dag_{sha256_hash(msg)}')             \n",
        "    print()\n",
        "\n",
        "    \n",
        "\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('SKT홈페이지:접속', '베어유쿠폰:수령', {'relation': '접속하여', 'path': None})\n",
            "('에이닷T멤버십:접속', '시크릿코드:입력', {'relation': '접속하여', 'path': None})\n"
          ]
        }
      ],
      "source": [
        "for edge in dag.edges(data=True):\n",
        "    print(edge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
