{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "from pygments import highlight\n",
        "from pygments.lexers import JsonLexer\n",
        "from pygments.formatters import HtmlFormatter\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\"  #우리꺼\n",
        "# llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님\n",
        "# llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "\n",
        "def ChatAnthropicSKT(model=\"skt/claude-3-7-sonnet-20250219\", max_tokens=4000):\n",
        "    llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\" #우리꺼 # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님  # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "    \n",
        "    llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "    \n",
        "    # llm_api_url = \"https://43.203.77.11:443/v1\"\n",
        "\n",
        "    # model = \"anthropic/claude-3-5-sonnet-20240620\"\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        openai_api_key=llm_api_key, \n",
        "        openai_api_base=llm_api_url, \n",
        "        model=model,\n",
        "        max_tokens=max_tokens\n",
        "        )\n",
        "    return model\n",
        "\n",
        "llm_cld37 = ChatAnthropicSKT()\n",
        "llm_gem3 = ChatAnthropicSKT(model='skt/gemma3-12b-it-dev')\n",
        "# llm_ax = ChatAnthropicSKT(model='skt/a.x-3-lg')\n",
        "\n",
        "\n",
        "# llm_cld37 = ChatAnthropic(\n",
        "#     api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "#     model=\"claude-3-7-sonnet-20250219\",\n",
        "#     max_tokens=3000\n",
        "# )\n",
        "\n",
        "llm_chat = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        model=\"gpt-4.1\",\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        max_tokens=2000,\n",
        ")\n",
        "\n",
        "llm_cld40 = ChatAnthropic(\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=3000\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \n",
        "    # json_str = broken_json.replace(\"'\",'\"')\n",
        "    \n",
        "    # Fix unquoted values (like NI00001863)\n",
        "    json_str = re.sub(r':\\s*([a-zA-Z0-9_]+)(\\s*[,}])', r': \"\\1\"\\2', broken_json)\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/a.x-3-lg\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n",
        "\n",
        "def convert_to_custom_format(json_items):\n",
        "    custom_format = []\n",
        "    \n",
        "    for item in json_items:\n",
        "        item_name = item.get(\"item_name_in_message\", \"\")\n",
        "        item_id = item.get(\"item_id\", \"\")\n",
        "        category = item.get(\"category\", \"\")\n",
        "        \n",
        "        # Create custom format for each item\n",
        "        custom_line = f\"[Item Name] {item_name} [Item ID] {item_id} [Item Category] {category}\"\n",
        "        custom_format.append(custom_line)\n",
        "    \n",
        "    return \"\\n\".join(custom_format)\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "    # Replace URLs with an empty string\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_custom_pattern(text, keyword=\"바로가기\"):\n",
        "    # Create a pattern that matches any text followed by the specified keyword\n",
        "    # We escape the keyword to handle any special regex characters it might contain\n",
        "    escaped_keyword = re.escape(keyword)\n",
        "    pattern = re.compile(r'.*? ' + escaped_keyword)\n",
        "    \n",
        "    # Replace the matched pattern with an empty string\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "def select_most_comprehensive(strings):\n",
        "    \"\"\"\n",
        "    Select the most comprehensive string from a list of overlapping strings.\n",
        "    Returns the longest string that contains other strings as substrings.\n",
        "    \n",
        "    Args:\n",
        "        strings: List of strings to filter\n",
        "        \n",
        "    Returns:\n",
        "        List of most comprehensive strings (usually one, but could be multiple if no containment)\n",
        "    \"\"\"\n",
        "    if not strings:\n",
        "        return []\n",
        "    \n",
        "    # Remove duplicates and sort by length (longest first)\n",
        "    unique_strings = list(set(strings))\n",
        "    unique_strings.sort(key=len, reverse=True)\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for current in unique_strings:\n",
        "        # Check if current string contains any of the strings already in result\n",
        "        is_contained = any(current in existing for existing in result)\n",
        "        \n",
        "        # Check if current string contains other strings not yet in result\n",
        "        contains_others = any(other in current for other in unique_strings if other != current and other not in result)\n",
        "        \n",
        "        # If current is not contained by existing results and either:\n",
        "        # 1. It contains other strings, or \n",
        "        # 2. No strings contain each other (keep all unique)\n",
        "        if not is_contained:\n",
        "            # Remove any strings from result that are contained in current\n",
        "            result = [r for r in result if r not in current]\n",
        "            result.append(current)\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "from typing import List, Dict, Set, Tuple, Any\n",
        "from collections import defaultdict\n",
        "\n",
        "class KoreanNERSpacy:\n",
        "    def __init__(self, entity_vocab: List[Tuple[str, Dict[str, Any]]]):\n",
        "        \"\"\"\n",
        "        Initialize with new entity vocabulary schema\n",
        "        entity_vocab: List of (alias, metadata) tuples where metadata contains:\n",
        "        - item_nm: canonical name\n",
        "        - item_id: unique identifier  \n",
        "        - description: entity description\n",
        "        - domain: entity domain/category\n",
        "        - item_nm_alias: display alias\n",
        "        \"\"\"\n",
        "        self.entity_vocab = entity_vocab\n",
        "        self.nlp = spacy.load(\"ko_core_news_sm\")\n",
        "        \n",
        "        # Build efficient lookup structures\n",
        "        self.alias_to_metadata = {}\n",
        "        self.canonical_to_metadata = {}\n",
        "        self.aliases_by_canonical = defaultdict(list)\n",
        "        self.entities_by_domain = defaultdict(list)\n",
        "        self.entities_by_length = defaultdict(list)\n",
        "        self.all_aliases = set()\n",
        "        \n",
        "        self._build_lookup_structures()\n",
        "        \n",
        "        # Pre-compute normalized variants for fuzzy matching\n",
        "        self.normalized_mappings = self._create_normalized_mappings()\n",
        "\n",
        "    def _build_lookup_structures(self):\n",
        "        \"\"\"Build efficient lookup structures from the entity vocab\"\"\"\n",
        "        for alias, metadata in self.entity_vocab:\n",
        "            # Skip invalid entries (like '#')\n",
        "            if not alias or alias == '#':\n",
        "                continue\n",
        "                \n",
        "            canonical_name = metadata['item_nm']\n",
        "            domain = metadata['domain']\n",
        "            \n",
        "            # Store mappings\n",
        "            self.alias_to_metadata[alias] = metadata\n",
        "            self.canonical_to_metadata[canonical_name] = metadata\n",
        "            self.aliases_by_canonical[canonical_name].append(alias)\n",
        "            self.entities_by_domain[domain].append(alias)\n",
        "            self.entities_by_length[len(alias)].append(alias)\n",
        "            self.all_aliases.add(alias)\n",
        "        \n",
        "        # Sort lengths for efficient processing (longest first)\n",
        "        self.sorted_lengths = sorted(self.entities_by_length.keys(), reverse=True)\n",
        "\n",
        "    def _create_normalized_mappings(self) -> Dict[str, str]:\n",
        "        \"\"\"Create normalized forms for fuzzy matching\"\"\"\n",
        "        mappings = {}\n",
        "        for alias in self.all_aliases:\n",
        "            # Multiple normalization strategies for Korean\n",
        "            normalized_forms = [\n",
        "                re.sub(r'\\s+', '', alias.lower()),  # Remove spaces\n",
        "                re.sub(r'[^\\w가-힣]', '', alias.lower()),  # Remove special chars\n",
        "                alias.lower().replace('ipad', '아이패드').replace('iphone', '아이폰'),  # EN->KR\n",
        "                alias.lower().replace('아이패드', 'ipad').replace('아이폰', 'iphone'),  # KR->EN\n",
        "                re.sub(r'프로|PRO', 'pro', alias.lower(), flags=re.IGNORECASE),  # Normalize \"pro\"\n",
        "            ]\n",
        "            \n",
        "            for normalized in normalized_forms:\n",
        "                if normalized and normalized != alias.lower():\n",
        "                    mappings[normalized] = alias\n",
        "                    \n",
        "        return mappings\n",
        "\n",
        "    def extract_entities(self, text: str, method: str = \"optimized\") -> List[Dict]:\n",
        "        \"\"\"Main entity extraction method\"\"\"\n",
        "        if method == \"optimized\":\n",
        "            return self._extract_optimized(text)\n",
        "        elif method == \"fuzzy\":\n",
        "            return self._extract_fuzzy(text)\n",
        "        elif method == \"hybrid\":\n",
        "            return self._extract_hybrid(text)\n",
        "        else:\n",
        "            return self._extract_exact(text)\n",
        "\n",
        "    def _extract_exact(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Enhanced exact matching with comprehensive case handling\"\"\"\n",
        "        results = []\n",
        "        processed_positions = set()\n",
        "        \n",
        "        # Step 1: Exact case-sensitive matching (highest priority)\n",
        "        for length in self.sorted_lengths:\n",
        "            if length > len(text):\n",
        "                continue\n",
        "                \n",
        "            aliases = self.entities_by_length[length]\n",
        "            for alias in aliases:\n",
        "                start = 0\n",
        "                while True:\n",
        "                    pos = text.find(alias, start)  # Case-sensitive\n",
        "                    if pos == -1:\n",
        "                        break\n",
        "                    \n",
        "                    if any(p in processed_positions for p in range(pos, pos + length)):\n",
        "                        start = pos + 1\n",
        "                        continue\n",
        "                    \n",
        "                    metadata = self.alias_to_metadata[alias]\n",
        "                    results.append({\n",
        "                        \"text\": text[pos:pos + length],\n",
        "                        \"alias\": alias,\n",
        "                        \"canonical_name\": metadata['item_nm'],\n",
        "                        \"item_id\": metadata['item_id'],\n",
        "                        \"domain\": metadata['domain'],\n",
        "                        \"description\": metadata['description'],\n",
        "                        \"start\": pos,\n",
        "                        \"end\": pos + length,\n",
        "                        \"confidence\": 1.0,\n",
        "                        \"method\": \"exact_case_match\",\n",
        "                        \"case_match\": \"exact\"\n",
        "                    })\n",
        "                    \n",
        "                    for p in range(pos, pos + length):\n",
        "                        processed_positions.add(p)\n",
        "                    start = pos + 1\n",
        "        \n",
        "        # Step 2: Case-insensitive matching for remaining text\n",
        "        text_lower = text.lower()\n",
        "        for length in self.sorted_lengths:\n",
        "            if length > len(text):\n",
        "                continue\n",
        "                \n",
        "            aliases = self.entities_by_length[length]\n",
        "            for alias in aliases:\n",
        "                alias_lower = alias.lower()\n",
        "                start = 0\n",
        "                \n",
        "                while True:\n",
        "                    pos = text_lower.find(alias_lower, start)\n",
        "                    if pos == -1:\n",
        "                        break\n",
        "                    \n",
        "                    if any(p in processed_positions for p in range(pos, pos + length)):\n",
        "                        start = pos + 1\n",
        "                        continue\n",
        "                    \n",
        "                    # Check if this is actually a case variation (not exact match)\n",
        "                    original_text = text[pos:pos + length]\n",
        "                    if original_text != alias:  # Only add if case differs\n",
        "                        metadata = self.alias_to_metadata[alias]\n",
        "                        case_type = self._analyze_case_type(original_text, alias)\n",
        "                        \n",
        "                        results.append({\n",
        "                            \"text\": original_text,\n",
        "                            \"alias\": alias,\n",
        "                            \"canonical_name\": metadata['item_nm'],\n",
        "                            \"item_id\": metadata['item_id'],\n",
        "                            \"domain\": metadata['domain'],\n",
        "                            \"description\": metadata['description'],\n",
        "                            \"start\": pos,\n",
        "                            \"end\": pos + length,\n",
        "                            \"confidence\": 0.9,\n",
        "                            \"method\": \"case_insensitive_match\",\n",
        "                            \"case_match\": case_type\n",
        "                        })\n",
        "                        \n",
        "                        for p in range(pos, pos + length):\n",
        "                            processed_positions.add(p)\n",
        "                    \n",
        "                    start = pos + 1\n",
        "        \n",
        "        return sorted(results, key=lambda x: x['start'])\n",
        "    \n",
        "    def _analyze_case_type(self, found_text: str, original_alias: str) -> str:\n",
        "        \"\"\"Analyze the type of case difference\"\"\"\n",
        "        if found_text.isupper():\n",
        "            return \"all_upper\"\n",
        "        elif found_text.islower():\n",
        "            return \"all_lower\"\n",
        "        elif found_text.istitle():\n",
        "            return \"title_case\"\n",
        "        else:\n",
        "            return \"mixed_case\"\n",
        "\n",
        "    def _extract_fuzzy(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Fuzzy matching for Korean variations with deduplication\"\"\"\n",
        "        results = []\n",
        "        \n",
        "        # First do exact matching\n",
        "        exact_results = self._extract_exact(text)\n",
        "        results.extend(exact_results)\n",
        "        \n",
        "        # Get processed positions from exact matches\n",
        "        processed_positions = set()\n",
        "        found_entities = set()  # Track unique entities\n",
        "        \n",
        "        for result in exact_results:\n",
        "            for pos in range(result['start'], result['end']):\n",
        "                processed_positions.add(pos)\n",
        "            # Add to found entities set\n",
        "            entity_key = (result['start'], result['end'], result['canonical_name'])\n",
        "            found_entities.add(entity_key)\n",
        "        \n",
        "        # Try normalized matching for unprocessed parts\n",
        "        doc = self.nlp(text)\n",
        "        \n",
        "        for token in doc:\n",
        "            # Skip if this token overlaps with exact matches\n",
        "            if any(pos in processed_positions for pos in range(token.idx, token.idx + len(token.text))):\n",
        "                continue\n",
        "\n",
        "            # Try various normalizations\n",
        "            token_text = token.text\n",
        "            normalized_forms = [\n",
        "                re.sub(r'\\s+', '', token_text.lower()),\n",
        "                re.sub(r'[^\\w가-힣]', '', token_text.lower()),\n",
        "                token_text.lower(),\n",
        "            ]\n",
        "            \n",
        "            for normalized in normalized_forms:\n",
        "                if normalized in self.normalized_mappings:\n",
        "                    original_alias = self.normalized_mappings[normalized]\n",
        "                    metadata = self.alias_to_metadata[original_alias]\n",
        "                    \n",
        "                    # Create unique entity key\n",
        "                    entity_key = (token.idx, token.idx + len(token.text), metadata['item_nm'])\n",
        "                    \n",
        "                    # Skip if we already found this entity at this position\n",
        "                    if entity_key in found_entities:\n",
        "                        continue\n",
        "                    \n",
        "                    results.append({\n",
        "                        \"text\": token_text,\n",
        "                        \"alias\": original_alias,\n",
        "                        \"canonical_name\": metadata['item_nm'],\n",
        "                        \"item_id\": metadata['item_id'],\n",
        "                        \"domain\": metadata['domain'],\n",
        "                        \"description\": metadata['description'],\n",
        "                        \"start\": token.idx,\n",
        "                        \"end\": token.idx + len(token.text),\n",
        "                        \"confidence\": 0.85,\n",
        "                        \"method\": \"fuzzy_match\"\n",
        "                    })\n",
        "                    \n",
        "                    found_entities.add(entity_key)\n",
        "                    break  # Stop after first match for this token\n",
        "        \n",
        "        return self._deduplicate_method_results(results)\n",
        "\n",
        "    def _extract_optimized(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Optimized extraction combining exact + smart fuzzy matching with deduplication\"\"\"\n",
        "        results = []\n",
        "        found_entities = set()  # Global entity tracking\n",
        "        \n",
        "        # Step 1: Exact matching (highest priority)\n",
        "        exact_results = self._extract_exact(text)\n",
        "        results.extend(exact_results)\n",
        "        \n",
        "        # Track found entities\n",
        "        for result in exact_results:\n",
        "            entity_key = (result['start'], result['end'], result['canonical_name'])\n",
        "            found_entities.add(entity_key)\n",
        "        \n",
        "        # Step 2: Handle Korean-specific patterns in remaining text\n",
        "        processed_positions = set()\n",
        "        for result in exact_results:\n",
        "            for pos in range(result['start'], result['end']):\n",
        "                processed_positions.add(pos)\n",
        "        \n",
        "        # Step 3: Multi-token matching for compound device names\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token for token in doc if not token.is_space]\n",
        "        \n",
        "        # Try 2-4 token combinations\n",
        "        for i in range(len(tokens)):\n",
        "            for j in range(i + 2, min(i + 5, len(tokens) + 1)):\n",
        "                # Skip if any token in range is already processed\n",
        "                token_range = tokens[i:j]\n",
        "                if any(any(pos in processed_positions \n",
        "                          for pos in range(token.idx, token.idx + len(token.text))) \n",
        "                       for token in token_range):\n",
        "                    continue\n",
        "                \n",
        "                start_pos = tokens[i].idx\n",
        "                end_pos = tokens[j-1].idx + len(tokens[j-1].text)\n",
        "                \n",
        "                # Create entity key for this position range\n",
        "                entity_key_base = (start_pos, end_pos)\n",
        "                \n",
        "                # Create candidate strings\n",
        "                candidates = [\n",
        "                    \"\".join([t.text for t in token_range]),  # No spaces\n",
        "                    \" \".join([t.text for t in token_range]), # With spaces\n",
        "                ]\n",
        "                \n",
        "                found_match = False\n",
        "                for candidate in candidates:\n",
        "                    if candidate.lower() in [alias.lower() for alias in self.all_aliases]:\n",
        "                        # Find the original alias\n",
        "                        original_alias = next(alias for alias in self.all_aliases \n",
        "                                            if alias.lower() == candidate.lower())\n",
        "                        metadata = self.alias_to_metadata[original_alias]\n",
        "                        \n",
        "                        # Create unique entity key\n",
        "                        entity_key = (start_pos, end_pos, metadata['item_nm'])\n",
        "                        \n",
        "                        # Skip if we already found this entity at this position\n",
        "                        if entity_key in found_entities:\n",
        "                            continue\n",
        "                        \n",
        "                        results.append({\n",
        "                            \"text\": text[start_pos:end_pos],\n",
        "                            \"alias\": original_alias,\n",
        "                            \"canonical_name\": metadata['item_nm'],\n",
        "                            \"item_id\": metadata['item_id'],\n",
        "                            \"domain\": metadata['domain'],\n",
        "                            \"description\": metadata['description'],\n",
        "                            \"start\": start_pos,\n",
        "                            \"end\": end_pos,\n",
        "                            \"confidence\": 0.9,\n",
        "                            \"method\": \"multi_token_match\"\n",
        "                        })\n",
        "                        \n",
        "                        # Mark as processed and found\n",
        "                        for token in token_range:\n",
        "                            for pos in range(token.idx, token.idx + len(token.text)):\n",
        "                                processed_positions.add(pos)\n",
        "                        found_entities.add(entity_key)\n",
        "                        found_match = True\n",
        "                        break\n",
        "                \n",
        "                if found_match:\n",
        "                    break  # Don't try longer combinations if we found a match\n",
        "        \n",
        "        return self._deduplicate_method_results(results)\n",
        "\n",
        "    def _extract_hybrid(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Hybrid approach combining all methods with comprehensive deduplication\"\"\"\n",
        "        all_results = []\n",
        "        global_found_entities = set()  # Track entities across all methods\n",
        "        \n",
        "        # Method 1: Optimized extraction\n",
        "        optimized_results = self._extract_optimized(text)\n",
        "        all_results.extend(optimized_results)\n",
        "        \n",
        "        # Track entities from optimized method\n",
        "        for result in optimized_results:\n",
        "            entity_key = (result['start'], result['end'], result['canonical_name'])\n",
        "            global_found_entities.add(entity_key)\n",
        "        \n",
        "        # Method 2: Additional fuzzy results for uncovered areas\n",
        "        fuzzy_results = self._extract_fuzzy(text)\n",
        "        \n",
        "        # Only add fuzzy results that don't duplicate existing entities\n",
        "        for result in fuzzy_results:\n",
        "            entity_key = (result['start'], result['end'], result['canonical_name'])\n",
        "            if entity_key not in global_found_entities:\n",
        "                all_results.append(result)\n",
        "                global_found_entities.add(entity_key)\n",
        "        \n",
        "        return self._deduplicate_method_results(all_results)\n",
        "\n",
        "    def _deduplicate_method_results(self, results: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Enhanced deduplication for method-level results\"\"\"\n",
        "        if not results:\n",
        "            return results\n",
        "        \n",
        "        # Step 1: Remove exact duplicates by creating unique keys\n",
        "        seen_entities = {}  # key -> best_result\n",
        "        \n",
        "        for result in results:\n",
        "            # Create comprehensive unique key\n",
        "            unique_key = (\n",
        "                result['start'],\n",
        "                result['end'], \n",
        "                result['canonical_name'],\n",
        "                result['item_id']\n",
        "            )\n",
        "            \n",
        "            # Keep the result with highest confidence, or first one if tied\n",
        "            if (unique_key not in seen_entities or \n",
        "                result['confidence'] > seen_entities[unique_key]['confidence']):\n",
        "                seen_entities[unique_key] = result\n",
        "        \n",
        "        # Step 2: Handle overlapping entities (keep highest confidence)\n",
        "        deduplicated_results = list(seen_entities.values())\n",
        "        return self._remove_overlaps(deduplicated_results)\n",
        "\n",
        "    def _remove_overlaps(self, results: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Enhanced overlap removal with better prioritization\"\"\"\n",
        "        if not results:\n",
        "            return results\n",
        "        \n",
        "        # Sort by multiple criteria for better conflict resolution\n",
        "        def sort_key(result):\n",
        "            # Priority: confidence, exactness of method, length, position\n",
        "            method_priority = {\n",
        "                \"exact_case_match\": 4,\n",
        "                \"case_insensitive_match\": 3,\n",
        "                \"multi_token_match\": 2,\n",
        "                \"fuzzy_match\": 1\n",
        "            }\n",
        "            \n",
        "            return (\n",
        "                -result['confidence'],                                    # Higher confidence first\n",
        "                -method_priority.get(result['method'], 0),               # Better methods first\n",
        "                -(result['end'] - result['start']),                     # Longer entities first\n",
        "                result['start']                                          # Earlier position first\n",
        "            )\n",
        "        \n",
        "        sorted_results = sorted(results, key=sort_key)\n",
        "        final_results = []\n",
        "        \n",
        "        for result in sorted_results:\n",
        "            # Check for overlap with existing results\n",
        "            overlap = False\n",
        "            for existing in final_results:\n",
        "                if (result['start'] < existing['end'] and result['end'] > existing['start']):\n",
        "                    overlap = True\n",
        "                    break\n",
        "            \n",
        "            if not overlap:\n",
        "                final_results.append(result)\n",
        "        \n",
        "        return sorted(final_results, key=lambda x: x['start'])\n",
        "\n",
        "    def get_entities_by_domain(self, domain: str) -> List[str]:\n",
        "        \"\"\"Get all entities in a specific domain\"\"\"\n",
        "        return self.entities_by_domain.get(domain, [])\n",
        "    \n",
        "    def get_canonical_variants(self, canonical_name: str) -> List[str]:\n",
        "        \"\"\"Get all aliases for a canonical name\"\"\"\n",
        "        return self.aliases_by_canonical.get(canonical_name, [])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "\n",
        "class KoreanEntityMatcher:\n",
        "    def __init__(self, min_similarity=70, ngram_size=2, min_entity_length=2, token_similarity=True):\n",
        "        self.min_similarity = min_similarity\n",
        "        self.ngram_size = ngram_size\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.token_similarity = token_similarity  # 토큰 단위 유사도 비교 옵션 추가\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "    def build_from_list(self, entities):\n",
        "        \"\"\"Build entity index from a list of entities\"\"\"\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "        for i, entity in enumerate(entities):\n",
        "            if isinstance(entity, tuple) and len(entity) == 2:\n",
        "                entity_name, data = entity\n",
        "                self.entities.append(entity_name)\n",
        "                self.entity_data[entity_name] = data\n",
        "            else:\n",
        "                self.entities.append(entity)\n",
        "                self.entity_data[entity] = {'id': i, 'entity': entity}\n",
        "                \n",
        "        # 각 엔티티의 정규화된 형태를 저장 (검색 최적화)\n",
        "        self.normalized_entities = {}\n",
        "        for entity in self.entities:\n",
        "            normalized = self._normalize_text(entity)\n",
        "            self.normalized_entities[normalized] = entity\n",
        "                \n",
        "        # Create n-gram index for faster candidate selection\n",
        "        self._build_ngram_index(n=self.ngram_size)\n",
        "    \n",
        "    def _normalize_text(self, text):\n",
        "        \"\"\"텍스트 정규화 - 소문자 변환, 공백 제거 등\"\"\"\n",
        "        # 소문자로 변환\n",
        "        text = text.lower()\n",
        "        # 연속된 공백을 하나로 통일\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"텍스트를 토큰으로 분리 (한글, 영문, 숫자 분리)\"\"\"\n",
        "        # 한글, 영문, 숫자 토큰 추출\n",
        "        tokens = re.findall(r'[가-힣]+|[a-z0-9]+', self._normalize_text(text))\n",
        "        return tokens\n",
        "    \n",
        "    def _build_ngram_index(self, n=2):\n",
        "        \"\"\"Build n-gram index optimized for Korean characters\"\"\"\n",
        "        self.ngram_index = {}\n",
        "        \n",
        "        for entity in self.entities:\n",
        "            # Skip entities shorter than min_entity_length\n",
        "            if len(entity) < self.min_entity_length:\n",
        "                continue\n",
        "                \n",
        "            # 정규화된 엔티티 사용\n",
        "            normalized_entity = self._normalize_text(entity)\n",
        "            \n",
        "            # Create n-grams for the entity\n",
        "            entity_chars = list(normalized_entity)  # Split into characters for proper Korean handling\n",
        "            ngrams = []\n",
        "            \n",
        "            # Create character-level n-grams (better for Korean)\n",
        "            for i in range(len(entity_chars) - n + 1):\n",
        "                ngram = ''.join(entity_chars[i:i+n])\n",
        "                ngrams.append(ngram)\n",
        "            \n",
        "            # Add entity to the index for each n-gram\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram_index:\n",
        "                    self.ngram_index[ngram] = set()\n",
        "                self.ngram_index[ngram].add(entity)\n",
        "                \n",
        "            # 토큰 기반 n-gram도 추가 (실험적)\n",
        "            tokens = self._tokenize(normalized_entity)\n",
        "            for token in tokens:\n",
        "                if len(token) >= n:\n",
        "                    token_key = f\"TOKEN:{token}\"\n",
        "                    if token_key not in self.ngram_index:\n",
        "                        self.ngram_index[token_key] = set()\n",
        "                    self.ngram_index[token_key].add(entity)\n",
        "    \n",
        "    def _get_candidates(self, text, n=None):\n",
        "        \"\"\"Get candidate entities based on n-gram overlap (optimized for Korean)\"\"\"\n",
        "        if n is None:\n",
        "            n = self.ngram_size\n",
        "            \n",
        "        # 텍스트 정규화\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        \n",
        "        # 정규화된 텍스트가 정확히 일치하는지 확인 (빠른 경로)\n",
        "        if normalized_text in self.normalized_entities:\n",
        "            entity = self.normalized_entities[normalized_text]\n",
        "            return [(entity, float('inf'))]  # 정확한 일치는 무한대 점수로 표시\n",
        "        \n",
        "        text_chars = list(normalized_text)  # Split into characters for proper Korean handling\n",
        "        text_ngrams = set()\n",
        "        \n",
        "        # Create character-level n-grams\n",
        "        for i in range(len(text_chars) - n + 1):\n",
        "            ngram = ''.join(text_chars[i:i+n])\n",
        "            text_ngrams.add(ngram)\n",
        "        \n",
        "        # 토큰 기반 n-gram 추가\n",
        "        tokens = self._tokenize(normalized_text)\n",
        "        for token in tokens:\n",
        "            if len(token) >= n:\n",
        "                text_ngrams.add(f\"TOKEN:{token}\")\n",
        "        \n",
        "        candidates = set()\n",
        "        for ngram in text_ngrams:\n",
        "            if ngram in self.ngram_index:\n",
        "                candidates.update(self.ngram_index[ngram])\n",
        "        \n",
        "        # Prioritize candidates with multiple n-gram matches\n",
        "        candidate_scores = {}\n",
        "        for candidate in candidates:\n",
        "            candidate_normalized = self._normalize_text(candidate)\n",
        "            candidate_chars = list(candidate_normalized)\n",
        "            candidate_ngrams = set()\n",
        "            \n",
        "            # 문자 n-gram\n",
        "            for i in range(len(candidate_chars) - n + 1):\n",
        "                ngram = ''.join(candidate_chars[i:i+n])\n",
        "                candidate_ngrams.add(ngram)\n",
        "            \n",
        "            # 토큰 기반 n-gram\n",
        "            candidate_tokens = self._tokenize(candidate_normalized)\n",
        "            for token in candidate_tokens:\n",
        "                if len(token) >= n:\n",
        "                    candidate_ngrams.add(f\"TOKEN:{token}\")\n",
        "            \n",
        "            # n-gram 교집합 크기로 초기 점수 계산\n",
        "            overlap = len(candidate_ngrams.intersection(text_ngrams))\n",
        "            \n",
        "            # 토큰 수준 유사도 보너스 점수 추가\n",
        "            token_bonus = 0\n",
        "            if self.token_similarity:\n",
        "                query_tokens = set(tokens)\n",
        "                cand_tokens = set(candidate_tokens)\n",
        "                \n",
        "                # 공통 토큰 비율 계산\n",
        "                if query_tokens and cand_tokens:\n",
        "                    common = query_tokens.intersection(cand_tokens)\n",
        "                    token_bonus = len(common) * 2  # 토큰 일치에 높은 가중치 부여\n",
        "            \n",
        "            candidate_scores[candidate] = overlap + token_bonus\n",
        "        \n",
        "        # Return candidates sorted by n-gram overlap score\n",
        "        return sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    def _calculate_similarity(self, text, entity):\n",
        "        \"\"\"다양한 유사도 측정 방법을 결합하여 더 정확한 유사도 계산\"\"\"\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        normalized_entity = self._normalize_text(entity)\n",
        "        \n",
        "        # 정확히 일치하면 100점 반환\n",
        "        if normalized_text == normalized_entity:\n",
        "            return 100\n",
        "        \n",
        "        # 기본 문자열 유사도 (fuzz.ratio)\n",
        "        ratio_score = fuzz.ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 부분 문자열 체크 (한 문자열이 다른 문자열의 부분 문자열인 경우)\n",
        "        partial_score = 0\n",
        "        if normalized_text in normalized_entity:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (text_len / entity_len) * 100 if entity_len > 0 else 0\n",
        "        elif normalized_entity in normalized_text:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (entity_len / text_len) * 100 if text_len > 0 else 0\n",
        "        \n",
        "        # 토큰 유사도 (토큰 단위로 비교)\n",
        "        token_score = 0\n",
        "        if self.token_similarity:\n",
        "            text_tokens = set(self._tokenize(normalized_text))\n",
        "            entity_tokens = set(self._tokenize(normalized_entity))\n",
        "            \n",
        "            if text_tokens and entity_tokens:\n",
        "                common_tokens = text_tokens.intersection(entity_tokens)\n",
        "                all_tokens = text_tokens.union(entity_tokens)\n",
        "                \n",
        "                if all_tokens:\n",
        "                    token_score = (len(common_tokens) / len(all_tokens)) * 100\n",
        "        \n",
        "        # 토큰 순서 무시 유사도 (fuzz.token_sort_ratio)\n",
        "        token_sort_score = fuzz.token_sort_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 토큰 집합 유사도 (fuzz.token_set_ratio)\n",
        "        token_set_score = fuzz.token_set_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 최종 유사도는 여러 점수의 가중 평균\n",
        "        # 토큰 유사도에 높은 가중치를 부여하여 \"T우주\"와 \"T 우주패스\"의 매칭 향상\n",
        "        final_score = (\n",
        "            ratio_score * 0.3 +  # 기본 유사도\n",
        "            max(partial_score, 0) * 0.1 +  # 부분 문자열 유사도\n",
        "            token_score * 0.2 +  # 토큰 유사도\n",
        "            token_sort_score * 0.2 +  # 토큰 순서 무시 유사도\n",
        "            token_set_score * 0.2  # 토큰 집합 유사도\n",
        "        )\n",
        "        \n",
        "        return final_score\n",
        "    \n",
        "    def find_entities(self, text, max_candidates_per_span=10):\n",
        "        \"\"\"Find entity matches in Korean text using fuzzy matching\"\"\"\n",
        "        # Extract spans that might contain entities\n",
        "        potential_spans = self._extract_korean_spans(text)\n",
        "        matches = []\n",
        "        \n",
        "        for span_text, start, end in potential_spans:\n",
        "            if len(span_text.strip()) < self.min_entity_length:  # Skip spans shorter than min_entity_length\n",
        "                continue\n",
        "\n",
        "            # Get candidate entities based on n-gram overlap\n",
        "            candidates = self._get_candidates(span_text)\n",
        "\n",
        "            # If no candidates found through n-gram filtering, skip\n",
        "            if not candidates:\n",
        "                continue\n",
        "            \n",
        "            # Limit the number of candidates to check\n",
        "            top_candidates = [c[0] for c in candidates[:max_candidates_per_span]]\n",
        "            \n",
        "            # 각 후보 엔티티에 대해 개선된 유사도 계산\n",
        "            scored_matches = []\n",
        "            for entity in top_candidates:\n",
        "                score = self._calculate_similarity(span_text, entity)\n",
        "                \n",
        "                if score >= self.min_similarity:\n",
        "                    scored_matches.append((entity, score, 0))  # 호환성을 위해 3번째 매개변수 추가\n",
        "\n",
        "            # 기존 process.extract 대신 개선된 유사도 계산 사용\n",
        "            best_matches = scored_matches\n",
        "\n",
        "            for entity, score, _ in best_matches:\n",
        "                matches.append({\n",
        "                    'text': span_text,\n",
        "                    'matched_entity': entity,\n",
        "                    'score': score,\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'data': self.entity_data.get(entity, {})\n",
        "                })\n",
        "\n",
        "        # Sort by position in text\n",
        "        matches.sort(key=lambda x: (x['start'], -x['score']))\n",
        "        \n",
        "        # Handle overlapping matches by keeping the best match\n",
        "        final_matches = self._resolve_overlapping_matches(matches)\n",
        "\n",
        "        return final_matches\n",
        "    \n",
        "    def _extract_korean_spans(self, text):\n",
        "        \"\"\"한국어와 영어가 혼합된 텍스트에서 엔티티일 수 있는 잠재적 텍스트 범위 추출\"\"\"\n",
        "        spans = []\n",
        "        min_len = self.min_entity_length\n",
        "        \n",
        "        # 1. 영문+한글 혼합 패턴 (붙여쓰기) 예: \"T우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 2. 영문+한글 혼합 패턴 (띄어쓰기) 예: \"T 우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+\\s+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 3. 특수한 혼합 패턴 - 영문+한글+숫자 (예: \"T우주365\", \"SK텔레콤\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:[0-9]+)?', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 4. 연속된 두 단어까지 확장 (예: \"T우주 패스\")\n",
        "        # 영문+한글 후 공백 하나를 두고 다른 한글 단어가 나오는 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "                \n",
        "        # 5. 연속된 세 단어까지 확장 (예: \"T우주 멤버십 패스\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 6. 브랜드명 + 제품명 패턴 (예: \"SK텔레콤 T우주\")\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣]+(?:\\s+[a-zA-Z가-힣]+){1,3}', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 7. 숫자와 영문 결합 패턴 (숫자 space 영문 패턴, e.g. \"0 day\")\n",
        "        for match in re.finditer(r'\\d+\\s+[a-zA-Z]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 8. 더 일반적인 영한 혼합 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣0-9]+(?:\\s+[a-zA-Z가-힣0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 9. 일반적인 구분자로 분리된 텍스트 조각도 추출\n",
        "        for span in re.split(r'[,\\.!?;:\"\\'…\\(\\)\\[\\]\\{\\}\\s_/]+', text):\n",
        "            if span and len(span) >= min_len:\n",
        "                span_pos = text.find(span)\n",
        "                if span_pos != -1:\n",
        "                    spans.append((span, span_pos, span_pos + len(span)))\n",
        "                \n",
        "        return spans\n",
        "    \n",
        "    def _remove_duplicate_entities(self, matches):\n",
        "        \"\"\"Keep only one instance of each unique entity\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Dictionary to track highest-scoring match for each entity\n",
        "        best_matches = {}\n",
        "        \n",
        "        for match in matches:\n",
        "            entity_key = match['matched_entity']\n",
        "            \n",
        "            # If we haven't seen this entity before, or if this match has a higher score\n",
        "            # than the previously saved match for this entity, save this one\n",
        "            if (entity_key not in best_matches or \n",
        "                match['score'] > best_matches[entity_key]['score']):\n",
        "                best_matches[entity_key] = match\n",
        "        \n",
        "        # Return the best matches sorted by start position\n",
        "        return sorted(best_matches.values(), key=lambda x: x['start'])\n",
        "    \n",
        "    def _resolve_overlapping_matches(self, matches, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # 점수 내림차순, 길이 오름차순으로 정렬 (높은 점수, 짧은 매치 우선)\n",
        "        sorted_matches = sorted(matches, key=lambda x: (-x['score'], x['end'] - x['start']))\n",
        "        \n",
        "        final_matches = []\n",
        "        \n",
        "        for current_match in sorted_matches:\n",
        "            current_score = current_match['score']\n",
        "            current_start, current_end = current_match['start'], current_match['end']\n",
        "            current_range = set(range(current_start, current_end))\n",
        "            current_len = len(current_range)\n",
        "            \n",
        "            # 수정: overlap_ratio를 0으로 초기화\n",
        "            current_match['overlap_ratio'] = 0.0\n",
        "            \n",
        "            # 높은 점수의 매치는 항상 포함\n",
        "            if current_score >= high_score_threshold:\n",
        "                # 기존 매치들과 비교하여 너무 많은 중복이 있는지 확인\n",
        "                is_too_similar = False\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    if existing_match['score'] < high_score_threshold:\n",
        "                        continue  # 낮은 점수의 기존 매치와는 비교하지 않음\n",
        "                        \n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    \n",
        "                    # 현재 매치에 대한 중복 비율 \n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 수정: 중복 비율 저장 - 가장 높은 중복 비율 저장\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하고, 동일한 엔티티면 추가하지 않음\n",
        "                    if (current_overlap_ratio > overlap_tolerance\n",
        "                        and current_match['matched_entity'] == existing_match['matched_entity']\n",
        "                        ):\n",
        "                        is_too_similar = True\n",
        "                        break\n",
        "                \n",
        "                if not is_too_similar:\n",
        "                    final_matches.append(current_match)\n",
        "            else:\n",
        "                # 낮은 점수의 매치는 기존 로직 적용 (중복 확인)\n",
        "                should_add = True\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 수정: 중복 비율 저장 - 가장 높은 중복 비율 저장\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하면 추가하지 않음\n",
        "                    if current_overlap_ratio > (1 - overlap_tolerance):\n",
        "                        should_add = False\n",
        "                        break\n",
        "                \n",
        "                if should_add:\n",
        "                    final_matches.append(current_match)\n",
        "        \n",
        "        # 시작 위치별로 정렬\n",
        "        final_matches.sort(key=lambda x: x['start'])\n",
        "        \n",
        "        return final_matches\n",
        "\n",
        "def find_entities_in_text(text, entity_list, min_similarity=70, ngram_size=3, min_entity_length=2, \n",
        "                         token_similarity=True, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "    \"\"\"\n",
        "    Find entity matches in text using fuzzy matching.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The text to search for entities\n",
        "    entity_list : list\n",
        "        List of entities to match against\n",
        "    min_similarity : int, default=70\n",
        "        Minimum similarity score (0-100) for fuzzy matching\n",
        "    ngram_size : int, default=2\n",
        "        Size of character n-grams to use for indexing (2 or 3 recommended for Korean)\n",
        "    min_entity_length : int, default=2\n",
        "        Minimum length of entities to consider (characters)\n",
        "    token_similarity : bool, default=True\n",
        "        Whether to use token-based similarity measures\n",
        "    high_score_threshold : int, default=50\n",
        "        Score threshold above which matches are always kept regardless of overlap\n",
        "    overlap_tolerance : float, default=0.5\n",
        "        Overlap tolerance ratio (0-1), higher values allow more overlapping matches\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        List of matched entities with position and metadata\n",
        "    \"\"\"\n",
        "    matcher = KoreanEntityMatcher(\n",
        "        min_similarity=min_similarity,\n",
        "        ngram_size=ngram_size,\n",
        "        min_entity_length=min_entity_length,\n",
        "        token_similarity=token_similarity\n",
        "    )\n",
        "    matcher.build_from_list(entity_list)\n",
        "    \n",
        "    matches = matcher.find_entities(text)\n",
        "    \n",
        "    # 기존 _resolve_overlapping_matches 메서드 대신 직접 호출\n",
        "    final_matches = matcher._resolve_overlapping_matches(\n",
        "        matches, \n",
        "        high_score_threshold=high_score_threshold,\n",
        "        overlap_tolerance=overlap_tolerance\n",
        "    )\n",
        "    \n",
        "    return final_matches\n",
        "# Function to highlight entities in text\n",
        "def highlight_entities(text, matches):\n",
        "    marked_text = text\n",
        "    offset = 0\n",
        "    for match in sorted(matches, key=lambda x: x['start'], reverse=True):\n",
        "        start = match['start'] + offset\n",
        "        end = match['end'] + offset\n",
        "        entity = match['matched_entity']\n",
        "        score = match['score']\n",
        "        marked_text = marked_text[:start] + f\"[{marked_text[start:end]}→{entity} ({score:.1f}%)]\" + marked_text[end:]\n",
        "        offset += len(f\"[→{entity} ({score:.1f}%)]\") + 2\n",
        "    \n",
        "    return marked_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def advanced_sequential_similarity(str1, str2, metrics=None, visualize=False):\n",
        "    \"\"\"\n",
        "    Calculate multiple character-level similarity metrics between two strings.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    str1 : str\n",
        "        First string\n",
        "    str2 : str\n",
        "        Second string\n",
        "    metrics : list\n",
        "        List of metrics to compute. Options: \n",
        "        ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "        If None, all metrics will be computed\n",
        "    visualize : bool\n",
        "        If True, visualize the differences between strings\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing similarity scores for each metric\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Handle empty strings\n",
        "    if not str1 or not str2:\n",
        "        return {metric: 0.0 for metric in metrics}\n",
        "    \n",
        "    # Prepare strings\n",
        "    s1, s2 = str1.lower(), str2.lower()\n",
        "    \n",
        "    # 1. N-gram similarity (with multiple window sizes)\n",
        "    if 'ngram' in metrics:\n",
        "        ngram_scores = {}\n",
        "        for window in range(min([len(s1),len(s2),2]), min([5,max([len(s1),len(s2)])+1])):\n",
        "            # Skip if strings are shorter than window\n",
        "            if len(s1) < window or len(s2) < window:\n",
        "                ngram_scores[f'window_{window}'] = 0.0\n",
        "                continue\n",
        "                \n",
        "            # Generate character n-grams\n",
        "            ngrams1 = [s1[i:i+window] for i in range(len(s1) - window + 1)]\n",
        "            ngrams2 = [s2[i:i+window] for i in range(len(s2) - window + 1)]\n",
        "            \n",
        "            # Count matches\n",
        "            matches = sum(1 for ng in ngrams1 if ng in ngrams2)\n",
        "            max_possible = max(len(ngrams1), len(ngrams2))\n",
        "            \n",
        "            # Normalize\n",
        "            score = matches / max_possible if max_possible > 0 else 0.0\n",
        "            ngram_scores[f'window_{window}'] = score\n",
        "            \n",
        "        # Average of all n-gram scores\n",
        "        results['ngram'] = max(ngram_scores.values())#sum(ngram_scores.values()) / len(ngram_scores)\n",
        "        results['ngram_details'] = ngram_scores\n",
        "    \n",
        "    # 2. Longest Common Substring (LCS)\n",
        "    if 'lcs' in metrics:\n",
        "        def longest_common_substring(s1, s2):\n",
        "            # Dynamic programming approach\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            max_length = 0\n",
        "            \n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                        max_length = max(max_length, dp[i][j])\n",
        "            \n",
        "            return max_length\n",
        "        \n",
        "        lcs_length = longest_common_substring(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['lcs'] = lcs_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    # 3. Longest Common Subsequence\n",
        "    if 'subsequence' in metrics:\n",
        "        def longest_common_subsequence(s1, s2):\n",
        "            # Dynamic programming approach for subsequence\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            \n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "            \n",
        "            return dp[m][n]\n",
        "        \n",
        "        subseq_length = longest_common_subsequence(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['subsequence'] = subseq_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    # 4. SequenceMatcher from difflib\n",
        "    if 'difflib' in metrics:\n",
        "        sm = SequenceMatcher(None, s1, s2)\n",
        "        results['difflib'] = sm.ratio()\n",
        "    \n",
        "    # Visualization of differences\n",
        "    if visualize:\n",
        "        try:\n",
        "            # Only works in notebooks or environments that support plotting\n",
        "            sm = SequenceMatcher(None, s1, s2)\n",
        "            matches = sm.get_matching_blocks()\n",
        "            \n",
        "            # Prepare for visualization\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            \n",
        "            # Draw strings as horizontal bars\n",
        "            ax.barh(0, len(s1), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            ax.barh(1, len(s2), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            \n",
        "            # Draw matching parts\n",
        "            for match in matches:\n",
        "                i, j, size = match\n",
        "                if size > 0:  # Ignore zero-length matches\n",
        "                    ax.barh(0, size, height=0.4, left=i, color='green', alpha=0.5)\n",
        "                    ax.barh(1, size, height=0.4, left=j, color='green', alpha=0.5)\n",
        "                    \n",
        "                    # Draw connection lines between matches\n",
        "                    ax.plot([i + size/2, j + size/2], [0.2, 0.8], 'k-', alpha=0.3)\n",
        "            \n",
        "            # Add string texts\n",
        "            for i, c in enumerate(s1):\n",
        "                ax.text(i + 0.5, 0, c, ha='center', va='center')\n",
        "            for i, c in enumerate(s2):\n",
        "                ax.text(i + 0.5, 1, c, ha='center', va='center')\n",
        "                \n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_yticklabels(['String 1', 'String 2'])\n",
        "            ax.set_xlabel('Character Position')\n",
        "            ax.set_title('Character-Level String Comparison')\n",
        "            ax.grid(False)\n",
        "            plt.tight_layout()\n",
        "            # plt.show()  # Uncomment to display\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "    \n",
        "    # Calculate overall similarity score (average of all metrics)\n",
        "    metrics_to_average = [m for m in results.keys() if not m.endswith('_details')]\n",
        "    results['overall'] = sum(results[m] for m in metrics_to_average) / len(metrics_to_average)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# advanced_sequential_similarity('시크릿', '시크릿', metrics='ngram')\n",
        "# advanced_sequential_similarity('에이닷_자사', '에이닷')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "item_pdf_raw = pd.read_csv(\"./data/item_info_all_250527.csv\")\n",
        "\n",
        "item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','domain','start_dt','end_dt','rank']].copy()\n",
        "\n",
        "# item_pdf_all.query(\"rank<1000\")[['item_nm']].drop_duplicates().to_csv(\"./data/item_nm_1000.csv\", index=False)\n",
        "alias_pdf = pd.read_csv(\"./data/alias_rules.csv\")\n",
        "alia_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))\n",
        "\n",
        "def apply_alias_rule(item_nm):\n",
        "    item_nm_list = [item_nm]\n",
        "\n",
        "    for r in alia_rule_set:\n",
        "        if r[0] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[0], r[1]))\n",
        "        if r[1] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[1], r[0]))\n",
        "    return item_nm_list\n",
        "\n",
        "item_pdf_all['item_nm_alias'] = item_pdf_all['item_nm'].apply(apply_alias_rule)\n",
        "\n",
        "item_pdf_all = item_pdf_all.explode('item_nm_alias')\n",
        "\n",
        "item_pdf_all.query(\"rank<1000 and item_nm.str.contains('넷플릭스', case=False) or item_nm.str.contains('웨이브', case=False)\")[['item_nm','item_nm_alias','item_id']]\n",
        "\n",
        "user_defined_entity = ['AIA Vitality' , '부스트 파크 건대입구' , 'Boost Park 건대입구']\n",
        "item_pdf_ext = pd.DataFrame([{'item_nm':e,'item_id':e,'item_desc':e, 'domain':'user_defined', 'start_dt':20250101, 'end_dt':99991231, 'rank':1, 'item_nm_alias':e} for e in user_defined_entity])\n",
        "item_pdf_all = pd.concat([item_pdf_all,item_pdf_ext])\n",
        "\n",
        "stop_item_names = pd.read_csv(\"./data/stop_words.csv\")['stop_words'].to_list()\n",
        "\n",
        "entity_vocab = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    if row['item_nm_alias'] in stop_item_names:\n",
        "        continue\n",
        "    entity_vocab.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'domain':row['domain'],'item_nm_alias':row['item_nm_alias']}))\n",
        "\n",
        "\n",
        "entity_list_for_fuzzy = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    entity_list_for_fuzzy.append((row['item_nm_alias'], {'item_nm':row['item_nm'], 'item_id':row['item_id'], 'description':row['item_desc'], 'domain':row['domain'], 'start_dt':row['start_dt'], 'end_dt':row['end_dt'], 'rank':1, 'item_nm_alias':row['item_nm_alias']}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "mms_pdf = pd.read_csv(\"./data/mms_data_250408.csv\")\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "mms_pdf = mms_pdf.reset_index()\n",
        "mms_pdf = mms_pdf.astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "emb_model = SentenceTransformer('jhgan/ko-sbert-nli')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수문자를 공백으로 변환\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # 여러 공백을 하나로 통일\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 앞뒤 공백 제거\n",
        "    return text.strip()\n",
        "\n",
        "item_embeddings = emb_model.encode([x.lower() for x in item_pdf_all['item_nm'].tolist()], convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "num_cand_pgms = 5\n",
        "\n",
        "pgm_pdf = pd.read_csv(\"./data/pgm_tag_ext_250516.csv\")\n",
        "\n",
        "clue_embeddings = emb_model.encode(pgm_pdf[[\"pgm_nm\",\"clue_tag\"]].apply(lambda x: preprocess_text(x['pgm_nm'].lower())+\" \"+x['clue_tag'].lower(), axis=1).tolist(), convert_to_tensor=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = KoreanNERSpacy(entity_vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test text: 인천중구대리점  부천역직영점 오픈 안내\n",
            "(광고)[SKT] 인천중구대리점  부천역직영점 오픈 안내  고객님, 안녕하세요. SK텔레콤이 부천역 4번출구 알라딘 문구 1층에 새로운 T프리미엄스토어를 열었습니다. ★매장위치 확인:  http://t-mms.kr/t.do?m=#61&u=http://naver.me/FaV6yr6v ★  ★우리 매장 친구맺기 혜택 안내★ http://t-mms.kr/t.do?m=#61&u=https://tworldfriends.co.kr/d131240160    ■ 오픈 기념 혜택 - 휴대폰구매고객: 6종 사은품   ① 저주파 무선 안마기   ② 카카오타월   ③ 그릇셋트   ④ 카카오무선선풍기   ⑤ 올인원케이블(모든타입 사용 가능)   ⑥ 고급 고속 충전기 - 인터넷/TV 설치시 상품권 최대 40만원 - 단골등록고객: 스마트폰 터치펜  또는 그립톡증정 및 휴대폰 액정필름 무료교체 ※ 위 사은품은 조기 종료 또는 대체될 수 있습니다.  ★ 혜택 내용 자세히 보기:  http://t-mms.kr/t.do?m=#61&u=http://www.sktbestshop.com/ * 이 문자를 받으신 고객님만 혜택을 받으실 수 있습니다. ------------------------------------------------------------------  ■ 매장위치 및 영업시간 안내 - 주소 : 경기 부천시 부천로4(경동빌딩) - 연락처 : 032-887-0001 (문의사항 있을시 매장연락처로 문의주십시오) - 영업시간 : 평일 오전 10시~오후 8시 (주말 오전11시~오후7시)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\n",
            "fuzzy_old:  MDL-0001 (mobile_device), T 연락처 (product), 티봇_887 (product), 티봇_0001 (product)\n",
            "\n",
            "\n",
            "fuzzy:  카카오 (affiliated_service)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for test_text in mms_pdf.sample(10)['msg'].tolist():\n",
        "# for test_text in msg_text_list:\n",
        "    print(f\"Test text: {test_text.strip()}\")\n",
        "\n",
        "    matches = []\n",
        "    matches.extend(find_entities_in_text(\n",
        "            test_text.strip(), \n",
        "            entity_list_for_fuzzy, \n",
        "            min_similarity=60,\n",
        "            high_score_threshold=50,\n",
        "            overlap_tolerance=0.5\n",
        "        ))\n",
        "\n",
        "    mdf = pd.DataFrame(matches)\n",
        "    \n",
        "\n",
        "    print(\"fuzzy_old: \",', '.join(set([f\"{p['matched_entity']} ({p['data']['domain']})\" for p in matches if p['matched_entity'] not in stop_item_names and p['text'] not in stop_item_names])))\n",
        "    print()\n",
        "\n",
        "    # Test each method\n",
        "    methods = [\"fuzzy\"]\n",
        "\n",
        "    for method in methods:\n",
        "        print()\n",
        "        results = ner.extract_entities(test_text, method)\n",
        "        \n",
        "        # print(f\"Found {len(results)} unique entities:\")\n",
        "        \n",
        "        # Group by canonical name to show deduplication\n",
        "        canonical_groups = {}\n",
        "        for result in results:\n",
        "            canonical = result['canonical_name']\n",
        "\n",
        "            if canonical in stop_item_names:\n",
        "                continue\n",
        "            \n",
        "            if canonical not in canonical_groups:\n",
        "                canonical_groups[canonical] = []\n",
        "            canonical_groups[canonical].append(result)\n",
        "\n",
        "        print(method+\": \",', '.join([f\"{canonical} ({group[0]['domain']})\" for canonical, group in canonical_groups.items()]))\n",
        "    print()\n",
        "\n",
        "    break\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "msg_text_list = [\"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:통화 부가서비스를 패키지로 저렴하게!\n",
        "광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ', \n",
        "\"\"\",\n",
        "\"\"\"\n",
        "(광고)[SKT] 1월 0 day 혜택 안내_ _[1월 20일(토) 혜택]_만 13~34세 고객이라면 _CU에서 핫바 1,000원에 구매 하세요!_(선착순 1만 명 증정)_▶ 자세히 보기 : http://t-mms.kr/t.do?m=#61&s=24264&a=&u=https://bit.ly/3H2OHSs__■ 에이닷 X T 멤버십 구독캘린더 이벤트_0 day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요! _알림 설정하면 추첨을 통해 [스타벅스 카페 라떼tall 모바일쿠폰]을 드립니다. _▶ 이벤트 참여하기 : https://bit.ly/3RVSojv_ _■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "'[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:[SK텔레콤] T건강습관 X AIA Vitality, 우리 가족의 든든한 보험!\n",
        "광고 내용:(광고)[SKT] 가족의 든든한 보험 (무배당)AIA Vitality 베스트핏 보장보험 안내  고객님, 안녕하세요. 4인 가족 표준생계비, 준비하고 계시나요? (무배당)AIA Vitality 베스트핏 보장보험(디지털 전용)으로 최대 20% 보험료 할인과 가족의 든든한 보험 보장까지 누려 보세요.   ▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/36oWjgX  ■ AIA Vitality  혜택 - 매달 리워드 최대 12,000원 - 등급 업그레이드 시 특별 리워드 - T건강습관 제휴 할인 최대 40% ※ 제휴사별 할인 조건과 주간 미션 달성 혜택 등 자세한 내용은 AIA Vitality 사이트에서 확인하세요. ※ 이 광고는 AIA생명의 광고이며 SK텔레콤은 모집 행위를 하지 않습니다.  - 보험료 납입 기간 중 피보험자가 장해분류표 중 동일한 재해 또는 재해 이외의 동일한 원인으로 여러 신체 부위의 장해지급률을 더하여 50% 이상인 장해 상태가 된 경우 차회 이후의 보험료 납입 면제 - 사망보험금은 계약일(부활일/효력회복일)로부터 2년 안에 자살한 경우 보장하지 않음 - 일부 특약 갱신 시 보험료 인상 가능 - 기존 계약 해지 후 신계약 체결 시 보험인수 거절, 보험료 인상, 보장 내용 변경 가능 - 해약 환급금(또는 만기 시 보험금이나 사고보험금)에 기타 지급금을 합해 5천만 원까지(본 보험 회사 모든 상품 합산) 예금자 보호 - 계약 체결 전 상품 설명서 및 약관 참조 - 월 보험료 5,500원(부가세 포함)  * 생명보험협회 심의필 제2020-03026호(2020-09-22) COM-2020-09-32426  ■문의: 청약 관련(1600-0880)  무료 수신거부 1504    \n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "message_idx = 1\n",
        "\n",
        "test_text = msg_text_list[message_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test text: [SK텔레콤] 와이즈대리점 태평점 행사안내\n",
            "(광고)[SKT]와이즈대리점 태평직영점 갤럭시 S23 출시 이벤트__* 단골 문자 받으신 고객님 매장 방문해주시면 따뜻한 핫팩 증정!!__SK텔레콤 공식인증 태평직영점에서 갤럭시 S23 출시 이벤트 안내드립니다.__■ 갤럭시 S23 구매 시 갤럭시 워치 또는 60만 원 상당 삼성노트북(갤럭시북GO) 증정_ - 갤럭시 S23 구매 시 최대 128만 원 할인(삼성보상, 제휴 카드 할인)_ - 24개월 후 반납 시 새 휴대폰으로 교체가능__■ 포켓몬 에디션 키즈폰 한정판 출시!! 새학기 사은품 증정!!_ - 태평동 당근마켓에서 많은 판매를 기록한 초등학생용폰__■SKT B tv air 이벤트_- 유선+휴대폰 가족 결합 3인 시 테블릿PC+넷플릭스등 OTT서비스(일부 요금 발생) 시청 가능__■ SK쉴더스(ADT캡스) 6개월 무료 체험 이벤트(선착순 10명)_ - 집앞 CCTV +도난, 화재보험_ - 무료 체험 후 불필요 시 해지 가능하며, 설치 및 해지 시 별도 비용 발생하지 않습니다._  _■ 인터넷+IPTV 가입 시 최대 사은품 증정__인터넷과 IPTV 가입 시 최대 사은품! 테블릿PC로 B tv 시청 무료 체험권!_항상 정직하고 친절한 SK텔레콤 직영점이 되겠습니다.__▶SKT 와이즈 태평점_- 주소: 경기도 성남시 수정구 수정로 119번길 4_- 연락처: 070-7865-8708__▶ 매장홈페이지: http://t-mms.kr/t.do?m=#61&s=18305&a=&u=https://tworldfriends.co.kr/D136650122__SK텔레콤과 함께해주셔서 감사합니다.__무료 수신거부 1504\n",
            "\n",
            "=============================================claude sonnet (emb)=============================================\n",
            "\n",
            "{\n",
            "    \"title\": \"갤럭시 S23 출시 기념 특별 이벤트 및 SKT 다양한 혜택 안내\",\n",
            "    \"purpose\": [\n",
            "        \"상품 가입 유도\",\n",
            "        \"대리점/매장 방문 유도\",\n",
            "        \"혜택 안내\",\n",
            "        \"이벤트 응모 유도\"\n",
            "    ],\n",
            "    \"product\": [\n",
            "        {\n",
            "            \"item_name_in_msg\": \"갤럭시 S23\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 S22\",\n",
            "                    \"item_id\": [\n",
            "                        \"A47T\",\n",
            "                        \"A3UV\",\n",
            "                        \"A3UU\",\n",
            "                        \"A3US\",\n",
            "                        \"A47S\",\n",
            "                        \"A3UW\",\n",
            "                        \"A3UX\",\n",
            "                        \"A3T3\",\n",
            "                        \"A3UT\",\n",
            "                        \"A3UY\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 S21\",\n",
            "                    \"item_id\": [\n",
            "                        \"A2VY\",\n",
            "                        \"A2VU\",\n",
            "                        \"A2SP\",\n",
            "                        \"A2VZ\",\n",
            "                        \"A2VV\",\n",
            "                        \"A2VW\",\n",
            "                        \"A2VX\",\n",
            "                        \"A2W1\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 S23 FE\",\n",
            "                    \"item_id\": [\n",
            "                        \"A5GX\",\n",
            "                        \"A5GZ\",\n",
            "                        \"A5GY\",\n",
            "                        \"A5H2\",\n",
            "                        \"A5H4\",\n",
            "                        \"A5FW\",\n",
            "                        \"A5H1\",\n",
            "                        \"A5H3\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 S25\",\n",
            "                    \"item_id\": [\n",
            "                        \"A683\",\n",
            "                        \"A682\",\n",
            "                        \"A684\",\n",
            "                        \"A685\",\n",
            "                        \"A687\",\n",
            "                        \"A686\",\n",
            "                        \"A688\",\n",
            "                        \"A65Y\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 S25 엣지\",\n",
            "                    \"item_id\": [\n",
            "                        \"A6DG\",\n",
            "                        \"A6DH\",\n",
            "                        \"A6DF\",\n",
            "                        \"A6AS\",\n",
            "                        \"A6DK\",\n",
            "                        \"A6DJ\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"갤럭시 워치\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"TEST 갤럭시 워치\",\n",
            "                    \"item_id\": [\n",
            "                        \"A59J\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 워치5 프로\",\n",
            "                    \"item_id\": [\n",
            "                        \"A458\",\n",
            "                        \"A463\",\n",
            "                        \"A462\",\n",
            "                        \"A465\",\n",
            "                        \"A464\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 워치 울트라\",\n",
            "                    \"item_id\": [\n",
            "                        \"A5RY\",\n",
            "                        \"A6MP\",\n",
            "                        \"A6M9\",\n",
            "                        \"A6MR\",\n",
            "                        \"A5RW\",\n",
            "                        \"A5RZ\",\n",
            "                        \"A6ML\",\n",
            "                        \"A5RX\",\n",
            "                        \"A6MN\",\n",
            "                        \"A6MQ\",\n",
            "                        \"A6MM\",\n",
            "                        \"A5LF\",\n",
            "                        \"A5RV\",\n",
            "                        \"A6M8\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"(LGU)갤럭시 워치 울트라\",\n",
            "                    \"item_id\": [\n",
            "                        \"A5LR\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"갤럭시 워치8 44MM\",\n",
            "                    \"item_id\": [\n",
            "                        \"A6KN\",\n",
            "                        \"A6MF\",\n",
            "                        \"A6ME\",\n",
            "                        \"A6MG\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"삼성 노트북 (갤럭시북GO)\",\n",
            "            \"item_in_voca\": []\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"포켓몬 에디션 키즈폰\",\n",
            "            \"item_in_voca\": []\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"SKT B tv air\",\n",
            "            \"item_in_voca\": []\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"SK쉴더스(ADT캡스)\",\n",
            "            \"item_in_voca\": []\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"인터넷+IPTV\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"인터넷D/U\",\n",
            "                    \"item_id\": [\n",
            "                        \"NI00000038\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"인터넷접속\",\n",
            "                    \"item_id\": [\n",
            "                        \"NB00000201\",\n",
            "                        \"NJ00000207\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_name_in_voca\": \"인터넷\",\n",
            "                    \"item_id\": [\n",
            "                        \"NI00000861\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"channel\": [\n",
            "        {\n",
            "            \"type\": \"대리점\",\n",
            "            \"value\": \"SK텔레콤 와이즈 태평점\",\n",
            "            \"action\": \"방문\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"070-7865-8708\",\n",
            "            \"action\": \"문의\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"https://tworldfriends.co.kr/D136650122\",\n",
            "            \"action\": \"추가 정보\"\n",
            "        }\n",
            "    ],\n",
            "    \"pgm\": [\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Sales]상품및부가서비스가입유도_단말\",\n",
            "            \"pgm_id\": \"2019SCTDH04\"\n",
            "        },\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Care]사용법/혜택/이벤트안내_단말\",\n",
            "            \"pgm_id\": \"#2025ABCD01\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "=============================================claude sonnet (fuzzy)=============================================\n",
            "\n",
            "{\n",
            "    \"title\": \"갤럭시 S23 출시 기념 특별 이벤트 및 SKT 다양한 혜택 안내\",\n",
            "    \"purpose\": [\n",
            "        \"상품 가입 유도\",\n",
            "        \"대리점/매장 방문 유도\",\n",
            "        \"혜택 안내\",\n",
            "        \"이벤트 응모 유도\"\n",
            "    ],\n",
            "    \"product\": [\n",
            "        {\n",
            "            \"item_name_in_msg\": \"갤럭시 S23\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 A23\",\n",
            "                    \"item_id\": [\n",
            "                        \"A3X1\",\n",
            "                        \"A3VX\",\n",
            "                        \"A3VU\",\n",
            "                        \"A3WY\",\n",
            "                        \"A3X2\",\n",
            "                        \"A3WZ\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S21\",\n",
            "                    \"item_id\": [\n",
            "                        \"A2VY\",\n",
            "                        \"A2VU\",\n",
            "                        \"A2SP\",\n",
            "                        \"A2VZ\",\n",
            "                        \"A2VV\",\n",
            "                        \"A2VW\",\n",
            "                        \"A2VX\",\n",
            "                        \"A2W1\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S22\",\n",
            "                    \"item_id\": [\n",
            "                        \"A47T\",\n",
            "                        \"A3UV\",\n",
            "                        \"A3UU\",\n",
            "                        \"A3US\",\n",
            "                        \"A47S\",\n",
            "                        \"A3UW\",\n",
            "                        \"A3UX\",\n",
            "                        \"A3T3\",\n",
            "                        \"A3UT\",\n",
            "                        \"A3UY\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S25\",\n",
            "                    \"item_id\": [\n",
            "                        \"A683\",\n",
            "                        \"A682\",\n",
            "                        \"A684\",\n",
            "                        \"A685\",\n",
            "                        \"A687\",\n",
            "                        \"A686\",\n",
            "                        \"A688\",\n",
            "                        \"A65Y\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시S2\",\n",
            "                    \"item_id\": [\n",
            "                        \"SSPG\",\n",
            "                        \"SSUS\",\n",
            "                        \"SSN7\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시S3\",\n",
            "                    \"item_id\": [\n",
            "                        \"SSO1\",\n",
            "                        \"SSP1\",\n",
            "                        \"SSAO\",\n",
            "                        \"SSI1\",\n",
            "                        \"SSEO\",\n",
            "                        \"SSBO\",\n",
            "                        \"SSOO\",\n",
            "                        \"SSON\",\n",
            "                        \"SSGO\",\n",
            "                        \"SSFO\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S23 FE\",\n",
            "                    \"item_id\": [\n",
            "                        \"A5GX\",\n",
            "                        \"A5GZ\",\n",
            "                        \"A5GY\",\n",
            "                        \"A5H2\",\n",
            "                        \"A5H4\",\n",
            "                        \"A5FW\",\n",
            "                        \"A5H1\",\n",
            "                        \"A5H3\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S21+\",\n",
            "                    \"item_id\": [\n",
            "                        \"A2W3\",\n",
            "                        \"A2W8\",\n",
            "                        \"A2W6\",\n",
            "                        \"A2W2\",\n",
            "                        \"A2W7\",\n",
            "                        \"A2SQ\",\n",
            "                        \"A2W5\",\n",
            "                        \"A2W4\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S22+\",\n",
            "                    \"item_id\": [\n",
            "                        \"A3V2\",\n",
            "                        \"A3T4\",\n",
            "                        \"A3UZ\",\n",
            "                        \"A3V6\",\n",
            "                        \"A3V3\",\n",
            "                        \"A3V1\",\n",
            "                        \"A3V5\",\n",
            "                        \"A3V4\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 S25+\",\n",
            "                    \"item_id\": [\n",
            "                        \"A65Z\",\n",
            "                        \"A68M\",\n",
            "                        \"A68N\",\n",
            "                        \"A68L\",\n",
            "                        \"A68J\",\n",
            "                        \"A68P\",\n",
            "                        \"A68K\",\n",
            "                        \"A68Q\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 TAB S2\",\n",
            "                    \"item_id\": [\n",
            "                        \"SS42\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 북S\",\n",
            "                    \"item_id\": [\n",
            "                        \"A1ST\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시탭S3\",\n",
            "                    \"item_id\": [\n",
            "                        \"A00S\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        },\n",
            "        {\n",
            "            \"item_name_in_msg\": \"갤럭시 워치\",\n",
            "            \"item_in_voca\": [\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 워치 울트라\",\n",
            "                    \"item_id\": [\n",
            "                        \"A5RY\",\n",
            "                        \"A6MP\",\n",
            "                        \"A6M9\",\n",
            "                        \"A6MR\",\n",
            "                        \"A5RW\",\n",
            "                        \"A5RZ\",\n",
            "                        \"A6ML\",\n",
            "                        \"A5RX\",\n",
            "                        \"A6MN\",\n",
            "                        \"A6MQ\",\n",
            "                        \"A6MM\",\n",
            "                        \"A5LF\",\n",
            "                        \"A5RV\",\n",
            "                        \"A6M8\"\n",
            "                    ]\n",
            "                },\n",
            "                {\n",
            "                    \"item_nm\": \"갤럭시 워치5 프로\",\n",
            "                    \"item_id\": [\n",
            "                        \"A458\",\n",
            "                        \"A463\",\n",
            "                        \"A462\",\n",
            "                        \"A465\",\n",
            "                        \"A464\"\n",
            "                    ]\n",
            "                }\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"channel\": [\n",
            "        {\n",
            "            \"type\": \"대리점\",\n",
            "            \"value\": \"SK텔레콤 와이즈 태평점\",\n",
            "            \"action\": \"방문\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"070-7865-8708\",\n",
            "            \"action\": \"문의\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"https://tworldfriends.co.kr/D136650122\",\n",
            "            \"action\": \"추가 정보\"\n",
            "        }\n",
            "    ],\n",
            "    \"pgm\": [\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Sales]상품및부가서비스가입유도_단말\",\n",
            "            \"pgm_id\": \"2019SCTDH04\"\n",
            "        },\n",
            "        {\n",
            "            \"pgm_nm\": \"[마케팅_Care]사용법/혜택/이벤트안내_단말\",\n",
            "            \"pgm_id\": \"#2025ABCD01\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "schema_prd = {\n",
        "    \"title\": {\n",
        "        \"type\": \"string\", \n",
        "        'description': '광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성'\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점/매장 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 수신 거부 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array',\n",
        "        'items': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "            'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름'},\n",
        "            # 'category': {'type': 'string', 'description': '광고 상품의 카테고리. [요금제, 부가서비스, 구독서비스, 제휴서비스, 모바일 단말기, 기타] 중에서 선택'},\n",
        "            'position': {'type': 'string', 'description': '광고 상품의 분류. [main, sub] 중에서 선택'},\n",
        "            'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                # 'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'pgm':{\n",
        "        'type': 'array', \n",
        "        'description': '아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공'\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "schema_prompt = f\"\"\"\n",
        "    아래와 같은 스키마로 결과를 제공해 주세요.\n",
        "\n",
        "    {schema_prd}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "for test_text in mms_pdf.sample(10)['msg'].tolist():\n",
        "    print(f\"Test text: {test_text.strip()}\")\n",
        "    msg = test_text.strip()\n",
        "\n",
        "    mms_embedding = emb_model.encode([msg.lower()], convert_to_tensor=True)\n",
        "\n",
        "    similarities = torch.nn.functional.cosine_similarity(\n",
        "        mms_embedding,  \n",
        "        clue_embeddings,  \n",
        "        dim=1 \n",
        "    ).cpu().numpy()\n",
        "\n",
        "    pgm_pdf_tmp = pgm_pdf.copy()\n",
        "    pgm_pdf_tmp['sim'] = similarities\n",
        "\n",
        "    pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)\n",
        "\n",
        "    pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "    rag_context = f\"\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    아래 광고 메시지에서 광고 목적과 상품 이름을 추출해 주세요.\n",
        "\n",
        "    ### 광고 메시지 ###\n",
        "    {msg}\n",
        "\n",
        "    ### 추출 작업 순서 ###\n",
        "    1. 광고 목적을 먼저 파악한다.\n",
        "    2. 파악된 목적에 기반하여 Main 상품을 추출한다.\n",
        "    3. 추출한 Main 상품에 관련되는 Sub 상품을 추출한다.\n",
        "    4. 추출된 상품 정보를 고려하여 채널 정보를 제공한다.\n",
        "\n",
        "    ### 추출 작업 가이드 ###\n",
        "    * 상품 추출시 정확도(precision) 보다는 재현율(recall)에 중심을 두어라.\n",
        "    * 광고 목적에 대리점 방문이 포함되어 있으면 대리점 채널 정보를 제공해라.\n",
        "\n",
        "\n",
        "    {schema_prompt}\n",
        "\n",
        "    {rag_context}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print()\n",
        "\n",
        "    # result_json_text = llm_cld40.invoke(prompt).content\n",
        "    result_json_text = llm_gem3.invoke(prompt).content\n",
        "\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    item_info_list = []\n",
        "    for item in json_objects['product']:\n",
        "\n",
        "        if item['name'] in stop_item_names:\n",
        "            continue\n",
        "\n",
        "        _embedding = emb_model.encode([item['name'].lower()], convert_to_tensor=True)\n",
        "\n",
        "        similarities = torch.nn.functional.cosine_similarity(\n",
        "            _embedding,  \n",
        "            item_embeddings,  \n",
        "            dim=1 \n",
        "        ).cpu().numpy()\n",
        "\n",
        "        item_pdf_tmp = item_pdf_all.copy()\n",
        "        item_pdf_tmp['sim'] = similarities.round(5)\n",
        "        item_pdf_tmp = item_pdf_tmp.drop_duplicates(subset=['item_nm','item_id']).query(\"sim>=0.8\")\n",
        "\n",
        "        item_pdf_tmp = item_pdf_tmp.groupby(['item_nm','sim'])['item_id'].apply(list).reset_index(name=\"item_id\").sort_values('sim', ascending=False)\n",
        "\n",
        "        sim_item_list = item_pdf_tmp[['item_nm','item_id']].rename(columns={'item_nm':'item_name_in_voca'}).head(5).to_dict(orient='records')\n",
        "\n",
        "        item_info_list.append({\"item_name_in_msg\":item['name'],\"item_in_voca\":sim_item_list})\n",
        "\n",
        "        # break\n",
        "\n",
        "    final_result = json_objects.copy()\n",
        "    final_result['product'] = item_info_list\n",
        "\n",
        "    if num_cand_pgms>0:\n",
        "        pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "        final_result['pgm'] = pgm_json\n",
        "\n",
        "    print(\"===\"*15+\"claude sonnet (emb)\"+\"===\"*15+\"\\n\")\n",
        "    print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "\n",
        "    matches = []\n",
        "    for item_name_message in json_objects['product']:\n",
        "        matches.extend([x for x in [{**e[1], 'entity_in_msg':item_name_message['name'], \"sim\":advanced_sequential_similarity(e[0], item_name_message['name'], metrics='difflib')} for e in entity_list_for_fuzzy] if x['sim']['difflib']>=0.75])\n",
        "\n",
        "    mdf = pd.DataFrame([p for p in matches if p['entity_in_msg'] in select_most_comprehensive([p['entity_in_msg'] for p in matches])]).drop_duplicates(['item_nm','item_id','entity_in_msg'])\n",
        "\n",
        "    if len(matches)>0:\n",
        "        mdf = mdf.query(\"item_nm_alias.str.lower() not in @stop_item_names or entity_in_msg.str.lower() not in @stop_item_names\")\n",
        "        mdf['sim'] = mdf['sim'].apply(lambda x: x['difflib'])\n",
        "\n",
        "    if mdf.shape[0]>0:\n",
        "        mdf['rank'] = mdf.groupby('entity_in_msg')['sim'].rank(method='dense',ascending=False)\n",
        "        mdf = mdf.query(\"rank<=5\")\n",
        "\n",
        "        product_tag = [{\"item_name_in_msg\":entity_in_msg, \"item_in_voca\":mdf.query(\"entity_in_msg==@entity_in_msg\").groupby(['item_nm','sim'])['item_id'].apply(list).reset_index(name=\"item_id\").sort_values('sim', ascending=False).drop('sim', axis=1).to_dict(orient='records')} for entity_in_msg in mdf['entity_in_msg'].unique()]\n",
        "\n",
        "        final_result = {\n",
        "            \"title\":json_objects['title'],\n",
        "            \"purpose\":json_objects['purpose'],\n",
        "            \"product\":product_tag,\n",
        "            \"channel\":json_objects['channel'],\n",
        "            \"pgm\":json_objects['pgm']\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        final_result = json_objects.copy()\n",
        "        final_result['product'] = [{'item_name_in_msg':d['name'], 'item_in_voca':[{'item_name_in_voca':d['name'], 'item_id': ['#']}]} for d in final_result['product']]\n",
        "\n",
        "    if num_cand_pgms>0:\n",
        "        pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "        final_result['pgm'] = pgm_json\n",
        "\n",
        "\n",
        "    print(\"===\"*15+\"claude sonnet (fuzzy)\"+\"===\"*15+\"\\n\")\n",
        "    print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}