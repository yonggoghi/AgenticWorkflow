# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor) - AI ê¸°ë°˜ ê´‘ê³  í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ
================================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ”
AI ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. LLM(Large Language Model)ì„ í™œìš©í•˜ì—¬ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ
ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì , ì—”í‹°í‹° ê´€ê³„ ë“±ì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
-----------
1. **ì—”í‹°í‹° ì¶”ì¶œ**: ìƒí’ˆëª…, ë¸Œëœë“œëª…, ì„œë¹„ìŠ¤ëª… ë“± í•µì‹¬ ì—”í‹°í‹° ì‹ë³„
2. **ì±„ë„ ë¶„ì„**: URL, ì „í™”ë²ˆí˜¸, ì•± ë§í¬ ë“± ê³ ê° ì ‘ì  ì±„ë„ ì¶”ì¶œ
3. **ëª©ì  ë¶„ë¥˜**: ê´‘ê³ ì˜ ì£¼ìš” ëª©ì  ë° ì•¡ì…˜ íƒ€ì… ë¶„ì„
4. **í”„ë¡œê·¸ë¨ ë§¤ì¹­**: ì‚¬ì „ ì •ì˜ëœ í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”

ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­
--------------
- **ëª¨ë“ˆí™” ì„¤ê³„**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì™¸ë¶€ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ ìš©ì´ì„± ì¦ëŒ€
- **ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”**: LLM í˜¸ì¶œ ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ë¡œê¹… ë° ì‹¤í–‰ ì‹œê°„ ì¶”ì ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” ì§€ì›
- **ë°ì´í„° ê²€ì¦**: ì¶”ì¶œ ê²°ê³¼ì˜ í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ
- **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©

ğŸ—ï¸ ì•„í‚¤í…ì²˜
-----------
- **MMSExtractor**: ë©”ì¸ ì¶”ì¶œ ì—”ì§„ í´ë˜ìŠ¤
- **DataManager**: ë°ì´í„° ë¡œë”© ë° ê´€ë¦¬ ë‹´ë‹¹
- **LLMProcessor**: LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
- **EntityMatcher**: ì—”í‹°í‹° ë§¤ì¹­ ë° ìœ ì‚¬ë„ ê³„ì‚°
- **PromptModule**: ì™¸ë¶€í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

âš™ï¸ ì„¤ì • ë° í™˜ê²½
--------------
- Python 3.8+
- LangChain, OpenAI, Anthropic API ì§€ì›
- Oracle Database ì—°ë™ (ì„ íƒì‚¬í•­)
- GPU ê°€ì† (CUDA ì§€ì› ì‹œ)

ğŸ“Š ì„±ëŠ¥ ì§€í‘œ
-----------
- í‰ê·  ì²˜ë¦¬ ì‹œê°„: ~30ì´ˆ/ë©”ì‹œì§€
- ì •í™•ë„: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
- ì²˜ë¦¬ëŸ‰: ~120 ë©”ì‹œì§€/ì‹œê°„ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

ì‘ì„±ì: MMS ë¶„ì„íŒ€
ìµœì¢… ìˆ˜ì •: 2024-09
ë²„ì „: 2.0.0
"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
import warnings
from functools import wraps
from typing import List, Tuple, Union, Dict, Any, Optional
from abc import ABC, abstractmethod
import traceback
import json
import re
import ast
import glob
import os
import copy
import pandas as pd
import numpy as np
from langchain_core.prompts import PromptTemplate

# joblibê³¼ multiprocessing ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="joblib")
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")
warnings.filterwarnings("ignore", message=".*resource_tracker.*")
warnings.filterwarnings("ignore", message=".*leaked.*")
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle
from contextlib import contextmanager

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from rapidfuzz import fuzz, process
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
from .entity_dag_extractor import DAGParser, extract_dag

# í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from prompts import (
    build_extraction_prompt,
    enhance_prompt_for_retry,
    get_fallback_result,
    build_entity_extraction_prompt,
    DEFAULT_ENTITY_EXTRACTION_PROMPT,
    DETAILED_ENTITY_EXTRACTION_PROMPT,
    SIMPLE_ENTITY_EXTRACTION_PROMPT,
    HYBRID_DAG_EXTRACTION_PROMPT
    )



# Helpers ëª¨ë“ˆ ì„í¬íŠ¸
from utils import PromptManager

# Workflow ëª¨ë“ˆ ì„í¬íŠ¸
from .workflow_core import WorkflowEngine, WorkflowState
from .mms_workflow_steps import (
    InputValidationStep,
    EntityExtractionStep,
    ProgramClassificationStep,
    ContextPreparationStep,
    LLMExtractionStep,
    ResponseParsingStep,
    ResultConstructionStep,
    ValidationStep,
    DAGExtractionStep
)
from services.entity_recognizer import EntityRecognizer
from services.program_classifier import ProgramClassifier
from services.store_matcher import StoreMatcher
from services.result_builder import ResultBuilder
from core.mms_extractor_data import MMSExtractorDataMixin


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸
from utils import (
    select_most_comprehensive,
    log_performance,
    safe_execute,
    validate_text_input,
    safe_check_empty,
    dataframe_to_markdown_prompt,
    extract_json_objects,
    preprocess_text,
    fuzzy_similarities,
    get_fuzzy_similarities,
    parallel_fuzzy_similarity,
    longest_common_subsequence_ratio,
    sequence_matcher_similarity,
    substring_aware_similarity,
    token_sequence_similarity,
    combined_sequence_similarity,
    calculate_seq_similarity,
    parallel_seq_similarity,
    load_sentence_transformer,
    Token,
)

# Database utilities ì„í¬íŠ¸
from utils.db_utils import (
    get_database_connection,
    database_connection,
    load_program_from_database,
    load_org_from_database
)

from utils import (
    Sentence,
    filter_text_by_exc_patterns,
    filter_specific_terms,
    convert_df_to_json_list,
    create_dag_diagram,
    sha256_hash,
    replace_special_chars_with_space,
    extract_ngram_candidates
)

# ì„¤ì • ë° ì˜ì¡´ì„± ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë¡œê¹… ì„¤ì • - api.pyì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê¸°ë³¸ ì„¤ì • ì ìš©
logger = logging.getLogger(__name__)

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë¡œê¹… ì„¤ì • (api.pyì—ì„œ ì„í¬íŠ¸ë  ë•ŒëŠ” api.pyì˜ ì„¤ì • ì‚¬ìš©)
if __name__ == '__main__':
    import sys
    from pathlib import Path
    
    # MongoDB ìœ í‹¸ë¦¬í‹°ëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'mms_extractor.log'),
            logging.StreamHandler()
        ]
    )

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ì¶”ìƒ í´ë˜ìŠ¤ ë° ì „ëµ íŒ¨í„´ =====

class EntityExtractionStrategy(ABC):
    """ì—”í‹°í‹° ì¶”ì¶œ ì „ëµ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def extract(self, text: str, **kwargs) -> pd.DataFrame:
        """ì—”í‹°í‹° ì¶”ì¶œ ë©”ì†Œë“œ"""
        pass

class DataLoader(ABC):
    """ë°ì´í„° ë¡œë” ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def load_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë“œ ë©”ì†Œë“œ"""
        pass

# ===== ê°œì„ ëœ MMSExtractor í´ë˜ìŠ¤ =====

class MMSExtractor(MMSExtractorDataMixin):
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ AI ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì¶”ì¶œ ì—”ì§„
    ================================================================
    
    ğŸ¨ ê°œìš”
    -------
    ì´ í´ë˜ìŠ¤ëŠ” MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.
    LLM(Large Language Model), ì„ë² ë”© ëª¨ë¸, NLP ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬
    ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
    -----------
    1. **ë‹¤ë‹¨ê³„ ì—”í‹°í‹° ì¶”ì¶œ**: Kiwi NLP + ì„ë² ë”© ìœ ì‚¬ë„ + LLM ê¸°ë°˜ ì¶”ì¶œ
    2. **ì§€ëŠ¥í˜• í”„ë¡œê·¸ë¨ ë¶„ë¥˜**: ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ë§¤ì¹­
    3. **RAG ê¸°ë°˜ ì»¬í…ìŠ¤íŠ¸ ì¦ê°•**: ê´€ë ¨ ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•ë„ í–¥ìƒ
    4. **ë‹¤ì¤‘ LLM ì§€ì›**: OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
    5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    
    ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•
    -----------
    - **ì •í™•ë„**: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
    - **ì²˜ë¦¬ ì†ë„**: í‰ê·  30ì´ˆ/ë©”ì‹œì§€
    - **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ìš©ì´
    - **ì•ˆì •ì„±**: ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    
    âš™ï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
    --------------
    - **ì•„í‚¤í…ì²˜ ëª¨ë“ˆí™”**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
    - **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
    - **ë‹¤ì¸µ ì˜ˆì™¸ ì²˜ë¦¬**: LLM API ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
    - **ìƒì„¸ ë¡œê¹…**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…, ê°ì‚¬ ë¡œê·¸ë¥¼ ìœ„í•œ í¬ê´„ì  ë¡œê¹… ì‹œìŠ¤í…œ
    - **ë°ì´í„° ê²€ì¦**: ì…ë ¥/ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ê²€ì¦
    - **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©
    
    ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    -----------
    ```python
    # ê¸°ë³¸ ì´ˆê¸°í™”
    extractor = MMSExtractor(
        llm_model='ax',
        entity_extraction_mode='llm',
        extract_entity_dag=True
    )
    
    # ë©”ì‹œì§€ ì²˜ë¦¬
    result = extractor.process_message("ìƒ˜í”Œ MMS í…ìŠ¤íŠ¸")
    
    # ê²°ê³¼ í™œìš©
    products = result['product']
    channels = result['channel']
    entity_dag = result.get('entity_dag', [])
    ```
    
    ğŸ’¼ ì˜ì¡´ì„±
    ---------
    - LangChain (LLM ì¸í„°í˜ì´ìŠ¤)
    - SentenceTransformers (ì„ë² ë”©)
    - KiwiPiePy (NLP)
    - cx_Oracle (ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™)
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='ax', 
                 entity_llm_model='ax', extract_entity_dag=False):
        """
        MMSExtractor ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        ì‹œìŠ¤í…œì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:
        - LLM ëª¨ë¸ ì„¤ì • ë° ì—°ê²°
        - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        - NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™”
        - ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ (CSV/DB)
        - ê°ì¢… ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
        
        Args:
            model_path (str, optional): ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ. ê¸°ë³¸ê°’: 'jhgan/ko-sroberta-multitask'
            data_dir (str, optional): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ. ê¸°ë³¸ê°’: './data/'
            product_info_extraction_mode (str, optional): ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ ('nlp' ë˜ëŠ” 'llm')
            entity_extraction_mode (str, optional): ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ ('nlp', 'llm', 'hybrid')
            offer_info_data_src (str, optional): ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ('local' ë˜ëŠ” 'db')
            llm_model (str, optional): ì‚¬ìš©í•  LLM ëª¨ë¸. ê¸°ë³¸ê°’: 'ax'
            extract_entity_dag (bool, optional): DAG ì¶”ì¶œ ì—¬ë¶€. ê¸°ë³¸ê°’: False
            
        Raises:
            Exception: ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜¤ë¥˜
            
        Example:
            >>> extractor = MMSExtractor(
            ...     llm_model='gpt-4',
            ...     entity_extraction_mode='hybrid',
            ...     extract_entity_dag=True
            ... )
        """
        logger.info("ğŸš€ MMSExtractor ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
            logger.info("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì ìš© ì¤‘...")
            self._set_default_config(model_path, data_dir, product_info_extraction_mode, 
                                   entity_extraction_mode, offer_info_data_src, llm_model, entity_llm_model, extract_entity_dag)
            
            # 2ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±)
            logger.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
            load_dotenv()
            
            # 3ë‹¨ê³„: ì£¼ìš” êµ¬ì„± ìš”ì†Œë“¤ ìˆœì°¨ ì´ˆê¸°í™”
            logger.info("ğŸ’» ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...")
            self._initialize_device()
            
            logger.info("ğŸ¤– LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self._initialize_llm()
            
            logger.info("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self._initialize_embedding_model()
            
            logger.info("ğŸ“ NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™” ì¤‘...")
            self._initialize_kiwi()
            
            logger.info("ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
            self._load_data()
            
            # Initialize Services
            logger.info("ğŸ› ï¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
            self.entity_recognizer = EntityRecognizer(
                self.kiwi, 
                self.item_pdf_all, 
                self.stop_item_names, 
                self.llm_model, 
                self.alias_pdf_raw,
                self.entity_extraction_mode
            )
            self.program_classifier = ProgramClassifier(
                self.emb_model, 
                self.pgm_pdf, 
                self.clue_embeddings,
                self.num_cand_pgms
            )
            self.store_matcher = StoreMatcher(self.org_pdf)
            self.result_builder = ResultBuilder(
                self.entity_recognizer,
                self.store_matcher,
                self.alias_pdf_raw,
                self.stop_item_names,
                self.num_cand_pgms,
                self.entity_extraction_mode,
                self._initialize_multiple_llm_models,
                self.entity_llm_model_name
            )
            logger.info("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # Workflow ì—”ì§„ ì´ˆê¸°í™”
            logger.info("âš™ï¸ Workflow ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
            self.workflow_engine = WorkflowEngine("MMS Extraction Workflow")
            self.workflow_engine.add_step(InputValidationStep())
            self.workflow_engine.add_step(EntityExtractionStep(self.entity_recognizer))
            self.workflow_engine.add_step(ProgramClassificationStep(self.program_classifier))
            self.workflow_engine.add_step(ContextPreparationStep())
            self.workflow_engine.add_step(LLMExtractionStep())
            self.workflow_engine.add_step(ResponseParsingStep())
            self.workflow_engine.add_step(ResultConstructionStep(self.result_builder))
            self.workflow_engine.add_step(ValidationStep())
            
            # DAG ì¶”ì¶œ ë‹¨ê³„ëŠ” í”Œë˜ê·¸ê°€ í™œì„±í™”ëœ ê²½ìš°ë§Œ ë“±ë¡
            if self.extract_entity_dag:
                self.workflow_engine.add_step(DAGExtractionStep())
                logger.info("ğŸ¯ DAG ì¶”ì¶œ ë‹¨ê³„ ë“±ë¡ë¨")
            
            logger.info(f"âœ… Workflow ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ ({len(self.workflow_engine.steps)}ê°œ ë‹¨ê³„)")
            
            logger.info("âœ… MMSExtractor ì´ˆê¸°í™” ì™„ë£Œ")

            
        except Exception as e:
            logger.error(f"âŒ MMSExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise

    def _set_default_config(self, model_path, data_dir, product_info_extraction_mode, 
                          entity_extraction_mode, offer_info_data_src, llm_model, entity_llm_model, extract_entity_dag):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì ìš©"""
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else getattr(EMBEDDING_CONFIG, 'ko_sbert_model_path', 'jhgan/ko-sroberta-multitask')
        self.offer_info_data_src = offer_info_data_src
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'product_info_extraction_mode', 'nlp')
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'entity_extraction_mode', 'llm')
        self.llm_model_name = llm_model
        self.entity_llm_model_name = entity_llm_model
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)
        self.extract_entity_dag = extract_entity_dag
        
        # DAG ì¶”ì¶œ ì„¤ì • ë¡œê¹…
        # extract_entity_dag: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ DAG(Directed Acyclic Graph)ë¡œ ì¶”ì¶œ
        # Trueì¸ ê²½ìš° ì¶”ê°€ì ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹° ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ 
        # NetworkX + Graphvizë¥¼ í†µí•´ ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±
        if self.extract_entity_dag:
            logger.info("ğŸ¯ DAG ì¶”ì¶œ ëª¨ë“œ í™œì„±í™”ë¨")
        else:
            logger.info("ğŸ“‹ í‘œì¤€ ì¶”ì¶œ ëª¨ë“œ (DAG ë¹„í™œì„±í™”)")

    @log_performance
    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    @log_performance
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì„¤ì • ë§¤í•‘
            model_mapping = {
                "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "gem": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),  # 'gem'ì€ 'gemma'ì˜ ì¤„ì„ë§
                "ax": getattr(MODEL_CONFIG, 'ax_model', 'ax-4'),
                "claude": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),
                "cld": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),  # 'cld'ëŠ” 'claude'ì˜ ì¤„ì„ë§
                "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),  # 'gen'ì€ 'gemini'ì˜ ì¤„ì„ë§
                "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'gpt-4')
            }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'gemini-pro'))
            
            # LLM ëª¨ë¸ë³„ ì¼ê´€ì„± ì„¤ì •
            model_kwargs = {
                "temperature": 0.0,  # ì™„ì „ ê²°ì •ì  ì¶œë ¥ì„ ìœ„í•´ 0.0 ê³ ì •
                "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ê°•í™”
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42  # ê³ ì • ì‹œë“œë¡œ ì¼ê´€ì„± ë³´ì¥
                
            self.llm_model = ChatOpenAI(**model_kwargs)
            
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ë¹„í™œì„±í™” ì˜µì…˜ í™•ì¸
        if MODEL_CONFIG.disable_embedding:
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¹„í™œì„±í™” ëª¨ë“œ (DISABLE_EMBEDDING=true)")
            self.emb_model = None
            return
            
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            try:
                self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)
            except Exception as e2:
                logger.error(f"Fallback ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                logger.warning("ì„ë² ë”© ëª¨ë¸ ì—†ì´ ë™ì‘ ëª¨ë“œë¡œ ì „í™˜")
                self.emb_model = None

    def _initialize_multiple_llm_models(self, model_names: List[str]) -> List:
        """
        ë³µìˆ˜ì˜ LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ
        
        Args:
            model_names (List[str]): ì´ˆê¸°í™”í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['ax', 'gpt', 'gen'])
            
        Returns:
            List: ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        llm_models = []
        
        # ëª¨ë¸ëª… ë§¤í•‘ (ê¸°ì¡´ LLM ì´ˆê¸°í™” ë¡œì§ê³¼ ë™ì¼)
        model_mapping = {
            "cld": getattr(MODEL_CONFIG, 'anthropic_model', 'amazon/anthropic/claude-sonnet-4-20250514'),
            "ax": getattr(MODEL_CONFIG, 'ax_model', 'skt/ax4'),
            "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'azure/openai/gpt-4o-2024-08-06'),
            "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gcp/gemini-2.5-flash')
        }
        
        for model_name in model_names:
            try:
                actual_model_name = model_mapping.get(model_name, model_name)
                
                # ëª¨ë¸ë³„ ì„¤ì • (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
                model_kwargs = {
                    "temperature": 0.0,
                    "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                    "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                    "model": actual_model_name,
                    "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000),
                    "seed": getattr(MODEL_CONFIG, 'llm_seed', 42)
                }
                
                llm_model = ChatOpenAI(**model_kwargs)
                llm_models.append(llm_model)
                logger.info(f"âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name} ({actual_model_name})")
                
            except Exception as e:
                logger.error(f"âŒ LLM ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {model_name} - {e}")
                continue
        
        return llm_models

    @log_performance
    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _load_data(self):
        """í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ëª¨ë“œ: {self.offer_info_data_src}")
            
            # ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ (ë³„ì¹­ ê·œì¹™ ì ìš© í¬í•¨)
            logger.info("1ï¸âƒ£ ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì¤‘...")
            self._load_and_prepare_item_data()
            logger.info(f"ìƒí’ˆ ì •ë³´ ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì •ì§€ì–´ ë¡œë“œ
            logger.info("2ï¸âƒ£ ì •ì§€ì–´ ë¡œë“œ ì¤‘...")
            self._load_stop_words()
            logger.info(f"ë¡œë“œëœ ì •ì§€ì–´ ìˆ˜: {len(self.stop_item_names)}ê°œ")
            
            # Kiwiì— ìƒí’ˆëª… ë“±ë¡
            logger.info("3ï¸âƒ£ Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì¤‘...")
            self._register_items_to_kiwi()
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ
            logger.info("4ï¸âƒ£ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_program_data()
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì¡°ì§ ì •ë³´ ë¡œë“œ
            logger.info("5ï¸âƒ£ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_organization_data()
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
            
            # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ - ìµœì¢… ìƒíƒœ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ìƒí’ˆ ë°ì´í„°: {self.item_pdf_all.shape}")
            logger.info(f"âœ… í”„ë¡œê·¸ë¨ ë°ì´í„°: {self.pgm_pdf.shape}")
            logger.info(f"âœ… ì¡°ì§ ë°ì´í„°: {self.org_pdf.shape}")
            logger.info(f"âœ… ì •ì§€ì–´: {len(self.stop_item_names)}ê°œ")
            
            # ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒíƒœ ë¹„êµë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´
            if hasattr(self, 'item_pdf_all') and not self.item_pdf_all.empty:
                logger.info("=== ìƒí’ˆ ë°ì´í„° ìƒì„¸ ì •ë³´ ===")
                if 'item_nm' in self.item_pdf_all.columns:
                    unique_items = self.item_pdf_all['item_nm'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆëª… ìˆ˜: {unique_items}ê°œ")
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    unique_aliases = self.item_pdf_all['item_nm_alias'].nunique()
                    logger.info(f"ê³ ìœ  ë³„ì¹­ ìˆ˜: {unique_aliases}ê°œ")
                if 'item_id' in self.item_pdf_all.columns:
                    unique_ids = self.item_pdf_all['item_id'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆID ìˆ˜: {unique_ids}ê°œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_and_prepare_item_data(self):
        """ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ (ipynb ì½”ë“œ ê¸°ì¤€ìœ¼ë¡œ í†µí•©)"""
        try:
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            # ===== 1ë‹¨ê³„: ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ =====
            if self.offer_info_data_src == "local":
                logger.info("ğŸ“ ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ")
                csv_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/items.csv')
                item_pdf_raw = pd.read_csv(csv_path)
            elif self.offer_info_data_src == "db":
                logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ")
                with self._database_connection() as conn:
                    sql = "SELECT * FROM TCAM_RC_OFER_MST"
                    item_pdf_raw = pd.read_sql(sql, conn)
            
            logger.info(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {item_pdf_raw.shape}")
            
            # ===== 2ë‹¨ê³„: ê³µí†µ ì „ì²˜ë¦¬ (ë°ì´í„° ì†ŒìŠ¤ ë¬´ê´€) =====
            # ITEM_DESCë¥¼ strë¡œ ë³€í™˜
            item_pdf_raw['ITEM_DESC'] = item_pdf_raw['ITEM_DESC'].astype('str')
            
            # ë‹¨ë§ê¸°ì¸ ê²½ìš° ì„¤ëª…ì„ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©
            item_pdf_raw['ITEM_NM'] = item_pdf_raw.apply(
                lambda x: x['ITEM_DESC'] if x['ITEM_DMN']=='E' else x['ITEM_NM'], axis=1
            )
            
            # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
            item_pdf_all = item_pdf_raw.rename(columns={c: c.lower() for c in item_pdf_raw.columns})
            logger.info(f"ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")
            
            # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
            item_pdf_all['item_ctg'] = None
            item_pdf_all['item_emb_vec'] = None
            item_pdf_all['ofer_cd'] = item_pdf_all['item_id']
            item_pdf_all['oper_dt_hms'] = '20250101000000'
            
            # ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ í•„í„°ë§
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
            if excluded_domains:
                before_filter = len(item_pdf_all)
                item_pdf_all = item_pdf_all.query("item_dmn not in @excluded_domains")
                logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter} -> {len(item_pdf_all)}")
            
            # ===== 3ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì²˜ë¦¬ (ë°ì´í„° ì†ŒìŠ¤ ë¬´ê´€) =====
            logger.info("ğŸ”— ë³„ì¹­ ê·œì¹™ ë¡œë“œ ì¤‘...")
            self.alias_pdf_raw = pd.read_csv(getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv'))
            alias_pdf = self.alias_pdf_raw.copy()
            alias_pdf['alias_1'] = alias_pdf['alias_1'].str.split("&&")
            alias_pdf['alias_2'] = alias_pdf['alias_2'].str.split("&&")
            alias_pdf = alias_pdf.explode('alias_1')
            alias_pdf = alias_pdf.explode('alias_2')
            
            # build íƒ€ì… ë³„ì¹­ í™•ì¥
            alias_list_ext = alias_pdf.query("type=='build'")[['alias_1','category','direction','type']].to_dict('records')
            for alias in alias_list_ext:
                adf = item_pdf_all.query(
                    "item_nm.str.contains(@alias['alias_1']) and item_dmn==@alias['category']"
                )[['item_nm','item_desc','item_dmn']].rename(
                    columns={'item_nm':'alias_2','item_desc':'description','item_dmn':'category'}
                ).drop_duplicates()
                adf['alias_1'] = alias['alias_1']
                adf['direction'] = alias['direction']
                adf['type'] = alias['type']
                adf = adf[alias_pdf.columns]
                alias_pdf = pd.concat([alias_pdf.query(f"alias_1!='{alias['alias_1']}'"), adf])
            
            alias_pdf = alias_pdf.drop_duplicates()
            
            # ì–‘ë°©í–¥(B) ë³„ì¹­ ì¶”ê°€
            alias_pdf = pd.concat([
                alias_pdf, 
                alias_pdf.query("direction=='B'").rename(
                    columns={'alias_1':'alias_2', 'alias_2':'alias_1'}
                )[alias_pdf.columns]
            ])
            
            alias_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2'], alias_pdf['type']))
            logger.info(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")
            
            # ===== 4ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬) =====
            def apply_alias_rule_cascade_parallel(args_dict):
                """ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš©"""
                item_nm = args_dict['item_nm']
                max_depth = args_dict['max_depth']
                
                processed = set()
                result_dict = {item_nm: '#' * len(item_nm)}
                to_process = [(item_nm, 0, frozenset())]
                
                while to_process:
                    current_item, depth, path_applied_rules = to_process.pop(0)
                    
                    if depth >= max_depth or current_item in processed:
                        continue
                    
                    processed.add(current_item)
                    
                    for r in alias_rule_set:
                        alias_from, alias_to, alias_type = r[0], r[1], r[2]
                        rule_key = (alias_from, alias_to, alias_type)
                        
                        if rule_key in path_applied_rules:
                            continue
                        
                        # íƒ€ì…ì— ë”°ë¥¸ ë§¤ì¹­
                        if alias_type == 'exact':
                            matched = (current_item == alias_from)
                        else:
                            matched = (alias_from in current_item)
                        
                        if matched:
                            new_item = alias_to.strip() if alias_type == 'exact' else current_item.replace(alias_from.strip(), alias_to.strip())
                            
                            if new_item not in result_dict:
                                result_dict[new_item] = alias_from.strip()
                                to_process.append((new_item, depth + 1, path_applied_rules | {rule_key}))
                
                item_nm_list = [{'item_nm': k, 'item_nm_alias': v} for k, v in result_dict.items()]
                adf = pd.DataFrame(item_nm_list)
                selected_alias = select_most_comprehensive(adf['item_nm_alias'].tolist())
                result_aliases = list(adf.query("item_nm_alias in @selected_alias")['item_nm'].unique())
                
                if item_nm not in result_aliases:
                    result_aliases.append(item_nm)
                
                return {'item_nm': item_nm, 'item_nm_alias': result_aliases}
            
            def parallel_alias_rule_cascade(texts, max_depth=5, n_jobs=None):
                """ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš©"""
                if n_jobs is None:
                    n_jobs = min(os.cpu_count()-1, 4)
                
                batches = [{"item_nm": text, "max_depth": max_depth} for text in texts]
                with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                    batch_results = parallel(delayed(apply_alias_rule_cascade_parallel)(args) for args in batches)
                
                return pd.DataFrame(batch_results)
            
            logger.info("ğŸ”„ ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© ì¤‘...")
            item_alias_pdf = parallel_alias_rule_cascade(item_pdf_all['item_nm'], max_depth=3)
            
            # ë³„ì¹­ ë³‘í•© ë° explode
            item_pdf_all = item_pdf_all.merge(item_alias_pdf, on='item_nm', how='left')
            before_explode = len(item_pdf_all)
            item_pdf_all = item_pdf_all.explode('item_nm_alias').drop_duplicates()
            logger.info(f"ë³„ì¹­ explode: {before_explode} -> {len(item_pdf_all)}")
            
            # ===== 5ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€ =====
            user_defined_entity = ['AIA Vitality', 'ë¶€ìŠ¤íŠ¸ íŒŒí¬ ê±´ëŒ€ì…êµ¬', 'Boost Park ê±´ëŒ€ì…êµ¬']
            item_pdf_ext = pd.DataFrame([{
                'item_nm': e, 'item_id': e, 'item_desc': e, 'item_dmn': 'user_defined',
                'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e
            } for e in user_defined_entity])
            item_pdf_all = pd.concat([item_pdf_all, item_pdf_ext])
            
            # ===== 6ë‹¨ê³„: item_dmn_nm ì»¬ëŸ¼ ì¶”ê°€ =====
            item_dmn_map = pd.DataFrame([
                {"item_dmn": 'P', 'item_dmn_nm': 'ìš”ê¸ˆì œ ë° ê´€ë ¨ ìƒí’ˆ'},
                {"item_dmn": 'E', 'item_dmn_nm': 'ë‹¨ë§ê¸°'},
                {"item_dmn": 'S', 'item_dmn_nm': 'êµ¬ë… ìƒí’ˆ'},
                {"item_dmn": 'C', 'item_dmn_nm': 'ì¿ í°'},
                {"item_dmn": 'X', 'item_dmn_nm': 'ê°€ìƒ ìƒí’ˆ'}
            ])
            item_pdf_all = item_pdf_all.merge(item_dmn_map, on='item_dmn', how='left')
            item_pdf_all['item_dmn_nm'] = item_pdf_all['item_dmn_nm'].fillna('ê¸°íƒ€')
            
            # ===== 7ë‹¨ê³„: TEST í•„í„°ë§ =====
            before_test = len(item_pdf_all)
            item_pdf_all = item_pdf_all.query("not item_nm_alias.str.contains('TEST', case=False, na=False)")
            logger.info(f"TEST í•„í„°ë§: {before_test} -> {len(item_pdf_all)}")
            
            self.item_pdf_all = item_pdf_all
            
            # ìµœì¢… í™•ì¸
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ì¤€ë¹„ ì™„ë£Œ ===")
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in self.item_pdf_all.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            else:
                logger.info("âœ… ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not self.item_pdf_all.empty:
                logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {self.item_pdf_all['item_nm'].dropna().head(3).tolist()}")
                logger.info(f"ë³„ì¹­ ìƒ˜í”Œ: {self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()}")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.item_pdf_all = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_nm_alias'])
            logger.warning("ë¹ˆ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")

    # Database methods moved to utils/db_utils.py

    def _load_stop_words(self):
        """ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ"""
        try:
            self.stop_item_names = pd.read_csv(getattr(METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv'))['stop_words'].to_list()
            logger.info(f"ì •ì§€ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.stop_item_names)}ê°œ")
        except Exception as e:
            logger.warning(f"ì •ì§€ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stop_item_names = []

    def _register_items_to_kiwi(self):
        """Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡"""
        try:
            logger.info("=== Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… ë³„ì¹­ ë°ì´í„° í™•ì¸
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                return
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë“±ë¡í•  ê³ ìœ  ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # nullì´ ì•„ë‹Œ ìœ íš¨í•œ ë³„ì¹­ë“¤ë§Œ í•„í„°ë§
            valid_aliases = [w for w in unique_aliases if isinstance(w, str) and len(w.strip()) > 0]
            logger.info(f"ìœ íš¨í•œ ë³„ì¹­ ìˆ˜: {len(valid_aliases)}ê°œ")
            
            if len(valid_aliases) > 0:
                sample_aliases = valid_aliases[:5]
                logger.info(f"ë“±ë¡í•  ë³„ì¹­ ìƒ˜í”Œ: {sample_aliases}")
            
            registered_count = 0
            failed_count = 0
            
            for w in valid_aliases:
                try:
                    self.kiwi.add_user_word(w, "NNP")
                    registered_count += 1
                except Exception as reg_error:
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ë¡œê¹…
                        logger.warning(f"Kiwi ë“±ë¡ ì‹¤íŒ¨ - '{w}': {reg_error}")
            
            logger.info(f"Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"Kiwi ìƒí’ˆëª… ë“±ë¡ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

    def _load_program_data(self):
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
        try:
            logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë”© ì‹œì‘...")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                self.pgm_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'pgm_info_path', './data/program_info.csv'))
                logger.info(f"ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                self.pgm_pdf = load_program_from_database()
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
            if not self.pgm_pdf.empty:
                logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
                clue_texts = self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                    lambda x: preprocess_text(x['pgm_nm'].lower()) + " " + x['clue_tag'].lower(), axis=1
                ).tolist()
                
                if self.emb_model is not None:
                    self.clue_embeddings = self.emb_model.encode(
                        clue_texts, convert_to_tensor=True, show_progress_bar=False
                    )
                else:
                    logger.warning("ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë¹ˆ tensor ì‚¬ìš©")
                    self.clue_embeddings = torch.empty((0, 768))
                
                logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ í”„ë¡œê·¸ë¨")
            else:
                logger.warning("í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.clue_embeddings = torch.tensor([])
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            self.clue_embeddings = torch.tensor([])

    def _load_organization_data(self):
        """ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'org_info_path', './data/org_info_all_250605.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                org_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ì¡°ì§ ë°ì´í„° í¬ê¸°: {org_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(org_pdf_raw.columns)}")
                
                # ITEM_DMN='R' ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§
                if 'ITEM_DMN' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("ITEM_DMN=='R'").copy()
                elif 'item_dmn' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("item_dmn=='R'").copy()
                else:
                    logger.warning("ITEM_DMN/item_dmn ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.org_pdf = org_pdf_raw.copy()
                
                # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë¦¬ë„¤ì„
                self.org_pdf = self.org_pdf.rename(columns={c: c.lower() for c in self.org_pdf.columns})
                
                logger.info(f"ë¡œì»¬ ëª¨ë“œ: ITEM_DMN='R' í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_org_from_database()
            
            # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
            if not self.org_pdf.empty:
                sample_orgs = self.org_pdf.head(3).to_dict('records')
                logger.info(f"ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ (3ê°œ í–‰): {sample_orgs}")
            
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.org_pdf)}ê°œ ì¡°ì§ ===")
            logger.info(f"ìµœì¢… ì¡°ì§ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: {list(self.org_pdf.columns)}")
            
            # ì¡°ì§ ë°ì´í„° ìµœì¢… ê²€ì¦
            if not self.org_pdf.empty:
                critical_org_columns = ['item_nm', 'item_id']
                missing_org_columns = [col for col in critical_org_columns if col not in self.org_pdf.columns]
                if missing_org_columns:
                    logger.error(f"ì¡°ì§ ë°ì´í„°ì—ì„œ ì¤‘ìš” ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_org_columns}")
                    logger.error("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    logger.info("ëª¨ë“  ì¤‘ìš” ì¡°ì§ ì»¬ëŸ¼ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¡°ì§/ë§¤ì¥ ì¶”ì¶œì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback (ì¡°ì§ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ ì¡°ì§ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")
            logger.warning("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    def _load_org_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ (ITEM_DMN='R')"""
        self.org_pdf = load_org_from_database()

    def _store_prompt_for_preview(self, prompt: str, prompt_type: str):
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ìš©ìœ¼ë¡œ ì €ì¥ (PromptManager ì‚¬ìš©)"""
        PromptManager.store_prompt_for_preview(prompt, prompt_type)


    def _safe_llm_invoke(self, prompt: str, max_retries: int = 3) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì†Œë“œ"""
        for attempt in range(max_retries):
            try:
                # LLM í˜¸ì¶œ
                response = self.llm_model.invoke(prompt)
                result_text = response.content if hasattr(response, 'content') else str(response)
                
                # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
                json_objects_list = extract_json_objects(result_text)
                if json_objects_list:
                    json_objects = json_objects_list[-1]
                    if self._detect_schema_response(json_objects):
                        logger.warning(f"ì‹œë„ {attempt + 1}: LLMì´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        
                        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µì¸ ê²½ìš° ë” ê°•í•œ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            enhanced_prompt = self._enhance_prompt_for_retry(prompt)
                            response = self.llm_model.invoke(enhanced_prompt)
                            result_text = response.content if hasattr(response, 'content') else str(response)
                
                return result_text
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_extraction(prompt)
                else:
                    logger.warning(f"LLM í˜¸ì¶œ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return ""

    def _enhance_prompt_for_retry(self, original_prompt: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”"""
        return enhance_prompt_for_retry(original_prompt)

    def _fallback_extraction(self, prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ fallback ì¶”ì¶œ ë¡œì§"""
        logger.info("Fallback ì¶”ì¶œ ë¡œì§ ì‹¤í–‰")
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì—ì„œ fallback ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        fallback_result = get_fallback_result()
        
        return json.dumps(fallback_result, ensure_ascii=False)


    # _extract_entities and _classify_programs removed (moved to services)

    def _build_extraction_prompt(self, msg: str, rag_context: str, product_element: Optional[List[Dict]]) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì‚¬ìš©"""
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©
        prompt = build_extraction_prompt(
            message=msg,
            rag_context=rag_context,
            product_element=product_element,
            product_info_extraction_mode=self.product_info_extraction_mode
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (LLM ëª¨ë“œì—ì„œë§Œ)
        if self.product_info_extraction_mode == 'llm':
            logger.debug(f"LLM ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í›„ë³´ ìƒí’ˆ ëª©ë¡ í¬í•¨ ì—¬ë¶€: {'ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡' in rag_context}")
            
        return prompt

    # _extract_channels removed (moved to ResultBuilder)

    # _match_store_info removed (moved to services)

    def _validate_extraction_result(self, result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'purpose', 'sales_script', 'product', 'channel', 'offer']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    if field == 'title':
                        result[field] = "ê´‘ê³  ë©”ì‹œì§€"
                    elif field == 'sales_script':
                        result[field] = ""
                    elif field == 'offer':
                        result[field] = {"type": "product", "value": []}
                    else:
                        result[field] = []

            # ì±„ë„ ì •ë³´ ê²€ì¦
            validated_channels = []
            for channel in result.get('channel', []):
                if isinstance(channel, dict) and channel.get('value'):
                    validated_channels.append(channel)
            
            result['channel'] = validated_channels
            
            # offer ì •ë³´ ê²€ì¦
            if not isinstance(result.get('offer'), dict):
                logger.warning("offer í•„ë“œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
                result['offer'] = {"type": "product", "value": []}
            elif 'type' not in result['offer'] or 'value' not in result['offer']:
                logger.warning("offer í•„ë“œì— type ë˜ëŠ” valueê°€ ì—†ìŒ, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
                result['offer'] = {"type": "product", "value": result.get('product', [])}

            return result
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return result

    @log_performance
    def process_message(self, message: str, message_id: str = '#') -> Dict[str, Any]:
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (Workflow ê¸°ë°˜)
        
        Args:
            message: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
        """
        try:
            # ì´ˆê¸° ìƒíƒœ ìƒì„± (typed dataclass)
            initial_state = WorkflowState(
                mms_msg=message,
                extractor=self,
                message_id=message_id # message_id ì¶”ê°€
            )
            
            # Workflow ì‹¤í–‰
            final_state = self.workflow_engine.run(initial_state)
            
            # Fallback ì²˜ë¦¬
            if final_state.get("is_fallback"):
                logger.warning("Workflowì—ì„œ Fallback ê²°ê³¼ ë°˜í™˜")
                return self._create_fallback_result(message, message_id) # message_id ì „ë‹¬
            
            # ê²°ê³¼ ì¶”ì¶œ
            final_result = final_state.get("final_result", {})
            raw_result = final_state.get("raw_result", {})

            # message_idë¥¼ ê²°ê³¼ì— í¬í•¨
            final_result['message_id'] = message_id
            raw_result['message_id'] = message_id
            
            # í”„ë¡¬í”„íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            actual_prompts = PromptManager.get_stored_prompts_from_thread()
            
            return {
                "ext_result": final_result,
                "raw_result": raw_result,
                "prompts": actual_prompts
            }
            
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_fallback_result(message, message_id) # message_id ì „ë‹¬
    
    @log_performance
    def extract_json_objects_only(self, mms_msg: str) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ì—ì„œ 7ë‹¨ê³„(ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±) ì „ì˜ json_objectsë§Œ ì¶”ì¶œ
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€
            
        Returns:
            Dict: LLMì´ ìƒì„±í•œ json_objects (ì—”í‹°í‹° ë§¤ì¹­ ì „)
        """
        try:
            msg = mms_msg.strip()
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì‹œì‘ - ë©”ì‹œì§€ ê¸¸ì´: {len(msg)}ì")
            
            # 1-4ë‹¨ê³„: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤
            pgm_info = self._prepare_program_classification(msg)
            
            # RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (product_info_extraction_modeê°€ 'rag'ì¸ ê²½ìš°)
            rag_context = ""
            if self.product_info_extraction_mode == 'rag':
                rag_context = self._prepare_rag_context(msg)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
            prompt = self._build_extraction_prompt(msg, pgm_info, rag_context)
            result_json_text = self._safe_llm_invoke(prompt)
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            json_objects_list = extract_json_objects(result_json_text)
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {}
            
            json_objects = json_objects_list[-1]
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.warning("LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return {}
            
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì™„ë£Œ - í‚¤: {list(json_objects.keys())}")
            return json_objects
            
        except Exception as e:
            logger.error(f"JSON ê°ì²´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _prepare_program_classification(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì¤€ë¹„ (_classify_programs ë©”ì†Œë“œì™€ ë™ì¼)"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _detect_schema_response(self, json_objects: Dict) -> bool:
        """LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ ê°ì§€"""
        try:
            # purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            purpose = json_objects.get('purpose', {})
            if isinstance(purpose, dict) and 'type' in purpose and purpose.get('type') == 'array':
                logger.warning("purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸  
            product = json_objects.get('product', {})
            if isinstance(product, dict) and 'type' in product and product.get('type') == 'array':
                logger.warning("product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            channel = json_objects.get('channel', {})
            if isinstance(channel, dict) and 'type' in channel and channel.get('type') == 'array':
                logger.warning("channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def convert_df_to_json_list(self, df: pd.DataFrame) -> List[Dict]:
        """
        DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜
        ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ: item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ëª¨ë“  item_name_in_msgë¥¼ ë°°ì—´ë¡œ ìˆ˜ì§‘
        
        Schema:
        {
            "item_nm": "ìƒí’ˆëª…",
            "item_id": ["ID1", "ID2"],
            "item_name_in_msg": ["ë©”ì‹œì§€ë‚´í‘œí˜„1", "ë©”ì‹œì§€ë‚´í‘œí˜„2"]
        }
        """
        result = []
        # item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
        grouped = df.groupby('item_nm')
        for item_nm, group in grouped:
            # ë©”ì¸ ì•„ì´í…œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            item_name_in_msg_raw = list(group['item_name_in_msg'].unique())
            item_dict = {
                'item_nm': item_nm,
                'item_id': list(group['item_id'].unique()),
                'item_name_in_msg': select_most_comprehensive(item_name_in_msg_raw)
            }
            result.append(item_dict)
        return result

    def _create_fallback_result(self, msg: str, message_id: str = '#') -> Dict[str, Any]:
        """ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "message_id": message_id,
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "sales_script": "",
            "product": [],
            "channel": [],
            "pgm": [],
            "offer": {"type": "product", "value": []},
            "entity_dag": []
        }

    # _build_final_result and _map_program_classification removed (moved to ResultBuilder)



def process_message_with_dag(extractor, message: str, extract_dag: bool = False, message_id: str = '#') -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ìš©)
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
        message_id: ë©”ì‹œì§€ ID (ì„ íƒ ì‚¬í•­)
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ (í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨)
    """
    try:
        # ìŠ¤ë ˆë“œ ë¡œì»¬ í”„ë¡¬í”„íŠ¸ ì €ì¥ì†Œ ì´ˆê¸°í™” (ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ìŠ¤ë ˆë“œ ì¬ì‚¬ìš© ë¬¸ì œ ë°©ì§€)
        PromptManager.clear_stored_prompts()
        
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")

        # 1. ë©”ì¸ ì¶”ì¶œ
        result = extractor.process_message(message, message_id) # message_id ì „ë‹¬
        dag_list = []
        
        if extract_dag:
            # ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë³€ê²½ (í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ë¥¼ ìœ„í•´)
            # ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ë©´ ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œê°€ ë¶„ë¦¬ë˜ì–´ í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ê°€ ì•ˆë¨
            logger.info("ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë©”ì¸ ì¶”ì¶œ ë° DAG ì¶”ì¶œ ìˆ˜í–‰")
            
            # 2. DAG ì¶”ì¶œ
            dag_result = make_entity_dag(message, extractor.llm_model)
            dag_list = sorted([d for d in dag_result['dag_section'].split('\n') if d!=''])

        extracted_result = result.get('ext_result', {})
        extracted_result['entity_dag'] = dag_list
        result['ext_result'] = extracted_result

        raw_result = result.get('raw_result', {})
        raw_result['entity_dag'] = dag_list
        result['raw_result'] = raw_result

        result['error'] = ""
        
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "ext_result": {
                "message_id": message_id, # message_id ì¶”ê°€
                "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                "purpose": ["ì˜¤ë¥˜"],
                "sales_script": "",
                "product": [],
                "channel": [],
                "pgm": [],
                "offer": {"type": "product", "value": []},
                "entity_dag": []
            },
            "raw_result": {
                "message_id": message_id, # message_id ì¶”ê°€
            },
            "prompts": {},
            "error": str(e)
        }

def process_messages_batch(extractor, messages: List[str], extract_dag: bool = False, max_workers: int = None) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        messages: ì²˜ë¦¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
    
    Returns:
        list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    from concurrent.futures import as_completed
    
    if max_workers is None:
        max_workers = min(len(messages), os.cpu_count())
    
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€, {max_workers}ê°œ ì›Œì»¤")
    
    start_time = time.time()
    results = [None] * len(messages)  # ê²°ê³¼ë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë©”ì‹œì§€ì— ëŒ€í•´ ì‘ì—… ì œì¶œ (ì¸ë±ìŠ¤ì™€ í•¨ê»˜)
        future_to_index = {
            executor.submit(process_message_with_dag, extractor, msg, extract_dag, f"batch_{idx}"): idx # message_id ì „ë‹¬
            for idx, msg in enumerate(messages)
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
        completed = 0
        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                result = future.result()
                results[idx] = result
                completed += 1
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {completed}/{len(messages)} ({(completed/len(messages)*100):.1f}%)")
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë©”ì‹œì§€ {idx+1}): {e}")
                results[idx] = {
                    "ext_result": {
                        "message_id": f"batch_{idx}", # message_id ì¶”ê°€
                        "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                        "purpose": ["ì˜¤ë¥˜"],
                        "sales_script": "",
                        "product": [],
                        "channel": [],
                        "pgm": [],
                        "offer": {"type": "product", "value": []},
                        "entity_dag": []
                    },
                    "raw_result": {
                        "message_id": f"batch_{idx}", # message_id ì¶”ê°€
                    },
                    "prompts": {},
                    "error": str(e)
                }
    
    elapsed_time = time.time() - start_time
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ")
    logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(messages):.2f}ì´ˆ/ë©”ì‹œì§€")
    
    return results

def make_entity_dag(msg: str, llm_model, save_dag_image=True):

    # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
    # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
    extract_dag_result = {}
    logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
    try:
        dag_start_time = time.time()
        # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
        extract_dag_result = extract_dag(DAGParser(), msg, llm_model)
        dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
        dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
        dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
        
        # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
        dag_filename = ""
        if save_dag_image:
            dag_filename = f'dag_{sha256_hash(msg)}'
            create_dag_diagram(dag, filename=dag_filename)
            logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")

        extract_dag_result['dag_filename'] = dag_filename
        
        dag_processing_time = time.time() - dag_start_time
        
        logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
        logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
        if dag_section:
            logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
        else:
            logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        dag_section = ""

    return extract_dag_result


def get_stored_prompts_from_thread():
    """í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (PromptManager ì‚¬ìš©)"""
    return PromptManager.get_stored_prompts_from_thread()

def save_result_to_mongodb_if_enabled(message: str, result: dict, args_or_data, extractor=None):
    """MongoDB ì €ì¥ì´ í™œì„±í™”ëœ ê²½ìš° ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
    
    Args:
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        result: ì²˜ë¦¬ ê²°ê³¼ (extracted_result, raw_result í¬í•¨)
        args_or_data: argparse.Namespace ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
    
    Returns:
        str: ì €ì¥ëœ ë¬¸ì„œ ID, ì‹¤íŒ¨ ì‹œ None
    """
    # args_or_dataê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Namespaceë¡œ ë³€í™˜
    if isinstance(args_or_data, dict):
        import argparse
        args = argparse.Namespace(**args_or_data)
    else:
        args = args_or_data
    
    # save_to_mongodb ì†ì„±ì´ ì—†ê±°ë‚˜ Falseì¸ ê²½ìš°
    if not getattr(args, 'save_to_mongodb', False):
        return None
        
    try:
        # MongoDB ì„í¬íŠ¸ ì‹œë„
        from utils.mongodb_utils import save_to_mongodb
        
        # ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œì—ì„œ í”„ë¡¬í”„íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stored_prompts = result.get('prompts', get_stored_prompts_from_thread()) 
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´ êµ¬ì„±
        prompts_data = {}
        for key, prompt_data in stored_prompts.items():
            prompts_data[key] = {
                'title': prompt_data.get('title', f'{key} í”„ë¡¬í”„íŠ¸'),
                'description': prompt_data.get('description', f'{key} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'),
                'content': prompt_data.get('content', ''),
                'length': len(prompt_data.get('content', ''))
            }
        
        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not prompts_data:
            prompts_data = {
                'main_extraction_prompt': {
                    'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                    'description': 'MMS ë©”ì‹œì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ',
                    'content': 'ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                    'length': 0
                }
            }
        
        extraction_prompts = {
            'success': True,
            'prompts': prompts_data,
            'settings': {
                'llm_model': getattr(args, 'llm_model', 'unknown'),
                'offer_data_source': getattr(args, 'offer_data_source', getattr(args, 'offer_info_data_src', 'unknown')),
                'product_info_extraction_mode': getattr(args, 'product_info_extraction_mode', 'unknown'),
                'entity_matching_mode': getattr(args, 'entity_matching_mode', getattr(args, 'entity_extraction_mode', 'unknown')),
                'extract_entity_dag': getattr(args, 'extract_entity_dag', False)
            }
        }
        
        # ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDB í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        extraction_result = {
            'success': not bool(result.get('error')),
            'result': result.get('ext_result', result.get('result', {})),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }

        raw_result_data = {
            'success': not bool(result.get('error')),
            'result': result.get('raw_result', {}),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }
        
        # MongoDBì— ì €ì¥
        user_id = getattr(args, 'user_id', 'DEFAULT_USER')
        saved_id = save_to_mongodb(message, extraction_result, raw_result_data, extraction_prompts, 
                                 user_id=user_id, message_id=None)
        
        if saved_id:
            print(f"ğŸ“„ ê²°ê³¼ê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {saved_id[:8]}...)")
            return saved_id
        else:
            print("âš ï¸ MongoDB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
            
    except ImportError:
        print("âŒ MongoDB ì €ì¥ì´ ìš”ì²­ë˜ì—ˆì§€ë§Œ mongodb_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# CLI interface moved to cli.py
# To run from command line: python cli.py --help
# %%
