"""
MMS Workflow Steps - MMS ì¶”ì¶œê¸° ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ êµ¬í˜„
===================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS ë©”ì‹œì§€ ì²˜ë¦¬ì˜ ê° ë‹¨ê³„ë¥¼ ë…ë¦½ì ì¸ í´ë˜ìŠ¤ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
ê° ë‹¨ê³„ëŠ” `WorkflowStep`ì„ ìƒì†ë°›ì•„ `execute` ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ë©°,
`WorkflowState`ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ìŠµë‹ˆë‹¤.

ğŸ”— ì˜ì¡´ì„±
---------
**ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ:**
- `workflow_core`: WorkflowStep, WorkflowState ê¸°ë°˜ í´ë˜ìŠ¤
- `services.*`: ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„œë¹„ìŠ¤ (EntityRecognizer, ProgramClassifier ë“±)
- `utils`: ê²€ì¦, íŒŒì‹±, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

**ì‚¬ìš©ë˜ëŠ” ê³³:**
- `core.mms_extractor`: MMSExtractor ì´ˆê¸°í™” ì‹œ ì›Œí¬í”Œë¡œìš° ì—”ì§„ì— ë‹¨ê³„ ë“±ë¡

ğŸ—ï¸ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ìˆœì„œ
-----------------------

```mermaid
graph TB
    A[InputValidationStep] -->|msg| B[EntityExtractionStep]
    B -->|entities_from_kiwi, cand_item_list| C[ProgramClassificationStep]
    C -->|pgm_info| D[ContextPreparationStep]
    D -->|rag_context, product_element| E[LLMExtractionStep]
    E -->|result_json_text| F[ResponseParsingStep]
    F -->|json_objects, raw_result| G[ResultConstructionStep]
    G -->|final_result| H[ValidationStep]
    H -->|validated final_result| I{extract_entity_dag?}
    I -->|Yes| J[DAGExtractionStep]
    I -->|No| K[End]
    J -->|entity_dag| K
    
    style A fill:#e1f5ff
    style E fill:#ffe1e1
    style G fill:#fff4e1
    style J fill:#e1ffe1
    style K fill:#d4edda
```

ğŸ“Š ê° ë‹¨ê³„ë³„ ì—­í• 
----------------

### 1. InputValidationStep
**ëª©ì **: ì…ë ¥ ë©”ì‹œì§€ ê²€ì¦ ë° ì„¤ì • ë¡œê¹…
**ì…ë ¥**: mms_msg (ì›ë³¸ ë©”ì‹œì§€)
**ì¶œë ¥**: msg (ê²€ì¦ëœ ë©”ì‹œì§€)
**ì£¼ìš” ì‘ì—…**:
- ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
- ì¶”ì¶œê¸° ì„¤ì • ìƒíƒœ ë¡œê¹…
- ë©”ì‹œì§€ ê¸¸ì´ ë° ë‚´ìš© í™•ì¸

### 2. EntityExtractionStep
**ëª©ì **: Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
**ì…ë ¥**: msg
**ì¶œë ¥**: entities_from_kiwi, cand_item_list, extra_item_pdf
**ì£¼ìš” ì‘ì—…**:
- Kiwi í˜•íƒœì†Œ ë¶„ì„ (NNP íƒœê·¸ ì¶”ì¶œ)
- ì„ë² ë”© ìœ ì‚¬ë„ ë§¤ì¹­
- DB ëª¨ë“œ ì§„ë‹¨ ë° ê²°ê³¼ ë¶„ì„

### 3. ProgramClassificationStep
**ëª©ì **: í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
**ì…ë ¥**: msg
**ì¶œë ¥**: pgm_info (í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´)
**ì£¼ìš” ì‘ì—…**:
- ì„ë² ë”© ê¸°ë°˜ í”„ë¡œê·¸ë¨ ìœ ì‚¬ë„ ê³„ì‚°
- ìƒìœ„ Nê°œ í›„ë³´ í”„ë¡œê·¸ë¨ ì„ íƒ

### 4. ContextPreparationStep
**ëª©ì **: RAG ì»¨í…ìŠ¤íŠ¸ ë° ì œí’ˆ ì •ë³´ ì¤€ë¹„
**ì…ë ¥**: pgm_info, cand_item_list, extra_item_pdf
**ì¶œë ¥**: rag_context, product_element
**ì£¼ìš” ì‘ì—…**:
- RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í”„ë¡œê·¸ë¨ ì •ë³´ í¬í•¨)
- ëª¨ë“œë³„ ì œí’ˆ ì •ë³´ ì¤€ë¹„ (nlp/llm/rag)
- NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì§ì ‘ ìƒì„±

### 5. LLMExtractionStep
**ëª©ì **: LLM í˜¸ì¶œ ë° ì •ë³´ ì¶”ì¶œ
**ì…ë ¥**: msg, rag_context, product_element
**ì¶œë ¥**: result_json_text (LLM ì‘ë‹µ)
**ì£¼ìš” ì‘ì—…**:
- í”„ë¡¬í”„íŠ¸ êµ¬ì„± (build_extraction_prompt)
- LLM í˜¸ì¶œ (safe_llm_invoke)
- í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…ìš©)

### 6. ResponseParsingStep
**ëª©ì **: LLM ì‘ë‹µ JSON íŒŒì‹±
**ì…ë ¥**: result_json_text
**ì¶œë ¥**: json_objects, raw_result
**ì£¼ìš” ì‘ì—…**:
- JSON íŒŒì‹± (extract_json_objects)
- ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ (detect_schema_response)
- raw_result ìƒì„±

### 7. ResultConstructionStep
**ëª©ì **: ìµœì¢… ê²°ê³¼ êµ¬ì„±
**ì…ë ¥**: json_objects, msg, pgm_info, entities_from_kiwi
**ì¶œë ¥**: final_result
**ì£¼ìš” ì‘ì—…**:
- ì—”í‹°í‹° ë§¤ì¹­ (ResultBuilder)
- ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë³´ê°•
- í”„ë¡œê·¸ë¨ ë§¤í•‘
- offer ê°ì²´ ìƒì„±

### 8. ValidationStep
**ëª©ì **: ê²°ê³¼ ê²€ì¦ ë° ìš”ì•½
**ì…ë ¥**: final_result
**ì¶œë ¥**: validated final_result
**ì£¼ìš” ì‘ì—…**:
- ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦ (validate_extraction_result)
- ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…

### 9. DAGExtractionStep (ì„ íƒì )
**ëª©ì **: ì—”í‹°í‹° ê°„ ê´€ê³„ ê·¸ë˜í”„ ìƒì„±
**ì…ë ¥**: msg, extract_entity_dag í”Œë˜ê·¸
**ì¶œë ¥**: entity_dag (DAG ë¦¬ìŠ¤íŠ¸)
**ì£¼ìš” ì‘ì—…**:
- LLM ê¸°ë°˜ DAG ì¶”ì¶œ (extract_dag)
- NetworkX ê·¸ë˜í”„ ìƒì„±
- Graphviz ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ
-----------
```python
from core.workflow_core import WorkflowEngine, WorkflowState
from core.mms_workflow_steps import (
    InputValidationStep,
    EntityExtractionStep,
    # ... ê¸°íƒ€ ë‹¨ê³„ë“¤
)

# ì›Œí¬í”Œë¡œìš° ì—”ì§„ ì´ˆê¸°í™”
engine = WorkflowEngine("MMS Extraction")

# ë‹¨ê³„ ë“±ë¡
engine.add_step(InputValidationStep())
engine.add_step(EntityExtractionStep(entity_recognizer))
engine.add_step(ProgramClassificationStep(program_classifier))
# ... ê¸°íƒ€ ë‹¨ê³„ë“¤

# ì´ˆê¸° ìƒíƒœ ì„¤ì •
state = WorkflowState()
state.set("mms_msg", "ìƒ˜í”Œ MMS ë©”ì‹œì§€")
state.set("extractor", extractor_instance)

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
final_state = engine.execute(state)

# ê²°ê³¼ í™•ì¸
if final_state.has_error():
    errors = final_state.get_errors()
else:
    result = final_state.get("final_result")
```

ğŸ“ ì°¸ê³ ì‚¬í•­
----------
- ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- ì—ëŸ¬ ë°œìƒ ì‹œ `state.add_error()`ë¡œ ê¸°ë¡
- ì—ëŸ¬ê°€ ìˆìœ¼ë©´ í›„ì† ë‹¨ê³„ëŠ” ìë™ìœ¼ë¡œ ìŠ¤í‚µ
- DAGExtractionStepì€ `extract_entity_dag=True`ì¼ ë•Œë§Œ ì‹¤í–‰
- ëª¨ë“  ë‹¨ê³„ëŠ” WorkflowStateë¥¼ í†µí•´ ë°ì´í„° ê³µìœ 

"""

import logging
import copy
import traceback
from typing import Any, Dict, List
import pandas as pd
from .workflow_core import WorkflowStep, WorkflowState
from utils import (
    validate_text_input,
    safe_check_empty,
    extract_json_objects,
    replace_special_chars_with_space
)
from utils import PromptManager, validate_extraction_result, detect_schema_response


logger = logging.getLogger(__name__)


class InputValidationStep(WorkflowStep):
    """
    ì…ë ¥ ë©”ì‹œì§€ ê²€ì¦ ë‹¨ê³„ (Step 1/9)
    
    ì±…ì„:
        - ì›ë³¸ MMS ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        - í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì „ì²˜ë¦¬
        - ì¶”ì¶œê¸° ì„¤ì • ìƒíƒœ ë¡œê¹…
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: mms_msg (ì›ë³¸ ë©”ì‹œì§€), extractor (ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤)
        ì¶œë ¥: msg (ì „ì²˜ë¦¬ëœ ë©”ì‹œì§€)
    
    ì—ëŸ¬ ì²˜ë¦¬:
        - ê²€ì¦ ì‹¤íŒ¨ ì‹œ is_fallback=True ì„¤ì •
        - ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ stateì— ê¸°ë¡
    """
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        mms_msg = state.get("mms_msg")
        extractor = state.get("extractor")
        
        self._log_message_info(mms_msg)
        self._log_extractor_config(extractor)
        
        try:
            msg = validate_text_input(mms_msg)
            state.set("msg", msg)
        except Exception as e:
            logger.error(f"ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            state.add_error(f"ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            state.set("is_fallback", True)
        
        return state
    
    def _log_message_info(self, mms_msg: str):
        """ë©”ì‹œì§€ ì •ë³´ ë¡œê¹…"""
        logger.info(f"ë©”ì‹œì§€ ë‚´ìš©: {mms_msg[:200]}...")
        logger.info(f"ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
    
    def _log_extractor_config(self, extractor):
        """ì¶”ì¶œê¸° ì„¤ì • ë¡œê¹…"""
        logger.info("=== í˜„ì¬ ì¶”ì¶œê¸° ì„¤ì • ===")
        logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {extractor.offer_info_data_src}")
        logger.info(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: {extractor.product_info_extraction_mode}")
        logger.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ: {extractor.entity_extraction_mode}")
        logger.info(f"LLM ëª¨ë¸: {extractor.llm_model_name}")
        logger.info(f"ìƒí’ˆ ë°ì´í„° í¬ê¸°: {extractor.item_pdf_all.shape}")
        logger.info(f"í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {extractor.pgm_pdf.shape}")


class EntityExtractionStep(WorkflowStep):
    """
    ì—”í‹°í‹° ì¶”ì¶œ ë‹¨ê³„ (Step 2)

    ì±…ì„:
        - Kiwi í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ NNP íƒœê·¸ ì¶”ì¶œ
        - ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­
        - í›„ë³´ ìƒí’ˆ ëª©ë¡ ìƒì„±
        - DB ëª¨ë“œ ì§„ë‹¨ ë° ë°ì´í„° í’ˆì§ˆ ê²€ì¦

    í˜‘ë ¥ ê°ì²´:
        - EntityRecognizer: ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ ìˆ˜í–‰

    ë°ì´í„° íë¦„:
        ì…ë ¥: msg (ê²€ì¦ëœ ë©”ì‹œì§€)
        ì¶œë ¥: entities_from_kiwi (Kiwi ì¶”ì¶œ ì—”í‹°í‹°)
              cand_item_list (í›„ë³´ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸)
              extra_item_pdf (ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´)

    íŠ¹ì´ì‚¬í•­:
        - skip_entity_extraction=Trueì´ë©´ should_execute() â†’ Falseë¡œ ìŠ¤í‚µ
        - DB ëª¨ë“œì—ì„œëŠ” ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ìˆ˜í–‰
        - í›„ë³´ ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ ê²½ê³  ë¡œê·¸ ì¶œë ¥
    """

    def __init__(self, entity_recognizer, skip_entity_extraction: bool = False):
        self.entity_recognizer = entity_recognizer
        self.skip_entity_extraction = skip_entity_extraction

    def should_execute(self, state: WorkflowState) -> bool:
        if self.skip_entity_extraction:
            return False
        return not state.has_error()

    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state

        msg = state.get("msg")
        extractor = state.get("extractor")

        # DB ëª¨ë“œ ì§„ë‹¨
        if extractor.offer_info_data_src == "db":
            self._diagnose_db_mode(extractor)

        # ì—”í‹°í‹° ì¶”ì¶œ
        entities_from_kiwi, cand_item_list, extra_item_pdf = self.entity_recognizer.extract_entities_hybrid(msg)

        self._log_extraction_results(entities_from_kiwi, cand_item_list, extra_item_pdf)

        # DB ëª¨ë“œ ê²°ê³¼ ë¶„ì„
        if extractor.offer_info_data_src == "db":
            self._analyze_db_results(cand_item_list)

        # ONT ëª¨ë“œì¼ ê²½ìš° LLM ê¸°ë°˜ ì¶”ì¶œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
        if hasattr(extractor, 'entity_extraction_context_mode') and extractor.entity_extraction_context_mode == 'ont':
            logger.info("ğŸ” ONT ëª¨ë“œ ê°ì§€: LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ìˆ˜í–‰")
            try:
                ont_result = self.entity_recognizer.extract_entities_with_llm(
                    msg_text=msg,
                    rank_limit=50,
                    llm_models=[extractor.llm_model],
                    external_cand_entities=cand_item_list,
                    context_mode='ont'
                )

                # ONT ê²°ê³¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
                if isinstance(ont_result, dict) and 'ont_metadata' in ont_result:
                    ont_metadata = ont_result.get('ont_metadata')
                    if ont_metadata:
                        state.set("ont_extraction_result", ont_metadata)
                        logger.info(f"âœ… ONT ë©”íƒ€ë°ì´í„° ì €ì¥: entity_types={len(ont_metadata.get('entity_types', {}))}, "
                                   f"relationships={len(ont_metadata.get('relationships', []))}")
            except Exception as e:
                logger.warning(f"ONT ëª¨ë“œ ì¶”ì¶œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        state.set("entities_from_kiwi", entities_from_kiwi)
        state.set("cand_item_list", cand_item_list)
        state.set("extra_item_pdf", extra_item_pdf)

        return state
    
    def _diagnose_db_mode(self, extractor):
        """DB ëª¨ë“œ ì§„ë‹¨"""
        logger.info("ğŸ” DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨ ì‹œì‘")
        logger.info(f"ìƒí’ˆ ë°ì´í„° ìƒíƒœ: {extractor.item_pdf_all.shape}")
        
        required_columns = ['item_nm', 'item_id', 'item_nm_alias']
        missing_columns = [col for col in required_columns if col not in extractor.item_pdf_all.columns]
        if missing_columns:
            logger.error(f"ğŸš¨ DB ëª¨ë“œì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
        
        if 'item_nm_alias' in extractor.item_pdf_all.columns:
            null_aliases = extractor.item_pdf_all['item_nm_alias'].isnull().sum()
            total_aliases = len(extractor.item_pdf_all)
            logger.info(f"DB ëª¨ë“œ ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ: {total_aliases - null_aliases}/{total_aliases} ìœ íš¨")
    
    def _log_extraction_results(self, entities_from_kiwi, cand_item_list, extra_item_pdf):
        """ì¶”ì¶œ ê²°ê³¼ ë¡œê¹…"""
        logger.info(f"ì¶”ì¶œëœ Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")
        logger.info(f"ì¶”ì¶œëœ í›„ë³´ ì—”í‹°í‹°: {cand_item_list}")
        logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
    
    def _analyze_db_results(self, cand_item_list):
        """DB ëª¨ë“œ ê²°ê³¼ ë¶„ì„"""
        logger.info("ğŸ” DB ëª¨ë“œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
        if safe_check_empty(cand_item_list):
            logger.error("ğŸš¨ DB ëª¨ë“œì—ì„œ í›„ë³´ ì—”í‹°í‹°ê°€ ì „í˜€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
            logger.error("1. ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ìƒí’ˆì´ ì—†ìŒ")
            logger.error("2. ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨")
            logger.error("3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ")
            logger.error("4. Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨")


class ProgramClassificationStep(WorkflowStep):
    """
    í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë‹¨ê³„ (Step 3/9)
    
    ì±…ì„:
        - ë©”ì‹œì§€ë¥¼ ì‚¬ì „ ì •ì˜ëœ í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
        - ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
        - ìƒìœ„ Nê°œ í›„ë³´ í”„ë¡œê·¸ë¨ ì„ íƒ
    
    í˜‘ë ¥ ê°ì²´:
        - ProgramClassifier: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìˆ˜í–‰
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: msg (ê²€ì¦ëœ ë©”ì‹œì§€)
        ì¶œë ¥: pgm_info (í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´)
              - pgm_cand_info: í›„ë³´ í”„ë¡œê·¸ë¨ ì •ë³´
              - pgm_pdf_tmp: í›„ë³´ í”„ë¡œê·¸ë¨ DataFrame
    """
    
    def __init__(self, program_classifier):
        self.program_classifier = program_classifier

    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state
        
        msg = state.get("msg")
        extractor = state.get("extractor")
        
        pgm_info = self.program_classifier.classify(msg)
        logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ê²°ê³¼ í‚¤: {list(pgm_info.keys())}")
        
        state.set("pgm_info", pgm_info)
        
        return state


class ContextPreparationStep(WorkflowStep):
    """
    RAG ì»¨í…ìŠ¤íŠ¸ ë° ì œí’ˆ ì •ë³´ ì¤€ë¹„ ë‹¨ê³„ (Step 4/9)
    
    ì±…ì„:
        - RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ í¬í•¨)
        - ëª¨ë“œë³„ ì œí’ˆ ì •ë³´ ì¤€ë¹„ (nlp/llm/rag)
        - NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì§ì ‘ ìƒì„±
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: pgm_info, cand_item_list, extra_item_pdf
        ì¶œë ¥: rag_context (RAG ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´)
              product_element (NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ, ì„ íƒì )
    
    ëª¨ë“œë³„ ë™ì‘:
        - rag: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        - llm: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ ì¶”ê°€
        - nlp: product_element ì§ì ‘ ìƒì„± (name, action)
    """
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state
        
        extractor = state.get("extractor")
        pgm_info = state.get("pgm_info")
        cand_item_list = state.get("cand_item_list")
        extra_item_pdf = state.get("extra_item_pdf")
        
        # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        rag_context = self._build_ad_classification_rag_context(extractor, pgm_info)
        
        # ì œí’ˆ ì •ë³´ ì¤€ë¹„
        product_element = None
        
        if not safe_check_empty(cand_item_list):
            self._log_candidate_items(cand_item_list, extra_item_pdf)
            rag_context, product_element = self._build_product_rag_context(
                extractor, rag_context, cand_item_list, extra_item_pdf
            )
        else:
            self._log_no_candidates()
        
        state.set("rag_context", rag_context)
        state.set("product_element", product_element)
        
        return state
    
    def _build_ad_classification_rag_context(self, extractor, pgm_info) -> str:
        """ê´‘ê³  ë¶„ë¥˜ìš© RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_info['pgm_cand_info']}" if extractor.num_cand_pgms > 0 else ""
        logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} ë¬¸ì")
        return rag_context
    
    def _log_candidate_items(self, cand_item_list, extra_item_pdf):
        """í›„ë³´ ì•„ì´í…œ ë¡œê¹…"""
        logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°: {len(cand_item_list)}ê°œ")
        logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
        logger.info(f"extra_item_pdf í¬ê¸°: {extra_item_pdf.shape}")
        if not extra_item_pdf.empty:
            logger.info(f"extra_item_pdf ì»¬ëŸ¼ë“¤: {list(extra_item_pdf.columns)}")
            logger.info(f"extra_item_pdf ìƒ˜í”Œ: {extra_item_pdf.head(2).to_dict('records')}")
    
    def _build_product_rag_context(self, extractor, rag_context, cand_item_list, extra_item_pdf):
        """ì œí’ˆ ì •ë³´ìš© RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ëª¨ë“œë³„)"""
        product_element = None
        
        if extractor.product_info_extraction_mode == 'rag':
            rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
            logger.info("RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
        elif extractor.product_info_extraction_mode == 'llm':
            rag_context += f"\n\n### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
            logger.info("LLM ëª¨ë“œ: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
        elif extractor.product_info_extraction_mode == 'nlp':
            product_element = self._build_nlp_product_element(extractor, extra_item_pdf)
        
        return rag_context, product_element
    
    def _build_nlp_product_element(self, extractor, extra_item_pdf):
        """NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ êµ¬ì„±"""
        if not extra_item_pdf.empty and 'item_nm' in extra_item_pdf.columns:
            product_df = extra_item_pdf.rename(columns={'item_nm': 'name'}).query(
                "not name in @extractor.stop_item_names"
            )[['name']]
            product_df['action'] = 'ê¸°íƒ€'
            product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
            logger.info(f"NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ - {len(product_element) if product_element else 0}ê°œ")
            if product_element:
                logger.info(f"NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ ìƒ˜í”Œ: {product_element[:2]}")
            return product_element
        else:
            logger.warning("NLP ëª¨ë“œ: extra_item_pdfê°€ ë¹„ì–´ìˆê±°ë‚˜ item_nm ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return None
    
    def _log_no_candidates(self):
        """í›„ë³´ ì•„ì´í…œ ì—†ìŒ ê²½ê³ """
        logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
        logger.warning("ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        logger.warning("1. ìƒí’ˆ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        logger.warning("2. ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨")
        logger.warning("3. ìœ ì‚¬ë„ ë§¤ì¹­ ì„ê³„ê°’ ë¬¸ì œ")


class LLMExtractionStep(WorkflowStep):
    """
    LLM í˜¸ì¶œ ë° ì¶”ì¶œ ë‹¨ê³„ (Step 5/9)
    
    ì±…ì„:
        - ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë©”ì‹œì§€ + RAG ì»¨í…ìŠ¤íŠ¸ + ì œí’ˆ ì •ë³´)
        - LLM í˜¸ì¶œ ë° ì‘ë‹µ ìˆ˜ì‹ 
        - í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹… ë° ê²€í† ìš©)
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: msg, rag_context, product_element
        ì¶œë ¥: result_json_text (LLM JSON ì‘ë‹µ)
    
    ì£¼ìš” ì‘ì—…:
        1. build_extraction_prompt()ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        2. PromptManager.store_prompt_for_preview()ë¡œ ì €ì¥
        3. safe_llm_invoke()ë¡œ LLM í˜¸ì¶œ
    
    íŠ¹ì´ì‚¬í•­:
        - í”„ë¡¬í”„íŠ¸ëŠ” ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œì— ìºì‹œë¨
        - LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ì ìš©
    """
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state
        
        msg = state.get("msg")
        extractor = state.get("extractor")
        rag_context = state.get("rag_context")
        product_element = state.get("product_element")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = extractor._build_extraction_prompt(msg, rag_context, product_element)
        logger.info(f"êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
        logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€: {'í›„ë³´ ìƒí’ˆ' in rag_context}")
        
        # í”„ë¡¬í”„íŠ¸ ì €ì¥ (helpers ëª¨ë“ˆ ì‚¬ìš©)
        PromptManager.store_prompt_for_preview(prompt, "main_extraction")
        
        # LLM í˜¸ì¶œ
        result_json_text = extractor._safe_llm_invoke(prompt)
        logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(result_json_text)} ë¬¸ì")
        logger.info(f"LLM ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {result_json_text[:500]}...")
        
        state.set("result_json_text", result_json_text)
        
        return state


class ResponseParsingStep(WorkflowStep):
    """
    LLM ì‘ë‹µ JSON íŒŒì‹± ë‹¨ê³„ (Step 6/9)
    
    ì±…ì„:
        - LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ ì¶”ì¶œ
        - ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ë° í•„í„°ë§
        - raw_result ìƒì„±
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: result_json_text (LLM ì‘ë‹µ)
        ì¶œë ¥: json_objects (íŒŒì‹±ëœ JSON)
              raw_result (ì›ë³¸ ê²°ê³¼, message_id í¬í•¨)
    
    ì—ëŸ¬ ì²˜ë¦¬:
        - JSON íŒŒì‹± ì‹¤íŒ¨: is_fallback=True ì„¤ì •
        - ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€: is_fallback=True ì„¤ì •
    
    íŠ¹ì´ì‚¬í•­:
        - ì—¬ëŸ¬ JSON ê°ì²´ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒ ì‚¬ìš©
        - detect_schema_response()ë¡œ ìŠ¤í‚¤ë§ˆ ì •ì˜ í•„í„°ë§
    """
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state
        
        result_json_text = state.get("result_json_text")
        # extractor = state.get("extractor") # No longer needed for detect_schema_response
        # msg = state.get("msg") # No longer needed
        
        # JSON íŒŒì‹±
        json_objects_list = extract_json_objects(result_json_text)
        logger.info(f"ì¶”ì¶œëœ JSON ê°ì²´ ìˆ˜: {len(json_objects_list)}ê°œ")
        
        if not json_objects_list:
            logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ: {result_json_text}")
            state.add_error("JSON íŒŒì‹± ì‹¤íŒ¨")
            state.set("is_fallback", True)
            return state
        
        json_objects = json_objects_list[-1]
        logger.info(f"íŒŒì‹±ëœ JSON ê°ì²´ í‚¤: {list(json_objects.keys())}")
        logger.info(f"íŒŒì‹±ëœ JSON ë‚´ìš©: {json_objects}")
        
        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ (helpers ëª¨ë“ˆ ì‚¬ìš©)
        if detect_schema_response(json_objects):
            logger.error("ğŸš¨ LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤! ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            logger.error("ì¬ì‹œë„ ë˜ëŠ” fallback ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            state.add_error("ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€")
            state.set("is_fallback", True)
            return state
        
        raw_result = copy.deepcopy(json_objects)
        
        # message_id ì¶”ê°€
        message_id = state.get("message_id", "#")
        raw_result['message_id'] = message_id
        
        state.set("json_objects", json_objects)
        state.set("raw_result", raw_result)
        
        return state


class EntityContextExtractionStep(WorkflowStep):
    """
    ì—”í‹°í‹° + ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ë‹¨ê³„ (Step 7)

    ì±…ì„:
        - ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹°ì™€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (Stage 1)
        - ë‘ ê°€ì§€ ì¶”ì¶œ ë°©ì‹ ì§€ì›:
          1. langextract: Google langextract ê¸°ë°˜ 6-type ë¶„ë¥˜
          2. default: entity_recognizer._extract_entities_stage1() í˜¸ì¶œ
        - ì¶”ì¶œ ê²°ê³¼ë¥¼ state.extracted_entitiesì— ì €ì¥

    ë°ì´í„° íë¦„:
        ì…ë ¥: msg, entities_from_kiwi, json_objects
        ì¶œë ¥: extracted_entities (entities, context_text, entity_types, relationships)
    """

    def __init__(self, entity_recognizer,
                 llm_factory=None, llm_model: str = 'ax',
                 entity_extraction_context_mode: str = 'dag',
                 use_external_candidates: bool = True,
                 extraction_engine: str = 'default',
                 stop_item_names: List[str] = None,
                 entity_extraction_mode: str = 'llm'):
        self.entity_recognizer = entity_recognizer
        self.llm_factory = llm_factory
        self.llm_model = llm_model
        self.entity_extraction_context_mode = entity_extraction_context_mode
        self.use_external_candidates = use_external_candidates
        self.extraction_engine = extraction_engine
        self.stop_item_names = stop_item_names or []
        self.entity_extraction_mode = entity_extraction_mode

    def should_execute(self, state: WorkflowState) -> bool:
        """Skip if there's an error or in logic mode (logic mode doesn't use Stage 1 output)"""
        if state.has_error():
            return False
        # Skip in logic mode - VocabularyFilteringStep doesn't use extracted_entities in logic mode
        if self.entity_extraction_mode == 'logic':
            return False
        return True

    def execute(self, state: WorkflowState) -> WorkflowState:
        msg = state.msg
        entities_from_kiwi = state.entities_from_kiwi
        json_objects = state.json_objects

        # Extract product items from json_objects
        product_items = json_objects.get('product', [])
        if isinstance(product_items, dict):
            product_items = product_items.get('items', [])

        primary_llm_extracted_entities = [x.get('name', '') for x in product_items]
        logger.debug(f"LLM ì¶”ì¶œ ì—”í‹°í‹°: {primary_llm_extracted_entities}")
        logger.debug(f"Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")

        # Build external candidate list
        if self.use_external_candidates:
            external_cand = list(set(entities_from_kiwi + primary_llm_extracted_entities))
        else:
            external_cand = []
            logger.info("ì™¸ë¶€ í›„ë³´ ì—”í‹°í‹° ë¹„í™œì„±í™” (use_external_candidates=False)")

        # Stage 1: Entity + Context Extraction
        if self.extraction_engine == 'langextract':
            # Method A: LangExtract-based extraction
            try:
                from core.lx_extractor import extract_mms_entities
                logger.info("ğŸ”— langextract ì—”ì§„ìœ¼ë¡œ Stage 1 ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘...")
                doc = extract_mms_entities(msg, model_id=self.llm_model)
                entities = []
                type_pairs = []
                for ext in (doc.extractions or []):
                    name = ext.extraction_text
                    if ext.extraction_class in ('Channel', 'Purpose'):
                        continue
                    if name not in self.stop_item_names and len(name) >= 2:
                        entities.append(name)
                        type_pairs.append(f"{name}({ext.extraction_class})")

                state.extracted_entities = {
                    'entities': entities,
                    'context_text': ", ".join(type_pairs),
                    'entity_types': {},  # langextract doesn't provide this
                    'relationships': []  # langextract doesn't provide this
                }
                logger.info(f"âœ… langextract Stage 1 ì™„ë£Œ: {len(entities)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ")
                logger.info(f"   ì—”í‹°í‹°: {entities}")
                logger.info(f"   ì»¨í…ìŠ¤íŠ¸: {state.extracted_entities['context_text']}")
            except Exception as e:
                logger.error(f"âŒ langextract ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
                state.extracted_entities = None
        else:
            # Method B: Standard LLM-based extraction
            if self.llm_factory:
                llm_models = self.llm_factory.create_models([self.llm_model])
            else:
                logger.warning("llm_factoryê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                llm_models = []

            try:
                logger.info(f"ğŸ” entity_recognizerë¡œ Stage 1 ì¶”ì¶œ ì‹œì‘ (context_mode={self.entity_extraction_context_mode})...")
                stage1_result = self.entity_recognizer._extract_entities_stage1(
                    msg_text=msg,
                    context_mode=self.entity_extraction_context_mode,
                    llm_models=llm_models,
                    external_cand_entities=external_cand
                )
                state.extracted_entities = stage1_result
                logger.info(f"âœ… Stage 1 ì™„ë£Œ: {len(stage1_result.get('entities', []))}ê°œ ì—”í‹°í‹° ì¶”ì¶œ")
                logger.info(f"   ì—”í‹°í‹°: {stage1_result.get('entities', [])}")
            except Exception as e:
                logger.error(f"âŒ Stage 1 ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                state.extracted_entities = None

        return state


class VocabularyFilteringStep(WorkflowStep):
    """
    ì–´íœ˜ ê¸°ë°˜ í•„í„°ë§ ë‹¨ê³„ (Step 8)

    ì±…ì„:
        - Stage 1ì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë¥¼ ìƒí’ˆ DBì™€ ë§¤ì¹­
        - logic ëª¨ë“œ: fuzzy + sequence ìœ ì‚¬ë„ ë§¤ì¹­
        - llm ëª¨ë“œ: LLM ê¸°ë°˜ ì–´íœ˜ í•„í„°ë§
        - alias íƒ€ì… í•„í„°ë§ (non-expansion)
        - ë§¤ì¹­ ê²°ê³¼ë¥¼ state.matched_productsì— ì €ì¥

    ë°ì´í„° íë¦„:
        ì…ë ¥: extracted_entities, msg, json_objects
        ì¶œë ¥: matched_products
    """

    def __init__(self, entity_recognizer, alias_pdf_raw: pd.DataFrame,
                 stop_item_names: List[str], entity_extraction_mode: str,
                 llm_factory=None, llm_model: str = 'ax',
                 entity_extraction_context_mode: str = 'dag'):
        self.entity_recognizer = entity_recognizer
        self.alias_pdf_raw = alias_pdf_raw
        self.stop_item_names = stop_item_names
        self.entity_extraction_mode = entity_extraction_mode
        self.llm_factory = llm_factory
        self.llm_model = llm_model
        self.entity_extraction_context_mode = entity_extraction_context_mode

    def should_execute(self, state: WorkflowState) -> bool:
        """Skip if error, fallback, or no entities"""
        if state.has_error():
            return False
        if state.is_fallback:
            return False

        # Check if we have extracted entities from Stage 1
        extracted_entities = state.extracted_entities
        if extracted_entities and len(extracted_entities.get('entities', [])) > 0:
            return True

        # Fallback: check if we have product items or kiwi entities
        json_objects = state.json_objects
        product_items = json_objects.get('product', [])
        if isinstance(product_items, dict):
            product_items = product_items.get('items', [])
        has_entities = len(product_items) > 0 or len(state.entities_from_kiwi) > 0
        return has_entities

    def execute(self, state: WorkflowState) -> WorkflowState:
        msg = state.msg
        json_objects = state.json_objects
        extracted_entities = state.extracted_entities

        # Get product items for fallback logic
        product_items = json_objects.get('product', [])
        if isinstance(product_items, dict):
            product_items = product_items.get('items', [])

        # Stage 2: Vocabulary Filtering
        if self.entity_extraction_mode == 'logic':
            # Logic mode: fuzzy matching
            entities_from_kiwi = state.entities_from_kiwi
            cand_entities = list(set(
                entities_from_kiwi + [item.get('name', '') for item in product_items if item.get('name')]
            ))
            logger.debug(f"ë¡œì§ ëª¨ë“œ cand_entities: {cand_entities}")
            similarities_fuzzy = self.entity_recognizer.extract_entities_with_fuzzy_matching(cand_entities)
        else:
            # LLM mode: vocabulary filtering
            if self.llm_factory:
                llm_models = self.llm_factory.create_models([self.llm_model])
            else:
                logger.warning("llm_factoryê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                llm_models = []

            if extracted_entities:
                # Use extracted entities from Stage 1
                entities = extracted_entities.get('entities', [])
                context_text = extracted_entities.get('context_text', '')
                logger.info(f"ğŸ” Stage 2 ì‹œì‘: {len(entities)}ê°œ ì—”í‹°í‹° í•„í„°ë§ (context_mode={self.entity_extraction_context_mode})")

                similarities_fuzzy = self.entity_recognizer._filter_with_vocabulary(
                    entities=entities,
                    context_text=context_text,
                    context_mode=self.entity_extraction_context_mode,
                    msg_text=msg,
                    rank_limit=100,
                    llm_model=llm_models[0] if llm_models else None
                )
                logger.info(f"âœ… Stage 2 ì™„ë£Œ: {similarities_fuzzy.shape[0] if not similarities_fuzzy.empty else 0}ê°œ ì—”í‹°í‹° í•„í„°ë§ë¨")
            else:
                # Fallback: no extracted entities, use wrapper
                logger.warning("extracted_entitiesê°€ ì—†ìŠµë‹ˆë‹¤. wrapperë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                llm_result = self.entity_recognizer.extract_entities_with_llm(
                    msg,
                    llm_models=llm_models,
                    rank_limit=100,
                    external_cand_entities=[],
                    context_mode=self.entity_extraction_context_mode,
                    pre_extracted=None,
                )

                if isinstance(llm_result, dict):
                    similarities_fuzzy = llm_result.get('similarities_df', pd.DataFrame())
                else:
                    similarities_fuzzy = llm_result

        logger.info(f"similarities_fuzzy í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")

        # Alias type filtering
        if not similarities_fuzzy.empty:
            merged_df = similarities_fuzzy.merge(
                self.alias_pdf_raw[['alias_1', 'type']].drop_duplicates(),
                left_on='item_name_in_msg',
                right_on='alias_1',
                how='left'
            )
            filtered_df = merged_df[merged_df.apply(
                lambda x: (
                    replace_special_chars_with_space(x['item_nm_alias']) in replace_special_chars_with_space(x['item_name_in_msg']) or
                    replace_special_chars_with_space(x['item_name_in_msg']) in replace_special_chars_with_space(x['item_nm_alias'])
                ) if x['type'] != 'expansion' else True,
                axis=1
            )]
            logger.debug(f"alias í•„í„°ë§ í›„ í¬ê¸°: {filtered_df.shape}")

        # Map products to entities
        if not similarities_fuzzy.empty:
            matched_products = self.entity_recognizer.map_products_to_entities(similarities_fuzzy, json_objects)
            logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ìˆ˜: {len(matched_products)}ê°œ")
        else:
            # Fallback: use LLM results directly with item_id='#'
            filtered_product_items = [
                d for d in product_items
                if d.get('name') and d['name'] not in self.stop_item_names
            ]
            matched_products = [
                {
                    'item_nm': d.get('name', ''),
                    'item_id': ['#'],
                    'item_name_in_msg': [d.get('name', '')],
                    'expected_action': [d.get('action', 'ê¸°íƒ€')]
                }
                for d in filtered_product_items
            ]
            logger.info(f"í´ë°± ìƒí’ˆ ìˆ˜ (item_id=#): {len(matched_products)}ê°œ")

        state.matched_products = matched_products
        return state


class ResultConstructionStep(WorkflowStep):
    """
    ìµœì¢… ê²°ê³¼ êµ¬ì„± ë‹¨ê³„ (Step 9)

    ì±…ì„:
        - matched_productsë¥¼ final_resultì— ë°˜ì˜
        - offer ê°ì²´ ìƒì„± (product/org íƒ€ì…)
        - ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¥ ì •ë³´ ë§¤ì¹­
        - í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘
        - entity_dag ì´ˆê¸°í™”, message_id ì¶”ê°€

    í˜‘ë ¥ ê°ì²´:
        - ResultBuilder: ê²°ê³¼ ì¡°ë¦½ ë¡œì§ ìˆ˜í–‰

    ë°ì´í„° íë¦„:
        ì…ë ¥: json_objects, matched_products, msg, pgm_info, message_id
        ì¶œë ¥: final_result (ìµœì¢… ì¶”ì¶œ ê²°ê³¼)
    """

    def __init__(self, result_builder):
        self.result_builder = result_builder

    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state

        json_objects = state.json_objects
        msg = state.msg
        pgm_info = state.pgm_info
        matched_products = state.matched_products
        message_id = state.message_id

        final_result = self.result_builder.assemble_result(
            json_objects, matched_products, msg, pgm_info, message_id
        )

        state.final_result = final_result
        return state


class ValidationStep(WorkflowStep):
    """
    ê²°ê³¼ ê²€ì¦ ë‹¨ê³„ (Step 10)

    ì±…ì„:
        - ìµœì¢… ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
        - í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        - ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: final_result
        ì¶œë ¥: validated final_result
    
    ê²€ì¦ í•­ëª©:
        - í•„ìˆ˜ í•„ë“œ: title, purpose, product, channel, pgm, offer
        - ë°ì´í„° íƒ€ì… ê²€ì¦
        - ë¹ˆ ê°’ ì²˜ë¦¬
    
    ë¡œê¹… ì •ë³´:
        - ì œëª©, ëª©ì , íŒë§¤ ìŠ¤í¬ë¦½íŠ¸
        - ìƒí’ˆ/ì±„ë„/í”„ë¡œê·¸ë¨ ê°œìˆ˜
        - offer íƒ€ì… ë° í•­ëª© ìˆ˜
    """
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        if state.has_error():
            return state
        
        final_result = state.get("final_result")
        # extractor = state.get("extractor") # No longer needed for validate_extraction_result
        
        # ê²°ê³¼ ê²€ì¦ (helpers ëª¨ë“ˆ ì‚¬ìš©)
        validated_result = validate_extraction_result(final_result)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
        self._log_final_summary(validated_result)
        
        state.set("final_result", validated_result)
        
        return state
    
    def _log_final_summary(self, result: Dict[str, Any]):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…"""
        logger.info("=== ìµœì¢… ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì œëª©: {result.get('title', 'N/A')}")
        logger.info(f"ëª©ì : {result.get('purpose', [])}")
        
        sales_script = result.get('sales_script', '')
        if sales_script:
            preview = sales_script[:100] + "..." if len(sales_script) > 100 else sales_script
            logger.info(f"íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {preview}")
        
        logger.info(f"ìƒí’ˆ ìˆ˜: {len(result.get('product', []))}ê°œ")
        logger.info(f"ì±„ë„ ìˆ˜: {len(result.get('channel', []))}ê°œ")
        logger.info(f"í”„ë¡œê·¸ë¨ ìˆ˜: {len(result.get('pgm', []))}ê°œ")
        
        offer_info = result.get('offer', {})
        logger.info(f"ì˜¤í¼ íƒ€ì…: {offer_info.get('type', 'N/A')}")
        logger.info(f"ì˜¤í¼ í•­ëª© ìˆ˜: {len(offer_info.get('value', []))}ê°œ")


class DAGExtractionStep(WorkflowStep):
    """
    DAG ì¶”ì¶œ ë‹¨ê³„ (Step 11, ì„ íƒì )

    ì±…ì„:
        - LLM ê¸°ë°˜ ì—”í‹°í‹° ê°„ ê´€ê³„ ë¶„ì„
        - DAG(Directed Acyclic Graph) ìƒì„±
        - NetworkX ê·¸ë˜í”„ êµ¬ì¡° ìƒì„±
        - Graphviz ë‹¤ì´ì–´ê·¸ë¨ ì´ë¯¸ì§€ ìƒì„±
    
    í˜‘ë ¥ ê°ì²´:
        - DAGParser: DAG í…ìŠ¤íŠ¸ íŒŒì‹±
        - extract_dag: LLM ê¸°ë°˜ DAG ì¶”ì¶œ
    
    ë°ì´í„° íë¦„:
        ì…ë ¥: msg, extract_entity_dag í”Œë˜ê·¸, message_id
        ì¶œë ¥: entity_dag (DAG ì—£ì§€ ë¦¬ìŠ¤íŠ¸)
    
    ì¶œë ¥ í˜•ì‹:
        entity_dag: [
            "(ìƒí’ˆA:êµ¬ë§¤) -[íšë“]-> (í˜œíƒB:ì œê³µ)",
            "(ì´ë²¤íŠ¸C:ì°¸ì—¬) -[ì‘ëª¨]-> (í˜œíƒB:ì œê³µ)"
        ]
    
    íŠ¹ì´ì‚¬í•­:
        - extract_entity_dag=Falseì´ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
        - DAG ë‹¤ì´ì–´ê·¸ë¨ì€ ./dag_images/ ë””ë ‰í† ë¦¬ì— ì €ì¥
        - ì‹¤íŒ¨ ì‹œì—ë„ ë¹ˆ ë°°ì—´ë¡œ ì²˜ë¦¬ (ì—ëŸ¬ ì „íŒŒ ì•ˆ í•¨)
    """
    
    def __init__(self, dag_parser=None):
        """
        Args:
            dag_parser: DAGParser ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­, Noneì´ë©´ ìë™ ìƒì„±)
        """
        from .entity_dag_extractor import DAGParser
        self.dag_parser = dag_parser or DAGParser()
    
    def execute(self, state: WorkflowState) -> WorkflowState:
        """
        DAG ì¶”ì¶œ ì‹¤í–‰

        Args:
            state: í˜„ì¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ

        Returns:
            ì—…ë°ì´íŠ¸ëœ ì›Œí¬í”Œë¡œìš° ìƒíƒœ (entity_dag í•„ë“œ ì¶”ê°€)
        """
        # extract_entity_dag í”Œë˜ê·¸ í™•ì¸
        extractor = state.get("extractor")
        if not extractor.extract_entity_dag:
            logger.info("DAG ì¶”ì¶œì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            # ë¹„í™œì„±í™”ëœ ê²½ìš° ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            final_result = state.get("final_result", {})
            final_result['entity_dag'] = []
            state.set("final_result", final_result)

            raw_result = state.get("raw_result", {})
            raw_result['entity_dag'] = []
            state.set("raw_result", raw_result)
            return state

        msg = state.get("msg")
        message_id = state.get("message_id", "#")

        # NOTE: ONT ìµœì í™” ì œê±° - ëª¨ë“  context modeì—ì„œ ë™ì¼í•˜ê²Œ fresh LLM callë¡œ DAG ì¶”ì¶œ
        # (ì´ì „: ONT ëª¨ë“œì—ì„œ ont_extraction_result ì¬ì‚¬ìš©ìœ¼ë¡œ LLM ì¬í˜¸ì¶œ ë°©ì§€)
        logger.info("ğŸ”— DAG ì¶”ì¶œ ì‹œì‘...")

        try:
            # entity_dag_extractorì˜ extract_dag í•¨ìˆ˜ í˜¸ì¶œ
            from .entity_dag_extractor import extract_dag

            dag_result = extract_dag(
                self.dag_parser,
                msg,
                extractor.llm_model,
                prompt_mode='cot'
            )

            # DAG ì„¹ì…˜ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë¹ˆ ì¤„ ì œê±° ë° ì •ë ¬)
            dag_list = sorted([
                d.strip() for d in dag_result['dag_section'].split('\n')
                if d.strip()
            ])

            logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {len(dag_list)}ê°œ ì—£ì§€")

            # final_resultì— entity_dag ì¶”ê°€
            final_result = state.get("final_result", {})
            final_result['entity_dag'] = dag_list
            state.set("final_result", final_result)

            # raw_resultì—ë„ ì¶”ê°€
            raw_result = state.get("raw_result", {})
            raw_result['entity_dag'] = dag_list
            state.set("raw_result", raw_result)

            # DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (ì„ íƒì )
            if dag_result['dag'].number_of_nodes() > 0:
                try:
                    from utils import create_dag_diagram, sha256_hash
                    dag_filename = f'dag_{message_id}_{sha256_hash(msg)}'
                    create_dag_diagram(dag_result['dag'], filename=dag_filename)
                    logger.info(f"ğŸ“Š DAG ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥: {dag_filename}.png")
                except Exception as e:
                    logger.warning(f"DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        except Exception as e:
            logger.error(f"âŒ DAG ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            final_result = state.get("final_result", {})
            final_result['entity_dag'] = []
            state.set("final_result", final_result)

            raw_result = state.get("raw_result", {})
            raw_result['entity_dag'] = []
            state.set("raw_result", raw_result)

        return state

    def _execute_from_ont(self, state: WorkflowState, ont_result: dict, msg: str, message_id: str) -> WorkflowState:
        """
        ONT ê²°ê³¼ì—ì„œ DAG ìƒì„± (LLM í˜¸ì¶œ ì—†ìŒ)

        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            ont_result: ONT ëª¨ë“œì—ì„œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°
            msg: ì›ë³¸ ë©”ì‹œì§€
            message_id: ë©”ì‹œì§€ ID

        Returns:
            ì—…ë°ì´íŠ¸ëœ ì›Œí¬í”Œë¡œìš° ìƒíƒœ
        """
        from .entity_dag_extractor import build_dag_from_ontology

        try:
            # 1. relationshipsì—ì„œ DAG ë¦¬ìŠ¤íŠ¸ ìƒì„±
            # í˜•ì‹: (entity value:entity type) -[relationship]-> (entity value:entity type)
            entity_types = ont_result.get('entity_types', {})
            relationships = ont_result.get('relationships', [])
            dag_lines = []

            for rel in relationships:
                src = rel.get('source', '')
                tgt = rel.get('target', '')
                rel_type = rel.get('type', '')

                if src and tgt and rel_type:
                    src_type = entity_types.get(src, 'Unknown')
                    tgt_type = entity_types.get(tgt, 'Unknown')
                    # DAG ëª¨ë“œì™€ ë™ì¼í•œ í˜•ì‹: (entity:type) -[relation]-> (entity:type)
                    dag_line = f"({src}:{src_type}) -[{rel_type}]-> ({tgt}:{tgt_type})"
                    dag_lines.append(dag_line)

            dag_list = sorted([d for d in dag_lines if d])

            # 2. NetworkX ê·¸ë˜í”„ ìƒì„±
            dag = build_dag_from_ontology(ont_result)

            logger.info(f"âœ… ONT ê¸°ë°˜ DAG ìƒì„± ì™„ë£Œ: {len(dag_list)}ê°œ ì—£ì§€, {dag.number_of_nodes()} ë…¸ë“œ")

            # 3. ê²°ê³¼ ì €ì¥
            final_result = state.get("final_result", {})
            final_result['entity_dag'] = dag_list
            state.set("final_result", final_result)

            raw_result = state.get("raw_result", {})
            raw_result['entity_dag'] = dag_list
            state.set("raw_result", raw_result)

            # 4. ì´ë¯¸ì§€ ìƒì„±
            if dag.number_of_nodes() > 0:
                try:
                    from utils import create_dag_diagram, sha256_hash
                    dag_filename = f'dag_{message_id}_{sha256_hash(msg)}'
                    create_dag_diagram(dag, filename=dag_filename)
                    logger.info(f"ğŸ“Š DAG ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ (ONT): {dag_filename}.png")
                except Exception as e:
                    logger.warning(f"DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        except Exception as e:
            logger.error(f"âŒ ONT ê¸°ë°˜ DAG ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            final_result = state.get("final_result", {})
            final_result['entity_dag'] = []
            state.set("final_result", final_result)

            raw_result = state.get("raw_result", {})
            raw_result['entity_dag'] = []
            state.set("raw_result", raw_result)

        return state

