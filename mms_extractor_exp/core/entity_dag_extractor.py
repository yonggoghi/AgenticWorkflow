"""
Entity DAG ì¶”ì¶œê¸° (Entity DAG Extractor) - ì—”í‹°í‹° ê´€ê³„ ê·¸ë˜í”„ ë¶„ì„ ì‹œìŠ¤í…œ
=====================================================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬
DAG(Directed Acyclic Graph) í˜•íƒœë¡œ ì‹œê°í™”í•˜ëŠ” ì „ë¬¸ ë„êµ¬ì…ë‹ˆë‹¤.
LLMì„ í™œìš©í•˜ì—¬ ê´‘ê³  ë‚´ìš©ì—ì„œ ì—”í‹°í‹°ë“¤ ê°„ì˜ ì¸ê³¼ê´€ê³„, ìˆœì°¨ì  ì•¡ì…˜,
ì˜ì¡´ì„± ë“±ì„ íŒŒì•…í•˜ì—¬ êµ¬ì¡°í™”ëœ ê·¸ë˜í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
-----------
1. **ì—”í‹°í‹° ê´€ê³„ ë¶„ì„**: í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ ì—°ê²° ê´€ê³„ ì‹ë³„
2. **DAG ìƒì„±**: ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„ êµ¬ì¡° ìƒì„±
3. **ì‹œê°í™”**: NetworkXì™€ Graphvizë¥¼ ì‚¬ìš©í•œ ê·¸ë˜í”„ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
4. **ê´€ê³„ ë¶„ë¥˜**: ì—ì´ì „íŠ¸-ì•¡ì…˜, ì›ì¸-ê²°ê³¼, ìˆœì°¨ì  í”„ë¡œì„¸ìŠ¤ ë“± ë‹¤ì–‘í•œ ê´€ê³„ íƒ€ì… ì§€ì›
5. **ê²€ì¦ ë° ì •ì œ**: ìƒì„±ëœ DAGì˜ ìœ íš¨ì„± ê²€ì‚¬ ë° ìˆœí™˜ ì°¸ì¡° ë°©ì§€

ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ
-----------
- **LLM ëª¨ë¸**: OpenAI GPT, Anthropic Claude ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
- **ê·¸ë˜í”„ ë¼ì´ë¸ŒëŸ¬ë¦¬**: NetworkX (DAG ì¡°ì‘ ë° ê²€ì¦)
- **ì‹œê°í™”**: Graphviz (PNG/SVG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±)
- **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: ì™¸ë¶€í™”ëœ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ

"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
import traceback
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from prompts.dag_extraction_prompt import build_dag_extraction_prompt
from langchain_core.output_parsers import JsonOutputParser
import json
import re
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from openai import OpenAI
from typing import List, Tuple, Union, Dict, Any, Optional, Set
import ast
from rapidfuzz import fuzz, process
import glob
import os
from config import settings
import networkx as nx
import random
from utils import create_dag_diagram, sha256_hash

llm_api_key = settings.API_CONFIG.llm_api_key
llm_api_url = settings.API_CONFIG.llm_api_url
client = OpenAI(
    api_key = llm_api_key,
    base_url = llm_api_url
)

# LLM ëª¨ë¸ ì„¤ì •
llm_gem = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gemma_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_ax = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.ax_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_cld = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.claude_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_gen = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gemini_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

llm_gpt = ChatOpenAI(
        temperature=0,
        openai_api_key=llm_api_key,
        openai_api_base=llm_api_url,
        model=settings.ModelConfig.gpt_model,
        max_tokens=settings.ModelConfig.llm_max_tokens
        )

# ë°ì´í„° íŒŒì¼ë“¤ì„ ì¡°ê±´ë¶€ë¡œ ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•  ë•Œë§Œ)
stop_item_names = []
mms_pdf = pd.DataFrame()

# Stop words ë¡œë“œ
try:
    stop_words_path = getattr(settings.METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv')
    if os.path.exists(stop_words_path):
        logger.info(f"Stop words íŒŒì¼ ë¡œë“œ ì¤‘: {stop_words_path}")
        stop_item_names = pd.read_csv(stop_words_path)['stop_words'].to_list()
        logger.info(f"Stop words ë¡œë“œ ì™„ë£Œ: {len(stop_item_names)}ê°œ")
    else:
        logger.warning(f"Stop words íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stop_words_path}")
except Exception as e:
    logger.warning(f"Stop words íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    stop_item_names = []

# MMS ë©”ì‹œì§€ ë°ì´í„° ë¡œë“œ
try:
    mms_msg_path = getattr(settings.METADATA_CONFIG, 'mms_msg_path', './data/mms_messages.csv')
    
    if os.path.exists(mms_msg_path):
        logger.info(f"MMS ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {mms_msg_path}")
        mms_pdf = pd.read_csv(mms_msg_path)
        logger.info(f"MMS ë°ì´í„° ì›ë³¸ í¬ê¸°: {mms_pdf.shape}")
        logger.info(f"MMS ë°ì´í„° ì»¬ëŸ¼ë“¤: {list(mms_pdf.columns)}")
        
        # ì»¬ëŸ¼ëª… í™•ì¸ ë° í‘œì¤€í™”
        if 'msg' not in mms_pdf.columns:
            # 1. ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ msg ì»¬ëŸ¼ ì°¾ê¸°
            msg_col_candidates = [col for col in mms_pdf.columns if col.lower() == 'msg']
            if msg_col_candidates:
                logger.info(f"'msg' ì»¬ëŸ¼ì„ '{msg_col_candidates[0]}'ë¡œ ë¦¬ë„¤ì„")
                mms_pdf = mms_pdf.rename(columns={msg_col_candidates[0]: 'msg'})
            # 2. mms_phrs ì»¬ëŸ¼ í™•ì¸ (ì¼ë°˜ì ì¸ MMS ë©”ì‹œì§€ ì»¬ëŸ¼ëª…)
            elif 'mms_phrs' in mms_pdf.columns:
                logger.info("'mms_phrs' ì»¬ëŸ¼ì„ 'msg'ë¡œ ë¦¬ë„¤ì„")
                mms_pdf = mms_pdf.rename(columns={'mms_phrs': 'msg'})
            # 3. MMS_PHRS ì»¬ëŸ¼ í™•ì¸ (ëŒ€ë¬¸ì ë²„ì „)
            elif 'MMS_PHRS' in mms_pdf.columns:
                logger.info("'MMS_PHRS' ì»¬ëŸ¼ì„ 'msg'ë¡œ ë¦¬ë„¤ì„")
                mms_pdf = mms_pdf.rename(columns={'MMS_PHRS': 'msg'})
            # 4. msg_nm ì»¬ëŸ¼ í™•ì¸ (ë©”ì‹œì§€ ì´ë¦„)
            elif 'msg_nm' in mms_pdf.columns:
                logger.info("'msg_nm' ì»¬ëŸ¼ì„ 'msg'ë¡œ ë¦¬ë„¤ì„")
                mms_pdf = mms_pdf.rename(columns={'msg_nm': 'msg'})
            else:
                logger.warning("'msg' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤:")
                logger.warning(f"{list(mms_pdf.columns)}")
                # ë¹ˆ DataFrameìœ¼ë¡œ ì„¤ì •
                mms_pdf = pd.DataFrame()
        
        # ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        if 'msg' in mms_pdf.columns:
            mms_pdf['msg'] = mms_pdf['msg'].astype('str')
            logger.info(f"'msg' ì»¬ëŸ¼ì„ ë¬¸ìì—´ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ")
            
            # ë°ì´í„° í’ˆì§ˆ í™•ì¸
            null_count = mms_pdf['msg'].isnull().sum()
            empty_count = (mms_pdf['msg'] == '').sum()
            valid_count = len(mms_pdf) - null_count - empty_count
            logger.info(f"MMS ë©”ì‹œì§€ í’ˆì§ˆ: ìœ íš¨={valid_count}, ë¹ˆê°’={empty_count}, null={null_count}")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not mms_pdf.empty and valid_count > 0:
                sample_msgs = mms_pdf['msg'].dropna().head(2).tolist()
                logger.info(f"MMS ë©”ì‹œì§€ ìƒ˜í”Œ: {[msg[:50]+'...' if len(msg) > 50 else msg for msg in sample_msgs]}")
        
        logger.info(f"MMS ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(mms_pdf)}ê°œ í–‰")
    else:
        logger.warning(f"MMS ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mms_msg_path}")
        logger.warning("ìƒ˜í”Œ ë©”ì‹œì§€ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ --prompt_mode simple ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        mms_pdf = pd.DataFrame()
        
except Exception as e:
    logger.error(f"MMS ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
    mms_pdf = pd.DataFrame()  # ë¹ˆ DataFrameìœ¼ë¡œ ì´ˆê¸°í™”

###############################################################################
# 1) ê¸°ì¡´ ì •ê·œì‹ ë° íŒŒì„œ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
###############################################################################
PAT = re.compile(r"\((.*?)\)\s*-\[(.*?)\]->\s*\((.*?)\)")
NODE_ONLY = re.compile(r"\((.*?)\)\s*$")

def parse_dag_block(text: str):
    nodes: Set[str] = set()
    edges: List[Tuple[str,str,str]] = []
    for line in filter(None, map(str.strip, text.splitlines())):
        m = PAT.match(line)
        if m:                            # (A) -[rel]-> (B)
            src, rel, dst = map(str.strip, m.groups())
            nodes.update([src, dst])
            edges.append((src, dst, rel))
            continue

        m2 = NODE_ONLY.match(line)       # (A:B:C)
        if m2:                           # â† ë…ë¦½ ë…¸ë“œ
            nodes.add(m2.group(1).strip())
            continue

        raise ValueError(f"ëª»ì½ì€ ë¼ì¸:\n  {line}")
    return list(nodes), edges

###############################################################################
# 2) ê°œì„ ëœ ë…¸ë“œ ìŠ¤í”Œë¦¬í„° â€“ ìœ ì—°í•œ íŒŒíŠ¸ ì²˜ë¦¬
###############################################################################
def split_node_parts(raw: str) -> Dict[str,str]:
    parts = [p.strip() for p in raw.split(":")]
    
    if len(parts) == 1:                       # entity only
        ent, act, kpi = parts[0], "", ""
    elif len(parts) == 2:                     # entity:metric
        ent, act = parts
        kpi = ""
    elif len(parts) == 3:                     # entity:action:metric
        ent, act, kpi = parts
    elif len(parts) == 4:                     # entity:action:behavior:metric
        ent, act, behavior, kpi = parts
        # Combine action and behavior, or handle separately
        act = f"{act}:{behavior}"  # Option 1: combine them
        # Or you could add a new field:
        # return {"entity": ent, "action": act, "behavior": behavior, "metric": kpi}
    elif len(parts) >= 5:                     # Handle even more parts flexibly
        ent = parts[0]
        kpi = parts[-1]  # Last part is usually the metric
        act = ":".join(parts[1:-1])  # Everything in between becomes action
    else:
        # This shouldn't happen with len(parts) >= 1, but just in case
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë…¸ë“œ í˜•ì‹: {raw}")
    
    return {"entity": ent, "action": act, "metric": kpi}

###############################################################################
# 3) ê¸°ì¡´ DAG ë¹Œë” (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
###############################################################################
def build_dag_graph(nodes: List[str], edges: List[Tuple[str,str,str]]) -> nx.DiGraph:
    g = nx.DiGraph()
    
    # Add nodes with error handling
    for n in nodes:
        try:
            node_attrs = split_node_parts(n)
            g.add_node(n, **node_attrs)
        except ValueError as e:
            print(f"Warning: {e}")
            # Add node with minimal attributes if parsing fails
            g.add_node(n, entity=n, action="", metric="")
    
    # Add edges
    for src, dst, rel in edges:
        g.add_edge(src, dst, relation=rel)
    
    return g

###############################################################################
# 4) ê¸°ì¡´ Path Finder (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
###############################################################################
def extract_root_to_leaf_paths(dag):
    """Generate all paths from root nodes (no predecessors) to leaf nodes (no successors)"""
    # Find root nodes (no incoming edges)
    root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]
    
    # Find leaf nodes (no outgoing edges)
    leaf_nodes = [node for node in dag.nodes() if dag.out_degree(node) == 0]
    
    all_paths = []
    for root in root_nodes:
        for leaf in leaf_nodes:
            try:
                paths = list(nx.all_simple_paths(dag, root, leaf))
                all_paths.extend(paths)
            except nx.NetworkXNoPath:
                continue
    
    return all_paths, root_nodes, leaf_nodes

###############################################################################
# 5) ìƒˆë¡œìš´ ê°œì„ ëœ DAGParser í´ë˜ìŠ¤
###############################################################################
class DAGParser:
    """
    DAG íŒŒì‹± í´ë˜ìŠ¤
    
    LLMì´ ìƒì„±í•œ DAG í…ìŠ¤íŠ¸ë¥¼ NetworkX ê·¸ë˜í”„ ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - LLM ì‘ë‹µì—ì„œ DAG ì„¹ì…˜ ì¶”ì¶œ
    - DAG í…ìŠ¤íŠ¸ë¥¼ NetworkX DiGraphë¡œ íŒŒì‹±
    - ë…¸ë“œì™€ ì—£ì§€ ê´€ê³„ ë¶„ì„
    
    ì§€ì›í•˜ëŠ” DAG í˜•ì‹:
    - ì—£ì§€: (ì—”í‹°í‹°:í–‰ë™) -[ê´€ê³„ë™ì‚¬]-> (ì—”í‹°í‹°:í–‰ë™)
    - ë…ë¦½ ë…¸ë“œ: (ì—”í‹°í‹°:í–‰ë™)
    """
    
    def __init__(self):
        # ê°œì„ ëœ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ - ê´€ê³„ ë¶€ë¶„ì— ì‰¼í‘œì™€ ê³µë°± í—ˆìš©
        # ê´€ê³„ ë¶€ë¶„([...])ì— ëª¨ë“  ë¬¸ì í—ˆìš© (]ë¥¼ ì œì™¸í•˜ê³ )
        self.edge_pattern = r'\(([^:)]+):([^)]+)\)\s*-\[([^\]]+)\]->\s*\(([^:)]+):([^)]+)\)'
        # ë…ë¦½í˜• ë…¸ë“œ íŒ¨í„´ ì¶”ê°€
        self.standalone_node_pattern = r'\(([^:)]+):([^)]+)\)\s*$'
        # ì„¹ì…˜ íŒ¨í„´ ìˆ˜ì •: ## ë˜ëŠ” ###ë¡œ ì‹œì‘í•˜ëŠ” 2. ì¶”ì¶œëœ DAG ì„¹ì…˜
        self.section_pattern = r'#{2,3}\s*2\.\s*ì¶”ì¶œëœ\s*DAG'
        
    def parse_dag_line(self, line: str) -> Optional[Union[Tuple[str, str, str, str, str], Tuple[str, str]]]:
        """ë‹¨ì¼ DAG ë¼ì¸ì„ íŒŒì‹±í•˜ì—¬ êµ¬ì„± ìš”ì†Œ ë°˜í™˜"""
        # ë¨¼ì € ì—£ì§€ íŒ¨í„´ í™•ì¸
        edge_match = re.match(self.edge_pattern, line)
        if edge_match:
            return (
                edge_match.group(1).strip(),  # src_entity
                edge_match.group(2).strip(),  # src_action
                edge_match.group(3).strip(),  # relation (ì‰¼í‘œ, ì¡°ê±´ í¬í•¨ ê°€ëŠ¥)
                edge_match.group(4).strip(),  # dst_entity
                edge_match.group(5).strip()   # dst_action
            )
        
        # ë…ë¦½í˜• ë…¸ë“œ íŒ¨í„´ í™•ì¸
        standalone_match = re.match(self.standalone_node_pattern, line)
        if standalone_match:
            return (
                standalone_match.group(1).strip(),  # entity
                standalone_match.group(2).strip()   # action
            )
        
        return None
        
    def extract_dag_section(self, full_text: str) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ DAG ì„¹ì…˜ë§Œ ì¶”ì¶œ"""
        lines = full_text.split('\n')
        
        # ë” ìœ ì—°í•œ DAG ì„¹ì…˜ ì°¾ê¸°
        dag_section_patterns = [
            r'#{2,3}\s*ìµœì¢…\s*DAG',            # ìµœì¢… DAG
            r'#{2,3}\s*4\.\s*ìˆ˜ì •ëœ\s*DAG',    # 4. ìˆ˜ì •ëœ DAG ìµœìš°ì„ 
            r'#{2,3}\s*ìˆ˜ì •ëœ\s*DAG',          # ìˆ˜ì •ëœ DAG
            r'#{2,3}\s*2\.\s*ì¶”ì¶œëœ\s*DAG',    # 2. ì¶”ì¶œëœ DAG (ê¸°ë³¸)
            r'#{2,3}\s*DAG',
            r'ì¶”ì¶œëœ\s*DAG',
            r'2\.\s*ì¶”ì¶œëœ\s*DAG'
        ]
        
        # DAG ì„¹ì…˜ ì°¾ê¸°
        start_idx = -1
        end_idx = len(lines)
        in_dag_section = False
        in_code_block = False
        
        # íŒ¨í„´ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        for pattern in dag_section_patterns:
            for i, line in enumerate(lines):
                if re.search(pattern, line, re.IGNORECASE):
                    in_dag_section = True
                    
                    # DAG í—¤ë” ë‹¤ìŒì—ì„œ ``` ì°¾ê¸° (ë¹ˆ ì¤„ì´ ìˆì–´ë„ ê´œì°®ìŒ)
                    code_block_found = False
                    for j in range(i + 1, min(i + 4, len(lines))):  # ìµœëŒ€ 3ì¤„ê¹Œì§€ í™•ì¸
                        next_line = lines[j].strip()
                        if next_line == '```':
                            start_idx = j + 1
                            in_code_block = True
                            code_block_found = True
                            break
                        elif next_line and not next_line == '':  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë‚´ìš©ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                            # DAG íŒ¨í„´ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì´ë©´ ì½”ë“œë¸”ë¡ ì—†ì´ ì‹œì‘
                            if (re.match(self.edge_pattern, next_line) or 
                                re.match(self.standalone_node_pattern, next_line)):
                                start_idx = j
                                in_code_block = False
                                code_block_found = True
                                break
                            else:
                                break
                    
                    # ì½”ë“œë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ í—¤ë” ë‹¤ìŒ ì¤„ë¶€í„° ì‹œì‘
                    if not code_block_found:
                        start_idx = i + 1
                        in_code_block = False
                    
                    break
            if start_idx != -1:
                break
        
        # DAG ì„¹ì…˜ ì¢…ë£Œ ì¡°ê±´ ì°¾ê¸°
        if start_idx != -1:
            for i in range(start_idx, len(lines)):
                line = lines[i]
                if in_code_block and line.strip() == '```':
                    end_idx = i
                    break
                elif not in_code_block and re.match(r'#{2,3}\s*[3-9]\.', line):  # 3ë²ˆ ì´ìƒ ì„¹ì…˜ì—ì„œ ì¢…ë£Œ
                    end_idx = i
                    break
        
        if start_idx == -1:
            # ì„¹ì…˜ í—¤ë”ê°€ ì—†ëŠ” ê²½ìš°, DAG íŒ¨í„´ì„ ì§ì ‘ ì°¾ê¸°
            dag_lines = []
            for line in lines:
                if (re.match(self.edge_pattern, line) or 
                    re.match(self.standalone_node_pattern, line) or 
                    line.strip().startswith('#')):
                    dag_lines.append(line)
            
            if dag_lines:
                result = '\n'.join(dag_lines)
                return result
            else:
                raise ValueError("DAG ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        result = '\n'.join(lines[start_idx:end_idx])
        return result
    
    def parse_dag(self, dag_text: str) -> nx.DiGraph:
        """DAG í…ìŠ¤íŠ¸ë¥¼ NetworkX DiGraphë¡œ ë³€í™˜"""
        G = nx.DiGraph()
        
        # í†µê³„ ì •ë³´ ì €ì¥
        stats = {
            'total_edges': 0,
            'comment_lines': 0,
            'empty_lines': 0,
            'paths': [],
            'parse_errors': [],
            'parsed_lines': []
        }
        
        current_path = None
        
        for line_num, line in enumerate(dag_text.strip().split('\n'), 1):
            line = line.strip()
            
            # ë¹ˆ ë¼ì¸ ì²˜ë¦¬
            if not line:
                stats['empty_lines'] += 1
                continue
            
            # ì£¼ì„ ë¼ì¸ ì²˜ë¦¬ (ê²½ë¡œ ì •ë³´ ì¶”ì¶œ)
            if line.startswith('#'):
                stats['comment_lines'] += 1
                current_path = line[1:].strip()
                if current_path:
                    stats['paths'].append(current_path)
                continue
            
            # DAG ì—£ì§€ ë˜ëŠ” ë…ë¦½í˜• ë…¸ë“œ íŒŒì‹±
            parsed = self.parse_dag_line(line)
            if parsed:
                try:
                    if len(parsed) == 5:  # ì—£ì§€ (src_entity, src_action, relation, dst_entity, dst_action)
                        src_entity, src_action, relation, dst_entity, dst_action = parsed
                        
                        # ë…¸ë“œ ID ìƒì„±
                        src_node = f"{src_entity}:{src_action}"
                        dst_node = f"{dst_entity}:{dst_action}"
                        
                        # ë…¸ë“œ ì¶”ê°€ (ì†ì„± í¬í•¨)
                        G.add_node(src_node, 
                                  entity=src_entity, 
                                  action=src_action,
                                  path=current_path)
                        G.add_node(dst_node, 
                                  entity=dst_entity, 
                                  action=dst_action,
                                  path=current_path)
                        
                        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„ì— ì‰¼í‘œë‚˜ ì¡°ê±´ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ)
                        G.add_edge(src_node, dst_node, 
                                  relation=relation,
                                  path=current_path)
                        
                        stats['total_edges'] += 1
                        stats['parsed_lines'].append(f"Line {line_num}: {src_node} -[{relation}]-> {dst_node}")
                        
                    elif len(parsed) == 2:  # ë…ë¦½í˜• ë…¸ë“œ (entity, action)
                        entity, action = parsed
                        
                        # ë…¸ë“œ ID ìƒì„±
                        node_id = f"{entity}:{action}"
                        
                        # ë…ë¦½í˜• ë…¸ë“œ ì¶”ê°€
                        G.add_node(node_id, 
                                  entity=entity, 
                                  action=action,
                                  path=current_path)
                        
                        stats['parsed_lines'].append(f"Line {line_num}: Standalone node {node_id}")
                    
                except Exception as e:
                    stats['parse_errors'].append(f"Line {line_num}: {str(e)}")
            else:
                # íŒŒì‹± ì‹¤íŒ¨í•œ ë¼ì¸ ê¸°ë¡ (ì£¼ì„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                if not line.startswith('#') and line.strip():
                    stats['parse_errors'].append(f"Line {line_num}: íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ - {line[:80]}...")
        
        # ê·¸ë˜í”„ì— í†µê³„ ì •ë³´ ì €ì¥
        G.graph['stats'] = stats
        
        return G
    
    def get_root_nodes(self, G: nx.DiGraph) -> List[str]:
        """Root ë…¸ë“œ(ë“¤ì–´ì˜¤ëŠ” ì—£ì§€ê°€ ì—†ëŠ” ë…¸ë“œ) ì°¾ê¸°"""
        return [node for node in G.nodes() if G.in_degree(node) == 0]
    
    def get_leaf_nodes(self, G: nx.DiGraph) -> List[str]:
        """Leaf ë…¸ë“œ(ë‚˜ê°€ëŠ” ì—£ì§€ê°€ ì—†ëŠ” ë…¸ë“œ) ì°¾ê¸°"""
        return [node for node in G.nodes() if G.out_degree(node) == 0]
    
    def get_paths_from_root_to_leaf(self, G: nx.DiGraph) -> List[List[str]]:
        """Rootì—ì„œ Leafê¹Œì§€ì˜ ëª¨ë“  ê²½ë¡œ ì°¾ê¸°"""
        roots = self.get_root_nodes(G)
        leaves = self.get_leaf_nodes(G)
        
        all_paths = []
        for root in roots:
            for leaf in leaves:
                try:
                    paths = list(nx.all_simple_paths(G, root, leaf))
                    all_paths.extend(paths)
                except nx.NetworkXNoPath:
                    continue
        
        return all_paths
    
    def analyze_graph(self, G: nx.DiGraph) -> Dict:
        """ê·¸ë˜í”„ ë¶„ì„ ì •ë³´ ìƒì„±"""
        analysis = {
            'num_nodes': G.number_of_nodes(),
            'num_edges': G.number_of_edges(),
            'root_nodes': self.get_root_nodes(G),
            'leaf_nodes': self.get_leaf_nodes(G),
            'is_dag': nx.is_directed_acyclic_graph(G),
            'num_components': nx.number_weakly_connected_components(G),
            'paths_info': G.graph.get('stats', {}).get('paths', []),
            'longest_path_length': 0
        }
        
        # ìµœì¥ ê²½ë¡œ ì°¾ê¸°
        if analysis['is_dag'] and G.number_of_nodes() > 0:
            try:
                longest = nx.dag_longest_path(G)
                analysis['longest_path_length'] = len(longest) - 1 if longest else 0
            except:
                analysis['longest_path_length'] = 0
        
        return analysis
    
    def to_json(self, G: nx.DiGraph) -> str:
        """ê·¸ë˜í”„ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        data = {
            'nodes': [
                {
                    'id': node,
                    'entity': G.nodes[node].get('entity', ''),
                    'action': G.nodes[node].get('action', ''),
                    'path': G.nodes[node].get('path', '')
                }
                for node in G.nodes()
            ],
            'edges': [
                {
                    'source': edge[0],
                    'target': edge[1],
                    'relation': G.edges[edge].get('relation', ''),
                    'path': G.edges[edge].get('path', '')
                }
                for edge in G.edges()
            ],
            'analysis': self.analyze_graph(G)
        }
        return json.dumps(data, ensure_ascii=False, indent=2)
    
    def visualize_paths(self, G: nx.DiGraph) -> str:
        """ê²½ë¡œë³„ë¡œ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥"""
        output = []
        paths_dict = {}
        
        # ê²½ë¡œë³„ë¡œ ì—£ì§€ ê·¸ë£¹í™”
        for edge in G.edges():
            path = G.edges[edge].get('path', 'Unknown')
            if path not in paths_dict:
                paths_dict[path] = []
            paths_dict[path].append(edge)
        
        # ê²½ë¡œë³„ ì¶œë ¥
        for path, edges in paths_dict.items():
            if path and path != 'Unknown':
                output.append(f"\n[{path}]")
            for edge in edges:
                relation = G.edges[edge].get('relation', '')
                output.append(f"  {edge[0]} -{relation}-> {edge[1]}")
        
        return '\n'.join(output)


def build_dag_from_ontology(ont_result: dict) -> nx.DiGraph:
    """
    âš ï¸ DEPRECATED: No longer used since commit a4e1ef0.
    DAGExtractionStep now always makes fresh LLM call, even in ONT mode.
    This function is kept for potential future use or rollback.

    ONT ê²°ê³¼ì—ì„œ NetworkX DiGraph ìƒì„± (LLM í˜¸ì¶œ ì—†ìŒ)

    Args:
        ont_result: ONT ëª¨ë“œì—ì„œ ì¶”ì¶œëœ ê²°ê³¼
            {
                'dag_text': str,
                'entity_types': dict,  # {entity_id: type}
                'relationships': list  # [{source, target, type}, ...]
            }

    Returns:
        nx.DiGraph: DAG ê·¸ë˜í”„
    """
    G = nx.DiGraph()

    entity_types = ont_result.get('entity_types', {})
    relationships = ont_result.get('relationships', [])
    dag_text = ont_result.get('dag_text', '')

    # ë°©ë²• 1: relationshipsì—ì„œ ê·¸ë˜í”„ ìƒì„± (ë” ì •í™•í•œ íƒ€ì… ì •ë³´ ë³´ì¡´)
    if relationships:
        for rel in relationships:
            src = rel.get('source', '')
            tgt = rel.get('target', '')
            rel_type = rel.get('type', '')

            if src and tgt:
                # ë…¸ë“œ ì¶”ê°€ (íƒ€ì… ì •ë³´ í¬í•¨)
                src_type = entity_types.get(src, 'Unknown')
                tgt_type = entity_types.get(tgt, 'Unknown')

                # ë…¸ë“œ IDì— íƒ€ì… í¬í•¨ (ì˜ˆ: "9ì›” T day:Campaign")
                src_node_id = f"{src}:{src_type}"
                tgt_node_id = f"{tgt}:{tgt_type}"

                G.add_node(src_node_id, entity=src, entity_type=src_type, action='')
                G.add_node(tgt_node_id, entity=tgt, entity_type=tgt_type, action='')

                # ì—£ì§€ ì¶”ê°€
                G.add_edge(src_node_id, tgt_node_id, relation=rel_type)

        logger.info(f"ğŸ“Š ONT ê·¸ë˜í”„ ìƒì„± (relationships ê¸°ë°˜): {G.number_of_nodes()} ë…¸ë“œ, {G.number_of_edges()} ì—£ì§€")
        return G

    # ë°©ë²• 2: dag_text íŒŒì‹± (relationshipsê°€ ì—†ëŠ” ê²½ìš°)
    if dag_text:
        # DAG íŒ¨í„´: (Entity:Action) -[Relation]-> (Entity:Action)
        dag_pattern = r'\(([^:)]+):([^)]+)\)\s*-\[([^\]]+)\]->\s*\(([^:)]+):([^)]+)\)'
        matches = re.findall(dag_pattern, dag_text)

        for match in matches:
            src_entity, src_action, relation, dst_entity, dst_action = match

            src_node = f"{src_entity.strip()}:{src_action.strip()}"
            dst_node = f"{dst_entity.strip()}:{dst_action.strip()}"

            src_type = entity_types.get(src_entity.strip(), 'Unknown')
            dst_type = entity_types.get(dst_entity.strip(), 'Unknown')

            G.add_node(src_node, entity=src_entity.strip(), action=src_action.strip(), entity_type=src_type)
            G.add_node(dst_node, entity=dst_entity.strip(), action=dst_action.strip(), entity_type=dst_type)
            G.add_edge(src_node, dst_node, relation=relation.strip())

        logger.info(f"ğŸ“Š ONT ê·¸ë˜í”„ ìƒì„± (dag_text íŒŒì‹±): {G.number_of_nodes()} ë…¸ë“œ, {G.number_of_edges()} ì—£ì§€")

    return G


def extract_dag(parser: DAGParser, msg: str, llm_model, prompt_mode: str = 'cot'):
    """
    ì—”í‹°í‹° ê´€ê³„ DAG ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜
    ========================================
    
    ğŸ¯ ëª©ì 
    -------
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬
    ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG) í˜•íƒœë¡œ ì‹œê°í™” ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ğŸ”„ ì²˜ë¦¬ ê³¼ì •
    -----------
    1. **LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ**: ì „ë¬¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹° ê°„ ê´€ê³„ ì‹ë³„
    2. **êµ¬ì¡°í™”ëœ íŒŒì‹±**: ìì—°ì–´ ì„¤ëª…ì—ì„œ DAG ì„¹ì…˜ ì¶”ì¶œ ë° ì •ë¦¬
    3. **ê·¸ë˜í”„ ë³€í™˜**: ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ NetworkX ê·¸ë˜í”„ ìƒì„±
    4. **ê²€ì¦ ë° ì •ì œ**: DAG ìœ íš¨ì„± ê²€ì‚¬ ë° ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    
    ğŸ“Š ì¶œë ¥ ë°ì´í„°
    -----------
    - **dag_section**: íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸ (ì¸ê°„ ê°€ë…)
    - **dag**: NetworkX DiGraph ê°ì²´ (í”„ë¡œê·¸ë˜ë° í™œìš©)
    - **dag_raw**: LLM ì›ë³¸ ì‘ë‹µ (ë””ë²„ê¹… ìš©ë„)
    
    Args:
        parser (DAGParser): DAG íŒŒì‹± ì „ë¬¸ ê°ì²´
        msg (str): ë¶„ì„í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        llm_model: Langchain í˜¸í™˜ LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        prompt_mode (str): í”„ë¡¬í”„íŠ¸ ëª¨ë“œ ('cot' ë˜ëŠ” 'simple'). ê¸°ë³¸ê°’ 'cot'.
        
    Returns:
        dict: DAG ì¶”ì¶œ ê²°ê³¼
            {
                'dag_section': str,      # êµ¬ì¡°í™”ëœ DAG í…ìŠ¤íŠ¸
                'dag': nx.DiGraph,       # NetworkX ê·¸ë˜í”„ ê°ì²´
                'dag_raw': str,          # LLM ì›ë³¸ ì‘ë‹µ
                'nodes': List[str],      # ì¶”ì¶œëœ ë…¸ë“œ ëª©ë¡
                'edges': List[Tuple],    # ì¶”ì¶œëœ ì—£ì§€ ëª©ë¡
            }
    
    Raises:
        Exception: LLM API í˜¸ì¶œ ì‹¤íŒ¨, íŒŒì‹± ì˜¤ë¥˜ ë“±
        
    Example:
        >>> parser = DAGParser()
        >>> result = extract_dag(parser, "SKí…”ë ˆì½¤ í˜œíƒ ì•ˆë‚´...", llm_model, prompt_mode='simple')
        >>> print(f"DAG ë…¸ë“œ ìˆ˜: {result['dag'].number_of_nodes()}")
        >>> print(f"DAG ì—£ì§€ ìˆ˜: {result['dag'].number_of_edges()}")
    """
    
    # ì´ˆê¸° ë¡œê¹… ë° ìƒíƒœ ì„¤ì •
    logger.info("ğŸš€ DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info(f"ğŸ“ ì…ë ¥ ë©”ì‹œì§€ ê¸¸ì´: {len(msg)}ì")
    logger.info(f"ğŸ¤– ì‚¬ìš© LLM ëª¨ë¸: {llm_model}")
    logger.info(f"âš™ï¸  í”„ë¡¬í”„íŠ¸ ëª¨ë“œ: {prompt_mode}")
    
    # ë‹¨ê³„ 1: ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì—ì„œ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = build_dag_extraction_prompt(msg, mode=prompt_mode)
    
    # LLM í˜¸ì¶œ ì¤€ë¹„ ë¡œê¹…
    logger.info("ğŸ¤– LLMì— DAG ì¶”ì¶œ ìš”ì²­ ì¤‘...")
    logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
    
    # ë‹¨ê³„ 2: LLM í˜¸ì¶œì„ í†µí•œ ì—”í‹°í‹° ê°„ ê´€ê³„ ë¶„ì„
    try:
        # í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…/ë¯¸ë¦¬ë³´ê¸°ìš©)
        if hasattr(llm_model, '_store_prompt_for_preview'):
            llm_model._store_prompt_for_preview(prompt, "dag_extraction")
        else:
            # ì „ì—­ í”„ë¡¬í”„íŠ¸ ì €ì¥ì†Œ ì‚¬ìš©
            import threading
            if not hasattr(threading.current_thread(), 'stored_prompts'):
                threading.current_thread().stored_prompts = {}
            threading.current_thread().stored_prompts['dag_extraction_prompt'] = {
                'title': 'DAG ê´€ê³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                'description': 'ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ê·¸ë˜í”„ í˜•íƒœë¡œ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸',
                'content': prompt,
                'length': len(prompt)
            }
        
        dag_raw = llm_model.invoke(prompt).content
        logger.info(f"ğŸ“ LLM ì‘ë‹µ ê¸¸ì´: {len(dag_raw)}ì")
        logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì): {dag_raw[:500]}...")
        print("\n" + "="*80)
        print("ğŸ” [DEBUG] LLM ì „ì²´ ì‘ë‹µ:")
        print("="*80)
        print(dag_raw)
        print("="*80 + "\n")
    except Exception as e:
        logger.error(f"âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

    # Step 2: DAG ì„¹ì…˜ ì¶”ì¶œ ë° ì •ë¦¬
    # LLM ì‘ë‹µì—ì„œ ì‹¤ì œ DAG êµ¬ì¡° ë¶€ë¶„ë§Œ ì¶”ì¶œ
    logger.info("ğŸ” DAG ì„¹ì…˜ ì¶”ì¶œ ì¤‘...")
    try:
        dag_section = parser.extract_dag_section(dag_raw)
        logger.info(f"ğŸ“„ ì¶”ì¶œëœ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
        if dag_section:
            logger.info(f"ğŸ“„ DAG ì„¹ì…˜ ë‚´ìš©:\n{dag_section}")
        else:
            logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ DAG ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ LLM ì‘ë‹µ ì „ì²´:\n{dag_raw}")
        raise
    
    # Step 3: NetworkX ê·¸ë˜í”„ êµ¬ì¡° ìƒì„±
    # í…ìŠ¤íŠ¸ DAGë¥¼ ì‹¤ì œ ê·¸ë˜í”„ ê°ì²´ë¡œ ë³€í™˜
    logger.info("ğŸ”— DAG íŒŒì‹± ì¤‘...")
    dag = parser.parse_dag(dag_section)
    logger.info(f"ğŸ“Š íŒŒì‹±ëœ DAG - ë…¸ë“œ ìˆ˜: {dag.number_of_nodes()}, ì—£ì§€ ìˆ˜: {dag.number_of_edges()}")
    
    # Step 4: ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…
    if dag.number_of_nodes() > 0:
        logger.info(f"ğŸ¯ DAG ë…¸ë“œ ëª©ë¡: {list(dag.nodes())}")
        logger.info(f"ğŸ”— DAG ì—£ì§€ ëª©ë¡: {list(dag.edges())}")
    else:
        logger.warning("âš ï¸ DAGì— ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤")

    logger.info("âœ… DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {
        'dag_section': dag_section,  # í…ìŠ¤íŠ¸ í˜•íƒœì˜ DAG í‘œí˜„
        'dag': dag,                  # NetworkX DiGraph ê°ì²´
        'dag_raw': dag_raw           # LLM ì›ë³¸ ì‘ë‹µ (ë””ë²„ê¹…ìš©)
    }

    # root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]
    # for root in root_nodes:
    #     node_data = dag.nodes[root]
    #     print(f"  {root} | {node_data}")

    # paths, roots, leaves = get_root_to_leaf_paths(dag)

    # for i, path in enumerate(paths):
    #     print(f"\nPath {i+1}:")
    #     for j, node in enumerate(path):
    #         if j < len(path) - 1:
    #             edge_data = dag.get_edge_data(node, path[j+1])
    #             relation = edge_data['relation'] if edge_data else ''
    #             print(f"  {node}")
    #             print(f"    --[{relation}]-->")
    #         else:
    #             print(f"  {node}")
                

###############################################################################
# 7) ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ + ê°œì„  ê¸°ëŠ¥ ì¶”ê°€)
###############################################################################
def dag_finder(num_msgs=50, llm_model_nm='ax', save_dag_image=True, prompt_mode='cot'):

    if llm_model_nm == 'ax':
        llm_model = llm_ax
    elif llm_model_nm == 'gem':
        llm_model = llm_gem
    elif llm_model_nm == 'cld':
        llm_model = llm_cld
    elif llm_model_nm == 'gen':
        llm_model = llm_gen
    elif llm_model_nm == 'gpt':
        llm_model = llm_gpt

    # ë°ì´í„° ê²€ì¦
    if mms_pdf is None or mms_pdf.empty or 'msg' not in mms_pdf.columns:
        logger.warning("MMS ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë©”ì‹œì§€ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        sample_messages = [
            "[SKT] T ìš°ì£¼íŒ¨ìŠ¤ ì‡¼í•‘ ì¶œì‹œ! ì§€ê¸ˆ ë§í¬ë¥¼ ëˆŒëŸ¬ ê°€ì…í•˜ë©´ ì²« ë‹¬ 1,000ì›ì— ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ê°€ì… ê³ ê° ì „ì›ì—ê²Œ 11ë²ˆê°€ í¬ì¸íŠ¸ 3,000Pì™€ ì•„ë§ˆì¡´ ë¬´ë£Œë°°ì†¡ ì¿ í°ì„ ë“œë¦½ë‹ˆë‹¤.",
            "[SKT] ì—ìŠ¤ì•ŒëŒ€ë¦¬ì  ì§€í–‰ì  9ì›” íŠ¹ê°€. ì•„ì´í°16 ì¦‰ì‹œ ê°œí†µ ê°€ëŠ¥! ë§¤ì¥ ë°©ë¬¸í•˜ì…”ì„œ ìƒë‹´ë§Œ ë°›ì•„ë„ ì‚¬ì€í’ˆì„ ë“œë¦½ë‹ˆë‹¤. ìœ„ì¹˜: ì§€í–‰ì—­ 2ë²ˆ ì¶œêµ¬"
        ]
        messages_to_process = sample_messages[:min(num_msgs, len(sample_messages))]
    else:
        # ê¸°ì¡´ ë¡œì§: mms_pdfì—ì„œ ëœë¤ ìƒ˜í”Œë§
        try:
            all_msgs = mms_pdf['msg'].unique().tolist()
            messages_to_process = random.sample(all_msgs, min(num_msgs, len(all_msgs)))
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ìƒ˜í”Œë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return

    # ì¶œë ¥ì„ íŒŒì¼ì— ì €ì¥í•˜ê¸° ìœ„í•œ ì„¤ì •
    output_file = "./logs/dag_extraction_output.txt"

    line_break_patterns = {"__":"\n", "â– ":"\nâ– ", "â–¶":"\nâ–¶", "_":"\n"}
    
    # ê°œì„ ëœ íŒŒì„œ ì´ˆê¸°í™”
    parser = DAGParser()
    dag = None  # dag ë³€ìˆ˜ ì´ˆê¸°í™”
    
    with open(output_file, 'a', encoding='utf-8') as f:
        # ì‹¤í–‰ ì‹œì‘ ì‹œì  ê¸°ë¡
        from datetime import datetime
        start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        f.write(f"\n{'='*80}\n")
        f.write(f"DAG ì¶”ì¶œ ì‹¤í–‰ ì‹œì‘: {start_time}\n")
        f.write(f"ì„¤ì •: ëª¨ë¸={llm_model_nm}, ëª¨ë“œ={prompt_mode}\n")
        f.write(f"{'='*80}\n\n")
        
        for msg in messages_to_process:
            dag = None  # ê° ë©”ì‹œì§€ë§ˆë‹¤ dag ì´ˆê¸°í™”
            try:
                for pattern, replacement in line_break_patterns.items():
                    msg = msg.replace(pattern, replacement)
                
                # ë©”ì‹œì§€ ì¶œë ¥
                msg_header = "==="*15+" Message "+"==="*15
                print(msg_header)
                f.write(msg_header + "\n")
                print(msg)
                f.write(msg + "\n")
                
                # DAG ì¶œë ¥
                dag_header = "==="*15+f" DAG ({llm_model_nm.upper()}) "+"==="*15
                print(dag_header)
                f.write(dag_header + "\n")
                
                print(f"ğŸš€ extract_dag í•¨ìˆ˜ í˜¸ì¶œ ì¤‘... (prompt_mode={prompt_mode})")
                extract_dag_result = extract_dag(parser, msg, llm_model, prompt_mode=prompt_mode)

                dag_raw = extract_dag_result['dag_raw']
                dag_section = extract_dag_result['dag_section']
                dag = extract_dag_result['dag']

                print("\n" + "="*80)
                print("ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ (dag_raw):")
                print("="*80)
                print(dag_raw)
                print("="*80 + "\n")
                f.write(dag_raw + "\n")

                # íŒŒì„œ ì„ íƒ ë° ì²˜ë¦¬
                if parser:
                    try:                    
                        # ë””ë²„ê¹…ì„ ìœ„í•´ dag_section ë‚´ìš© í™•ì¸
                        print("=== DAG Section Debug ===")
                        print(f"DAG Section Length: {len(dag_section)}")
                        print("DAG Section Content:")
                        print(dag_section)
                        print("=" * 50)

                        # ë¼ì¸ë³„ë¡œ í™•ì¸
                        lines = dag_section.strip().split('\n')
                        print(f"Total lines: {len(lines)}")
                        for i, line in enumerate(lines, 1):
                            line = line.strip()
                            if line:
                                print(f"Line {i}: '{line}'")
                        print("=" * 50)

                        # íŒŒì‹± ê³¼ì • ë””ë²„ê¹…
                        dag = parser.parse_dag(dag_section)

                        # íŒŒì‹± ê²°ê³¼ í™•ì¸
                        print("=== Parse Results ===")
                        print(f"Nodes: {dag.number_of_nodes()}")
                        print(f"Edges: {dag.number_of_edges()}")

                        # íŒŒì‹± í†µê³„ í™•ì¸
                        if dag.graph.get('stats'):
                            stats = dag.graph['stats']
                            print(f"Parsed lines: {len(stats['parsed_lines'])}")
                            print(f"Parse errors: {len(stats['parse_errors'])}")
                            
                            if stats['parse_errors']:
                                print("Parse Errors:")
                                for error in stats['parse_errors']:
                                    print(f"  - {error}")
                                    
                            if stats['parsed_lines']:
                                print("Successfully parsed lines:")
                                for parsed in stats['parsed_lines']:
                                    print(f"  + {parsed}")
                        
                        # ë¶„ì„ ì •ë³´ ì¶œë ¥
                        analysis = parser.analyze_graph(dag)
                        analysis_header = "==="*15+" Enhanced Analysis "+"==="*15
                        print(analysis_header)
                        f.write(analysis_header + "\n")
                        
                        analysis_info = f"""ê·¸ë˜í”„ ë¶„ì„:
- ë…¸ë“œ ìˆ˜: {analysis['num_nodes']}
- ì—£ì§€ ìˆ˜: {analysis['num_edges']}  
- Root ë…¸ë“œ: {analysis['root_nodes']}
- Leaf ë…¸ë“œ: {analysis['leaf_nodes']}
- DAG ì—¬ë¶€: {analysis['is_dag']}
- ìµœì¥ ê²½ë¡œ ê¸¸ì´: {analysis['longest_path_length']}"""
                        print(analysis_info)
                        f.write(analysis_info + "\n")
                        
                        # íŒŒì‹± ì—ëŸ¬ê°€ ìˆë‹¤ë©´ ì¶œë ¥
                        if dag.graph['stats'].get('parse_errors'):
                            error_info = "\níŒŒì‹± ì—ëŸ¬:"
                            print(error_info)
                            f.write(error_info + "\n")
                            for error in dag.graph['stats']['parse_errors'][:3]:  # ì²˜ìŒ 3ê°œë§Œ
                                error_line = f"  âœ— {error}"
                                print(error_line)
                                f.write(error_line + "\n")
                                
                    except Exception as e:
                        print(f"Enhanced parser ì‹¤íŒ¨, ê¸°ë³¸ íŒŒì„œë¡œ ì „í™˜: {e}")
                        f.write(f"Enhanced parser ì‹¤íŒ¨, ê¸°ë³¸ íŒŒì„œë¡œ ì „í™˜: {e}\n")
                        # ê¸°ë³¸ íŒŒì„œë¡œ í´ë°±
                        nodes, edges = parse_dag_block(re.sub(r'^```|```$', '', dag_raw.strip()))
                        dag = build_dag_graph(nodes, edges)
                else:
                    # ê¸°ë³¸ íŒŒì„œ ì‚¬ìš©
                    nodes, edges = parse_dag_block(re.sub(r'^```|```$', '', dag_raw.strip()))
                    dag = build_dag_graph(nodes, edges)

                # Root Nodes ì¶œë ¥
                root_header = "==="*15+" Root Nodes "+"==="*15
                print(root_header)
                f.write(root_header + "\n")
                root_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]
                for root in root_nodes:
                    node_data = dag.nodes[root]
                    root_info = f"  {root} | {node_data}"
                    print(root_info)
                    f.write(root_info + "\n")

                # Paths ì¶œë ¥
                paths_header = "==="*15+" Paths "+"==="*15
                print(paths_header)
                f.write(paths_header + "\n")
                paths, roots, leaves = extract_root_to_leaf_paths(dag)
                
                if not paths:
                    print("No paths found.")
                    f.write("No paths found.\n")

                for i, path in enumerate(paths):
                    path_info = f"\nPath {i+1}:"
                    print(path_info)
                    f.write(path_info + "\n")
                    for j, node in enumerate(path):
                        if j < len(path) - 1:
                            edge_data = dag.get_edge_data(node, path[j+1])
                            relation = edge_data['relation'] if edge_data else ''
                            node_info = f"  {node}"
                            relation_info = f"    --[{relation}]-->"
                            print(node_info)
                            print(relation_info)
                            f.write(node_info + "\n")
                            f.write(relation_info + "\n")
                        else:
                            final_node = f"  {node}"
                            print(final_node)
                            f.write(final_node + "\n")

                separator = "\n" + "#"*100 + "\n"
                print(separator)
                f.write(separator)

            except Exception as e:
                print(f"Error: {e}")
                f.write(f"Error: {e}\n")
                continue
    
    print(f"ì¶œë ¥ì´ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

    if save_dag_image and dag is not None:
        try:
            create_dag_diagram(dag, filename=f'dag_#_{sha256_hash(msg)}')
            print(f"DAG ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {f'dag_#_{sha256_hash(msg)}.png'}")
        except Exception as e:
            print(f"DAG ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    elif save_dag_image and dag is None:
        print("âš ï¸ DAG ê°ì²´ê°€ ìƒì„±ë˜ì§€ ì•Šì•„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    
    parser_arg = argparse.ArgumentParser(description='DAG ì¶”ì¶œê¸° - MMS ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê´€ê³„ ê·¸ë˜í”„ ì¶”ì¶œ')
    parser_arg.add_argument('--message', type=str, help='ë‹¨ì¼ ë©”ì‹œì§€ ì§ì ‘ ì…ë ¥')
    parser_arg.add_argument('--batch-file', type=str, help='ë°°ì¹˜ ì²˜ë¦¬í•  ë©”ì‹œì§€ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)')
    parser_arg.add_argument('--num_msgs', type=int, default=50, help='CSVì—ì„œ ì¶”ì¶œí•  ë©”ì‹œì§€ ìˆ˜ (ê¸°ë³¸ê°’: 50)')
    parser_arg.add_argument('--llm_model', type=str, default='ax', help='ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: ax)')
    parser_arg.add_argument('--save_dag_image', action='store_true', default=False, help='DAG ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€')
    parser_arg.add_argument('--prompt_mode', type=str, default='cot', choices=['cot', 'simple'], help='í”„ë¡¬í”„íŠ¸ ëª¨ë“œ (cot: Chain-of-Thought ìƒì„¸ë¶„ì„, simple: ê°„ë‹¨ë¶„ì„)')
    args = parser_arg.parse_args()

    args.message = """
  message: '(ê´‘ê³ )[SKT] iPhone ì‹ ì œí’ˆ êµ¬ë§¤ í˜œíƒ ì•ˆë‚´ __#04 ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”._SKí…”ë ˆì½¤ì—ì„œ iPhone ì‹ ì œí’ˆ êµ¬ë§¤í•˜ë©´, ìµœëŒ€ 22ë§Œ ì› ìºì‹œë°± ì´ë²¤íŠ¸ì— ì°¸ì—¬í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.__í˜„ëŒ€ì¹´ë“œë¡œ ì• í”Œ í˜ì´ë„ ë” í¸ë¦¬í•˜ê²Œ ì´ìš©í•´ ë³´ì„¸ìš”.__â–¶ í˜„ëŒ€ì¹´ë“œ ë°”ë¡œ ê°€ê¸°: https://t-mms.kr/ais/#74_ _ì• í”Œ í˜ì´ í‹°ë¨¸ë‹ˆ ì¶©ì „ ì¿ í° 96ë§Œ ì›, ìƒŒí”„ë€ì‹œìŠ¤ì½” ì™•ë³µ í•­ê³µê¶Œ, ì• í”Œ ì•¡ì„¸ì„œë¦¬ íŒ©ê¹Œì§€!_Lucky 1717 ì´ë²¤íŠ¸ ì‘ëª¨í•˜ê³  ê²½í’ˆ ë‹¹ì²¨ì˜ í–‰ìš´ì„ ëˆ„ë ¤ ë³´ì„¸ìš”.__â–¶ ì´ë²¤íŠ¸ ìì„¸íˆ ë³´ê¸°: https://t-mms.kr/aiN/#74_ _â–  ë¬¸ì˜: SKT ê³ ê°ì„¼í„°(1558, ë¬´ë£Œ)__SKTì™€ í•¨ê»˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.__ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ 1504',

    """
    
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
    if args.message:
        print("=" * 80)
        print("ğŸš€ ë‹¨ì¼ ë©”ì‹œì§€ DAG ì¶”ì¶œ ì‹œì‘")
        print("=" * 80)
        print(f"ë©”ì‹œì§€: {args.message[:100]}..." if len(args.message) > 100 else f"ë©”ì‹œì§€: {args.message}")
        print(f"LLM ëª¨ë¸: {args.llm_model}")
        print(f"í”„ë¡¬í”„íŠ¸ ëª¨ë“œ: {args.prompt_mode}")
        print("=" * 80 + "\n")
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        if args.llm_model == 'ax':
            llm_model = llm_ax
        elif args.llm_model == 'gem':
            llm_model = llm_gem
        elif args.llm_model == 'cld':
            llm_model = llm_cld
        elif args.llm_model == 'gen':
            llm_model = llm_gen
        elif args.llm_model == 'gpt':
            llm_model = llm_gpt
        else:
            llm_model = llm_ax
        
        # DAG ì¶”ì¶œ
        parser = DAGParser()
        try:
            result = extract_dag(parser, args.message, llm_model, prompt_mode=args.prompt_mode)
            
            print("\n" + "=" * 80)
            print("âœ… DAG ì¶”ì¶œ ì™„ë£Œ")
            print("=" * 80)
            print(f"ì¶”ì¶œëœ DAG:\n{result['dag_section']}")
            print("=" * 80)
            print(f"ë…¸ë“œ ìˆ˜: {result['dag'].number_of_nodes()}")
            print(f"ì—£ì§€ ìˆ˜: {result['dag'].number_of_edges()}")
            
            if args.save_dag_image and result['dag'].number_of_nodes() > 0:
                dag_filename = f"dag_#_{sha256_hash(args.message)}"
                create_dag_diagram(result['dag'], filename=dag_filename)
                print(f"âœ… DAG ì´ë¯¸ì§€ ì €ì¥: {dag_filename}.png")
                
        except Exception as e:
            print(f"âŒ DAG ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬
    elif args.batch_file:
        print("=" * 80)
        print("ğŸš€ ë°°ì¹˜ íŒŒì¼ DAG ì¶”ì¶œ ì‹œì‘")
        print("=" * 80)
        print(f"íŒŒì¼: {args.batch_file}")
        print(f"LLM ëª¨ë¸: {args.llm_model}")
        print(f"í”„ë¡¬í”„íŠ¸ ëª¨ë“œ: {args.prompt_mode}")
        print("=" * 80 + "\n")
        
        try:
            with open(args.batch_file, 'r', encoding='utf-8') as f:
                messages = [line.strip() for line in f if line.strip()]
            
            print(f"ğŸ“„ ë¡œë“œëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ\n")
            
            # LLM ëª¨ë¸ ì´ˆê¸°í™”
            if args.llm_model == 'ax':
                llm_model = llm_ax
            elif args.llm_model == 'gem':
                llm_model = llm_gem
            elif args.llm_model == 'cld':
                llm_model = llm_cld
            elif args.llm_model == 'gen':
                llm_model = llm_gen
            elif args.llm_model == 'gpt':
                llm_model = llm_gpt
            else:
                llm_model = llm_ax
            
            parser = DAGParser()
            
            for idx, msg in enumerate(messages, 1):
                print(f"\n{'='*80}")
                print(f"ì²˜ë¦¬ ì¤‘: {idx}/{len(messages)}")
                print(f"ë©”ì‹œì§€: {msg[:100]}..." if len(msg) > 100 else f"ë©”ì‹œì§€: {msg}")
                print('='*80)
                
                try:
                    result = extract_dag(parser, msg, llm_model, prompt_mode=args.prompt_mode)
                    print(f"âœ… ë…¸ë“œ: {result['dag'].number_of_nodes()}ê°œ, ì—£ì§€: {result['dag'].number_of_edges()}ê°œ")
                    
                    if args.save_dag_image and result['dag'].number_of_nodes() > 0:
                        dag_filename = f"dag_batch_{idx}_{sha256_hash(msg)}"
                        create_dag_diagram(result['dag'], filename=dag_filename)
                        print(f"âœ… ì´ë¯¸ì§€ ì €ì¥: {dag_filename}.png")
                        
                except Exception as e:
                    print(f"âŒ ì‹¤íŒ¨: {e}")
            
            print(f"\n{'='*80}")
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€")
            print('='*80)
            
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch_file}")
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # CSVì—ì„œ ëœë¤ ìƒ˜í”Œë§ (ê¸°ì¡´ ë°©ì‹)
    else:
        print("=" * 80)
        print("ğŸš€ CSV íŒŒì¼ì—ì„œ ëœë¤ ìƒ˜í”Œë§ DAG ì¶”ì¶œ")
        print("=" * 80)
        print(f"ì¶”ì¶œí•  ë©”ì‹œì§€ ìˆ˜: {args.num_msgs}ê°œ")
        print(f"LLM ëª¨ë¸: {args.llm_model}")
        print(f"í”„ë¡¬í”„íŠ¸ ëª¨ë“œ: {args.prompt_mode}")
        print("=" * 80 + "\n")
        
        dag_finder(num_msgs=args.num_msgs, llm_model_nm=args.llm_model, save_dag_image=args.save_dag_image, prompt_mode=args.prompt_mode)