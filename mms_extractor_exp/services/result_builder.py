"""
Result Builder Service
=======================

ğŸ“‹ ê°œìš”
-------
ìµœì¢… ì¶”ì¶œ ê²°ê³¼ë¥¼ êµ¬ì„±í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
ì—”í‹°í‹° ë§¤ì¹­, ì±„ë„ ì •ë³´ ì¶”ì¶œ, í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘, offer ê°ì²´ ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

ğŸ”— ì˜ì¡´ì„±
---------
**ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ:**
- `services.entity_recognizer`: ì—”í‹°í‹° ë§¤ì¹­ ë° ìƒí’ˆ ì •ë³´ ë§¤í•‘
- `services.store_matcher`: ëŒ€ë¦¬ì /ë§¤ì¥ ì •ë³´ ë§¤ì¹­
- `utils.llm_factory`: LLM ëª¨ë¸ ìƒì„±
- `utils`: í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì„±ëŠ¥ ë¡œê¹…

**ì‚¬ìš©ë˜ëŠ” ê³³:**
- `core.mms_workflow_steps.ResultConstructionStep`: ì›Œí¬í”Œë¡œìš°ì—ì„œ ìµœì¢… ê²°ê³¼ êµ¬ì„±

ğŸ—ï¸ ê²°ê³¼ êµ¬ì„± í”„ë¡œì„¸ìŠ¤
--------------------
```mermaid
graph TB
    A[JSON Objects from LLM] --> B[Extract Product Items]
    B --> C{Entity Extraction Mode}
    C -->|logic| D[Logic-based Matching]
    C -->|llm| E[LLM-based Matching]
    D --> F[Similarity DataFrame]
    E --> F
    F --> G[Map Products with Similarity]
    G --> H[Create Offer Object]
    H --> I[Extract Channels]
    I --> J{Channel Type}
    J -->|ëŒ€ë¦¬ì | K[Match Store Info]
    J -->|ê¸°íƒ€| L[Keep Original]
    K --> M[Update Offer to org type]
    L --> M
    M --> N[Map Program Classification]
    N --> O[Final Result]
    
    style B fill:#e1f5ff
    style G fill:#ffe1e1
    style H fill:#fff4e1
    style O fill:#e1ffe1
```

ğŸ“Š ìŠ¤í‚¤ë§ˆ ë³€í™˜
------------

### raw_result (LLM ì§ì ‘ ì¶œë ¥)
```json
{
  "product": [
    {"name": "ì•„ì´í° 17", "action": "êµ¬ë§¤"}
  ],
  "channel": [
    {"type": "ëŒ€ë¦¬ì ", "value": "ìƒˆìƒ˜ëŒ€ë¦¬ì  ì—­ê³¡ì "}
  ],
  "pgm": ["5GX í”„ë¼ì„"],
  "purpose": "ëŒ€ë¦¬ì /ë§¤ì¥ ë°©ë¬¸ ìœ ë„"
}
```

### ext_result (ë³€í™˜ í›„ ìµœì¢… ê²°ê³¼)
```json
{
  "product": [
    {
      "item_nm": "ì•„ì´í° 17",
      "item_id": ["ITEM001"],
      "item_name_in_msg": ["ì•„ì´í° 17"],
      "expected_action": ["êµ¬ë§¤"]
    }
  ],
  "channel": [
    {
      "type": "ëŒ€ë¦¬ì ",
      "value": "ìƒˆìƒ˜ëŒ€ë¦¬ì  ì—­ê³¡ì ",
      "store_info": [
        {"org_nm": "ìƒˆìƒ˜ëŒ€ë¦¬ì  ì—­ê³¡ì ", "org_cd": "ORG001"}
      ]
    }
  ],
  "offer": {
    "type": "org",  // or "product"
    "value": [...]
  },
  "pgm": [
    {"pgm_nm": "5GX í”„ë¼ì„", "pgm_id": "PGM001"}
  ],
  "entity_dag": [],
  "message_id": "MSG001"
}
```

### ìŠ¤í‚¤ë§ˆ ë³€í™˜ ê·œì¹™

1. **product ë³€í™˜**:
   - `name` â†’ `item_nm`
   - DB ë§¤ì¹­ìœ¼ë¡œ `item_id` ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸)
   - `name` â†’ `item_name_in_msg` (ë¦¬ìŠ¤íŠ¸)
   - `action` â†’ `expected_action` (ë¦¬ìŠ¤íŠ¸)

2. **offer ê°ì²´ ìƒì„±**:
   - ê¸°ë³¸: `type='product'`, `value=product ë¦¬ìŠ¤íŠ¸`
   - ëŒ€ë¦¬ì  ê°ì§€ ì‹œ: `type='org'`, `value=ë§¤ì¥ ì •ë³´`

3. **channel ë³´ê°•**:
   - ëŒ€ë¦¬ì  íƒ€ì…: `store_info` ì¶”ê°€ (StoreMatch ê²°ê³¼)
   - ê¸°íƒ€ íƒ€ì…: `store_info=[]`

4. **pgm ë§¤í•‘**:
   - LLM ì¶”ì¶œ í”„ë¡œê·¸ë¨ëª… â†’ DB ë§¤ì¹­
   - `pgm_nm`, `pgm_id` ì¶”ê°€

ğŸ—ï¸ ì£¼ìš” ì»´í¬ë„ŒíŠ¸
----------------
- **ResultBuilder**: ê²°ê³¼ êµ¬ì„± ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
  - `build_extraction_result()`: ì „ì²´ ê²°ê³¼ êµ¬ì„± íŒŒì´í”„ë¼ì¸
  - `_map_program_classification()`: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘
  - `_extract_channels()`: ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° offer ì—…ë°ì´íŠ¸

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ
-----------
```python
from services.result_builder import ResultBuilder
from utils.llm_factory import LLMFactory

# ì´ˆê¸°í™”
llm_factory = LLMFactory()
builder = ResultBuilder(
    entity_recognizer=recognizer,
    store_matcher=matcher,
    alias_pdf_raw=alias_df,
    stop_item_names=['ê´‘ê³ ', 'ì´ë²¤íŠ¸'],
    num_cand_pgms=10,
    entity_extraction_mode='llm',
    llm_factory=llm_factory,
    llm_model='ax',
    entity_extraction_context_mode='dag'
)

# ìµœì¢… ê²°ê³¼ êµ¬ì„±
final_result = builder.build_extraction_result(
    json_objects=llm_response,
    msg="ì•„ì´í° 17 êµ¬ë§¤ ì‹œ ìºì‹œë°± ì œê³µ",
    pgm_info=program_info,
    entities_from_kiwi=['ì•„ì´í°', 'ìºì‹œë°±'],
    message_id='MSG001'
)

print(f"ì¶”ì¶œëœ ìƒí’ˆ ìˆ˜: {len(final_result['product'])}")
print(f"Offer íƒ€ì…: {final_result['offer']['type']}")
```

ğŸ“ ì°¸ê³ ì‚¬í•­
----------
- entity_extraction_mode='logic': Fuzzy + Sequence ìœ ì‚¬ë„ ì‚¬ìš©
- entity_extraction_mode='llm': LLM ê¸°ë°˜ 2ë‹¨ê³„ ì¶”ì¶œ
- ëŒ€ë¦¬ì  ê°ì§€ ì‹œ offer íƒ€ì…ì´ ìë™ìœ¼ë¡œ 'org'ë¡œ ë³€ê²½
- ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ LLM ì›ë³¸ ê²°ê³¼ ì‚¬ìš© (item_id='#')
- ëª¨ë“  ë¦¬ìŠ¤íŠ¸ í•„ë“œëŠ” ì¤‘ë³µ ì œê±° ì ìš©

"""

import logging
import pandas as pd
import re
import os
import torch
from typing import List, Dict, Any, Tuple, Optional
from langchain_openai import ChatOpenAI
from utils import (
    replace_special_chars_with_space,
    log_performance
)
from config.settings import MODEL_CONFIG, API_CONFIG, PROCESSING_CONFIG

logger = logging.getLogger(__name__)

class ResultBuilder:
    """
    Service for building the final extraction result.
    Handles entity matching, channel extraction, and result assembly.
    """

    def __init__(self, entity_recognizer, store_matcher, alias_pdf_raw: pd.DataFrame, 
                 stop_item_names: List[str], num_cand_pgms: int, entity_extraction_mode: str,
                 llm_factory=None,  # LLMFactory ì¸ìŠ¤í„´ìŠ¤ (ê¸°ì¡´ callable ëŒ€ì²´)
                 llm_model: str = 'ax', 
                 entity_extraction_context_mode: str = 'dag'):
        self.entity_recognizer = entity_recognizer
        self.store_matcher = store_matcher
        self.alias_pdf_raw = alias_pdf_raw
        self.stop_item_names = stop_item_names
        self.num_cand_pgms = num_cand_pgms
        self.entity_extraction_mode = entity_extraction_mode
        self.llm_factory = llm_factory  # LLMFactory ì¸ìŠ¤í„´ìŠ¤ ì €ì¥
        self.llm_model = llm_model
        self.entity_extraction_context_mode = entity_extraction_context_mode

    def build_extraction_result(self, json_objects: Dict, msg: str, pgm_info: Dict, entities_from_kiwi: List[str], message_id: str = '#') -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        try:
            logger.info("=" * 80)
            logger.info("ğŸ” [PRODUCT DEBUG] build_extraction_result ì‹œì‘")
            logger.info("=" * 80)
            
            final_result = json_objects.copy()
            
            # offer_object ì´ˆê¸°í™”
            offer_object = {}
            
            # ìƒí’ˆ ì •ë³´ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("ğŸ“‹ [STEP 1] product_items ì¶”ì¶œ")
            product_items = json_objects.get('product', [])
            logger.info(f"   - ì›ë³¸ product íƒ€ì…: {type(product_items)}")
            logger.info(f"   - ì›ë³¸ product ë‚´ìš©: {product_items}")
            
            if isinstance(product_items, dict):
                logger.info("   - productê°€ dict íƒ€ì… â†’ 'items' í‚¤ë¡œ ì ‘ê·¼")
                product_items = product_items.get('items', [])
                logger.info(f"   - items ì¶”ì¶œ í›„: {product_items}")
            
            logger.info(f"   âœ… ìµœì¢… product_items ê°œìˆ˜: {len(product_items)}ê°œ")
            logger.info(f"   âœ… ìµœì¢… product_items ë‚´ìš©: {product_items}")

            primary_llm_extracted_entities = [x.get('name', '') for x in product_items]
            logger.info(f"ğŸ“‹ [STEP 2] LLM ì¶”ì¶œ ì—”í‹°í‹°: {primary_llm_extracted_entities}")
            logger.info(f"ğŸ“‹ [STEP 2] Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")
            logger.info(f"ğŸ“‹ [STEP 2] entity_extraction_mode: {self.entity_extraction_mode}")

            # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if self.entity_extraction_mode == 'logic':
                logger.info("ğŸ” [STEP 3] ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ë§¤ì¹­ ì‹œì‘")
                # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
                cand_entities = list(set(entities_from_kiwi+[item.get('name', '') for item in product_items if item.get('name')]))
                logger.info(f"   - cand_entities: {cand_entities}")
                similarities_fuzzy = self.entity_recognizer.extract_entities_with_fuzzy_matching(cand_entities)
                logger.info(f"   âœ… similarities_fuzzy ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            else:
                logger.info("ğŸ” [STEP 3] LLM ê¸°ë°˜ ì—”í‹°í‹° ë§¤ì¹­ ì‹œì‘")
                logger.info(f"   - ì‚¬ìš©í•  LLM ëª¨ë¸: {self.llm_model}")
                logger.info(f"   - llm_factory íƒ€ì…: {type(self.llm_factory)}")
                
                # LLMFactoryë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ìƒì„±
                if self.llm_factory:
                    default_llm_models = self.llm_factory.create_models([self.llm_model])
                    logger.info(f"   - ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ìˆ˜: {len(default_llm_models)}ê°œ")
                else:
                    logger.warning("âš ï¸ llm_factoryê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    default_llm_models = []
                logger.info(f"   - ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ìˆ˜: {len(default_llm_models)}ê°œ")
                llm_result = self.entity_recognizer.extract_entities_with_llm(
                    msg,
                    llm_models=default_llm_models,
                    rank_limit=100,
                    external_cand_entities=list(set(entities_from_kiwi+primary_llm_extracted_entities)),
                    context_mode=self.entity_extraction_context_mode
                )

                # ONT ëª¨ë“œì¼ ê²½ìš° dict ë°˜í™˜, ê·¸ ì™¸ëŠ” DataFrame ë°˜í™˜
                if isinstance(llm_result, dict):
                    similarities_fuzzy = llm_result.get('similarities_df', pd.DataFrame())
                    logger.info(f"   âœ… ONT ëª¨ë“œ: similarities_df ì¶”ì¶œ ì™„ë£Œ")
                else:
                    similarities_fuzzy = llm_result

                logger.info(f"   âœ… similarities_fuzzy ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if not similarities_fuzzy.empty:
                logger.info(f"   ğŸ“Š similarities_fuzzy ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                logger.info(f"{similarities_fuzzy.head(3).to_dict('records')}")
            else:
                logger.warning("   âš ï¸ similarities_fuzzyê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

            if not similarities_fuzzy.empty:
                logger.info("ğŸ” [STEP 4] alias_pdf_rawì™€ merge ì‹œì‘")
                logger.info(f"   - alias_pdf_raw í¬ê¸°: {self.alias_pdf_raw.shape}")
                merged_df = similarities_fuzzy.merge(
                    self.alias_pdf_raw[['alias_1','type']].drop_duplicates(), 
                    left_on='item_name_in_msg', 
                    right_on='alias_1', 
                    how='left'
                )
                logger.info(f"   âœ… merged_df í¬ê¸°: {merged_df.shape if not merged_df.empty else 'ë¹„ì–´ìˆìŒ'}")
                if not merged_df.empty:
                    logger.info(f"   ğŸ“Š merged_df ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                    logger.info(f"{merged_df.head(3).to_dict('records')}")

                logger.info("ğŸ” [STEP 5] filtered_df ìƒì„± (expansion íƒ€ì… í•„í„°ë§)")
                filtered_df = merged_df[merged_df.apply(
                    lambda x: (
                        replace_special_chars_with_space(x['item_nm_alias']) in replace_special_chars_with_space(x['item_name_in_msg']) or 
                        replace_special_chars_with_space(x['item_name_in_msg']) in replace_special_chars_with_space(x['item_nm_alias'])
                    ) if x['type'] != 'expansion' else True, 
                    axis=1
                )]
                logger.info(f"   âœ… filtered_df í¬ê¸°: {filtered_df.shape if not filtered_df.empty else 'ë¹„ì–´ìˆìŒ'}")
                if not filtered_df.empty:
                    logger.info(f"   ğŸ“Š filtered_df ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                    logger.info(f"{filtered_df.head(3).to_dict('records')}")

                # similarities_fuzzy = filtered_df[similarities_fuzzy.columns]

            # ìƒí’ˆ ì •ë³´ ë§¤í•‘
            logger.info("ğŸ” [STEP 6] ìƒí’ˆ ì •ë³´ ë§¤í•‘ ì‹œì‘")
            logger.info(f"   - similarities_fuzzy.empty: {similarities_fuzzy.empty}")
            
            if not similarities_fuzzy.empty:
                logger.info("   âœ… similarities_fuzzyê°€ ë¹„ì–´ìˆì§€ ì•ŠìŒ â†’ map_products_with_similarity í˜¸ì¶œ")
                final_result['product'] = self.entity_recognizer.map_products_to_entities(similarities_fuzzy, json_objects)
                logger.info(f"   âœ… ìµœì¢… product ê°œìˆ˜: {len(final_result['product'])}ê°œ")
                logger.info(f"   âœ… ìµœì¢… product ë‚´ìš©: {final_result['product']}")
            else:
                logger.warning("   âš ï¸ similarities_fuzzyê°€ ë¹„ì–´ìˆìŒ â†’ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (else ë¸Œëœì¹˜)")
                logger.info(f"   - product_items ê°œìˆ˜: {len(product_items)}ê°œ")
                logger.info(f"   - stop_item_names ê°œìˆ˜: {len(self.stop_item_names)}ê°œ")
                
                # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìƒˆ ìŠ¤í‚¤ë§ˆ + expected_action ë¦¬ìŠ¤íŠ¸)
                filtered_product_items = [
                    d for d in product_items 
                    if d.get('name') and d['name'] not in self.stop_item_names
                ]
                logger.info(f"   - í•„í„°ë§ í›„ product_items ê°œìˆ˜: {len(filtered_product_items)}ê°œ")
                logger.info(f"   - í•„í„°ë§ í›„ product_items: {filtered_product_items}")
                
                final_result['product'] = [
                    {
                        'item_nm': d.get('name', ''), 
                        'item_id': ['#'],
                        'item_name_in_msg': [d.get('name', '')],
                        'expected_action': [d.get('action', 'ê¸°íƒ€')]
                    } 
                    for d in filtered_product_items
                ]
                logger.info(f"   âœ… ìµœì¢… product ê°œìˆ˜: {len(final_result['product'])}ê°œ")
                logger.info(f"   âœ… ìµœì¢… product ë‚´ìš©: {final_result['product']}")

            # offer_objectì— product íƒ€ì…ìœ¼ë¡œ ì„¤ì •
            offer_object['type'] = 'product'
            offer_object['value'] = final_result['product']
            logger.info(f"ğŸ·ï¸  [STEP 7] offer_object ì´ˆê¸°í™”: type=product, value ê°œìˆ˜={len(offer_object['value'])}ê°œ")

            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
            final_result['pgm'] = self._map_programs_to_result(json_objects, pgm_info)
            
            # ì±„ë„ ì •ë³´ ì²˜ë¦¬ (offer_objectë„ í•¨ê»˜ ì „ë‹¬ ë° ë°˜í™˜)
            logger.info("ğŸ” [STEP 8] ì±„ë„ ì •ë³´ ì²˜ë¦¬ ë° offer_object ì—…ë°ì´íŠ¸")
            final_result['channel'], offer_object = self._extract_and_enrich_channels(json_objects, msg, offer_object)
            logger.info(f"   âœ… ìµœì¢… channel ê°œìˆ˜: {len(final_result['channel'])}ê°œ")
            logger.info(f"   âœ… ìµœì¢… offer_object type: {offer_object.get('type', 'N/A')}")
            logger.info(f"   âœ… ìµœì¢… offer_object value ê°œìˆ˜: {len(offer_object.get('value', []))}ê°œ")
            
            # offer í•„ë“œ ì¶”ê°€
            final_result['offer'] = offer_object
            logger.info(f"âœ… [STEP 9] final_resultì— offer í•„ë“œ ì¶”ê°€ ì™„ë£Œ")
            
            # entity_dag ì´ˆê¸°í™” (ë¹ˆ ë°°ì—´)
            final_result['entity_dag'] = []
            
            logger.info("=" * 80)
            logger.info("âœ… [PRODUCT DEBUG] build_extraction_result ì™„ë£Œ")
            logger.info(f"   ìµœì¢… final_result['product'] ê°œìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info("=" * 80)

            # message_id ì¶”ê°€
            final_result['message_id'] = message_id
            
            return final_result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return json_objects

    def _map_programs_to_result(self, json_objects: Dict, pgm_info: Dict) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘"""
        try:
            if (self.num_cand_pgms > 0 and 
                'pgm' in json_objects and 
                isinstance(json_objects['pgm'], list) and
                not pgm_info.get('pgm_pdf_tmp', pd.DataFrame()).empty):
                
                pgm_json = pgm_info['pgm_pdf_tmp'][
                    pgm_info['pgm_pdf_tmp']['pgm_nm'].apply(
                        lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm'])
                    )
                ][['pgm_nm', 'pgm_id']].to_dict('records')
                
                return pgm_json
            
            return []
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

    def _extract_and_enrich_channels(self, json_objects: Dict, msg: str, offer_object: Dict) -> Tuple[List[Dict], Dict]:
        """ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¹­ (offer_objectë„ í•¨ê»˜ ë°˜í™˜)"""
        try:
            channel_tag = []
            channel_items = json_objects.get('channel', [])
            if isinstance(channel_items, dict):
                channel_items = channel_items.get('items', [])

            for d in channel_items:
                if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                    # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                    store_info = self.store_matcher.match_store(d['value'])
                    d['store_info'] = store_info
                    
                    # offer_objectë¥¼ org íƒ€ì…ìœ¼ë¡œ ë³€ê²½
                    if store_info:
                        offer_object['type'] = 'org'
                        org_tmp = [
                            {
                                'item_nm': o['org_nm'], 
                                'item_id': o['org_cd'], 
                                'item_name_in_msg': d['value'], 
                                'expected_action': ['ë°©ë¬¸']
                            } 
                            for o in store_info
                        ]
                        offer_object['value'] = org_tmp
                    else:
                        if "ëŒ€ë¦¬ì /ë§¤ì¥ ë°©ë¬¸ ìœ ë„" in json_objects['purpose']:
                            offer_object['type'] = 'org'
                            org_tmp = [{'item_nm':d['value'], 'item_id':'#', 'item_name_in_msg':d['value'], 'expected_action':['ë°©ë¬¸']}]
                            offer_object['value'] = org_tmp
                else:
                    d['store_info'] = []
                channel_tag.append(d)

            return channel_tag, offer_object
            
        except Exception as e:
            logger.error(f"ì±„ë„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [], offer_object


