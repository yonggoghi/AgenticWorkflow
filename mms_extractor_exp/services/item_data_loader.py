"""
Item Data Loader Service
=========================

ğŸ“‹ ê°œìš”
-------
ìƒí’ˆ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ë¥¼ ì „ë‹´í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
ê¸°ì¡´ MMSExtractorì˜ `_load_item_data` ë©”ì„œë“œ(197ì¤„)ë¥¼ 
ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

ğŸ”— ì˜ì¡´ì„±
---------
**ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ:**
- `pandas`: ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
- `joblib.Parallel`: ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³„ì¹­ ê·œì¹™ ì ìš© ìµœì í™”
- `utils.select_most_comprehensive`: ì¤‘ë³µ ì œê±° ë¡œì§

**ì‚¬ìš©ë˜ëŠ” ê³³:**
- `core.mms_extractor`: MMSExtractor ì´ˆê¸°í™” ì‹œ ë°ì´í„° ë¡œë“œ
- `services.entity_recognizer`: ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ ì°¸ì¡°

ğŸ—ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸
-------------------
```mermaid
graph LR
    A[Raw Data Source] -->|CSV/DB| B[load_raw_data]
    B --> C[normalize_columns]
    C --> D[filter_by_domain]
    D --> E[load_alias_rules]
    E --> F[expand_build_aliases]
    F --> G[create_bidirectional_aliases]
    G --> H[apply_cascading_alias_rules]
    H --> I[add_user_defined_entities]
    I --> J[add_domain_name_column]
    J --> K[filter_test_items]
    K --> L[Final DataFrame]
    
    style B fill:#e1f5ff
    style H fill:#ffe1e1
    style L fill:#e1ffe1
```

### íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ëª…

1. **load_raw_data**: CSV ë˜ëŠ” DBì—ì„œ ì›ì‹œ ë°ì´í„° ë¡œë“œ
2. **normalize_columns**: ì»¬ëŸ¼ëª… ì •ê·œí™” (item_nm, item_id ë“±)
3. **filter_by_domain**: ë„ë©”ì¸ ì½”ë“œë¡œ í•„í„°ë§ (ì œì™¸ ë„ë©”ì¸ ì ìš©)
4. **load_alias_rules**: ë³„ì¹­ ê·œì¹™ CSV ë¡œë“œ
5. **expand_build_aliases**: 'build' íƒ€ì… ë³„ì¹­ í™•ì¥
6. **create_bidirectional_aliases**: ì–‘ë°©í–¥(B) ë³„ì¹­ ì¶”ê°€
7. **apply_cascading_alias_rules**: ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬, ìµœëŒ€ 3ë‹¨ê³„)
8. **add_user_defined_entities**: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
9. **add_domain_name_column**: ë„ë©”ì¸ëª… ì»¬ëŸ¼ ì¶”ê°€
10. **filter_test_items**: TEST ë¬¸ìì—´ í¬í•¨ í•­ëª© ì œê±°

ğŸ—ï¸ ì£¼ìš” ì»´í¬ë„ŒíŠ¸
----------------
- **ItemDataLoader**: ìƒí’ˆ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤
  - `load_and_prepare_items()`: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  - `apply_cascading_alias_rules()`: ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš© (ì„±ëŠ¥ ìµœì í™”)

ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ
-----------
```python
from services.item_data_loader import ItemDataLoader

# 1. Local CSV ëª¨ë“œ
loader = ItemDataLoader(data_source='local')
item_df, alias_df = loader.load_and_prepare_items(
    offer_data_path='./data/offer_info.csv',
    alias_rules_path='./data/alias_rules.csv',
    excluded_domains=['TEST', 'DEV'],
    user_entities=['íŠ¹ë³„ìƒí’ˆA', 'íŠ¹ë³„ìƒí’ˆB']
)

# 2. DB ëª¨ë“œ
def my_db_loader():
    return pd.read_sql("SELECT * FROM offer_info", conn)

loader = ItemDataLoader(data_source='db', db_loader=my_db_loader)
item_df, alias_df = loader.load_and_prepare_items()

# 3. ê²°ê³¼ í™•ì¸
print(f"ë¡œë“œëœ ìƒí’ˆ ìˆ˜: {len(item_df)}")
print(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_df)}")
print(f"ì»¬ëŸ¼: {list(item_df.columns)}")
```

ğŸ“Š ë°ì´í„° ìŠ¤í‚¤ë§ˆ
--------------

### ì…ë ¥ ìŠ¤í‚¤ë§ˆ (Raw Data)
```
offer_info.csv:
- item_nm: ìƒí’ˆëª…
- item_id: ìƒí’ˆ ID
- item_dmn_cd: ë„ë©”ì¸ ì½”ë“œ
- ... (ê¸°íƒ€ ì»¬ëŸ¼)

alias_rules.csv:
- item_nm: ì›ë³¸ ìƒí’ˆëª…
- alias: ë³„ì¹­
- alias_type: ë³„ì¹­ íƒ€ì… ('build', 'B', 'F' ë“±)
```

### ì¶œë ¥ ìŠ¤í‚¤ë§ˆ (Processed Data)
```
item_pdf_all:
- item_nm: ì •ê·œí™”ëœ ìƒí’ˆëª…
- item_nm_alias: ì ìš©ëœ ë³„ì¹­ (ì—°ì‡„ ê·œì¹™ ì ìš© í›„)
- item_id: ìƒí’ˆ ID
- item_dmn_cd: ë„ë©”ì¸ ì½”ë“œ
- item_dmn_nm: ë„ë©”ì¸ëª… (ì¶”ê°€ë¨)
- ... (ê¸°íƒ€ ì»¬ëŸ¼)
```

âš™ï¸ ì„±ëŠ¥ ìµœì í™”
-------------
**ë³‘ë ¬ ì²˜ë¦¬ (apply_cascading_alias_rules):**
- joblib.Parallel ì‚¬ìš©
- ê¸°ë³¸ n_jobs: CPU ì½”ì–´ ìˆ˜
- ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ

**ì—°ì‡„ ê¹Šì´ ì œí•œ:**
- ê¸°ë³¸ max_depth: 3
- ë¬´í•œ ë£¨í”„ ë°©ì§€

ğŸ“ ì°¸ê³ ì‚¬í•­
----------
- ë³„ì¹­ ê·œì¹™ì€ ìµœëŒ€ 3ë‹¨ê³„ê¹Œì§€ ì—°ì‡„ ì ìš©
- 'build' íƒ€ì…ì€ ìƒí’ˆëª… ì¡°í•©ìœ¼ë¡œ ìë™ í™•ì¥
- ì–‘ë°©í–¥(B) ë³„ì¹­ì€ Aâ†’B, Bâ†’A ëª¨ë‘ ìƒì„±
- TEST ë¬¸ìì—´ í¬í•¨ í•­ëª©ì€ ìë™ ì œê±°
- ë„ë©”ì¸ í•„í„°ë§ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë°ì´í„° ì‚¬ì „ ì œê±°

"""

import logging
import os
import pandas as pd
from typing import List, Tuple
from joblib import Parallel, delayed
from utils import select_most_comprehensive

logger = logging.getLogger(__name__)


class ItemDataLoader:
    """
    ìƒí’ˆ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤
    
    ì±…ì„:
    - ë°ì´í„° ì†ŒìŠ¤(CSV/DB)ì—ì„œ ì›ì‹œ ë°ì´í„° ë¡œë“œ
    - ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì ìš©
    - ë°ì´í„° í•„í„°ë§ ë° ì •ì œ
    - ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€
    """
    
    def __init__(self, data_source='local', db_loader=None):
        """
        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ('local' ë˜ëŠ” 'db')
            db_loader: DB ë¡œë” í•¨ìˆ˜ (data_source='db'ì¼ ë•Œ í•„ìš”)
        """
        self.data_source = data_source
        self.db_loader = db_loader
        self.alias_pdf_raw = None  # ë³„ì¹­ ê·œì¹™ ì›ë³¸ (ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
        
    def load_raw_data(self, offer_data_path: str = None) -> pd.DataFrame:
        """
        ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì›ì‹œ ìƒí’ˆ ë°ì´í„° ë¡œë“œ
        
        Args:
            offer_data_path: CSV íŒŒì¼ ê²½ë¡œ (local ëª¨ë“œì¼ ë•Œ)
            
        Returns:
            pd.DataFrame: ì›ì‹œ ìƒí’ˆ ë°ì´í„°
        """
        logger.info(f"ğŸ“Š ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ (ì†ŒìŠ¤: {self.data_source})")
        
        if self.data_source == 'db':
            if not self.db_loader:
                raise ValueError("DB ëª¨ë“œì—ì„œëŠ” db_loaderê°€ í•„ìš”í•©ë‹ˆë‹¤")
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ ì¤‘...")
            item_pdf_raw = self.db_loader()
        elif self.data_source == 'local':
            if offer_data_path is None:
                from config.settings import METADATA_CONFIG
                offer_data_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/offer_master_data.csv')
            
            logger.info(f"ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ìƒí’ˆ ë°ì´í„° ë¡œë“œ: {offer_data_path}")
            
            # Try multiple encodings to handle various file formats
            encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
            item_pdf_raw = None
            
            for encoding in encodings:
                try:
                    item_pdf_raw = pd.read_csv(offer_data_path, encoding=encoding)
                    logger.info(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ (encoding: {encoding})")
                    break
                except UnicodeDecodeError:
                    logger.debug(f"ì¸ì½”ë”© {encoding} ì‹¤íŒ¨, ë‹¤ìŒ ì¸ì½”ë”© ì‹œë„...")
                    continue
                except Exception as e:
                    logger.error(f"CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (encoding: {encoding}): {e}")
                    continue
            
            if item_pdf_raw is None:
                logger.error(f"ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨. íŒŒì¼: {offer_data_path}")
                return pd.DataFrame()
        
        logger.info(f"ì›ì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {item_pdf_raw.shape}")
        return item_pdf_raw
    
    def normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì»¬ëŸ¼ëª… ì •ê·œí™” ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        
        Args:
            df: ì›ì‹œ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ”§ ì»¬ëŸ¼ ì •ê·œí™” ì¤‘...")
        
        # # ITEM_NM ì²˜ë¦¬: ë‹¨ë§ê¸°(E)ëŠ” ITEM_DESC ì‚¬ìš©
        # if 'ITEM_DMN' in df.columns and 'ITEM_DESC' in df.columns:
        #     df['ITEM_NM'] = df.apply(
        #         lambda x: x['ITEM_DESC'] if x['ITEM_DMN']=='E' and pd.notna(x['ITEM_DESC']) else x['ITEM_NM'], 
        #         axis=1
        #     )
        
        # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
        df = df.rename(columns={c: c.lower() for c in df.columns})
        logger.info("ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")
        
        # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
        df['item_ctg'] = None
        df['item_emb_vec'] = None
        df['ofer_cd'] = df['item_id']
        df['oper_dt_hms'] = '20250101000000'
        
        return df
    
    def filter_by_domain(self, df: pd.DataFrame, excluded_domains: List[str] = None) -> pd.DataFrame:
        """
        ë„ë©”ì¸ ì½”ë“œë¡œ ë°ì´í„° í•„í„°ë§
        
        Args:
            df: ë°ì´í„°í”„ë ˆì„
            excluded_domains: ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        if not excluded_domains:
            from config.settings import PROCESSING_CONFIG
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
        
        if excluded_domains and 'item_dmn' in df.columns:
            before_filter = len(df)
            df = df.query("item_dmn not in @excluded_domains")
            logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter} -> {len(df)}")
        
        return df
    
    def load_alias_rules(self, alias_rules_path: str = None) -> pd.DataFrame:
        """
        ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            alias_rules_path: ë³„ì¹­ ê·œì¹™ CSV íŒŒì¼ ê²½ë¡œ
            
        Returns:
            pd.DataFrame: ì „ì²˜ë¦¬ëœ ë³„ì¹­ ê·œì¹™
        """
        logger.info("ğŸ”— ë³„ì¹­ ê·œì¹™ ë¡œë“œ ì¤‘...")
        
        if not alias_rules_path:
            from config.settings import METADATA_CONFIG
            alias_rules_path = getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv')
        
        # ì›ë³¸ ì €ì¥ (ì™¸ë¶€ ì ‘ê·¼ìš©)
        self.alias_pdf_raw = pd.read_csv(alias_rules_path)
        
        # ë³„ì¹­ ê·œì¹™ ì „ì²˜ë¦¬
        alias_pdf = self.alias_pdf_raw.copy()

        # Add default case sensitivity columns if not present (backward compatibility)
        if 'case_1' not in alias_pdf.columns:
            alias_pdf['case_1'] = 'S'
        if 'case_2' not in alias_pdf.columns:
            alias_pdf['case_2'] = 'S'
        # Fill NaN values with default 'S' (sensitive)
        alias_pdf['case_1'] = alias_pdf['case_1'].fillna('S')
        alias_pdf['case_2'] = alias_pdf['case_2'].fillna('S')

        alias_pdf['alias_1'] = alias_pdf['alias_1'].str.split("&&")
        alias_pdf['alias_2'] = alias_pdf['alias_2'].str.split("&&")
        alias_pdf = alias_pdf.explode('alias_1')
        alias_pdf = alias_pdf.explode('alias_2')

        return alias_pdf
    
    def expand_build_aliases(self, alias_pdf: pd.DataFrame, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        Type='build' ë³„ì¹­ í™•ì¥
        
        Args:
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: í™•ì¥ëœ ë³„ì¹­ ê·œì¹™
        """
        logger.info("ğŸ”¨ Build íƒ€ì… ë³„ì¹­ í™•ì¥ ì¤‘...")
        
        alias_list_ext = alias_pdf.query("type=='build'")[
            ['alias_1','category','direction','type','case_1','case_2']
        ].to_dict('records')

        for alias in alias_list_ext:
            # Use boolean indexing instead of query() for better pandas compatibility
            mask = (
                item_df['item_nm'].str.contains(alias['alias_1'], na=False) &
                (item_df['item_dmn'] == alias['category'])
            )
            adf = item_df.loc[mask, ['item_nm', 'item_desc', 'item_dmn']].rename(
                columns={'item_nm': 'alias_2', 'item_desc': 'description', 'item_dmn': 'category'}
            ).drop_duplicates()

            adf['alias_1'] = alias['alias_1']
            adf['direction'] = alias['direction']
            adf['type'] = alias['type']
            adf['case_1'] = alias['case_1']
            adf['case_2'] = alias['case_2']
            adf = adf[alias_pdf.columns]
            
            alias_pdf = pd.concat([
                alias_pdf.query(f"alias_1!='{alias['alias_1']}'"), 
                adf
            ])
        
        alias_pdf = alias_pdf.drop_duplicates()
        return alias_pdf
    
    def create_bidirectional_aliases(self, alias_pdf: pd.DataFrame) -> pd.DataFrame:
        """
        ë°©í–¥ì„± 'B'ì¸ ë³„ì¹­ì— ëŒ€í•´ ì—­ë°©í–¥ ì¶”ê°€

        Args:
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„

        Returns:
            pd.DataFrame: ì–‘ë°©í–¥ ë³„ì¹­ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("â†”ï¸ ì–‘ë°©í–¥ ë³„ì¹­ ì¶”ê°€ ì¤‘...")

        # For bidirectional rules, swap alias_1/alias_2 AND case_1/case_2
        bidirectional = alias_pdf.query("direction=='B'").rename(
            columns={'alias_1':'alias_2', 'alias_2':'alias_1', 'case_1':'case_2', 'case_2':'case_1'}
        )[alias_pdf.columns]

        alias_pdf = pd.concat([alias_pdf, bidirectional])
        return alias_pdf
    
    def apply_cascading_alias_rules(self, item_df: pd.DataFrame, alias_pdf: pd.DataFrame, 
                               max_depth: int = 7) -> pd.DataFrame:
        """
        ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
            max_depth: ìµœëŒ€ ì—°ì‡„ ê¹Šì´
            
        Returns:
            pd.DataFrame: ë³„ì¹­ì´ ì ìš©ëœ ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ”„ ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© ì¤‘...")
        
        alias_rule_set = list(zip(
            alias_pdf['alias_1'],
            alias_pdf['alias_2'],
            alias_pdf['type'],
            alias_pdf['case_1'],
            alias_pdf['case_2']
        ))
        logger.info(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")
        
        def apply_alias_rule_cascade_parallel(args_dict):
            """ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš©"""
            item_nm = args_dict['item_nm']
            max_depth = args_dict['max_depth']
            
            # Skip NaN values and ensure item_nm is a string
            if pd.isna(item_nm) or not isinstance(item_nm, str):
                return {'item_nm': item_nm, 'item_nm_alias': [item_nm]} # Return original if invalid
            
            processed = set()
            result_dict = {item_nm: '#' * len(item_nm)}
            to_process = [(item_nm, 0, frozenset())]
            
            while to_process:
                current_item, depth, path_applied_rules = to_process.pop(0)
                
                if depth >= max_depth or current_item in processed:
                    continue
                
                processed.add(current_item)
                
                for r in alias_rule_set:
                    alias_from, alias_to, alias_type, case_1, case_2 = r[0], r[1], r[2], r[3], r[4]
                    rule_key = (alias_from, alias_to, alias_type)

                    if rule_key in path_applied_rules:
                        continue

                    # íƒ€ì…ì— ë”°ë¥¸ ë§¤ì¹­ (case_1 ì ìš©)
                    if alias_type == 'exact':
                        if case_1 == 'I':
                            matched = (current_item.lower() == alias_from.lower())
                        else:
                            matched = (current_item == alias_from)
                    else:  # partial
                        if case_1 == 'I':
                            matched = (alias_from.lower() in current_item.lower())
                        else:
                            matched = (alias_from in current_item)

                    if matched:
                        # Generate alias variants based on case_2
                        if alias_type == 'exact':
                            new_items = [alias_to.strip()]
                        else:
                            # For partial match, replace the matched portion
                            if case_1 == 'I':
                                # Case-insensitive replacement
                                import re
                                pattern = re.compile(re.escape(alias_from), re.IGNORECASE)
                                new_items = [pattern.sub(alias_to.strip(), current_item, count=1)]
                            else:
                                new_items = [current_item.replace(alias_from.strip(), alias_to.strip())]

                        # If case_2 is 'I', also generate case variants
                        if case_2 == 'I':
                            base_items = new_items.copy()
                            for base_item in base_items:
                                # Add lowercase and uppercase variants
                                if base_item.lower() not in [x.lower() for x in new_items]:
                                    new_items.append(base_item.lower())
                                if base_item.upper() not in [x.upper() for x in new_items]:
                                    new_items.append(base_item.upper())

                        for new_item in new_items:
                            if new_item not in result_dict:
                                result_dict[new_item] = alias_from.strip()
                                to_process.append((new_item, depth + 1, path_applied_rules | {rule_key}))
            
            # Return all generated aliases without filtering
            # The select_most_comprehensive was filtering out aliases derived from shorter patterns
            # (e.g., 'iPhone 17' derived from 'ì•„ì´í°' pattern was filtered because 'ì•„ì´í°' is substring of 'ì•„ì´í° 17 PRO')
            result_aliases = list(result_dict.keys())

            if item_nm not in result_aliases:
                result_aliases.append(item_nm)

            return {'item_nm': item_nm, 'item_nm_alias': result_aliases}
        
        def parallel_alias_rule_cascade(texts, max_depth=5, n_jobs=None):
            """ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš©"""
            if n_jobs is None:
                n_jobs = min(os.cpu_count() - 1, 8)
            
            # Filter out NaN values before processing
            valid_texts = [t for t in texts if pd.notna(t) and isinstance(t, str)]
            
            if not valid_texts:
                logger.warning("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜.")
                return pd.DataFrame()
            
            batches = []
            batch_size = max(1, len(valid_texts) // (n_jobs * 2))
            for i in range(0, len(valid_texts), batch_size):
                batch = valid_texts[i:i + batch_size]
                batches.append([{'item_nm': text, 'max_depth': max_depth} for text in batch])
            
            with Parallel(n_jobs=n_jobs) as parallel:
                batch_results = parallel(delayed(apply_alias_rule_cascade_parallel)(args) for batch in batches for args in batch)
            
            return pd.DataFrame(batch_results)
        
        # ë³„ì¹­ ì ìš©
        item_alias_pdf = parallel_alias_rule_cascade(item_df['item_nm'], max_depth=max_depth)
        
        # ë³„ì¹­ ë³‘í•© ë° explode
        item_df = item_df.merge(item_alias_pdf, on='item_nm', how='left')
        before_explode = len(item_df)
        item_df = item_df.explode('item_nm_alias').drop_duplicates()
        logger.info(f"ë³„ì¹­ explode: {before_explode} -> {len(item_df)}")
        
        return item_df
    
    def add_user_defined_entities(self, item_df: pd.DataFrame, 
                                  user_entities: List[str] = None) -> pd.DataFrame:
        """
        ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            user_entities: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: ì‚¬ìš©ì ì—”í‹°í‹°ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        if not user_entities:
            from config.settings import PROCESSING_CONFIG
            user_entities = getattr(PROCESSING_CONFIG, 'user_defined_entities', [])
        
        if user_entities:
            logger.info(f"ğŸ‘¤ ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€: {len(user_entities)}ê°œ")
            item_pdf_ext = pd.DataFrame([{
                'item_nm': e, 'item_id': e, 'item_desc': e, 'item_dmn': 'user_defined',
                'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e
            } for e in user_entities])
            item_df = pd.concat([item_df, item_pdf_ext])
        
        return item_df
    
    def add_domain_name_column(self, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        item_dmn_nm ì»¬ëŸ¼ ì¶”ê°€ (ë„ë©”ì¸ ì½”ë“œ -> ë„ë©”ì¸ëª…)
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: ë„ë©”ì¸ëª…ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ·ï¸ ë„ë©”ì¸ëª… ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        
        item_dmn_map = pd.DataFrame([
            {"item_dmn": 'P', 'item_dmn_nm': 'ìš”ê¸ˆì œ ë° ê´€ë ¨ ìƒí’ˆ'},
            {"item_dmn": 'E', 'item_dmn_nm': 'ë‹¨ë§ê¸°'},
            {"item_dmn": 'S', 'item_dmn_nm': 'êµ¬ë… ìƒí’ˆ'},
            {"item_dmn": 'C', 'item_dmn_nm': 'ì¿ í°'},
            {"item_dmn": 'X', 'item_dmn_nm': 'ê°€ìƒ ìƒí’ˆ'}
        ])
        
        item_df = item_df.merge(item_dmn_map, on='item_dmn', how='left')
        item_df['item_dmn_nm'] = item_df['item_dmn_nm'].fillna('ê¸°íƒ€')
        
        return item_df
    
    def filter_test_items(self, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        TEST ë¬¸ìì—´ì´ í¬í•¨ëœ í•­ëª© í•„í„°ë§
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: TEST í•­ëª©ì´ ì œê±°ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ§¹ TEST í•­ëª© í•„í„°ë§ ì¤‘...")
        
        before_test = len(item_df)
        item_df = item_df.query("not item_nm_alias.str.contains('TEST', case=False, na=False)")
        logger.info(f"TEST í•„í„°ë§: {before_test} -> {len(item_df)}")
        
        return item_df
    
    def load_and_prepare_items(self, offer_data_path: str = None, 
                          alias_rules_path: str = None,
                          output_path: str = None,
                          data_source: str = 'local') -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì „ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            offer_data_path: ìƒí’ˆ ë°ì´í„° CSV ê²½ë¡œ
            alias_rules_path: ë³„ì¹­ ê·œì¹™ CSV ê²½ë¡œ
            output_path: (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            data_source: (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: ìµœì¢… ì „ì²˜ë¦¬ëœ ìƒí’ˆ ë°ì´í„°ì™€ ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸš€ ìƒí’ˆ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            logger.info("=" * 50)
            
            # 1. ì›ì‹œ ë°ì´í„° ë¡œë“œ
            raw_data = self.load_raw_data(offer_data_path)
            
            # 2. ì»¬ëŸ¼ ì •ê·œí™”
            normalized_data = self.normalize_columns(raw_data)
            
            # 3. ë„ë©”ì¸ í•„í„°ë§
            filtered_data = self.filter_by_domain(normalized_data)
            
            # 4. ë³„ì¹­ ê·œì¹™ ë¡œë“œ
            alias_pdf = self.load_alias_rules(alias_rules_path)

            # 4. build íƒ€ì… ë³„ì¹­ í™•ì¥
            logger.info("4ï¸âƒ£ build íƒ€ì… ë³„ì¹­ í™•ì¥")
            alias_pdf = self.expand_build_aliases(alias_pdf, filtered_data)
            alias_pdf = self.create_bidirectional_aliases(alias_pdf)
            
            # 5. ë³„ì¹­ ì—°ì‡„ ì ìš©
            with_aliases = self.apply_cascading_alias_rules(filtered_data, alias_pdf)
            
            # 6. ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
            with_user_entities = self.add_user_defined_entities(with_aliases, None)
            
            # 7. ë„ë©”ì¸ëª… ì»¬ëŸ¼ ì¶”ê°€
            with_domain_names = self.add_domain_name_column(with_user_entities)
            
            # 8. TEST í•­ëª© í•„í„°ë§
            final_data = self.filter_test_items(with_domain_names)
            
            # ìµœì¢… í™•ì¸
            logger.info("=" * 50)
            logger.info("âœ… ìƒí’ˆ ì •ë³´ ì¤€ë¹„ ì™„ë£Œ")
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {final_data.shape}")
            logger.info(f"ìµœì¢… ì»¬ëŸ¼ë“¤: {list(final_data.columns)}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in final_data.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            else:
                logger.info("âœ… ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not final_data.empty:
                logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {final_data['item_nm'].dropna().head(3).tolist()}")
                logger.info(f"ë³„ì¹­ ìƒ˜í”Œ: {final_data['item_nm_alias'].dropna().head(3).tolist()}")
            
            logger.info("=" * 50)
            
            return final_data, alias_pdf
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            return pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_nm_alias']), pd.DataFrame()
