"""
Item Data Loader Service
=========================

ìƒí’ˆ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ë¥¼ ì „ë‹´í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
ê¸°ì¡´ MMSExtractorì˜ _load_and_prepare_item_data ë©”ì„œë“œ(197ì¤„)ë¥¼ 
ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
"""

import logging
import os
import pandas as pd
from typing import List, Tuple
from joblib import Parallel, delayed
from utils import select_most_comprehensive

logger = logging.getLogger(__name__)


class ItemDataLoader:
    """
    ìƒí’ˆ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤
    
    ì±…ì„:
    - ë°ì´í„° ì†ŒìŠ¤(CSV/DB)ì—ì„œ ì›ì‹œ ë°ì´í„° ë¡œë“œ
    - ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì ìš©
    - ë°ì´í„° í•„í„°ë§ ë° ì •ì œ
    - ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€
    """
    
    def __init__(self, data_source='local', db_loader=None):
        """
        Args:
            data_source: ë°ì´í„° ì†ŒìŠ¤ ('local' ë˜ëŠ” 'db')
            db_loader: DB ë¡œë” í•¨ìˆ˜ (data_source='db'ì¼ ë•Œ í•„ìš”)
        """
        self.data_source = data_source
        self.db_loader = db_loader
        self.alias_pdf_raw = None  # ë³„ì¹­ ê·œì¹™ ì›ë³¸ (ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥)
        
    def load_raw_data(self, offer_data_path: str = None) -> pd.DataFrame:
        """
        ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì›ì‹œ ìƒí’ˆ ë°ì´í„° ë¡œë“œ
        
        Args:
            offer_data_path: CSV íŒŒì¼ ê²½ë¡œ (local ëª¨ë“œì¼ ë•Œ)
            
        Returns:
            pd.DataFrame: ì›ì‹œ ìƒí’ˆ ë°ì´í„°
        """
        logger.info(f"ğŸ“Š ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ (ì†ŒìŠ¤: {self.data_source})")
        
        if self.data_source == 'db':
            if not self.db_loader:
                raise ValueError("DB ëª¨ë“œì—ì„œëŠ” db_loaderê°€ í•„ìš”í•©ë‹ˆë‹¤")
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ ì¤‘...")
            item_pdf_raw = self.db_loader()
        else:
            if not offer_data_path:
                from config.settings import METADATA_CONFIG
                offer_data_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/offer_master_data.csv')
            logger.info(f"CSV íŒŒì¼ì—ì„œ ë¡œë“œ ì¤‘: {offer_data_path}")
            item_pdf_raw = pd.read_csv(offer_data_path, encoding='cp949')
        
        logger.info(f"ì›ì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {item_pdf_raw.shape}")
        return item_pdf_raw
    
    def normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì»¬ëŸ¼ëª… ì •ê·œí™” ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        
        Args:
            df: ì›ì‹œ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ”§ ì»¬ëŸ¼ ì •ê·œí™” ì¤‘...")
        
        # ITEM_NM ì²˜ë¦¬: ë‹¨ë§ê¸°(E)ëŠ” ITEM_DESC ì‚¬ìš©
        if 'ITEM_DMN' in df.columns and 'ITEM_DESC' in df.columns:
            df['ITEM_NM'] = df.apply(
                lambda x: x['ITEM_DESC'] if x['ITEM_DMN']=='E' else x['ITEM_NM'], 
                axis=1
            )
        
        # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
        df = df.rename(columns={c: c.lower() for c in df.columns})
        logger.info("ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")
        
        # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
        df['item_ctg'] = None
        df['item_emb_vec'] = None
        df['ofer_cd'] = df['item_id']
        df['oper_dt_hms'] = '20250101000000'
        
        return df
    
    def filter_by_domain(self, df: pd.DataFrame, excluded_domains: List[str] = None) -> pd.DataFrame:
        """
        ë„ë©”ì¸ ì½”ë“œë¡œ ë°ì´í„° í•„í„°ë§
        
        Args:
            df: ë°ì´í„°í”„ë ˆì„
            excluded_domains: ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        if not excluded_domains:
            from config.settings import PROCESSING_CONFIG
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
        
        if excluded_domains and 'item_dmn' in df.columns:
            before_filter = len(df)
            df = df.query("item_dmn not in @excluded_domains")
            logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter} -> {len(df)}")
        
        return df
    
    def load_alias_rules(self, alias_rules_path: str = None) -> pd.DataFrame:
        """
        ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            alias_rules_path: ë³„ì¹­ ê·œì¹™ CSV íŒŒì¼ ê²½ë¡œ
            
        Returns:
            pd.DataFrame: ì „ì²˜ë¦¬ëœ ë³„ì¹­ ê·œì¹™
        """
        logger.info("ğŸ”— ë³„ì¹­ ê·œì¹™ ë¡œë“œ ì¤‘...")
        
        if not alias_rules_path:
            from config.settings import METADATA_CONFIG
            alias_rules_path = getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv')
        
        # ì›ë³¸ ì €ì¥ (ì™¸ë¶€ ì ‘ê·¼ìš©)
        self.alias_pdf_raw = pd.read_csv(alias_rules_path)
        
        # ë³„ì¹­ ê·œì¹™ ì „ì²˜ë¦¬
        alias_pdf = self.alias_pdf_raw.copy()
        alias_pdf['alias_1'] = alias_pdf['alias_1'].str.split("&&")
        alias_pdf['alias_2'] = alias_pdf['alias_2'].str.split("&&")
        alias_pdf = alias_pdf.explode('alias_1')
        alias_pdf = alias_pdf.explode('alias_2')
        
        return alias_pdf
    
    def expand_build_type_aliases(self, alias_pdf: pd.DataFrame, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        'build' íƒ€ì… ë³„ì¹­ í™•ì¥
        
        Args:
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: í™•ì¥ëœ ë³„ì¹­ ê·œì¹™
        """
        logger.info("ğŸ”¨ Build íƒ€ì… ë³„ì¹­ í™•ì¥ ì¤‘...")
        
        alias_list_ext = alias_pdf.query("type=='build'")[
            ['alias_1','category','direction','type']
        ].to_dict('records')
        
        for alias in alias_list_ext:
            adf = item_df.query(
                "item_nm.str.contains(@alias['alias_1']) and item_dmn==@alias['category']"
            )[['item_nm','item_desc','item_dmn']].rename(
                columns={'item_nm':'alias_2','item_desc':'description','item_dmn':'category'}
            ).drop_duplicates()
            
            adf['alias_1'] = alias['alias_1']
            adf['direction'] = alias['direction']
            adf['type'] = alias['type']
            adf = adf[alias_pdf.columns]
            
            alias_pdf = pd.concat([
                alias_pdf.query(f"alias_1!='{alias['alias_1']}'"), 
                adf
            ])
        
        alias_pdf = alias_pdf.drop_duplicates()
        return alias_pdf
    
    def add_bidirectional_aliases(self, alias_pdf: pd.DataFrame) -> pd.DataFrame:
        """
        ì–‘ë°©í–¥(B) ë³„ì¹­ ì¶”ê°€
        
        Args:
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: ì–‘ë°©í–¥ ë³„ì¹­ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("â†”ï¸ ì–‘ë°©í–¥ ë³„ì¹­ ì¶”ê°€ ì¤‘...")
        
        bidirectional = alias_pdf.query("direction=='B'").rename(
            columns={'alias_1':'alias_2', 'alias_2':'alias_1'}
        )[alias_pdf.columns]
        
        alias_pdf = pd.concat([alias_pdf, bidirectional])
        return alias_pdf
    
    def apply_alias_cascade(self, item_df: pd.DataFrame, alias_pdf: pd.DataFrame, 
                           max_depth: int = 3, n_jobs: int = None) -> pd.DataFrame:
        """
        ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            alias_pdf: ë³„ì¹­ ê·œì¹™ ë°ì´í„°í”„ë ˆì„
            max_depth: ìµœëŒ€ ì—°ì‡„ ê¹Šì´
            n_jobs: ë³‘ë ¬ ì‘ì—… ìˆ˜
            
        Returns:
            pd.DataFrame: ë³„ì¹­ì´ ì ìš©ëœ ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ”„ ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© ì¤‘...")
        
        alias_rule_set = list(zip(
            alias_pdf['alias_1'], 
            alias_pdf['alias_2'], 
            alias_pdf['type']
        ))
        logger.info(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")
        
        def apply_alias_rule_cascade_parallel(args_dict):
            """ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš©"""
            item_nm = args_dict['item_nm']
            max_depth = args_dict['max_depth']
            
            processed = set()
            result_dict = {item_nm: '#' * len(item_nm)}
            to_process = [(item_nm, 0, frozenset())]
            
            while to_process:
                current_item, depth, path_applied_rules = to_process.pop(0)
                
                if depth >= max_depth or current_item in processed:
                    continue
                
                processed.add(current_item)
                
                for r in alias_rule_set:
                    alias_from, alias_to, alias_type = r[0], r[1], r[2]
                    rule_key = (alias_from, alias_to, alias_type)
                    
                    if rule_key in path_applied_rules:
                        continue
                    
                    # íƒ€ì…ì— ë”°ë¥¸ ë§¤ì¹­
                    if alias_type == 'exact':
                        matched = (current_item == alias_from)
                    else:
                        matched = (alias_from in current_item)
                    
                    if matched:
                        new_item = alias_to.strip() if alias_type == 'exact' else current_item.replace(alias_from.strip(), alias_to.strip())
                        
                        if new_item not in result_dict:
                            result_dict[new_item] = alias_from.strip()
                            to_process.append((new_item, depth + 1, path_applied_rules | {rule_key}))
            
            item_nm_list = [{'item_nm': k, 'item_nm_alias': v} for k, v in result_dict.items()]
            adf = pd.DataFrame(item_nm_list)
            selected_alias = select_most_comprehensive(adf['item_nm_alias'].tolist())
            result_aliases = list(adf.query("item_nm_alias in @selected_alias")['item_nm'].unique())
            
            if item_nm not in result_aliases:
                result_aliases.append(item_nm)
            
            return {'item_nm': item_nm, 'item_nm_alias': result_aliases}
        
        def parallel_alias_rule_cascade(texts, max_depth=5, n_jobs=None):
            """ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš©"""
            if n_jobs is None:
                n_jobs = min(os.cpu_count()-1, 4)
            
            batches = [{"item_nm": text, "max_depth": max_depth} for text in texts]
            with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                batch_results = parallel(delayed(apply_alias_rule_cascade_parallel)(args) for args in batches)
            
            return pd.DataFrame(batch_results)
        
        # ë³„ì¹­ ì ìš©
        item_alias_pdf = parallel_alias_rule_cascade(item_df['item_nm'], max_depth=max_depth, n_jobs=n_jobs)
        
        # ë³„ì¹­ ë³‘í•© ë° explode
        item_df = item_df.merge(item_alias_pdf, on='item_nm', how='left')
        before_explode = len(item_df)
        item_df = item_df.explode('item_nm_alias').drop_duplicates()
        logger.info(f"ë³„ì¹­ explode: {before_explode} -> {len(item_df)}")
        
        return item_df
    
    def add_user_defined_entities(self, item_df: pd.DataFrame, 
                                  user_entities: List[str] = None) -> pd.DataFrame:
        """
        ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            user_entities: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: ì‚¬ìš©ì ì—”í‹°í‹°ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        if not user_entities:
            from config.settings import PROCESSING_CONFIG
            user_entities = getattr(PROCESSING_CONFIG, 'user_defined_entities', [])
        
        if user_entities:
            logger.info(f"ğŸ‘¤ ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€: {len(user_entities)}ê°œ")
            item_pdf_ext = pd.DataFrame([{
                'item_nm': e, 'item_id': e, 'item_desc': e, 'item_dmn': 'user_defined',
                'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e
            } for e in user_entities])
            item_df = pd.concat([item_df, item_pdf_ext])
        
        return item_df
    
    def add_domain_name_column(self, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        item_dmn_nm ì»¬ëŸ¼ ì¶”ê°€ (ë„ë©”ì¸ ì½”ë“œ -> ë„ë©”ì¸ëª…)
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: ë„ë©”ì¸ëª…ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ·ï¸ ë„ë©”ì¸ëª… ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
        
        item_dmn_map = pd.DataFrame([
            {"item_dmn": 'P', 'item_dmn_nm': 'ìš”ê¸ˆì œ ë° ê´€ë ¨ ìƒí’ˆ'},
            {"item_dmn": 'E', 'item_dmn_nm': 'ë‹¨ë§ê¸°'},
            {"item_dmn": 'S', 'item_dmn_nm': 'êµ¬ë… ìƒí’ˆ'},
            {"item_dmn": 'C', 'item_dmn_nm': 'ì¿ í°'},
            {"item_dmn": 'X', 'item_dmn_nm': 'ê°€ìƒ ìƒí’ˆ'}
        ])
        
        item_df = item_df.merge(item_dmn_map, on='item_dmn', how='left')
        item_df['item_dmn_nm'] = item_df['item_dmn_nm'].fillna('ê¸°íƒ€')
        
        return item_df
    
    def filter_test_items(self, item_df: pd.DataFrame) -> pd.DataFrame:
        """
        TEST ë¬¸ìì—´ì´ í¬í•¨ëœ í•­ëª© í•„í„°ë§
        
        Args:
            item_df: ìƒí’ˆ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            pd.DataFrame: TEST í•­ëª©ì´ ì œê±°ëœ ë°ì´í„°í”„ë ˆì„
        """
        logger.info("ğŸ§¹ TEST í•­ëª© í•„í„°ë§ ì¤‘...")
        
        before_test = len(item_df)
        item_df = item_df.query("not item_nm_alias.str.contains('TEST', case=False, na=False)")
        logger.info(f"TEST í•„í„°ë§: {before_test} -> {len(item_df)}")
        
        return item_df
    
    def prepare_item_data(self, offer_data_path: str = None, 
                         alias_rules_path: str = None,
                         excluded_domains: List[str] = None,
                         user_entities: List[str] = None) -> pd.DataFrame:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ë°ì´í„° ë¡œë“œë¶€í„° ìµœì¢… ì „ì²˜ë¦¬ê¹Œì§€
        
        Args:
            offer_data_path: ìƒí’ˆ ë°ì´í„° CSV ê²½ë¡œ
            alias_rules_path: ë³„ì¹­ ê·œì¹™ CSV ê²½ë¡œ
            excluded_domains: ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            user_entities: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            pd.DataFrame: ìµœì¢… ì „ì²˜ë¦¬ëœ ìƒí’ˆ ë°ì´í„°
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸš€ ìƒí’ˆ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            logger.info("=" * 50)
            
            # 1. ì›ì‹œ ë°ì´í„° ë¡œë“œ
            raw_data = self.load_raw_data(offer_data_path)
            
            # 2. ì»¬ëŸ¼ ì •ê·œí™”
            normalized_data = self.normalize_columns(raw_data)
            
            # 3. ë„ë©”ì¸ í•„í„°ë§
            filtered_data = self.filter_by_domain(normalized_data, excluded_domains)
            
            # 4. ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì²˜ë¦¬
            alias_pdf = self.load_alias_rules(alias_rules_path)
            alias_pdf = self.expand_build_type_aliases(alias_pdf, filtered_data)
            alias_pdf = self.add_bidirectional_aliases(alias_pdf)
            
            # 5. ë³„ì¹­ ì—°ì‡„ ì ìš©
            with_aliases = self.apply_alias_cascade(filtered_data, alias_pdf)
            
            # 6. ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€
            with_user_entities = self.add_user_defined_entities(with_aliases, user_entities)
            
            # 7. ë„ë©”ì¸ëª… ì»¬ëŸ¼ ì¶”ê°€
            with_domain_names = self.add_domain_name_column(with_user_entities)
            
            # 8. TEST í•­ëª© í•„í„°ë§
            final_data = self.filter_test_items(with_domain_names)
            
            # ìµœì¢… í™•ì¸
            logger.info("=" * 50)
            logger.info("âœ… ìƒí’ˆ ì •ë³´ ì¤€ë¹„ ì™„ë£Œ")
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {final_data.shape}")
            logger.info(f"ìµœì¢… ì»¬ëŸ¼ë“¤: {list(final_data.columns)}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in final_data.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            else:
                logger.info("âœ… ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not final_data.empty:
                logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {final_data['item_nm'].dropna().head(3).tolist()}")
                logger.info(f"ë³„ì¹­ ìƒ˜í”Œ: {final_data['item_nm_alias'].dropna().head(3).tolist()}")
            
            logger.info("=" * 50)
            
            return final_data
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            return pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_nm_alias'])
