# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor) - AI ê¸°ë°˜ ê´‘ê³  í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ
================================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ”
AI ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. LLM(Large Language Model)ì„ í™œìš©í•˜ì—¬ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ
ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì , ì—”í‹°í‹° ê´€ê³„ ë“±ì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
-----------
1. **ì—”í‹°í‹° ì¶”ì¶œ**: ìƒí’ˆëª…, ë¸Œëœë“œëª…, ì„œë¹„ìŠ¤ëª… ë“± í•µì‹¬ ì—”í‹°í‹° ì‹ë³„
2. **ì±„ë„ ë¶„ì„**: URL, ì „í™”ë²ˆí˜¸, ì•± ë§í¬ ë“± ê³ ê° ì ‘ì  ì±„ë„ ì¶”ì¶œ
3. **ëª©ì  ë¶„ë¥˜**: ê´‘ê³ ì˜ ì£¼ìš” ëª©ì  ë° ì•¡ì…˜ íƒ€ì… ë¶„ì„
4. **í”„ë¡œê·¸ë¨ ë§¤ì¹­**: ì‚¬ì „ ì •ì˜ëœ í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”

ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­
--------------
- **ëª¨ë“ˆí™” ì„¤ê³„**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì™¸ë¶€ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ ìš©ì´ì„± ì¦ëŒ€
- **ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”**: LLM í˜¸ì¶œ ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ë¡œê¹… ë° ì‹¤í–‰ ì‹œê°„ ì¶”ì ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” ì§€ì›
- **ë°ì´í„° ê²€ì¦**: ì¶”ì¶œ ê²°ê³¼ì˜ í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ
- **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©

ğŸ—ï¸ ì•„í‚¤í…ì²˜
-----------
- **MMSExtractor**: ë©”ì¸ ì¶”ì¶œ ì—”ì§„ í´ë˜ìŠ¤
- **DataManager**: ë°ì´í„° ë¡œë”© ë° ê´€ë¦¬ ë‹´ë‹¹
- **LLMProcessor**: LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
- **EntityMatcher**: ì—”í‹°í‹° ë§¤ì¹­ ë° ìœ ì‚¬ë„ ê³„ì‚°
- **PromptModule**: ì™¸ë¶€í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

âš™ï¸ ì„¤ì • ë° í™˜ê²½
--------------
- Python 3.8+
- LangChain, OpenAI, Anthropic API ì§€ì›
- Oracle Database ì—°ë™ (ì„ íƒì‚¬í•­)
- GPU ê°€ì† (CUDA ì§€ì› ì‹œ)

ğŸ“Š ì„±ëŠ¥ ì§€í‘œ
-----------
- í‰ê·  ì²˜ë¦¬ ì‹œê°„: ~30ì´ˆ/ë©”ì‹œì§€
- ì •í™•ë„: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
- ì²˜ë¦¬ëŸ‰: ~120 ë©”ì‹œì§€/ì‹œê°„ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

ì‘ì„±ì: MMS ë¶„ì„íŒ€
ìµœì¢… ìˆ˜ì •: 2024-09
ë²„ì „: 2.0.0
"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
import warnings
from functools import wraps
from typing import List, Tuple, Union, Dict, Any, Optional
from abc import ABC, abstractmethod
import traceback
import json
import re
import ast
import glob
import os
from bson import raw_bson
import copy
import pandas as pd
import numpy as np

# joblibê³¼ multiprocessing ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="joblib")
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")
warnings.filterwarnings("ignore", message=".*resource_tracker.*")
warnings.filterwarnings("ignore", message=".*leaked.*")
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle
from contextlib import contextmanager

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from rapidfuzz import fuzz, process
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
from entity_dag_extractor import DAGParser, extract_dag, create_dag_diagram, sha256_hash

# í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from prompts import (
    build_extraction_prompt,
    enhance_prompt_for_retry,
    get_fallback_result,
    build_entity_extraction_prompt,
    DEFAULT_ENTITY_EXTRACTION_PROMPT,
    DETAILED_ENTITY_EXTRACTION_PROMPT
)

# ì„¤ì • ë° ì˜ì¡´ì„± ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë¡œê¹… ì„¤ì • - api.pyì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê¸°ë³¸ ì„¤ì • ì ìš©
logger = logging.getLogger(__name__)

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë¡œê¹… ì„¤ì • (api.pyì—ì„œ ì„í¬íŠ¸ë  ë•ŒëŠ” api.pyì˜ ì„¤ì • ì‚¬ìš©)
if __name__ == '__main__':
    import sys
    from pathlib import Path
    
    # MongoDB ìœ í‹¸ë¦¬í‹°ëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'mms_extractor.log'),
            logging.StreamHandler()
        ]
    )

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ë°ì½”ë ˆì´í„° ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def log_performance(func):
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ì„ ë¡œê¹…í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            logger.info(f"{func.__name__} ì‹¤í–‰ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            return result
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{func.__name__} ì‹¤í–‰ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            raise
    return wrapper

def safe_execute(func, *args, default_return=None, max_retries=2, **kwargs):
    """
    ì•ˆì „í•œ í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, API í˜¸ì¶œ ì‹¤íŒ¨ ë“±ì˜ ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€í•´
    ì§€ìˆ˜ ë°±ì˜¤í”„(exponential backoff)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    
    Args:
        func: ì‹¤í–‰í•  í•¨ìˆ˜
        *args: í•¨ìˆ˜ì— ì „ë‹¬í•  ìœ„ì¹˜ ì¸ìˆ˜
        default_return: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë°˜í™˜í•  ê¸°ë³¸ê°’
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (default: 2)
        **kwargs: í•¨ìˆ˜ì— ì „ë‹¬í•  í‚¤ì›Œë“œ ì¸ìˆ˜
        
    Returns:
        í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” default_return
        
    Example:
        result = safe_execute(api_call, data, default_return={}, max_retries=3)
    """
    for attempt in range(max_retries + 1):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries:
                # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… ë° ê¸°ë³¸ê°’ ë°˜í™˜
                logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
                return default_return
            else:
                # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„: 1ì´ˆ, 2ì´ˆ, 4ì´ˆ, 8ì´ˆ... (ì§€ìˆ˜ ë°±ì˜¤í”„)
                logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                time.sleep(2 ** attempt)
    return default_return

def validate_text_input(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ ê²€ì¦ ë° ì •ë¦¬ í•¨ìˆ˜
    
    MMS í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì „ì— ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ê³ 
    ì²˜ë¦¬ì— ì í•©í•œ í˜•íƒœë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ê²€ì¦í•  ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        
    Raises:
        ValueError: ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì˜ ì…ë ¥
        
    Example:
        clean_text = validate_text_input("  [SKí…”ë ˆì½¤] í˜œíƒ ì•ˆë‚´  ")
    """
    # íƒ€ì… ê²€ì¦: ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
    if not isinstance(text, str):
        raise ValueError(f"í…ìŠ¤íŠ¸ ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(text)}")
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬
    if not text:
        raise ValueError("ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # ìµœëŒ€ ê¸¸ì´ ì œí•œ (LLM í† í° ì œí•œ ë° ì„±ëŠ¥ ê³ ë ¤)
    if len(text) > 10000:
        logger.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(text)} ë¬¸ì). ì²˜ìŒ 10000ìë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        text = text[:10000]
    
    return text

def safe_check_empty(obj) -> bool:
    """ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°ì²´ê°€ ë¹„ì–´ìˆëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸"""
    try:
        if hasattr(obj, '__len__'):
            return len(obj) == 0
        elif hasattr(obj, 'size'):  # numpy ë°°ì—´
            return obj.size == 0
        elif hasattr(obj, 'empty'):  # pandas DataFrame/Series
            return obj.empty
        else:
            return not bool(obj)
    except (ValueError, TypeError):
        # numpy ë°°ì—´ì˜ truth value ì—ëŸ¬ ë“±ì„ ì²˜ë¦¬
        try:
            return getattr(obj, 'size', 1) == 0
        except:
            return True  # ì•ˆì „ì„ ìœ„í•´ ë¹„ì–´ìˆë‹¤ê³  ê°€ì •

# ===== ì›ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìœ ì§€) =====

def dataframe_to_markdown_prompt(df, max_rows=None):
    """DataFrameì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    if max_rows is not None and len(df) > max_rows:
        display_df = df.head(max_rows)
        truncation_note = f"\n[Note: Only showing first {max_rows} of {len(df)} rows]"
    else:
        display_df = df
        truncation_note = ""
    df_markdown = display_df.to_markdown()
    prompt = f"\n\n    {df_markdown}\n    {truncation_note}\n\n    "
    return prompt

def clean_segment(segment):
    """ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ì—ì„œ ë‚´ë¶€ì˜ ë™ì¼í•œ ë”°ì˜´í‘œ ì œê±°"""
    segment = segment.strip()
    if len(segment) >= 2 and segment[0] in ['"', "'"] and segment[-1] == segment[0]:
        q = segment[0]
        inner = segment[1:-1].replace(q, '')
        return q + inner + q
    return segment

def split_key_value(text):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ ì²« ë²ˆì§¸ ì½œë¡ ì„ ê¸°ì¤€ìœ¼ë¡œ í‚¤-ê°’ ë¶„ë¦¬"""
    in_quote = False
    quote_char = ''
    for i, char in enumerate(text):
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
        elif char == ':' and not in_quote:
            return text[:i], text[i+1:]
    return text, ''

def split_outside_quotes(text, delimiter=','):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ êµ¬ë¶„ìë¡œë§Œ í…ìŠ¤íŠ¸ ë¶„ë¦¬"""
    parts = []
    current = []
    in_quote = False
    quote_char = ''
    for char in text:
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
            current.append(char)
        elif char == delimiter and not in_quote:
            parts.append(''.join(current).strip())
            current = []
        else:
            current.append(char)
    if current:
        parts.append(''.join(current).strip())
    return parts

def clean_ill_structured_json(text):
    """ì˜ëª» êµ¬ì¡°í™”ëœ JSON í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    parts = split_outside_quotes(text, delimiter=',')
    cleaned_parts = []
    for part in parts:
        key, value = split_key_value(part)
        key_clean = clean_segment(key)
        value_clean = clean_segment(value) if value.strip() != "" else ""
        if value_clean:
            cleaned_parts.append(f"{key_clean}: {value_clean}")
        else:
            cleaned_parts.append(key_clean)
    return ', '.join(cleaned_parts)

def repair_json(broken_json):
    """ì†ìƒëœ JSON ë¬¸ìì—´ ë³µêµ¬"""
    json_str = broken_json
    # ë”°ì˜´í‘œ ì—†ëŠ” í‚¤ì— ë”°ì˜´í‘œ ì¶”ê°€
    json_str = re.sub(r'([{,])\s*([a-zA-Z0-9_]+)\s*:', r'\1 "\2":', json_str)
    # ë”°ì˜´í‘œ ì—†ëŠ” ê°’ ì²˜ë¦¬
    parts = json_str.split('"')
    for i in range(0, len(parts), 2):
        parts[i] = re.sub(r':\s*([a-zA-Z0-9_]+)(?=\s*[,\]\}])', r': "\1"', parts[i])
    json_str = '"'.join(parts)
    # í›„í–‰ ì‰¼í‘œ ì œê±°
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    return json_str

def extract_json_objects(text):
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì¶”ì¶œ"""
    pattern = r'(\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\})'
    result = []
    for match in re.finditer(pattern, text):
        potential_json = match.group(0)
        try:
            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))
            result.append(json_obj)
        except (json.JSONDecodeError, SyntaxError, ValueError):
            pass
    return result

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ê·œí™”)"""
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ===== ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ (ì›ë³¸ ìœ ì§€) =====

def fuzzy_similarities(text, entities):
    """í¼ì§€ ë§¤ì¹­ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    results = []
    for entity in entities:
        scores = {
            'ratio': fuzz.ratio(text, entity) / 100,
            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,
            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,
            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100
        }
        max_score = max(scores.values())
        results.append((entity, max_score))
    return results

def get_fuzzy_similarities(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜"""
    text = args_dict['text']
    entities = args_dict['entities']
    threshold = args_dict['threshold']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    
    text_processed = preprocess_text(text.lower())
    similarities = fuzzy_similarities(text_processed, entities)
    
    filtered_results = [
        {
            text_col_nm: text,
            item_col_nm: entity, 
            "sim": score
        } 
        for entity, score in similarities 
        if score >= threshold
    ]
    return filtered_results

def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, len(entities) // (n_jobs * 2))
    
    batches = []
    for text in texts:
        for i in range(0, len(entities), batch_size):
            batch = entities[i:i + batch_size]
            batches.append({"text": text, "entities": batch, "threshold": threshold, "text_col_nm": text_col_nm, "item_col_nm": item_col_nm})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def longest_common_subsequence_ratio(s1, s2, normalizaton_value):
    """ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ë¹„ìœ¨ ê³„ì‚°"""
    def lcs_length(x, y):
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i-1] == y[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_len = lcs_length(s1, s2)
    if normalizaton_value == 'max':
        max_len = max(len(s1), len(s2))
        return lcs_len / max_len if max_len > 0 else 1.0
    elif normalizaton_value == 'min':
        min_len = min(len(s1), len(s2))
        return lcs_len / min_len if min_len > 0 else 1.0
    elif normalizaton_value == 's1':
        return lcs_len / len(s1) if len(s1) > 0 else 1.0
    elif normalizaton_value == 's2':
        return lcs_len / len(s2) if len(s2) > 0 else 1.0
    else:
        raise ValueError(f"Invalid normalization value: {normalizaton_value}")

def sequence_matcher_similarity(s1, s2, normalizaton_value):
    """SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    matcher = difflib.SequenceMatcher(None, s1, s2)
    matches = sum(triple.size for triple in matcher.get_matching_blocks())
    
    normalization_length = min(len(s1), len(s2))
    if normalizaton_value == 'max':
        normalization_length = max(len(s1), len(s2))
    elif normalizaton_value == 's1':
        normalization_length = len(s1)
    elif normalizaton_value == 's2':
        normalization_length = len(s2)
        
    if normalization_length == 0: 
        return 0.0
    
    return matches / normalization_length

def substring_aware_similarity(s1, s2, normalizaton_value):
    """ë¶€ë¶„ë¬¸ìì—´ ê´€ê³„ë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    if s1 in s2 or s2 in s1:
        shorter = min(s1, s2, key=len)
        longer = max(s1, s2, key=len)
        base_score = len(shorter) / len(longer)
        return min(0.95 + base_score * 0.05, 1.0)
    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)

def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\s_\-]+'):
    """í† í° ì‹œí€€ìŠ¤ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    tokens1 = [t for t in re.split(separator_pattern, s1.strip()) if t]
    tokens2 = [t for t in re.split(separator_pattern, s2.strip()) if t]
    
    if not tokens1 or not tokens2:
        return 0.0
    
    def token_lcs_length(t1, t2):
        m, n = len(t1), len(t2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if t1[i-1] == t2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_tokens = token_lcs_length(tokens1, tokens2)
    normalization_tokens = max(len(tokens1), len(tokens2))
    if normalizaton_value == 'min':
        normalization_tokens = min(len(tokens1), len(tokens2))
    elif normalizaton_value == 's1':
        normalization_tokens = len(tokens1)
    elif normalizaton_value == 's2':
        normalization_tokens = len(tokens2)
    
    return lcs_tokens / normalization_tokens  

def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):
    """ì—¬ëŸ¬ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ì„ ê²°í•©í•œ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°"""
    if weights is None:
        weights = {'substring': 0.4, 'sequence_matcher': 0.4, 'token_sequence': 0.2}
    
    similarities = {
        'substring': substring_aware_similarity(s1, s2, normalizaton_value),
        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),
        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)
    }
    
    return sum(similarities[key] * weights[key] for key in weights), similarities

def calculate_seq_similarity(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    sent_item_batch = args_dict['sent_item_batch']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    normalizaton_value = args_dict['normalizaton_value']
    
    results = []
    for sent_item in sent_item_batch:
        sent = sent_item[text_col_nm]
        item = sent_item[item_col_nm]
        try:
            sent_processed = preprocess_text(sent.lower())
            item_processed = preprocess_text(item.lower())
            similarity = combined_sequence_similarity(sent_processed, item_processed, normalizaton_value=normalizaton_value)[0]
            results.append({text_col_nm:sent, item_col_nm:item, "sim":similarity})
        except Exception as e:
            logger.error(f"Error processing {item}: {e}")
            results.append({text_col_nm:sent, item_col_nm:item, "sim":0.0})
    
    return results

def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, normalizaton_value='s2'):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))
    
    batches = []
    for i in range(0, sent_item_pdf.shape[0], batch_size):
        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')
        batches.append({"sent_item_batch": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'normalizaton_value': normalizaton_value})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def load_sentence_transformer(model_path, device=None):
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Loading model from {model_path}...")
    model = SentenceTransformer(model_path).to(device)
    logger.info(f"Model loaded on {device}")
    return model

# ===== Kiwi í˜•íƒœì†Œ ë¶„ì„ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ (ì›ë³¸ ìœ ì§€) =====

class Token:
    """í˜•íƒœì†Œ ë¶„ì„ í† í° í´ë˜ìŠ¤"""
    def __init__(self, form, tag, start, length):
        self.form = form      # í† í° í˜•íƒœ
        self.tag = tag        # í’ˆì‚¬ íƒœê·¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.len = length     # ê¸¸ì´

class Sentence:
    """í˜•íƒœì†Œ ë¶„ì„ ë¬¸ì¥ í´ë˜ìŠ¤"""
    def __init__(self, text, start, end, tokens, subs=None):
        self.text = text      # ë¬¸ì¥ í…ìŠ¤íŠ¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.end = end        # ë ìœ„ì¹˜
        self.tokens = tokens  # í† í° ë¦¬ìŠ¤íŠ¸
        self.subs = subs or []  # í•˜ìœ„ ë¬¸ì¥ë“¤

def filter_text_by_exc_patterns(sentence, exc_tag_patterns):
    """ì œì™¸í•  í’ˆì‚¬ íŒ¨í„´ì— ë”°ë¼ í…ìŠ¤íŠ¸ í•„í„°ë§"""
    # ê°œë³„ íƒœê·¸ì™€ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ë¦¬
    individual_tags = set()
    sequences = []
    
    for pattern in exc_tag_patterns:
        if isinstance(pattern, list):
            if len(pattern) == 1:
                individual_tags.add(pattern[0])
            else:
                sequences.append(pattern)
        else:
            individual_tags.add(pattern)
    
    # ì œì™¸í•  í† í° ì¸ë±ìŠ¤ ìˆ˜ì§‘
    tokens_to_exclude = set()
    
    # ê°œë³„ íƒœê·¸ ë§¤ì¹­ í™•ì¸
    for i, token in enumerate(sentence.tokens):
        if token.tag in individual_tags:
            tokens_to_exclude.add(i)
    
    # ì‹œí€€ìŠ¤ íŒ¨í„´ ë§¤ì¹­ í™•ì¸
    for sequence in sequences:
        seq_len = len(sequence)
        for i in range(len(sentence.tokens) - seq_len + 1):
            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):
                for j in range(seq_len):
                    tokens_to_exclude.add(i + j)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸í•  í† í° ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    result_chars = list(sentence.text)
    for i, token in enumerate(sentence.tokens):
        if i in tokens_to_exclude:
            start_pos = token.start - sentence.start
            end_pos = start_pos + token.len
            for j in range(start_pos, end_pos):
                if j < len(result_chars) and result_chars[j] != ' ':
                    result_chars[j] = ' '
    
    filtered_text = ''.join(result_chars)
    return re.sub(r'\s+', ' ', filtered_text)

def filter_specific_terms(strings: List[str]) -> List[str]:
    """ì¤‘ë³µë˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì— ìˆëŠ” ìš©ì–´ë“¤ í•„í„°ë§"""
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)
    
    filtered = []
    for s in unique_strings:
        if not any(s in other for other in filtered):
            filtered.append(s)
    
    return filtered

def convert_df_to_json_list(df):
    """DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜"""
    result = []
    grouped = df.groupby('item_name_in_msg')
    
    for item_name_in_msg, group in grouped:
        item_dict = {
            'item_name_in_msg': item_name_in_msg,
            'item_in_voca': []
        }
        
        item_nm_groups = group.groupby('item_nm')
        for item_nm, item_group in item_nm_groups:
            item_ids = list(item_group['item_id'].unique())
            voca_item = {
                'item_nm': item_nm,
                'item_id': item_ids
            }
            item_dict['item_in_voca'].append(voca_item)
        result.append(item_dict)
    
    return result

# ===== ì¶”ìƒ í´ë˜ìŠ¤ ë° ì „ëµ íŒ¨í„´ =====

class EntityExtractionStrategy(ABC):
    """ì—”í‹°í‹° ì¶”ì¶œ ì „ëµ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def extract(self, text: str, **kwargs) -> pd.DataFrame:
        """ì—”í‹°í‹° ì¶”ì¶œ ë©”ì†Œë“œ"""
        pass

class DataLoader(ABC):
    """ë°ì´í„° ë¡œë” ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def load_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë“œ ë©”ì†Œë“œ"""
        pass

# ===== ê°œì„ ëœ MMSExtractor í´ë˜ìŠ¤ =====

class MMSExtractor:
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ AI ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì¶”ì¶œ ì—”ì§„
    ================================================================
    
    ğŸ¨ ê°œìš”
    -------
    ì´ í´ë˜ìŠ¤ëŠ” MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.
    LLM(Large Language Model), ì„ë² ë”© ëª¨ë¸, NLP ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬
    ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
    -----------
    1. **ë‹¤ë‹¨ê³„ ì—”í‹°í‹° ì¶”ì¶œ**: Kiwi NLP + ì„ë² ë”© ìœ ì‚¬ë„ + LLM ê¸°ë°˜ ì¶”ì¶œ
    2. **ì§€ëŠ¥í˜• í”„ë¡œê·¸ë¨ ë¶„ë¥˜**: ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ë§¤ì¹­
    3. **RAG ê¸°ë°˜ ì»¬í…ìŠ¤íŠ¸ ì¦ê°•**: ê´€ë ¨ ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•ë„ í–¥ìƒ
    4. **ë‹¤ì¤‘ LLM ì§€ì›**: OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
    5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    
    ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•
    -----------
    - **ì •í™•ë„**: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
    - **ì²˜ë¦¬ ì†ë„**: í‰ê·  30ì´ˆ/ë©”ì‹œì§€
    - **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ìš©ì´
    - **ì•ˆì •ì„±**: ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    
    âš™ï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
    --------------
    - **ì•„í‚¤í…ì²˜ ëª¨ë“ˆí™”**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
    - **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
    - **ë‹¤ì¸µ ì˜ˆì™¸ ì²˜ë¦¬**: LLM API ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
    - **ìƒì„¸ ë¡œê¹…**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…, ê°ì‚¬ ë¡œê·¸ë¥¼ ìœ„í•œ í¬ê´„ì  ë¡œê¹… ì‹œìŠ¤í…œ
    - **ë°ì´í„° ê²€ì¦**: ì…ë ¥/ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ê²€ì¦
    - **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©
    
    ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    -----------
    ```python
    # ê¸°ë³¸ ì´ˆê¸°í™”
    extractor = MMSExtractor(
        llm_model='ax',
        entity_extraction_mode='llm',
        extract_entity_dag=True
    )
    
    # ë©”ì‹œì§€ ì²˜ë¦¬
    result = extractor.process_message("ìƒ˜í”Œ MMS í…ìŠ¤íŠ¸")
    
    # ê²°ê³¼ í™œìš©
    products = result['product']
    channels = result['channel']
    entity_dag = result.get('entity_dag', [])
    ```
    
    ğŸ’¼ ì˜ì¡´ì„±
    ---------
    - LangChain (LLM ì¸í„°í˜ì´ìŠ¤)
    - SentenceTransformers (ì„ë² ë”©)
    - KiwiPiePy (NLP)
    - cx_Oracle (ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™)
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='ax', extract_entity_dag=False):
        """
        MMSExtractor ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        ì‹œìŠ¤í…œì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:
        - LLM ëª¨ë¸ ì„¤ì • ë° ì—°ê²°
        - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        - NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™”
        - ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ (CSV/DB)
        - ê°ì¢… ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
        
        Args:
            model_path (str, optional): ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ. ê¸°ë³¸ê°’: 'jhgan/ko-sroberta-multitask'
            data_dir (str, optional): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ. ê¸°ë³¸ê°’: './data/'
            product_info_extraction_mode (str, optional): ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ ('nlp' ë˜ëŠ” 'llm')
            entity_extraction_mode (str, optional): ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ ('nlp', 'llm', 'hybrid')
            offer_info_data_src (str, optional): ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ('local' ë˜ëŠ” 'db')
            llm_model (str, optional): ì‚¬ìš©í•  LLM ëª¨ë¸. ê¸°ë³¸ê°’: 'ax'
            extract_entity_dag (bool, optional): DAG ì¶”ì¶œ ì—¬ë¶€. ê¸°ë³¸ê°’: False
            
        Raises:
            Exception: ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜¤ë¥˜
            
        Example:
            >>> extractor = MMSExtractor(
            ...     llm_model='gpt-4',
            ...     entity_extraction_mode='hybrid',
            ...     extract_entity_dag=True
            ... )
        """
        logger.info("ğŸš€ MMSExtractor ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
            logger.info("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì ìš© ì¤‘...")
            self._set_default_config(model_path, data_dir, product_info_extraction_mode, 
                                   entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag)
            
            # 2ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±)
            logger.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
            load_dotenv()
            
            # 3ë‹¨ê³„: ì£¼ìš” êµ¬ì„± ìš”ì†Œë“¤ ìˆœì°¨ ì´ˆê¸°í™”
            logger.info("ğŸ’» ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...")
            self._initialize_device()
            
            logger.info("ğŸ¤– LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self._initialize_llm()
            
            logger.info("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self._initialize_embedding_model()
            
            logger.info("ğŸ“ NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™” ì¤‘...")
            self._initialize_kiwi()
            
            logger.info("ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
            self._load_data()
            
            logger.info("âœ… MMSExtractor ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ MMSExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise

    def _set_default_config(self, model_path, data_dir, product_info_extraction_mode, 
                          entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì ìš©"""
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else getattr(EMBEDDING_CONFIG, 'ko_sbert_model_path', 'jhgan/ko-sroberta-multitask')
        self.offer_info_data_src = offer_info_data_src
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'product_info_extraction_mode', 'nlp')
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'entity_extraction_mode', 'llm')
        self.llm_model_name = llm_model
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)
        self.extract_entity_dag = extract_entity_dag
        
        # DAG ì¶”ì¶œ ì„¤ì • ë¡œê¹…
        # extract_entity_dag: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ DAG(Directed Acyclic Graph)ë¡œ ì¶”ì¶œ
        # Trueì¸ ê²½ìš° ì¶”ê°€ì ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹° ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ 
        # NetworkX + Graphvizë¥¼ í†µí•´ ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±
        if self.extract_entity_dag:
            logger.info("ğŸ¯ DAG ì¶”ì¶œ ëª¨ë“œ í™œì„±í™”ë¨")
        else:
            logger.info("ğŸ“‹ í‘œì¤€ ì¶”ì¶œ ëª¨ë“œ (DAG ë¹„í™œì„±í™”)")

    @log_performance
    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    @log_performance
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì„¤ì • ë§¤í•‘
            model_mapping = {
                "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "gem": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),  # 'gem'ì€ 'gemma'ì˜ ì¤„ì„ë§
                "ax": getattr(MODEL_CONFIG, 'ax_model', 'ax-4'),
                "claude": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),
                "cld": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),  # 'cld'ëŠ” 'claude'ì˜ ì¤„ì„ë§
                "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),  # 'gen'ì€ 'gemini'ì˜ ì¤„ì„ë§
                "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'gpt-4')
            }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'ax-4'))
            
            # LLM ëª¨ë¸ë³„ ì¼ê´€ì„± ì„¤ì •
            model_kwargs = {
                "temperature": 0.0,  # ì™„ì „ ê²°ì •ì  ì¶œë ¥ì„ ìœ„í•´ 0.0 ê³ ì •
                "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ê°•í™”
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42  # ê³ ì • ì‹œë“œë¡œ ì¼ê´€ì„± ë³´ì¥
                
            self.llm_model = ChatOpenAI(**model_kwargs)
            
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ë¹„í™œì„±í™” ì˜µì…˜ í™•ì¸
        if MODEL_CONFIG.disable_embedding:
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¹„í™œì„±í™” ëª¨ë“œ (DISABLE_EMBEDDING=true)")
            self.emb_model = None
            return
            
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            try:
                self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)
            except Exception as e2:
                logger.error(f"Fallback ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                logger.warning("ì„ë² ë”© ëª¨ë¸ ì—†ì´ ë™ì‘ ëª¨ë“œë¡œ ì „í™˜")
                self.emb_model = None

    @log_performance
    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _load_data(self):
        """í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ëª¨ë“œ: {self.offer_info_data_src}")
            
            # ìƒí’ˆ ì •ë³´ ë¡œë“œ
            logger.info("1ï¸âƒ£ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_item_data()
            logger.info(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ë³„ì¹­ ê·œì¹™ ì ìš©
            logger.info("2ï¸âƒ£ ë³„ì¹­ ê·œì¹™ ì ìš© ì¤‘...")
            self._apply_alias_rules()
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            
            # ì •ì§€ì–´ ë¡œë“œ
            logger.info("3ï¸âƒ£ ì •ì§€ì–´ ë¡œë“œ ì¤‘...")
            self._load_stop_words()
            logger.info(f"ë¡œë“œëœ ì •ì§€ì–´ ìˆ˜: {len(self.stop_item_names)}ê°œ")
            
            # Kiwiì— ìƒí’ˆëª… ë“±ë¡
            logger.info("4ï¸âƒ£ Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì¤‘...")
            self._register_items_to_kiwi()
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ
            logger.info("5ï¸âƒ£ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_program_data()
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì¡°ì§ ì •ë³´ ë¡œë“œ
            logger.info("6ï¸âƒ£ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_organization_data()
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
            
            # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ - ìµœì¢… ìƒíƒœ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ìƒí’ˆ ë°ì´í„°: {self.item_pdf_all.shape}")
            logger.info(f"âœ… í”„ë¡œê·¸ë¨ ë°ì´í„°: {self.pgm_pdf.shape}")
            logger.info(f"âœ… ì¡°ì§ ë°ì´í„°: {self.org_pdf.shape}")
            logger.info(f"âœ… ì •ì§€ì–´: {len(self.stop_item_names)}ê°œ")
            
            # ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒíƒœ ë¹„êµë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´
            if hasattr(self, 'item_pdf_all') and not self.item_pdf_all.empty:
                logger.info("=== ìƒí’ˆ ë°ì´í„° ìƒì„¸ ì •ë³´ ===")
                if 'item_nm' in self.item_pdf_all.columns:
                    unique_items = self.item_pdf_all['item_nm'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆëª… ìˆ˜: {unique_items}ê°œ")
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    unique_aliases = self.item_pdf_all['item_nm_alias'].nunique()
                    logger.info(f"ê³ ìœ  ë³„ì¹­ ìˆ˜: {unique_aliases}ê°œ")
                if 'item_id' in self.item_pdf_all.columns:
                    unique_ids = self.item_pdf_all['item_id'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆID ìˆ˜: {unique_ids}ê°œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_item_data(self):
        """ìƒí’ˆ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/items.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                item_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ë°ì´í„° í¬ê¸°: {item_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(item_pdf_raw.columns)}")
                
                # ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ì²˜ë¦¬: ëŒ€ë¬¸ì/ì†Œë¬¸ì ì»¬ëŸ¼ëª… ëª¨ë‘ ì§€ì›
                available_columns = list(item_pdf_raw.columns)
                
                # í•„ìš”í•œ ì»¬ëŸ¼ëª… ë§¤í•‘ (ëŒ€ë¬¸ì ìš°ì„ , ì†Œë¬¸ì í´ë°±)
                column_mapping = {}
                required_cols = ['item_nm', 'item_id', 'item_desc', 'item_dmn']
                
                for req_col in required_cols:
                    if req_col.upper() in available_columns:
                        column_mapping[req_col.upper()] = req_col
                    elif req_col in available_columns:
                        column_mapping[req_col] = req_col
                    else:
                        logger.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ '{req_col}' ë˜ëŠ” '{req_col.upper()}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ITEM_ALS -> item_nm_alias ë§¤í•‘ ì²˜ë¦¬
                if 'ITEM_ALS' in available_columns:
                    column_mapping['ITEM_ALS'] = 'item_nm_alias'
                elif 'item_als' in available_columns:
                    column_mapping['item_als'] = 'item_nm_alias'
                elif 'item_nm_alias' in available_columns:
                    column_mapping['item_nm_alias'] = 'item_nm_alias'
                else:
                    logger.info("ITEM_ALS/item_nm_alias ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ìƒì„±ë©ë‹ˆë‹¤.")
                
                logger.info(f"ì»¬ëŸ¼ ë§¤í•‘: {column_mapping}")
                
                # ë°ì´í„° ì¶”ì¶œ ë° ì¤‘ë³µ ì œê±°
                mapped_columns = list(column_mapping.keys())
                if len(mapped_columns) >= 2:  # ìµœì†Œ 2ê°œ ì»¬ëŸ¼ì€ ìˆì–´ì•¼ í•¨
                    # ëŒ€ë¬¸ì ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                    dedup_cols = [col for col in ['ITEM_NM', 'ITEM_ID'] if col in available_columns]
                    if not dedup_cols:  # ëŒ€ë¬¸ìê°€ ì—†ìœ¼ë¥´ ì†Œë¬¸ì ì‚¬ìš©
                        dedup_cols = [col for col in ['item_nm', 'item_id'] if col in available_columns]
                    
                    self.item_pdf_all = item_pdf_raw.drop_duplicates(dedup_cols)[mapped_columns].copy()
                    
                    # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë¦¬ë„¤ì„
                    self.item_pdf_all = self.item_pdf_all.rename(columns=column_mapping)
                    logger.info(f"ì¤‘ë³µ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
                else:
                    logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì„ ì¶©ë¶„íˆ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_columns}")
                    raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼ ë¶€ì¡±")
                
                # ì¶”ê°€ ì»¬ëŸ¼ë“¤ ìƒì„± (DB ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜ì„±ì„ ìœ„í•´)
                if 'item_ctg' not in self.item_pdf_all.columns:
                    self.item_pdf_all['item_ctg'] = None
                if 'item_emb_vec' not in self.item_pdf_all.columns:
                    self.item_pdf_all['item_emb_vec'] = None
                if 'ofer_cd' not in self.item_pdf_all.columns:
                    self.item_pdf_all['ofer_cd'] = self.item_pdf_all['item_id']
                if 'oper_dt_hms' not in self.item_pdf_all.columns:
                    self.item_pdf_all['oper_dt_hms'] = '20250101000000'
                
                # item_nm_alias ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ item_nmì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                if 'item_nm_alias' not in self.item_pdf_all.columns:
                    logger.info("item_nm_alias ì»¬ëŸ¼ì´ ì—†ì–´ì„œ item_nmì„ ë³µì‚¬í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.")
                    self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm']
                else:
                    # item_nm_aliasê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° item_nmìœ¼ë¡œ ì±„ìš°ê¸°
                    null_count = self.item_pdf_all['item_nm_alias'].isnull().sum()
                    if null_count > 0:
                        logger.info(f"item_nm_aliasì—ì„œ {null_count}ê°œ null ê°’ì„ item_nmìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
                        self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm_alias'].fillna(self.item_pdf_all['item_nm'])
                
                # ì»¬ëŸ¼ëª…ì´ ì´ë¯¸ ì†Œë¬¸ìë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”
                logger.info(f"ë¡œì»¬ ëª¨ë“œ ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
                
                # item_nm_alias ì»¬ëŸ¼ í™•ì¸
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    alias_sample = self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()
                    logger.info(f"item_nm_alias ìƒ˜í”Œ: {alias_sample}")
                else:
                    logger.error("item_nm_alias ì»¬ëŸ¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤!")
                
                # ë¡œì»¬ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                if not self.item_pdf_all.empty:
                    sample_items = self.item_pdf_all['item_nm'].dropna().head(5).tolist()
                    logger.info(f"ë¡œì»¬ ëª¨ë“œ ìƒí’ˆëª… ìƒ˜í”Œ: {sample_items}")
                    logger.info(f"ë¡œì»¬ ëª¨ë“œ ë°ì´í„° ìƒ˜í”Œ (5ê°œ í–‰):")
                    logger.info(f"{self.item_pdf_all.head().to_dict('records')}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_item_from_database()
            
            # ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ í•„í„°ë§
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
            if excluded_domains:
                before_filter_size = len(self.item_pdf_all)
                self.item_pdf_all = self.item_pdf_all.query("item_dmn not in @excluded_domains")
                after_filter_size = len(self.item_pdf_all)
                logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter_size} -> {after_filter_size} (ì œì™¸ëœ ë„ë©”ì¸: {excluded_domains})")
                
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.item_pdf_all)}ê°œ ìƒí’ˆ ===")
            logger.info(f"ìµœì¢… ë°ì´í„° ìŠ¤í‚¤ë§ˆ: {list(self.item_pdf_all.columns)}")
            logger.info(f"ìµœì¢… ë°ì´í„° íƒ€ì…: {self.item_pdf_all.dtypes.to_dict()}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ìµœì¢… í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in self.item_pdf_all.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
            else:
                logger.info("ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.item_pdf_all = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")

    def _get_database_connection(self):
        """Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘ ===")
            
            username = os.getenv("DB_USERNAME")
            password = os.getenv("DB_PASSWORD")
            host = os.getenv("DB_HOST")
            port = os.getenv("DB_PORT")
            service_name = os.getenv("DB_NAME")
            
            # ì—°ê²° ì •ë³´ ë¡œê¹… (ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹)
            logger.info(f"DB ì—°ê²° ì •ë³´:")
            logger.info(f"  - ì‚¬ìš©ìëª…: {username if username else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ë¹„ë°€ë²ˆí˜¸: {'*' * len(password) if password else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í˜¸ìŠ¤íŠ¸: {host if host else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í¬íŠ¸: {port if port else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ì„œë¹„ìŠ¤ëª…: {service_name if service_name else '[ë¹„ì–´ìˆìŒ]'}")
            
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            missing_vars = []
            if not username: missing_vars.append('DB_USERNAME')
            if not password: missing_vars.append('DB_PASSWORD')
            if not host: missing_vars.append('DB_HOST')
            if not port: missing_vars.append('DB_PORT')
            if not service_name: missing_vars.append('DB_NAME')
            
            if missing_vars:
                logger.error(f"ëˆ„ë½ëœ í™˜ê²½ ë³€ìˆ˜: {missing_vars}")
                logger.error("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì„ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
                raise ValueError(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ëˆ„ë½: {missing_vars}")
            
            # DSN ìƒì„± ë° ë¡œê¹…
            logger.info(f"DSN ìƒì„± ì¤‘: {host}:{port}/{service_name}")
            dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
            logger.info(f"DSN ìƒì„± ì„±ê³µ: {dsn}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            conn = cx_Oracle.connect(user=username, password=password, dsn=dsn, encoding="UTF-8")
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            
            # LOB ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ outputtypehandler ì„¤ì •
            def output_type_handler(cursor, name, default_type, size, precision, scale):
                if default_type == cx_Oracle.CLOB:
                    return cursor.var(cx_Oracle.LONG_STRING, arraysize=cursor.arraysize)
                elif default_type == cx_Oracle.BLOB:
                    return cursor.var(cx_Oracle.LONG_BINARY, arraysize=cursor.arraysize)
            
            conn.outputtypehandler = output_type_handler
            
            # ì—°ê²° ì •ë³´ í™•ì¸
            logger.info(f"ì—°ê²°ëœ DB ë²„ì „: {conn.version}")
            
            return conn
            
        except cx_Oracle.DatabaseError as db_error:
            error_obj, = db_error.args
            logger.error(f"Oracle ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜:")
            logger.error(f"  - ì˜¤ë¥˜ ì½”ë“œ: {error_obj.code}")
            logger.error(f"  - ì˜¤ë¥˜ ë©”ì‹œì§€: {error_obj.message}")
            logger.error(f"  - ì „ì²´ ì˜¤ë¥˜: {db_error}")
            raise
        except ImportError as import_error:
            logger.error(f"cx_Oracle ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {import_error}")
            logger.error("ì½”ë§¨ë“œ: pip install cx_Oracle")
            raise
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    @contextmanager
    def _database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager"""
        conn = None
        start_time = time.time()
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager ì‹œì‘")
            conn = self._get_database_connection()
            connection_time = time.time() - start_time
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ ({connection_time:.2f}ì´ˆ)")
            yield conn
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise
        finally:
            if conn:
                try:
                    conn.close()
                    total_time = time.time() - start_time
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
                except Exception as close_error:
                    logger.warning(f"ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {close_error}")
            else:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _load_item_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìƒí’ˆ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            with self._database_connection() as conn:
                sql = "SELECT * FROM TCAM_RC_OFER_MST"
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.item_pdf_all = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì›ë³¸ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
                
                # ë°ì´í„° íƒ€ì… ì •ë³´ ë¡œê¹…
                logger.info("=== DB ë°ì´í„° íƒ€ì… ì •ë³´ ===")
                for col in self.item_pdf_all.columns:
                    dtype = self.item_pdf_all[col].dtype
                    null_count = self.item_pdf_all[col].isnull().sum()
                    logger.info(f"  {col}: {dtype}, nullê°’: {null_count}ê°œ")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ë° ITEM_ALS -> item_nm_alias ë§¤í•‘
                original_columns = list(self.item_pdf_all.columns)
                
                # ITEM_ALS ì»¬ëŸ¼ì„ item_nm_aliasë¡œ ë§¤í•‘
                column_mapping = {c: c.lower() for c in self.item_pdf_all.columns}
                if 'ITEM_ALS' in original_columns:
                    column_mapping['ITEM_ALS'] = 'item_nm_alias'
                    logger.info("ITEM_ALS ì»¬ëŸ¼ì„ item_nm_aliasë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.")
                
                self.item_pdf_all = self.item_pdf_all.rename(columns=column_mapping)
                logger.info(f"ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.item_pdf_all.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.item_pdf_all.empty:
                    logger.info("DataFrame ë°ì´í„° ê°•ì œ ë¡œë“œ ì‹œì‘...")
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.item_pdf_all.values  # ëª¨ë“  ë°ì´í„° ì ‘ê·¼í•˜ì—¬ LOB ë¡œë“œ ìœ ë„
                        logger.info("DataFrame ë°ì´í„° ê°•ì œ ë¡œë“œ ì™„ë£Œ")
                        
                        # ì£¼ìš” ì»¬ëŸ¼ë“¤ì˜ ìƒ˜í”Œ ë°ì´í„° í™•ì¸
                        if 'item_nm' in self.item_pdf_all.columns:
                            sample_items = self.item_pdf_all['item_nm'].dropna().head(5).tolist()
                            logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {sample_items}")
                        
                        if 'item_id' in self.item_pdf_all.columns:
                            sample_ids = self.item_pdf_all['item_id'].dropna().head(5).tolist()
                            logger.info(f"ìƒí’ˆID ìƒ˜í”Œ: {sample_ids}")
                        
                        # item_nm_alias ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
                        if 'item_nm_alias' not in self.item_pdf_all.columns:
                            logger.info("item_nm_alias ì»¬ëŸ¼ì´ ì—†ì–´ì„œ item_nmì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.")
                            if 'item_nm' in self.item_pdf_all.columns:
                                self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm']
                            else:
                                logger.error("item_nm ì»¬ëŸ¼ë„ ì—†ì–´ item_nm_aliasë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                                self.item_pdf_all['item_nm_alias'] = None
                        else:
                            # item_nm_aliasê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° item_nmìœ¼ë¡œ ì±„ìš°ê¸°
                            null_count = self.item_pdf_all['item_nm_alias'].isnull().sum()
                            if null_count > 0:
                                logger.info(f"DB ëª¨ë“œ: item_nm_aliasì—ì„œ {null_count}ê°œ null ê°’ì„ item_nmìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
                                self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm_alias'].fillna(self.item_pdf_all['item_nm'])
                        
                        # item_nm_alias ìƒ˜í”Œ í™•ì¸
                        if 'item_nm_alias' in self.item_pdf_all.columns:
                            alias_sample = self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()
                            logger.info(f"DB ëª¨ë“œ item_nm_alias ìƒ˜í”Œ: {alias_sample}")
                            
                        logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.item_pdf_all)}ê°œ ìƒí’ˆ")
                        logger.info(f"DB ëª¨ë“œ ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
                    except Exception as load_error:
                        logger.error(f"ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹„ìƒ ìƒí™©ì—ì„œ ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„± (item_nm_alias ì»¬ëŸ¼ í¬í•¨)
            logger.warning("ìƒí’ˆ ì •ë³´ DB ë¡œë“œ ì‹¤íŒ¨ë¡œ ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            self.item_pdf_all = pd.DataFrame(columns=[
                'item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_ctg', 
                'item_emb_vec', 'ofer_cd', 'oper_dt_hms', 'item_nm_alias'
            ])
            raise

    def _load_program_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            with self._database_connection() as conn:
                # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ì¿¼ë¦¬
                sql = """SELECT CMPGN_PGM_NUM pgm_id, CMPGN_PGM_NM pgm_nm, RMK clue_tag 
                         FROM TCAM_CMPGN_PGM_INFO
                         WHERE DEL_YN = 'N' 
                         AND APRV_OP_RSLT_CD = 'APPR'
                         AND EXPS_YN = 'Y'
                         AND CMPGN_PGM_NUM like '2025%' 
                         AND RMK is not null"""
                
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.pgm_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ë“¤: {list(self.pgm_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.pgm_pdf.columns)
                self.pgm_pdf = self.pgm_pdf.rename(columns={c:c.lower() for c in self.pgm_pdf.columns})
                logger.info(f"í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.pgm_pdf.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.pgm_pdf.empty:
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.pgm_pdf.values  # ëª¨ë“  ë°ì´í„° ì ‘ê·¼í•˜ì—¬ LOB ë¡œë“œ ìœ ë„
                        
                        # í”„ë¡œê·¸ë¨ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                        if 'pgm_nm' in self.pgm_pdf.columns:
                            sample_pgms = self.pgm_pdf['pgm_nm'].dropna().head(3).tolist()
                            logger.info(f"í”„ë¡œê·¸ë¨ëª… ìƒ˜í”Œ: {sample_pgms}")
                        
                        if 'clue_tag' in self.pgm_pdf.columns:
                            sample_clues = self.pgm_pdf['clue_tag'].dropna().head(3).tolist()
                            logger.info(f"í´ë£¨ íƒœê·¸ ìƒ˜í”Œ: {sample_clues}")
                            
                        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ")
                    except Exception as load_error:
                        logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            raise

    def _apply_alias_rules(self):
        """ë³„ì¹­ ê·œì¹™ ì ìš©"""
        try:
            logger.info("=== ë³„ì¹­ ê·œì¹™ ì ìš© ì‹œì‘ ===")
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì „ ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            
            alias_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv'))
            alias_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2']))
            logger.info(f"ë¡œë“œëœ ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")

            def apply_alias_rule(item_nm):
                if pd.isna(item_nm) or not isinstance(item_nm, str):
                    return [item_nm] if not pd.isna(item_nm) else []
                    
                item_nm_list = [item_nm]
                for r in alias_rule_set:
                    if r[0] in item_nm:
                        item_nm_list.append(item_nm.replace(r[0], r[1]))
                    if r[1] in item_nm:
                        item_nm_list.append(item_nm.replace(r[1], r[0]))
                return item_nm_list

            # ë³„ì¹­ ê·œì¹™ ì ìš© ì „ ë°ì´í„° ìƒíƒœ í™•ì¸
            if 'item_nm' in self.item_pdf_all.columns:
                non_null_items = self.item_pdf_all['item_nm'].dropna()
                logger.info(f"nullì´ ì•„ë‹Œ ìƒí’ˆëª… ìˆ˜: {len(non_null_items)}ê°œ")
                if len(non_null_items) > 0:
                    sample_before = non_null_items.head(3).tolist()
                    logger.info(f"ë³„ì¹­ ì ìš© ì „ ìƒí’ˆëª… ìƒ˜í”Œ: {sample_before}")
            
            # ì¤‘ìš”: ê¸°ì¡´ item_nm_alias ë°ì´í„° ë³´ì¡´ ì—¬ë¶€ í™•ì¸
            existing_alias_data = None
            has_existing_alias = 'item_nm_alias' in self.item_pdf_all.columns and not self.item_pdf_all['item_nm_alias'].isnull().all()
            
            if has_existing_alias:
                # ê¸°ì¡´ ITEM_ALS ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ë³´ì¡´
                logger.info("ê¸°ì¡´ item_nm_alias ë°ì´í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë³„ì¹­ ê·œì¹™ê³¼ ë³‘í•©í•©ë‹ˆë‹¤.")
                existing_alias_sample = self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()
                logger.info(f"ê¸°ì¡´ alias ìƒ˜í”Œ: {existing_alias_sample}")
                
                # ê¸°ì¡´ alias ë°ì´í„°ë¥¼ ë³„ë„ ì»´ëŸ¼ìœ¼ë¡œ ë³´ì¡´
                self.item_pdf_all['original_item_alias'] = self.item_pdf_all['item_nm_alias']
                
                # item_nmì— ë³„ì¹­ ê·œì¹™ì„ ì ìš©í•œ ë‹¤ìŒ ê¸°ì¡´ aliasì™€ ë³‘í•©
                generated_aliases = self.item_pdf_all['item_nm'].apply(apply_alias_rule)
                
                # ê¸°ì¡´ aliasì™€ ìƒì„±ëœ aliasë¥¼ ë³‘í•©
                def combine_aliases(row):
                    generated = row['generated_aliases'] if isinstance(row['generated_aliases'], list) else [row['generated_aliases']]
                    original = [row['original_item_alias']] if pd.notna(row['original_item_alias']) else []
                    combined = list(set(generated + original))  # ì¤‘ë³µ ì œê±°
                    return combined
                
                self.item_pdf_all['generated_aliases'] = generated_aliases
                self.item_pdf_all['item_nm_alias'] = self.item_pdf_all.apply(combine_aliases, axis=1)
                
                # ì„ì‹œ ì»´ëŸ¼ ì‚­ì œ
                self.item_pdf_all = self.item_pdf_all.drop(['original_item_alias', 'generated_aliases'], axis=1)
                
                logger.info("ê¸°ì¡´ ITEM_ALS ë°ì´í„°ì™€ ë³„ì¹­ ê·œì¹™ì´ ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            else:
                # ê¸°ì¡´ alias ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                logger.info("ê¸°ì¡´ item_nm_alias ë°ì´í„°ê°€ ì—†ì–´ item_nmì—ì„œ ë³„ì¹­ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm'].apply(apply_alias_rule)
            
            # explode ì „í›„ í¬ê¸° ë¹„êµ
            before_explode_size = len(self.item_pdf_all)
            self.item_pdf_all = self.item_pdf_all.explode('item_nm_alias')
            after_explode_size = len(self.item_pdf_all)
            
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© í›„ ë°ì´í„° í¬ê¸°: {before_explode_size} -> {after_explode_size}")
            
            # ë³„ì¹­ ì ìš© í›„ ìƒ˜í”Œ í™•ì¸
            if 'item_nm_alias' in self.item_pdf_all.columns:
                non_null_aliases = self.item_pdf_all['item_nm_alias'].dropna()
                if len(non_null_aliases) > 0:
                    sample_after = non_null_aliases.head(5).tolist()
                    logger.info(f"ë³„ì¹­ ì ìš© í›„ ìƒ˜í”Œ: {sample_after}")
            
            logger.info(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì™„ë£Œ: {len(alias_rule_set)}ê°œ ê·œì¹™")
            
        except Exception as e:
            logger.warning(f"ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨: {e}")
            logger.warning(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ì¡´ item_nm_alias ë°ì´í„° ë³´ì¡´ ë˜ëŠ” ì›ë³¸ ì´ë¦„ ì‚¬ìš©
            if 'item_nm_alias' not in self.item_pdf_all.columns or self.item_pdf_all['item_nm_alias'].isnull().all():
                if 'item_nm' in self.item_pdf_all.columns:
                    self.item_pdf_all['item_nm_alias'] = self.item_pdf_all['item_nm']
                    logger.info("ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì›ë³¸ ìƒí’ˆëª…ì„ ë³„ì¹­ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")
            else:
                logger.info("ê¸°ì¡´ item_nm_alias ë°ì´í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤")

    def _load_stop_words(self):
        """ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ"""
        try:
            self.stop_item_names = pd.read_csv(getattr(METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv'))['stop_words'].to_list()
            logger.info(f"ì •ì§€ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.stop_item_names)}ê°œ")
        except Exception as e:
            logger.warning(f"ì •ì§€ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stop_item_names = []

    def _register_items_to_kiwi(self):
        """Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡"""
        try:
            logger.info("=== Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… ë³„ì¹­ ë°ì´í„° í™•ì¸
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                return
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë“±ë¡í•  ê³ ìœ  ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # nullì´ ì•„ë‹Œ ìœ íš¨í•œ ë³„ì¹­ë“¤ë§Œ í•„í„°ë§
            valid_aliases = [w for w in unique_aliases if isinstance(w, str) and len(w.strip()) > 0]
            logger.info(f"ìœ íš¨í•œ ë³„ì¹­ ìˆ˜: {len(valid_aliases)}ê°œ")
            
            if len(valid_aliases) > 0:
                sample_aliases = valid_aliases[:5]
                logger.info(f"ë“±ë¡í•  ë³„ì¹­ ìƒ˜í”Œ: {sample_aliases}")
            
            registered_count = 0
            failed_count = 0
            
            for w in valid_aliases:
                try:
                    self.kiwi.add_user_word(w, "NNP")
                    registered_count += 1
                except Exception as reg_error:
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ë¡œê¹…
                        logger.warning(f"Kiwi ë“±ë¡ ì‹¤íŒ¨ - '{w}': {reg_error}")
            
            logger.info(f"Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"Kiwi ìƒí’ˆëª… ë“±ë¡ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

    def _load_program_data(self):
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
        try:
            logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë”© ì‹œì‘...")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                self.pgm_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'pgm_info_path', './data/program_info.csv'))
                logger.info(f"ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                self._load_program_from_database()
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
            if not self.pgm_pdf.empty:
                logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
                clue_texts = self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                    lambda x: preprocess_text(x['pgm_nm'].lower()) + " " + x['clue_tag'].lower(), axis=1
                ).tolist()
                
                if self.emb_model is not None:
                    self.clue_embeddings = self.emb_model.encode(
                        clue_texts, convert_to_tensor=True, show_progress_bar=False
                    )
                else:
                    logger.warning("ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë¹ˆ tensor ì‚¬ìš©")
                    self.clue_embeddings = torch.empty((0, 768))
                
                logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ í”„ë¡œê·¸ë¨")
            else:
                logger.warning("í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.clue_embeddings = torch.tensor([])
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            self.clue_embeddings = torch.tensor([])

    def _load_organization_data(self):
        """ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'org_info_path', './data/org_info_all_250605.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                org_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ì¡°ì§ ë°ì´í„° í¬ê¸°: {org_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(org_pdf_raw.columns)}")
                
                # ITEM_DMN='R' ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§
                if 'ITEM_DMN' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("ITEM_DMN=='R'").copy()
                elif 'item_dmn' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("item_dmn=='R'").copy()
                else:
                    logger.warning("ITEM_DMN/item_dmn ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.org_pdf = org_pdf_raw.copy()
                
                # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë¦¬ë„¤ì„
                self.org_pdf = self.org_pdf.rename(columns={c: c.lower() for c in self.org_pdf.columns})
                
                logger.info(f"ë¡œì»¬ ëª¨ë“œ: ITEM_DMN='R' í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_org_from_database()
            
            # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
            if not self.org_pdf.empty:
                sample_orgs = self.org_pdf.head(3).to_dict('records')
                logger.info(f"ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ (3ê°œ í–‰): {sample_orgs}")
            
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.org_pdf)}ê°œ ì¡°ì§ ===")
            logger.info(f"ìµœì¢… ì¡°ì§ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: {list(self.org_pdf.columns)}")
            
            # ì¡°ì§ ë°ì´í„° ìµœì¢… ê²€ì¦
            if not self.org_pdf.empty:
                critical_org_columns = ['item_nm', 'item_id']
                missing_org_columns = [col for col in critical_org_columns if col not in self.org_pdf.columns]
                if missing_org_columns:
                    logger.error(f"ì¡°ì§ ë°ì´í„°ì—ì„œ ì¤‘ìš” ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_org_columns}")
                    logger.error("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    logger.info("ëª¨ë“  ì¤‘ìš” ì¡°ì§ ì»¬ëŸ¼ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¡°ì§/ë§¤ì¥ ì¶”ì¶œì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback (ì¡°ì§ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ ì¡°ì§ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")
            logger.warning("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    def _load_org_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ (ITEM_DMN='R')"""
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            
            with self._database_connection() as conn:
                sql = "SELECT * FROM TCAM_RC_OFER_MST WHERE ITEM_DMN='R'"
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.org_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.org_pdf.columns)
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì›ë³¸ ì»¬ëŸ¼ë“¤: {original_columns}")
                
                # ì¡°ì§ ë°ì´í„°ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ë§¤í•‘ (ë™ì¼í•œ í…Œì´ë¸”ì´ì§€ë§Œ ì‚¬ìš© ëª©ì ì´ ë‹¤ë¦„)
                column_mapping = {c: c.lower() for c in self.org_pdf.columns}
                
                # ì¡°ì§ ë°ì´í„°ëŠ” item í…Œì´ë¸”ê³¼ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # ITEM_NM -> item_nm, ITEM_ID -> item_id, ITEM_DESC -> item_desc ë“±
                
                self.org_pdf = self.org_pdf.rename(columns=column_mapping)
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ì»¬ëŸ¼ëª… ë§¤í•‘ ì™„ë£Œ: {dict(zip(original_columns, self.org_pdf.columns))}")
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ë° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
                if not self.org_pdf.empty:
                    logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìµœì¢… í¬ê¸°: {self.org_pdf.shape}")
                    
                    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    required_columns = ['item_nm', 'item_id']
                    missing_columns = [col for col in required_columns if col not in self.org_pdf.columns]
                    if missing_columns:
                        logger.error(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                        logger.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                    else:
                        logger.info("ëª¨ë“  í•„ìˆ˜ ì¡°ì§ ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    
                    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
                    if 'item_nm' in self.org_pdf.columns:
                        sample_orgs = self.org_pdf['item_nm'].dropna().head(5).tolist()
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ëª… ìƒ˜í”Œ: {sample_orgs}")
                    else:
                        logger.error("item_nm ì»¬ëŸ¼ì´ ì—†ì–´ ìƒ˜í”Œì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ í‘œì‹œ
                        sample_data = self.org_pdf.head(3).to_dict('records')
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ: {sample_data}")
                else:
                    logger.warning("DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                
                logger.info(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.org_pdf)}ê°œ ì¡°ì§")
                
        except Exception as e:
            logger.error(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"DB ì¡°ì§ ë¡œë“œ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback (ì¡°ì§ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ì¡°ì§ ë°ì´í„° DB ë¡œë“œ ì‹¤íŒ¨ë¡œ ë¹ˆ DataFrame ì‚¬ìš©")
            
            raise

    def _store_prompt_for_preview(self, prompt: str, prompt_type: str):
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ìš©ìœ¼ë¡œ ì €ì¥"""
        import threading
        current_thread = threading.current_thread()
        
        if not hasattr(current_thread, 'stored_prompts'):
            current_thread.stored_prompts = {}
        
        # í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ ì œëª©ê³¼ ì„¤ëª… ë§¤í•‘
        prompt_info = {
            "main_extraction": {
                'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                'description': 'ê´‘ê³  ë©”ì‹œì§€ì—ì„œ ì œëª©, ëª©ì , ìƒí’ˆ, ì±„ë„, í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            },
            "entity_extraction": {
                'title': 'ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸', 
                'description': 'ë©”ì‹œì§€ì—ì„œ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            }
        }
        
        info = prompt_info.get(prompt_type, {
            'title': f'{prompt_type} í”„ë¡¬í”„íŠ¸',
            'description': f'{prompt_type} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'
        })
        
        prompt_key = f'{prompt_type}_prompt'
        prompt_data = {
            'title': info['title'],
            'description': info['description'],
            'content': prompt,
            'length': len(prompt)
        }
        
        current_thread.stored_prompts[prompt_key] = prompt_data
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        logger.info(f"í”„ë¡¬í”„íŠ¸ ì €ì¥ë¨: {prompt_key} (ê¸¸ì´: {len(prompt)})")
        logger.info(f"í˜„ì¬ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(current_thread.stored_prompts)}")
        logger.info(f"ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í‚¤ë“¤: {list(current_thread.stored_prompts.keys())}")

    def _safe_llm_invoke(self, prompt: str, max_retries: int = 3) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì†Œë“œ"""
        for attempt in range(max_retries):
            try:
                # LLM í˜¸ì¶œ
                response = self.llm_model.invoke(prompt)
                result_text = response.content if hasattr(response, 'content') else str(response)
                
                # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
                json_objects_list = extract_json_objects(result_text)
                if json_objects_list:
                    json_objects = json_objects_list[-1]
                    if self._detect_schema_response(json_objects):
                        logger.warning(f"ì‹œë„ {attempt + 1}: LLMì´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        
                        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µì¸ ê²½ìš° ë” ê°•í•œ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            enhanced_prompt = self._enhance_prompt_for_retry(prompt)
                            response = self.llm_model.invoke(enhanced_prompt)
                            result_text = response.content if hasattr(response, 'content') else str(response)
                
                return result_text
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_extraction(prompt)
                else:
                    logger.warning(f"LLM í˜¸ì¶œ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return ""

    def _enhance_prompt_for_retry(self, original_prompt: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”"""
        return enhance_prompt_for_retry(original_prompt)

    def _fallback_extraction(self, prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ fallback ì¶”ì¶œ ë¡œì§"""
        logger.info("Fallback ì¶”ì¶œ ë¡œì§ ì‹¤í–‰")
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì—ì„œ fallback ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        fallback_result = get_fallback_result()
        
        return json.dumps(fallback_result, ensure_ascii=False)

    @log_performance
    def extract_entities_from_kiwi(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            logger.info("=== Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘ ===")
            mms_msg = validate_text_input(mms_msg)
            logger.info(f"ì²˜ë¦¬í•  ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # ìƒí’ˆ ë°ì´í„° ìƒíƒœ í™•ì¸
            if self.item_pdf_all.empty:
                logger.error("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë§¤ì¹­í•  ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # ë¬¸ì¥ ë¶„í•  ë° í•˜ìœ„ ë¬¸ì¥ ì²˜ë¦¬
            sentences = sum(self.kiwi.split_into_sents(
                re.split(r"_+", mms_msg), return_tokens=True, return_sub_sents=True
            ), [])
            
            sentences_all = []
            for sent in sentences:
                if sent.subs:
                    sentences_all.extend(sent.subs)
                else:
                    sentences_all.append(sent)
            
            logger.info(f"ë¶„í• ëœ ë¬¸ì¥ ìˆ˜: {len(sentences_all)}ê°œ")
            
            # ì œì™¸ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ í•„í„°ë§
            sentence_list = [
                filter_text_by_exc_patterns(sent, self.exc_tag_patterns) 
                for sent in sentences_all
            ]
            
            logger.info(f"í•„í„°ë§ëœ ë¬¸ì¥ë“¤: {sentence_list[:3]}...")  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…

            # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
            result_msg = self.kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)
            all_tokens = [(token.form, token.tag) for token in result_msg]
            logger.info(f"ì „ì²´ í† í° ìˆ˜: {len(all_tokens)}ê°œ")
            
            # NNP íƒœê·¸ í† í°ë“¤ë§Œ ì¶”ì¶œ
            nnp_tokens = [token.form for token in result_msg if token.tag == 'NNP']
            logger.info(f"NNP íƒœê·¸ í† í°ë“¤: {nnp_tokens}")
            
            entities_from_kiwi = [
                token.form for token in result_msg 
                if token.tag == 'NNP' and 
                   token.form not in self.stop_item_names + ['-'] and 
                   len(token.form) >= 2 and 
                   not token.form.lower() in self.stop_item_names
            ]
            entities_from_kiwi = filter_specific_terms(entities_from_kiwi)
            
            logger.info(f"í•„í„°ë§ í›„ Kiwi ì¶”ì¶œ ì—”í‹°í‹°: {list(set(entities_from_kiwi))}")

            # í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ìœ ì‚¬ ìƒí’ˆëª… ì°¾ê¸°
            logger.info("í¼ì§€ ë§¤ì¹­ ì‹œì‘...")
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                sentence_list, 
                unique_aliases,
                threshold=getattr(PROCESSING_CONFIG, 'fuzzy_threshold', 0.5),
                text_col_nm='sent', 
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            logger.info(f"í¼ì§€ ë§¤ì¹­ ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if similarities_fuzzy.empty:
                logger.warning("í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©
                cand_item_list = list(entities_from_kiwi) if entities_from_kiwi else []
                logger.info(f"Kiwi ê¸°ë°˜ í›„ë³´ ì•„ì´í…œ: {cand_item_list}")
                
                if cand_item_list:
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                    logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
                else:
                    extra_item_pdf = pd.DataFrame()
                    logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                
                return cand_item_list, extra_item_pdf
            else:
                logger.info(f"í¼ì§€ ë§¤ì¹­ ì„±ê³µ: {len(similarities_fuzzy)}ê°œ ê²°ê³¼")
                if not similarities_fuzzy.empty:
                    sample_fuzzy = similarities_fuzzy.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                    logger.info(f"í¼ì§€ ë§¤ì¹­ ìƒ˜í”Œ: {sample_fuzzy}")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ í†µí•œ ì •ë°€ ë§¤ì¹­
            logger.info("ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")
            similarities_seq = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='sent',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )
            
            logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê²°ê³¼ í¬ê¸°: {similarities_seq.shape if not similarities_seq.empty else 'ë¹„ì–´ìˆìŒ'}")
            if not similarities_seq.empty:
                sample_seq = similarities_seq.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ìƒ˜í”Œ: {sample_seq}")
            
            # ì„ê³„ê°’ ì´ìƒì˜ í›„ë³´ ì•„ì´í…œë“¤ í•„í„°ë§
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold', 0.2)
            logger.info(f"ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
            
            cand_items = similarities_seq.query(
                "sim >= @similarity_threshold and "
                "item_nm_alias.str.contains('', case=False) and "
                "item_nm_alias not in @self.stop_item_names"
            )
            logger.info(f"ì„ê³„ê°’ í•„í„°ë§ í›„ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_items)}ê°œ")
            
            # Kiwiì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ ì¶”ê°€
            entities_from_kiwi_pdf = self.item_pdf_all.query("item_nm_alias in @entities_from_kiwi")[
                ['item_nm','item_nm_alias']
            ]
            entities_from_kiwi_pdf['sim'] = 1.0
            logger.info(f"Kiwi ì—”í‹°í‹° ë§¤ì¹­ ê²°ê³¼: {len(entities_from_kiwi_pdf)}ê°œ")

            # ê²°ê³¼ í†µí•© ë° ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            cand_item_pdf = pd.concat([cand_items, entities_from_kiwi_pdf])
            logger.info(f"í†µí•©ëœ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_item_pdf)}ê°œ")
            
            if not cand_item_pdf.empty:
                cand_item_array = cand_item_pdf.sort_values('sim', ascending=False).groupby([
                    "item_nm_alias"
                ])['sim'].max().reset_index(name='final_sim').sort_values(
                    'final_sim', ascending=False
                ).query("final_sim >= 0.2")['item_nm_alias'].unique()
                
                # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „ì„± ë³´ì¥
                cand_item_list = list(cand_item_array) if hasattr(cand_item_array, '__iter__') else []
                
                logger.info(f"ìµœì¢… í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                if cand_item_list:  # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¿¼ë¦¬ ì‹¤í–‰
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                else:
                    extra_item_pdf = pd.DataFrame()
                
                logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ DataFrame í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    sample_final = extra_item_pdf.head(3).to_dict('records')
                    logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ìƒ˜í”Œ: {sample_final}")
            else:
                logger.warning("í†µí•©ëœ í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                cand_item_list = []
                extra_item_pdf = pd.DataFrame()

            return cand_item_list, extra_item_pdf
            
        except Exception as e:
            logger.error(f"Kiwi ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ì™€ ë¹ˆ DataFrame
            return [], pd.DataFrame()

    def extract_entities_by_logic(self, cand_entities: List[str], threshold_for_fuzzy: float = 0.8) -> pd.DataFrame:
        """ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            if not cand_entities:
                return pd.DataFrame()
            
            # í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entities,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=threshold_for_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _calculate_combined_similarity(self, similarities_fuzzy: pd.DataFrame) -> pd.DataFrame:
        """s1, s2 ì •ê·œí™” ë°©ì‹ìœ¼ë¡œ ê°ê° ê³„ì‚° í›„ í•©ì‚°"""
        try:
            # s1 ì •ê·œí™”
            sim_s1 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s1',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s2',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s2'})
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            if not sim_s1.empty and not sim_s2.empty:
                combined = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
                combined = combined.groupby(['item_name_in_msg', 'item_nm_alias'])[
                    ['sim_s1', 'sim_s2']
                ].apply(lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()).reset_index(name='sim')
                return combined
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    @log_performance
    def extract_entities_by_llm(self, msg_text: str, rank_limit: int = 5) -> pd.DataFrame:
        """LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            msg_text = validate_text_input(msg_text)
            
            # ë¡œì§ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ í›„ë³´ ì—”í‹°í‹° ë¨¼ì € ì¶”ì¶œ
            cand_entities_by_sim = sorted([
                e.strip() for e in self.extract_entities_by_logic([msg_text], threshold_for_fuzzy=getattr(PROCESSING_CONFIG, 'fuzzy_threshold', 0.4))['item_nm_alias'].unique() 
                if e.strip() not in self.stop_item_names and len(e.strip()) >= 2
            ])

            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì‚¬ìš©
            # í”„ë¡¬í”„íŠ¸ë¥¼ prompts ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì„¤ì • íŒŒì¼ ëŒ€ì‹ )
            base_prompt = getattr(PROCESSING_CONFIG, 'entity_extraction_prompt', None)
            if base_prompt is None:
                # settings.pyì— í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ prompts ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                base_prompt = DETAILED_ENTITY_EXTRACTION_PROMPT
                logger.info("ì—”í‹°í‹° ì¶”ì¶œì— prompts ë””ë ‰í† ë¦¬ì˜ DETAILED_ENTITY_EXTRACTION_PROMPT ì‚¬ìš©")
            else:
                logger.info("ì—”í‹°í‹° ì¶”ì¶œì— settings.pyì˜ entity_extraction_prompt ì‚¬ìš©")
            prompt = build_entity_extraction_prompt(msg_text, base_prompt)
            
            # í›„ë³´ ì—”í‹°í‹° ì¶”ê°€
            prompt += f"""

            ## Candidate entities:
            {cand_entities_by_sim}
            """
            
            # í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…/ë¯¸ë¦¬ë³´ê¸°ìš©)
            self._store_prompt_for_preview(prompt, "entity_extraction")
            
            # LLM í˜¸ì¶œ (í”„ë¡¬í”„íŠ¸ ì €ì¥ì€ ì´ë¯¸ ìœ„ì—ì„œ í–ˆìœ¼ë¯€ë¡œ ì§ì ‘ í˜¸ì¶œ)
            response = self.llm_model.invoke(prompt)
            cand_entities = response.content if hasattr(response, 'content') else str(response)
            
            # LLM ì‘ë‹µ íŒŒì‹± ë° ì •ë¦¬
            cand_entity_list = [e.strip() for e in cand_entities.split(',') if e.strip()]
            cand_entity_list = [e for e in cand_entity_list if e not in self.stop_item_names and len(e) >= 2]

            if not cand_entity_list:
                return pd.DataFrame()

            # í›„ë³´ ì—”í‹°í‹°ë“¤ê³¼ ìƒí’ˆ DB ë§¤ì¹­
            return self._match_entities_with_products(cand_entity_list, rank_limit)
            
        except Exception as e:
            logger.error(f"LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _match_entities_with_products(self, cand_entity_list: List[str], rank_limit: int) -> pd.DataFrame:
        """í›„ë³´ ì—”í‹°í‹°ë“¤ì„ ìƒí’ˆ DBì™€ ë§¤ì¹­"""
        try:
            # í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entity_list,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=0.6,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì •ì§€ì–´ í•„í„°ë§
            similarities_fuzzy = similarities_fuzzy[
                ~similarities_fuzzy['item_nm_alias'].isin(self.stop_item_names)
            ]

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ë§¤ì¹­
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            if cand_entities_sim.empty:
                return pd.DataFrame()
            
            high_sim_threshold = getattr(PROCESSING_CONFIG, 'high_similarity_threshold', 1.5)
            cand_entities_sim = cand_entities_sim.query("sim >= @high_sim_threshold").copy()

            # ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ
            cand_entities_sim["rank"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(
                method='first', ascending=False
            )
            cand_entities_sim = cand_entities_sim.query(f"rank <= {rank_limit}").sort_values(
                ['item_name_in_msg', 'rank'], ascending=[True, True]
            )

            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _extract_entities(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """ì—”í‹°í‹° ì¶”ì¶œ (Kiwi ë˜ëŠ” LLM ë°©ì‹)"""
        try:
            if self.entity_extraction_mode == 'logic':
                # Kiwi ê¸°ë°˜ ì¶”ì¶œ
                return self.extract_entities_from_kiwi(mms_msg)
            else:
                # LLM ê¸°ë°˜ ì¶”ì¶œì„ ìœ„í•´ ë¨¼ì € Kiwië¡œ ê¸°ë³¸ ì¶”ì¶œ
                cand_item_list, extra_item_pdf = self.extract_entities_from_kiwi(mms_msg)
                return cand_item_list, extra_item_pdf
                
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return [], pd.DataFrame()

    def _classify_programs(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _build_extraction_prompt(self, msg: str, rag_context: str, product_element: Optional[List[Dict]]) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì‚¬ìš©"""
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©
        prompt = build_extraction_prompt(
            message=msg,
            rag_context=rag_context,
            product_element=product_element,
            product_info_extraction_mode=self.product_info_extraction_mode
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (LLM ëª¨ë“œì—ì„œë§Œ)
        if self.product_info_extraction_mode == 'llm':
            logger.debug(f"LLM ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í›„ë³´ ìƒí’ˆ ëª©ë¡ í¬í•¨ ì—¬ë¶€: {'ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡' in rag_context}")
            
        return prompt

    def _extract_channels(self, json_objects: Dict, msg: str) -> List[Dict]:
        """ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¹­"""
        try:
            channel_tag = []
            channel_items = json_objects.get('channel', [])
            if isinstance(channel_items, dict):
                channel_items = channel_items.get('items', [])

            for d in channel_items:
                if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                    # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                    store_info = self._match_store_info(d['value'])
                    d['store_info'] = store_info
                else:
                    d['store_info'] = []
                channel_tag.append(d)

            return channel_tag
            
        except Exception as e:
            logger.error(f"ì±„ë„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _match_store_info(self, store_name: str) -> List[Dict]:
        """ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­"""
        try:
            # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
            org_pdf_cand = safe_execute(
                parallel_fuzzy_similarity,
                [preprocess_text(store_name.lower())],
                self.org_pdf['item_nm'].unique(),
                threshold=0.5,
                text_col_nm='org_nm_in_msg',
                item_col_nm='item_nm',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )

            if org_pdf_cand.empty:
                return []

            org_pdf_cand = org_pdf_cand.drop('org_nm_in_msg', axis=1)
            org_pdf_cand = self.org_pdf.merge(org_pdf_cand, on=['item_nm'])
            org_pdf_cand['sim'] = org_pdf_cand.apply(
                lambda x: combined_sequence_similarity(store_name, x['item_nm'])[0], axis=1
            ).round(5)
            
            # ëŒ€ë¦¬ì  ì½”ë“œ('D'ë¡œ ì‹œì‘) ìš°ì„  ê²€ìƒ‰
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold_for_store', 0.2)
            org_pdf_tmp = org_pdf_cand.query(
                "sim >= @similarity_threshold", engine='python'
            ).sort_values('sim', ascending=False)
            
            if org_pdf_tmp.empty:
                # ëŒ€ë¦¬ì ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰
                org_pdf_tmp = org_pdf_cand.query("sim >= @similarity_threshold").sort_values('sim', ascending=False)
            
            if not org_pdf_tmp.empty:
                # ìµœê³  ìˆœìœ„ ì¡°ì§ë“¤ì˜ ì •ë³´ ì¶”ì¶œ
                org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense', ascending=False)
                org_pdf_tmp = org_pdf_tmp.rename(columns={'item_id':'org_cd','item_nm':'org_nm'})
                org_info = org_pdf_tmp.query("rank == 1").groupby('org_nm')['org_cd'].apply(list).reset_index(name='org_cd').to_dict('records')
                return org_info
            else:
                return []
                
        except Exception as e:
            logger.error(f"ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []

    def _validate_extraction_result(self, result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'purpose', 'product', 'channel']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    result[field] = [] if field != 'title' else "ê´‘ê³  ë©”ì‹œì§€"

            # ìƒí’ˆëª… ê¸¸ì´ ê²€ì¦
            validated_products = []
            for product in result.get('product', []):
                if isinstance(product, dict):
                    item_name = product.get('item_name_in_msg', product.get('name', ''))
                    if len(item_name) >= 2 and item_name not in self.stop_item_names:
                        validated_products.append(product)
                    else:
                        logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìƒí’ˆëª… ì œì™¸: {item_name}")
            
            result['product'] = validated_products

            # ì±„ë„ ì •ë³´ ê²€ì¦
            validated_channels = []
            for channel in result.get('channel', []):
                if isinstance(channel, dict) and channel.get('value'):
                    validated_channels.append(channel)
            
            result['channel'] = validated_channels

            return result
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return result

    @log_performance
    def process_message(self, mms_msg: str) -> Dict[str, Any]:
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜)
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ MMS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë©”ì‹œì§€ ë‚´ìš©: {mms_msg[:200]}...")
            logger.info(f"ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # í˜„ì¬ ì„¤ì • ìƒíƒœ ë¡œê¹…
            logger.info("=== í˜„ì¬ ì¶”ì¶œê¸° ì„¤ì • ===")
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.offer_info_data_src}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: {self.product_info_extraction_mode}")
            logger.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ: {self.entity_extraction_mode}")
            logger.info(f"LLM ëª¨ë¸: {self.llm_model_name}")
            logger.info(f"ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì…ë ¥ ê²€ì¦
            msg = validate_text_input(mms_msg)
            
            # 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("=" * 30 + " 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ " + "=" * 30)
            
            # DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨ ì‹œì‘")
                logger.info(f"ìƒí’ˆ ë°ì´í„° ìƒíƒœ: {self.item_pdf_all.shape}")
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                required_columns = ['item_nm', 'item_id', 'item_nm_alias']
                missing_columns = [col for col in required_columns if col not in self.item_pdf_all.columns]
                if missing_columns:
                    logger.error(f"ğŸš¨ DB ëª¨ë“œì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                
                # ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    null_aliases = self.item_pdf_all['item_nm_alias'].isnull().sum()
                    total_aliases = len(self.item_pdf_all)
                    logger.info(f"DB ëª¨ë“œ ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ: {total_aliases - null_aliases}/{total_aliases} ìœ íš¨")
            
            cand_item_list, extra_item_pdf = self._extract_entities(msg)
            logger.info(f"ì¶”ì¶œëœ í›„ë³´ ì—”í‹°í‹°: {cand_item_list}")
            logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
            
            # DB ëª¨ë“œì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ íŠ¹ë³„ ë¶„ì„
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
                # cand_item_listê°€ numpy ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•œ ê²€ì‚¬ ì‚¬ìš©
                if safe_check_empty(cand_item_list):
                    logger.error("ğŸš¨ DB ëª¨ë“œì—ì„œ í›„ë³´ ì—”í‹°í‹°ê°€ ì „í˜€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
                    logger.error("1. ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ìƒí’ˆì´ ì—†ìŒ")
                    logger.error("2. ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨")
                    logger.error("3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ")
                    logger.error("4. Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨")
            
            # 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜
            logger.info("=" * 30 + " 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ " + "=" * 30)
            pgm_info = self._classify_programs(msg)
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ê²°ê³¼ í‚¤: {list(pgm_info.keys())}")
            
            # 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            logger.info("=" * 30 + " 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± " + "=" * 30)
            rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_info['pgm_cand_info']}" if self.num_cand_pgms > 0 else ""
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ (ëª¨ë“œë³„ ì²˜ë¦¬)
            logger.info("=" * 30 + " 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ " + "=" * 30)
            product_element = None
            
            # cand_item_listê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ ì•ˆì „í•˜ê²Œ ê²€ì‚¬
            if not safe_check_empty(cand_item_list):
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°: {len(cand_item_list)}ê°œ")
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                # extra_item_pdf ìƒíƒœ í™•ì¸
                logger.info(f"extra_item_pdf í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    logger.info(f"extra_item_pdf ì»¬ëŸ¼ë“¤: {list(extra_item_pdf.columns)}")
                    logger.info(f"extra_item_pdf ìƒ˜í”Œ: {extra_item_pdf.head(2).to_dict('records')}")
                
                if self.product_info_extraction_mode == 'rag':
                    rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'llm':
                    # LLM ëª¨ë“œì—ë„ í›„ë³´ ëª©ë¡ ì œê³µí•˜ì—¬ ì¼ê´€ì„± í–¥ìƒ
                    rag_context += f"\n\n### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("LLM ëª¨ë“œ: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'nlp':
                    if not extra_item_pdf.empty and 'item_nm' in extra_item_pdf.columns:
                        product_df = extra_item_pdf.rename(columns={'item_nm': 'name'}).query(
                            "not name in @self.stop_item_names"
                        )[['name']]
                        product_df['action'] = 'ê¸°íƒ€'
                        product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
                        logger.info(f"NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ - {len(product_element) if product_element else 0}ê°œ")
                        if product_element:
                            logger.info(f"NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ ìƒ˜í”Œ: {product_element[:2]}")
                    else:
                        logger.warning("NLP ëª¨ë“œ: extra_item_pdfê°€ ë¹„ì–´ìˆê±°ë‚˜ item_nm ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            else:
                logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                logger.warning("ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                logger.warning("1. ìƒí’ˆ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                logger.warning("2. ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨") 
                logger.warning("3. ìœ ì‚¬ë„ ë§¤ì¹­ ì„ê³„ê°’ ë¬¸ì œ")

            # 5ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹¤í–‰
            logger.info("=" * 30 + " 5ë‹¨ê³„: LLM í˜¸ì¶œ " + "=" * 30)
            prompt = self._build_extraction_prompt(msg, rag_context, product_element)
            logger.info(f"êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€: {'í›„ë³´ ìƒí’ˆ' in rag_context}")
            
            # í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…/ë¯¸ë¦¬ë³´ê¸°ìš©)
            self._store_prompt_for_preview(prompt, "main_extraction")
            
            result_json_text = self._safe_llm_invoke(prompt)
            logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(result_json_text)} ë¬¸ì")
            logger.info(f"LLM ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {result_json_text[:500]}...")
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            logger.info("=" * 30 + " 6ë‹¨ê³„: JSON íŒŒì‹± " + "=" * 30)
            json_objects_list = extract_json_objects(result_json_text)
            logger.info(f"ì¶”ì¶œëœ JSON ê°ì²´ ìˆ˜: {len(json_objects_list)}ê°œ")
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ: {result_json_text}")
                return self._create_fallback_result(msg)
            
            json_objects = json_objects_list[-1]
            logger.info(f"íŒŒì‹±ëœ JSON ê°ì²´ í‚¤: {list(json_objects.keys())}")
            logger.info(f"íŒŒì‹±ëœ JSON ë‚´ìš©: {json_objects}")
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ë° ì²˜ë¦¬
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.error("ğŸš¨ LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤! ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                logger.error("ì¬ì‹œë„ ë˜ëŠ” fallback ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._create_fallback_result(msg)

            raw_result = copy.deepcopy(json_objects)
            
            # 7ë‹¨ê³„: ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±
            logger.info("=" * 30 + " 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„± " + "=" * 30)
            final_result = self._build_final_result(json_objects, msg, pgm_info)
            
            # 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
            logger.info("=" * 30 + " 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ " + "=" * 30)
            final_result = self._validate_extraction_result(final_result)

            # # DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ (ì„ íƒì )
            # # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
            # # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
            # dag_section = ""
            # if self.extract_entity_dag:
            #     logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
            #     try:
            #         dag_start_time = time.time()
            #         # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
            #         extract_dag_result = extract_dag(DAGParser(), msg, self.llm_model)
            #         dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
            #         dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
            #         dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
                    
            #         # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
            #         dag_filename = f'dag_{sha256_hash(msg)}'
            #         create_dag_diagram(dag, filename=dag_filename)
            #         dag_processing_time = time.time() - dag_start_time
                    
            #         logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")
            #         logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
            #         logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
            #         if dag_section:
            #             logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
            #         else:
            #             logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                        
            #     except Exception as e:
            #         logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            #         dag_section = ""

            # # ìµœì¢… ê²°ê³¼ì— DAG ì •ë³´ ì¶”ê°€ (ë¹„ì–´ìˆì„ ìˆ˜ë„ ìˆìŒ)
            # final_result['entity_dag'] = sorted([d for d in dag_section.split('\n') if d!=''])
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
            logger.info("=" * 60)
            logger.info("âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"ì œëª©: {final_result.get('title', 'N/A')}")
            logger.info(f"ëª©ì : {final_result.get('purpose', [])}")
            logger.info(f"ìƒí’ˆ ìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info(f"ì±„ë„ ìˆ˜: {len(final_result.get('channel', []))}ê°œ")
            logger.info(f"í”„ë¡œê·¸ë¨ ìˆ˜: {len(final_result.get('pgm', []))}ê°œ")

            actual_prompts = get_stored_prompts_from_thread()

            return {"extracted_result": final_result, "raw_result": raw_result, "prompts": actual_prompts}
            
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_fallback_result(mms_msg)
    
    @log_performance
    def extract_json_objects_only(self, mms_msg: str) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ì—ì„œ 7ë‹¨ê³„(ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±) ì „ì˜ json_objectsë§Œ ì¶”ì¶œ
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€
            
        Returns:
            Dict: LLMì´ ìƒì„±í•œ json_objects (ì—”í‹°í‹° ë§¤ì¹­ ì „)
        """
        try:
            msg = mms_msg.strip()
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì‹œì‘ - ë©”ì‹œì§€ ê¸¸ì´: {len(msg)}ì")
            
            # 1-4ë‹¨ê³„: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤
            pgm_info = self._prepare_program_classification(msg)
            
            # RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (product_info_extraction_modeê°€ 'rag'ì¸ ê²½ìš°)
            rag_context = ""
            if self.product_info_extraction_mode == 'rag':
                rag_context = self._prepare_rag_context(msg)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
            prompt = self._build_extraction_prompt(msg, pgm_info, rag_context)
            result_json_text = self._safe_llm_invoke(prompt)
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            json_objects_list = extract_json_objects(result_json_text)
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {}
            
            json_objects = json_objects_list[-1]
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.warning("LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return {}
            
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì™„ë£Œ - í‚¤: {list(json_objects.keys())}")
            return json_objects
            
        except Exception as e:
            logger.error(f"JSON ê°ì²´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _prepare_program_classification(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì¤€ë¹„ (_classify_programs ë©”ì†Œë“œì™€ ë™ì¼)"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _detect_schema_response(self, json_objects: Dict) -> bool:
        """LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ ê°ì§€"""
        try:
            # purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            purpose = json_objects.get('purpose', {})
            if isinstance(purpose, dict) and 'type' in purpose and purpose.get('type') == 'array':
                logger.warning("purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸  
            product = json_objects.get('product', {})
            if isinstance(product, dict) and 'type' in product and product.get('type') == 'array':
                logger.warning("product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            channel = json_objects.get('channel', {})
            if isinstance(channel, dict) and 'type' in channel and channel.get('type') == 'array':
                logger.warning("channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _create_fallback_result(self, msg: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "product": [],
            "channel": [],
            "pgm": []
        }

    def _build_final_result(self, json_objects: Dict, msg: str, pgm_info: Dict) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        try:
            final_result = json_objects.copy()
            
            # ìƒí’ˆ ì •ë³´ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            product_items = json_objects.get('product', [])
            if isinstance(product_items, dict):
                product_items = product_items.get('items', [])
            
            logger.info(f"LLM ì¶”ì¶œ ì—”í‹°í‹°: {[x.get('name', '') for x in product_items]}")

            # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if self.entity_extraction_mode == 'logic':
                # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
                cand_entities = [item.get('name', '') for item in product_items if item.get('name')]
                similarities_fuzzy = self.extract_entities_by_logic(cand_entities)
            else:
                # LLM ê¸°ë°˜: LLMì„ í†µí•œ ì—”í‹°í‹° ì¶”ì¶œ
                similarities_fuzzy = self.extract_entities_by_llm(msg)

            # ìƒí’ˆ ì •ë³´ ë§¤í•‘
            if not similarities_fuzzy.empty:
                final_result['product'] = self._map_products_with_similarity(similarities_fuzzy, json_objects)
            else:
                # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                final_result['product'] = [
                    {
                        'item_name_in_msg': d.get('name', ''), 
                        'expected_action': d.get('action', 'ê¸°íƒ€'),
                        'item_in_voca': [{'item_name_in_voca': d.get('name', ''), 'item_id': ['#']}]
                    } 
                    for d in product_items 
                    if d.get('name') and d['name'] not in self.stop_item_names
                ]

            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
            final_result['pgm'] = self._map_program_classification(json_objects, pgm_info)
            
            # ì±„ë„ ì •ë³´ ì²˜ë¦¬
            final_result['channel'] = self._extract_channels(json_objects, msg)

            return final_result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return json_objects

    def _map_products_with_similarity(self, similarities_fuzzy: pd.DataFrame, json_objects: Dict = None) -> List[Dict]:
        """ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ë§¤í•‘"""
        try:
            # ë†’ì€ ìœ ì‚¬ë„ ì•„ì´í…œë“¤ í•„í„°ë§
            high_sim_threshold = getattr(PROCESSING_CONFIG, 'high_similarity_threshold', 1.5)
            high_sim_items = similarities_fuzzy.query('sim >= @high_sim_threshold')['item_nm_alias'].unique()
            filtered_similarities = similarities_fuzzy[
                (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &
                (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &
                (~similarities_fuzzy['item_name_in_msg'].isin(self.stop_item_names))
            ]
            
            # ìƒí’ˆ ì •ë³´ì™€ ë§¤í•‘í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
            product_tag = convert_df_to_json_list(
                self.item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])
            )
            
            # Add action information from original json_objects
            if json_objects:
                action_mapping = self._create_action_mapping(json_objects)
                for product in product_tag:
                    item_name = product.get('item_name_in_msg', '')
                    product['expected_action'] = action_mapping.get(item_name, 'ê¸°íƒ€')
            
            return product_tag
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

    def _create_action_mapping(self, json_objects: Dict) -> Dict[str, str]:
        """LLM ì‘ë‹µì—ì„œ ìƒí’ˆëª…-ì•¡ì…˜ ë§¤í•‘ ìƒì„±"""
        try:
            action_mapping = {}
            product_data = json_objects.get('product', [])
            
            if isinstance(product_data, list):
                # ì •ìƒì ì¸ ë°°ì—´ êµ¬ì¡°
                for item in product_data:
                    if isinstance(item, dict) and 'name' in item and 'action' in item:
                        action_mapping[item['name']] = item['action']
            elif isinstance(product_data, dict):
                # ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ë˜ëŠ” ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                if 'items' in product_data:
                    # ìŠ¤í‚¤ë§ˆ êµ¬ì¡°: {"items": [...]}
                    items = product_data.get('items', [])
                    for item in items:
                        if isinstance(item, dict) and 'name' in item and 'action' in item:
                            action_mapping[item['name']] = item['action']
                elif 'type' in product_data and product_data.get('type') == 'array':
                    # ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡°ëŠ” ê±´ë„ˆë›°ê¸°
                    logger.debug("ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡° ê°ì§€ë¨, ì•¡ì…˜ ë§¤í•‘ ê±´ë„ˆë›°ê¸°")
                else:
                    # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                    if 'name' in product_data and 'action' in product_data:
                        action_mapping[product_data['name']] = product_data['action']
            
            logger.debug(f"ìƒì„±ëœ ì•¡ì…˜ ë§¤í•‘: {action_mapping}")
            return action_mapping
            
        except Exception as e:
            logger.error(f"ì•¡ì…˜ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _map_program_classification(self, json_objects: Dict, pgm_info: Dict) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘"""
        try:
            if (self.num_cand_pgms > 0 and 
                'pgm' in json_objects and 
                isinstance(json_objects['pgm'], list) and
                not pgm_info.get('pgm_pdf_tmp', pd.DataFrame()).empty):
                
                pgm_json = pgm_info['pgm_pdf_tmp'][
                    pgm_info['pgm_pdf_tmp']['pgm_nm'].apply(
                        lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm'])
                    )
                ][['pgm_nm', 'pgm_id']].to_dict('records')
                
                return pgm_json
            
            return []
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

def process_message_with_dag(extractor, message: str, extract_dag: bool = False) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ìš©)
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ (í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨)
    """
    try:
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")

        # 1. ë©”ì¸ ì¶”ì¶œ
        result = extractor.process_message(message)
        dag_list = []
        
        if extract_dag:
            # ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë³€ê²½ (í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ë¥¼ ìœ„í•´)
            # ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ë©´ ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œê°€ ë¶„ë¦¬ë˜ì–´ í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ê°€ ì•ˆë¨
            logger.info("ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë©”ì¸ ì¶”ì¶œ ë° DAG ì¶”ì¶œ ìˆ˜í–‰")
            
            # 2. DAG ì¶”ì¶œ
            dag_result = make_entity_dag(message, extractor.llm_model)
            dag_list = sorted([d for d in dag_result['dag_section'].split('\n') if d!=''])

        extracted_result = result.get('extracted_result', {})
        extracted_result['entity_dag'] = dag_list
        result['extracted_result'] = extracted_result

        raw_result = result.get('raw_result', {})
        raw_result['entity_dag'] = dag_list
        result['raw_result'] = raw_result

        result['error'] = ""
        
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
            "purpose": ["ì˜¤ë¥˜"],
            "product": [],
            "channel": [],
            "pgm": [],
            "entity_dag": [],
            "error": str(e)
        }

def process_messages_batch(extractor, messages: List[str], extract_dag: bool = False, max_workers: int = None) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        messages: ì²˜ë¦¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
    
    Returns:
        list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if max_workers is None:
        max_workers = min(len(messages), os.cpu_count())
    
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€, {max_workers}ê°œ ì›Œì»¤")
    
    start_time = time.time()
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë©”ì‹œì§€ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_message = {
            executor.submit(process_message_with_dag, extractor, msg, extract_dag): msg 
            for msg in messages
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
        for i, future in enumerate(future_to_message):
            try:
                result = future.result()
                results.append(result)
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {i+1}/{len(messages)} ({((i+1)/len(messages)*100):.1f}%)")
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                results.append({
                    "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                    "purpose": ["ì˜¤ë¥˜"],
                    "product": [],
                    "channel": [],
                    "pgm": [],
                    "entity_dag": [],
                    "error": str(e)
                })
    
    elapsed_time = time.time() - start_time
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ")
    logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(messages):.2f}ì´ˆ/ë©”ì‹œì§€")
    
    return results

def make_entity_dag(msg: str, llm_model, save_dag_image=True):

    # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
    # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
    extract_dag_result = {}
    logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
    try:
        dag_start_time = time.time()
        # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
        extract_dag_result = extract_dag(DAGParser(), msg, llm_model)
        dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
        dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
        dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
        
        # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
        dag_filename = ""
        if save_dag_image:
            dag_filename = f'dag_{sha256_hash(msg)}'
            create_dag_diagram(dag, filename=dag_filename)
            logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")

        extract_dag_result['dag_filename'] = dag_filename
        
        dag_processing_time = time.time() - dag_start_time
        
        logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
        logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
        if dag_section:
            logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
        else:
            logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        dag_section = ""

    return extract_dag_result


def get_stored_prompts_from_thread():
    """í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    import threading
    current_thread = threading.current_thread()
    
    if hasattr(current_thread, 'stored_prompts'):
        return current_thread.stored_prompts
    else:
        return {}

def save_result_to_mongodb_if_enabled(message: str, result: dict, args_or_data, extractor=None):
    """MongoDB ì €ì¥ì´ í™œì„±í™”ëœ ê²½ìš° ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
    
    Args:
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        result: ì²˜ë¦¬ ê²°ê³¼ (extracted_result, raw_result í¬í•¨)
        args_or_data: argparse.Namespace ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
    
    Returns:
        str: ì €ì¥ëœ ë¬¸ì„œ ID, ì‹¤íŒ¨ ì‹œ None
    """
    # args_or_dataê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Namespaceë¡œ ë³€í™˜
    if isinstance(args_or_data, dict):
        import argparse
        args = argparse.Namespace(**args_or_data)
    else:
        args = args_or_data
    
    # save_to_mongodb ì†ì„±ì´ ì—†ê±°ë‚˜ Falseì¸ ê²½ìš°
    if not getattr(args, 'save_to_mongodb', False):
        return None
        
    try:
        # MongoDB ì„í¬íŠ¸ ì‹œë„
        from mongodb_utils import save_to_mongodb
        
        # ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œì—ì„œ í”„ë¡¬í”„íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stored_prompts = result.get('prompts', get_stored_prompts_from_thread()) 
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´ êµ¬ì„±
        prompts_data = {}
        for key, prompt_data in stored_prompts.items():
            prompts_data[key] = {
                'title': prompt_data.get('title', f'{key} í”„ë¡¬í”„íŠ¸'),
                'description': prompt_data.get('description', f'{key} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'),
                'content': prompt_data.get('content', ''),
                'length': len(prompt_data.get('content', ''))
            }
        
        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not prompts_data:
            prompts_data = {
                'main_extraction_prompt': {
                    'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                    'description': 'MMS ë©”ì‹œì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ',
                    'content': 'ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                    'length': 0
                }
            }
        
        extraction_prompts = {
            'success': True,
            'prompts': prompts_data,
            'settings': {
                'llm_model': getattr(args, 'llm_model', 'unknown'),
                'offer_data_source': getattr(args, 'offer_data_source', getattr(args, 'offer_info_data_src', 'unknown')),
                'product_info_extraction_mode': getattr(args, 'product_info_extraction_mode', 'unknown'),
                'entity_matching_mode': getattr(args, 'entity_matching_mode', getattr(args, 'entity_extraction_mode', 'unknown')),
                'extract_entity_dag': getattr(args, 'extract_entity_dag', False)
            }
        }
        
        # ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDB í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        extraction_result = {
            'success': not bool(result.get('error')),
            'result': result.get('extracted_result', result.get('result', {})),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }

        raw_result_data = {
            'success': not bool(result.get('error')),
            'result': result.get('raw_result', {}),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }
        
        # MongoDBì— ì €ì¥
        user_id = getattr(args, 'user_id', 'DEFAULT_USER')
        saved_id = save_to_mongodb(message, extraction_result, raw_result_data, extraction_prompts, 
                                 user_id=user_id, message_id=None)
        
        if saved_id:
            print(f"ğŸ“„ ê²°ê³¼ê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {saved_id[:8]}...)")
            return saved_id
        else:
            print("âš ï¸ MongoDB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
            
    except ImportError:
        print("âŒ MongoDB ì €ì¥ì´ ìš”ì²­ë˜ì—ˆì§€ë§Œ mongodb_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

            
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ì¶”ì¶œê¸° ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --extract-entity-dag
    
    # ë°°ì¹˜ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ìŠ¤)
    python mms_extractor.py --batch-file messages.txt --max-workers 4 --extract-entity-dag
    
    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œë¡œ ë°°ì¹˜ ì²˜ë¦¬
    python mms_extractor.py --batch-file messages.txt --offer-data-source db --max-workers 8
    
    # MongoDBì— ê²°ê³¼ ì €ì¥
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --save-to-mongodb --extract-entity-dag
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° - ê°œì„ ëœ ë²„ì „')
    parser.add_argument('--message', type=str, help='í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€')
    parser.add_argument('--batch-file', type=str, help='ë°°ì¹˜ ì²˜ë¦¬í•  ë©”ì‹œì§€ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)')
    parser.add_argument('--max-workers', type=int, help='ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)')
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='local',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm', 'rag'], default='nlp',
                       help='ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ (nlp: í˜•íƒœì†Œë¶„ì„, llm: LLM ê¸°ë°˜, rag: ê²€ìƒ‰ì¦ê°•ìƒì„±)')
    parser.add_argument('--entity-matching-mode', choices=['logic', 'llm'], default='llm',
                       help='ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œ (logic: ë¡œì§ ê¸°ë°˜, llm: LLM ê¸°ë°˜)')
    parser.add_argument('--llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='ax',
                       help='ì‚¬ìš©í•  LLM ëª¨ë¸ (gem: Gemma, ax: ax, cld: Claude, gen: Gemini, gpt: GPT)')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                       help='ë¡œê·¸ ë ˆë²¨ ì„¤ì •')
    parser.add_argument('--extract-entity-dag', action='store_true', default=False, help='Entity DAG extraction (default: False)')
    parser.add_argument('--save-to-mongodb', action='store_true', default=False, 
                       help='ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥ (mongodb_utils.py í•„ìš”)')
    parser.add_argument('--test-mongodb', action='store_true', default=False,
                       help='MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì¢…ë£Œ')

    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.test_mongodb:
        try:
            from mongodb_utils import test_mongodb_connection
        except ImportError:
            print("âŒ MongoDB ìœ í‹¸ë¦¬í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("mongodb_utils.py íŒŒì¼ê³¼ pymongo íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
        
        print("ğŸ”Œ MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        if test_mongodb_connection():
            print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
            exit(0)
        else:
            print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨!")
            print("MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
    
    try:
                # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        logger.info("MMS ì¶”ì¶œê¸° ì´ˆê¸°í™” ì¤‘...")
        extractor = MMSExtractor(
            offer_info_data_src=args.offer_data_source,
            product_info_extraction_mode=args.product_info_extraction_mode,
            entity_extraction_mode=args.entity_matching_mode,
            llm_model=args.llm_model,
            extract_entity_dag=args.extract_entity_dag
        )
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
        if args.batch_file:
            # ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ë“¤ ë¡œë“œ
            logger.info(f"ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ ë¡œë“œ: {args.batch_file}")
            try:
                with open(args.batch_file, 'r', encoding='utf-8') as f:
                    messages = [line.strip() for line in f if line.strip()]
                
                logger.info(f"ë¡œë“œëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = process_messages_batch(
                    extractor, 
                    messages, 
                    extract_dag=args.extract_entity_dag,
                    max_workers=args.max_workers
                )
                
                # MongoDB ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                if args.save_to_mongodb:
                    print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                    args.processing_mode = 'batch'
                    saved_count = 0
                    for i, result in enumerate(results):
                        if i < len(messages):  # ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                            saved_id = save_result_to_mongodb_if_enabled(messages[i], result, args, extractor)
                            if saved_id:
                                saved_count += 1
                    print(f"ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(results)}ê°œ")
                
                # ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥
                print("\n" + "="*50)
                print("ğŸ¯ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼")
                print("="*50)
                
                for i, result in enumerate(results):
                    print(f"\n--- ë©”ì‹œì§€ {i+1} ---")
                    print(f"ì œëª©: {result.get('title', 'N/A')}")
                    print(f"ìƒí’ˆ: {len(result.get('product', []))}ê°œ")
                    if result.get('error'):
                        print(f"ì˜¤ë¥˜: {result['error']}")
                
                # ì „ì²´ ë°°ì¹˜ í†µê³„
                successful = len([r for r in results if not r.get('error')])
                failed = len(results) - successful
                print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í†µê³„")
                print(f"âœ… ì„±ê³µ: {successful}ê°œ")
                print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {(successful/len(results)*100):.1f}%")
                
                # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
                output_file = f"batch_results_{int(time.time())}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, ensure_ascii=False)
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
                
            except FileNotFoundError:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch_file}")
                exit(1)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                exit(1)
        
        else:
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
            test_message = args.message if args.message else """
[Webë°œì‹ ]
(ê´‘ê³ )[SKT (ì„ì§€ë¡œì )] ì‹ ìš©ìš± ë‹¨ê³¨ê³ ê°ë‹˜
9ì›”ì€ SKT ì§ì˜ì ì—ì„œ í˜œíƒë°›9, êµ¬ë§¤í•˜9

ã€ê°¤ëŸ­ì‹œ ë§ˆì§€ë§‰ íŠ¹ê°€ã€‘
â‘  ì™€ì´ë“œ8 â–¶ê¸°ê¸°ê°’ 5ë§Œì›
â‘¡ A36 â–¶ê¸°ê¸°ê°’ 10ë§Œì›
â‘¢ S24 FE â–¶ê¸°ê¸°ê°’ 20ë§Œì›
â˜ ì œíœ´ì¹´ë“œ ì‚¬ìš© ì‹œ ìµœëŒ€ 72ë§Œì› ì¶”ê°€í• ì¸

ã€SKë¡œ í†µì‹ ì‚¬ ì´ë™ ì‹œã€‘
â‘  ì“°ë˜ í° ê·¸ëŒ€ë¡œ ì´ë™ì‹œ ìƒí’ˆê¶Œ 20ë§Œì›
â‘¡ ì¸í„°ë„·+TV ê°€ì… ìµœëŒ€ 70ë§Œì›

â˜…9/9 ê¹Œì§€ ì„ ì°©ìˆœ í–‰ì‚¬ (ì¡°ê±´ì— ë”°ë¼ í• ì¸ê¸ˆì•¡ ìƒì´)

â™¥ì•„ì´í°17 ì‚¬ì „ì˜ˆì•½â™¥
ê³ ìš©ëŸ‰ ì „ìƒ‰ìƒ ë°”ë¡œ ê°œí†µê°€ëŠ¥â˜ https://naver.me/FTM8rdfj

â˜ ì„ì§€ë¡œì…êµ¬ì—­ 5ë²ˆì¶œêµ¬ í•˜ë‚˜ì€í–‰ ëª…ë™ì‚¬ì˜¥ ë§ì€í¸
https://naver.me/GipIR3Lg
â˜ 0507-1399-6011

(ë¬´ë£ŒARS)ìˆ˜ì‹ ê±°ë¶€ ë° ë‹¨ê³¨í•´ì§€ : 
080-801-0011            
"""
            
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
            logger.info("ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘ (ë©€í‹°ìŠ¤ë ˆë“œ)")
            result = process_message_with_dag(extractor, test_message, args.extract_entity_dag)
                    
            # MongoDB ì €ì¥ (ë‹¨ì¼ ë©”ì‹œì§€)
            if args.save_to_mongodb:
                print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                args.processing_mode = 'single'
                saved_id = save_result_to_mongodb_if_enabled(test_message, result, args, extractor)
                if saved_id:
                    print("ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ!")

            
            extracted_result = result.get('extracted_result', {})
        
            print("\n" + "="*50)
            print("ğŸ¯ ìµœì¢… ì¶”ì¶œëœ ì •ë³´")
            print("="*50)
            print(json.dumps(extracted_result, indent=4, ensure_ascii=False))

            # ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ì¶œë ¥
            print("\n" + "="*50)
            print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ")
            print("="*50)
            print(f"âœ… ì œëª©: {extracted_result.get('title', 'N/A')}")
            print(f"âœ… ëª©ì : {len(extracted_result.get('purpose', []))}ê°œ")
            print(f"âœ… ìƒí’ˆ: {len(extracted_result.get('product', []))}ê°œ")
            print(f"âœ… ì±„ë„: {len(extracted_result.get('channel', []))}ê°œ")
            print(f"âœ… í”„ë¡œê·¸ë¨: {len(extracted_result.get('pgm', []))}ê°œ")
            if extracted_result.get('error'):
                print(f"âŒ ì˜¤ë¥˜: {extracted_result['error']}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        exit(1)


if __name__ == '__main__':
    main()