# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor) - AI ê¸°ë°˜ ê´‘ê³  í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ
================================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ”
AI ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. LLM(Large Language Model)ì„ í™œìš©í•˜ì—¬ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ
ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì , ì—”í‹°í‹° ê´€ê³„ ë“±ì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
-----------
1. **ì—”í‹°í‹° ì¶”ì¶œ**: ìƒí’ˆëª…, ë¸Œëœë“œëª…, ì„œë¹„ìŠ¤ëª… ë“± í•µì‹¬ ì—”í‹°í‹° ì‹ë³„
2. **ì±„ë„ ë¶„ì„**: URL, ì „í™”ë²ˆí˜¸, ì•± ë§í¬ ë“± ê³ ê° ì ‘ì  ì±„ë„ ì¶”ì¶œ
3. **ëª©ì  ë¶„ë¥˜**: ê´‘ê³ ì˜ ì£¼ìš” ëª©ì  ë° ì•¡ì…˜ íƒ€ì… ë¶„ì„
4. **í”„ë¡œê·¸ë¨ ë§¤ì¹­**: ì‚¬ì „ ì •ì˜ëœ í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”

ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­
--------------
- **ëª¨ë“ˆí™” ì„¤ê³„**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì™¸ë¶€ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ ìš©ì´ì„± ì¦ëŒ€
- **ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”**: LLM í˜¸ì¶œ ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ë¡œê¹… ë° ì‹¤í–‰ ì‹œê°„ ì¶”ì ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” ì§€ì›
- **ë°ì´í„° ê²€ì¦**: ì¶”ì¶œ ê²°ê³¼ì˜ í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ
- **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©

ğŸ—ï¸ ì•„í‚¤í…ì²˜
-----------
- **MMSExtractor**: ë©”ì¸ ì¶”ì¶œ ì—”ì§„ í´ë˜ìŠ¤
- **DataManager**: ë°ì´í„° ë¡œë”© ë° ê´€ë¦¬ ë‹´ë‹¹
- **LLMProcessor**: LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
- **EntityMatcher**: ì—”í‹°í‹° ë§¤ì¹­ ë° ìœ ì‚¬ë„ ê³„ì‚°
- **PromptModule**: ì™¸ë¶€í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

âš™ï¸ ì„¤ì • ë° í™˜ê²½
--------------
- Python 3.8+
- LangChain, OpenAI, Anthropic API ì§€ì›
- Oracle Database ì—°ë™ (ì„ íƒì‚¬í•­)
- GPU ê°€ì† (CUDA ì§€ì› ì‹œ)

ğŸ“Š ì„±ëŠ¥ ì§€í‘œ
-----------
- í‰ê·  ì²˜ë¦¬ ì‹œê°„: ~30ì´ˆ/ë©”ì‹œì§€
- ì •í™•ë„: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
- ì²˜ë¦¬ëŸ‰: ~120 ë©”ì‹œì§€/ì‹œê°„ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

ì‘ì„±ì: MMS ë¶„ì„íŒ€
ìµœì¢… ìˆ˜ì •: 2024-09
ë²„ì „: 2.0.0
"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
import warnings
from functools import wraps
from typing import List, Tuple, Union, Dict, Any, Optional
from abc import ABC, abstractmethod
import traceback
import json
import re
import ast
import glob
import os
import copy
import pandas as pd
import numpy as np
from langchain.prompts import PromptTemplate

# joblibê³¼ multiprocessing ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="joblib")
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")
warnings.filterwarnings("ignore", message=".*resource_tracker.*")
warnings.filterwarnings("ignore", message=".*leaked.*")
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle
from contextlib import contextmanager

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from rapidfuzz import fuzz, process
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
from entity_dag_extractor import DAGParser, extract_dag

# í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from prompts import (
    build_extraction_prompt,
    enhance_prompt_for_retry,
    get_fallback_result,
    build_entity_extraction_prompt,
    DEFAULT_ENTITY_EXTRACTION_PROMPT,
    DETAILED_ENTITY_EXTRACTION_PROMPT,
    SIMPLE_ENTITY_EXTRACTION_PROMPT,
    HYBRID_DAG_EXTRACTION_PROMPT
    )

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸
from utils import (
    select_most_comprehensive,
    log_performance,
    safe_execute,
    validate_text_input,
    safe_check_empty,
    dataframe_to_markdown_prompt,
    extract_json_objects,
    preprocess_text,
    fuzzy_similarities,
    get_fuzzy_similarities,
    parallel_fuzzy_similarity,
    longest_common_subsequence_ratio,
    sequence_matcher_similarity,
    substring_aware_similarity,
    token_sequence_similarity,
    combined_sequence_similarity,
    calculate_seq_similarity,
    parallel_seq_similarity,
    load_sentence_transformer,
    Token,
    Sentence,
    filter_text_by_exc_patterns,
    filter_specific_terms,
    convert_df_to_json_list,
    create_dag_diagram,
    sha256_hash,
    replace_special_chars_with_space,
    extract_ngram_candidates
)

# Mixin í´ë˜ìŠ¤ ì„í¬íŠ¸
from mms_extractor_data import MMSExtractorDataMixin
from mms_extractor_entity import MMSExtractorEntityMixin

# ì„¤ì • ë° ì˜ì¡´ì„± ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë¡œê¹… ì„¤ì • - api.pyì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê¸°ë³¸ ì„¤ì • ì ìš©
logger = logging.getLogger(__name__)

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë¡œê¹… ì„¤ì • (api.pyì—ì„œ ì„í¬íŠ¸ë  ë•ŒëŠ” api.pyì˜ ì„¤ì • ì‚¬ìš©)
if __name__ == '__main__':
    import sys
    from pathlib import Path
    
    # MongoDB ìœ í‹¸ë¦¬í‹°ëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'mms_extractor.log'),
            logging.StreamHandler()
        ]
    )

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ê°œì„ ëœ MMSExtractor í´ë˜ìŠ¤ =====


class MMSExtractor(MMSExtractorDataMixin, MMSExtractorEntityMixin):
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ AI ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì¶”ì¶œ ì—”ì§„
    ================================================================
    
    ğŸ¨ ê°œìš”
    -------
    ì´ í´ë˜ìŠ¤ëŠ” MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.
    LLM(Large Language Model), ì„ë² ë”© ëª¨ë¸, NLP ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬
    ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ—ï¸ ì•„í‚¤í…ì²˜
    -----------
    ì´ í´ë˜ìŠ¤ëŠ” Mixin íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ëŠ¥ë³„ë¡œ ëª¨ë“ˆí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
    - **MMSExtractorDataMixin**: ë°ì´í„° ë¡œë”© ë° ì´ˆê¸°í™” ê¸°ëŠ¥
    - **MMSExtractorEntityMixin**: ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ ê¸°ëŠ¥
    - **MMSExtractor**: í•µì‹¬ ì¶”ì¶œ ë¡œì§ ë° í†µí•©
    
    ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
    -----------
    1. **ë‹¤ë‹¨ê³„ ì—”í‹°í‹° ì¶”ì¶œ**: Kiwi NLP + ì„ë² ë”© ìœ ì‚¬ë„ + LLM ê¸°ë°˜ ì¶”ì¶œ
    2. **ì§€ëŠ¥í˜• í”„ë¡œê·¸ë¨ ë¶„ë¥˜**: ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ë§¤ì¹­
    3. **RAG ê¸°ë°˜ ì»¬í…ìŠ¤íŠ¸ ì¦ê°•**: ê´€ë ¨ ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•ë„ í–¥ìƒ
    4. **ë‹¤ì¤‘ LLM ì§€ì›**: OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
    5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    
    ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•
    -----------
    - **ì •í™•ë„**: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
    - **ì²˜ë¦¬ ì†ë„**: í‰ê·  30ì´ˆ/ë©”ì‹œì§€
    - **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ìš©ì´
    - **ì•ˆì •ì„±**: ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    
    âš™ï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
    --------------
    - **ì•„í‚¤í…ì²˜ ëª¨ë“ˆí™”**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
    - **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
    - **ë‹¤ì¸µ ì˜ˆì™¸ ì²˜ë¦¬**: LLM API ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
    - **ìƒì„¸ ë¡œê¹…**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…, ê°ì‚¬ ë¡œê·¸ë¥¼ ìœ„í•œ í¬ê´„ì  ë¡œê¹… ì‹œìŠ¤í…œ
    - **ë°ì´í„° ê²€ì¦**: ì…ë ¥/ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ê²€ì¦
    - **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©
    
    ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    -----------
    ```python
    # ê¸°ë³¸ ì´ˆê¸°í™”
    extractor = MMSExtractor(
        llm_model='ax',
        entity_extraction_mode='llm',
        extract_entity_dag=True
    )
    
    # ë©”ì‹œì§€ ì²˜ë¦¬
    result = extractor.process_message("ìƒ˜í”Œ MMS í…ìŠ¤íŠ¸")
    
    # ê²°ê³¼ í™œìš©
    products = result['product']
    channels = result['channel']
    entity_dag = result.get('entity_dag', [])
    ```
    
    ğŸ’¼ ì˜ì¡´ì„±
    ---------
    - LangChain (LLM ì¸í„°í˜ì´ìŠ¤)
    - SentenceTransformers (ì„ë² ë”©)
    - KiwiPiePy (NLP)
    - cx_Oracle (ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™)
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='ax', extract_entity_dag=False):
        """
        MMSExtractor ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        ì‹œìŠ¤í…œì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:
        - LLM ëª¨ë¸ ì„¤ì • ë° ì—°ê²°
        - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        - NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™”
        - ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ (CSV/DB)
        - ê°ì¢… ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
        
        Args:
            model_path (str, optional): ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ. ê¸°ë³¸ê°’: 'jhgan/ko-sroberta-multitask'
            data_dir (str, optional): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ. ê¸°ë³¸ê°’: './data/'
            product_info_extraction_mode (str, optional): ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ ('nlp' ë˜ëŠ” 'llm')
            entity_extraction_mode (str, optional): ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ ('nlp', 'llm', 'hybrid')
            offer_info_data_src (str, optional): ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ('local' ë˜ëŠ” 'db')
            llm_model (str, optional): ì‚¬ìš©í•  LLM ëª¨ë¸. ê¸°ë³¸ê°’: 'ax'
            extract_entity_dag (bool, optional): DAG ì¶”ì¶œ ì—¬ë¶€. ê¸°ë³¸ê°’: False
            
        Raises:
            Exception: ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜¤ë¥˜
            
        Example:
            >>> extractor = MMSExtractor(
            ...     llm_model='gpt-4',
            ...     entity_extraction_mode='hybrid',
            ...     extract_entity_dag=True
            ... )
        """
        logger.info("ğŸš€ MMSExtractor ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
            logger.info("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì ìš© ì¤‘...")
            self._set_default_config(model_path, data_dir, product_info_extraction_mode, 
                                   entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag)
            
            # 2ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±)
            logger.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
            load_dotenv()
            
            # 3ë‹¨ê³„: ì£¼ìš” êµ¬ì„± ìš”ì†Œë“¤ ìˆœì°¨ ì´ˆê¸°í™”
            logger.info("ğŸ’» ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...")
            self._initialize_device()
            
            logger.info("ğŸ¤– LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self._initialize_llm()
            
            logger.info("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self._initialize_embedding_model()
            
            logger.info("ğŸ“ NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™” ì¤‘...")
            self._initialize_kiwi()
            
            logger.info("ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
            self._load_data()
            
            logger.info("âœ… MMSExtractor ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ MMSExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise

    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ë¹„í™œì„±í™” ì˜µì…˜ í™•ì¸
        if MODEL_CONFIG.disable_embedding:
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¹„í™œì„±í™” ëª¨ë“œ (DISABLE_EMBEDDING=true)")
            self.emb_model = None
            return
            
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            try:
                self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)
            except Exception as e2:
                logger.error(f"Fallback ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                logger.warning("ì„ë² ë”© ëª¨ë¸ ì—†ì´ ë™ì‘ ëª¨ë“œë¡œ ì „í™˜")
                self.emb_model = None

    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        except cx_Oracle.DatabaseError as db_error:
            error_obj, = db_error.args
            logger.error(f"Oracle ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜:")
            logger.error(f"  - ì˜¤ë¥˜ ì½”ë“œ: {error_obj.code}")
            logger.error(f"  - ì˜¤ë¥˜ ë©”ì‹œì§€: {error_obj.message}")
            logger.error(f"  - ì „ì²´ ì˜¤ë¥˜: {db_error}")
            raise
        except ImportError as import_error:
            logger.error(f"cx_Oracle ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {import_error}")
            logger.error("ì½”ë§¨ë“œ: pip install cx_Oracle")
            raise
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise


    def _store_prompt_for_preview(self, prompt: str, prompt_type: str):
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ìš©ìœ¼ë¡œ ì €ì¥"""
        import threading
        current_thread = threading.current_thread()
        
        if not hasattr(current_thread, 'stored_prompts'):
            current_thread.stored_prompts = {}
        
        # í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ ì œëª©ê³¼ ì„¤ëª… ë§¤í•‘
        prompt_info = {
            "main_extraction": {
                'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                'description': 'ê´‘ê³  ë©”ì‹œì§€ì—ì„œ ì œëª©, ëª©ì , ìƒí’ˆ, ì±„ë„, í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            },
            "entity_extraction": {
                'title': 'ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸', 
                'description': 'ë©”ì‹œì§€ì—ì„œ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            }
        }
        
        info = prompt_info.get(prompt_type, {
            'title': f'{prompt_type} í”„ë¡¬í”„íŠ¸',
            'description': f'{prompt_type} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'
        })
        
        prompt_key = f'{prompt_type}_prompt'
        prompt_data = {
            'title': info['title'],
            'description': info['description'],
            'content': prompt,
            'length': len(prompt)
        }
        
        current_thread.stored_prompts[prompt_key] = prompt_data
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        prompt_length = len(prompt)
        logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì €ì¥ë¨: {prompt_key}")
        logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length:,} ë¬¸ì")
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë§¤ìš° ê¸´ ê²½ìš° ê²½ê³ 
        if prompt_length > 20000:
            logger.warning(f"âš ï¸ ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸ê°€ ì €ì¥ë¨: {prompt_length:,} ë¬¸ì")
            logger.warning("ì´ëŠ” UI í‘œì‹œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë¶„ì„ (ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ì¸ ê²½ìš°)
            if 'entity' in prompt_key.lower():
                entity_section_start = prompt.find("## Candidate entities:")
                if entity_section_start > 0:
                    entity_section = prompt[entity_section_start:]
                    entity_lines = entity_section.split('\n')
                    entity_count = len([line for line in entity_lines if line.strip().startswith('-')])
                    logger.warning(f"ğŸ” í›„ë³´ ì—”í‹°í‹° ê°œìˆ˜: {entity_count}ê°œ")
        
        logger.info(f"ğŸ“ í˜„ì¬ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(current_thread.stored_prompts)}")
        logger.info(f"ğŸ“ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í‚¤ë“¤: {list(current_thread.stored_prompts.keys())}")

    def _safe_llm_invoke(self, prompt: str, max_retries: int = 3) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì†Œë“œ"""
        for attempt in range(max_retries):
            try:
                # LLM í˜¸ì¶œ
                response = self.llm_model.invoke(prompt)
                result_text = response.content if hasattr(response, 'content') else str(response)
                
                # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
                json_objects_list = extract_json_objects(result_text)
                if json_objects_list:
                    json_objects = json_objects_list[-1]
                    if self._detect_schema_response(json_objects):
                        logger.warning(f"ì‹œë„ {attempt + 1}: LLMì´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        
                        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µì¸ ê²½ìš° ë” ê°•í•œ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            enhanced_prompt = self._enhance_prompt_for_retry(prompt)
                            response = self.llm_model.invoke(enhanced_prompt)
                            result_text = response.content if hasattr(response, 'content') else str(response)
                
                return result_text
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_extraction(prompt)
                else:
                    logger.warning(f"LLM í˜¸ì¶œ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return ""

    def _enhance_prompt_for_retry(self, original_prompt: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”"""
        return enhance_prompt_for_retry(original_prompt)

    def _fallback_extraction(self, prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ fallback ì¶”ì¶œ ë¡œì§"""
        logger.info("Fallback ì¶”ì¶œ ë¡œì§ ì‹¤í–‰")
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì—ì„œ fallback ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        fallback_result = get_fallback_result()
        
        return json.dumps(fallback_result, ensure_ascii=False)


    def _extract_entities(self, mms_msg: str) -> Tuple[List[str], List[str], pd.DataFrame]:
        """ì—”í‹°í‹° ì¶”ì¶œ (Kiwi ë˜ëŠ” LLM ë°©ì‹)"""
        try:
            if self.entity_extraction_mode == 'logic':
                # Kiwi ê¸°ë°˜ ì¶”ì¶œ
                return self.extract_entities_from_kiwi(mms_msg)
            else:
                # LLM ê¸°ë°˜ ì¶”ì¶œì„ ìœ„í•´ ë¨¼ì € Kiwië¡œ ê¸°ë³¸ ì¶”ì¶œ
                entities_from_kiwi, cand_item_list, extra_item_pdf = self.extract_entities_from_kiwi(mms_msg)
                return entities_from_kiwi, cand_item_list, extra_item_pdf
                
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return [], [], pd.DataFrame()

    def _classify_programs(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _build_extraction_prompt(self, msg: str, rag_context: str, product_element: Optional[List[Dict]]) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì‚¬ìš©"""
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©
        prompt = build_extraction_prompt(
            message=msg,
            rag_context=rag_context,
            product_element=product_element,
            product_info_extraction_mode=self.product_info_extraction_mode
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (LLM ëª¨ë“œì—ì„œë§Œ)
        if self.product_info_extraction_mode == 'llm':
            logger.debug(f"LLM ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í›„ë³´ ìƒí’ˆ ëª©ë¡ í¬í•¨ ì—¬ë¶€: {'ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡' in rag_context}")
            
        return prompt

    def _extract_channels(self, json_objects: Dict, msg: str, offer_object: Dict) -> tuple[List[Dict], Dict]:
        """ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¹­ (offer_objectë„ í•¨ê»˜ ë°˜í™˜)"""
        try:
            channel_tag = []
            channel_items = json_objects.get('channel', [])
            if isinstance(channel_items, dict):
                channel_items = channel_items.get('items', [])

            for d in channel_items:
                if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                    # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                    store_info = self._match_store_info(d['value'])
                    d['store_info'] = store_info
                    
                    # offer_objectë¥¼ org íƒ€ì…ìœ¼ë¡œ ë³€ê²½
                    if store_info:
                        offer_object['type'] = 'org'
                        org_tmp = [
                            {
                                'item_nm': o['org_nm'], 
                                'item_id': o['org_cd'], 
                                'item_name_in_msg': d['value'], 
                                'expected_action': ['ë°©ë¬¸']
                            } 
                            for o in store_info
                        ]
                        offer_object['value'] = org_tmp
                else:
                    d['store_info'] = []
                channel_tag.append(d)

            return channel_tag, offer_object
            
        except Exception as e:
            logger.error(f"ì±„ë„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [], offer_object

    def _match_store_info(self, store_name: str) -> List[Dict]:
        """ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­"""
        try:
            # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
            org_pdf_cand = safe_execute(
                parallel_fuzzy_similarity,
                [preprocess_text(store_name.lower())],
                self.org_pdf['item_nm'].unique(),
                threshold=0.5,
                text_col_nm='org_nm_in_msg',
                item_col_nm='item_nm',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )

            if org_pdf_cand.empty:
                return []

            org_pdf_cand = org_pdf_cand.drop('org_nm_in_msg', axis=1)
            org_pdf_cand = self.org_pdf.merge(org_pdf_cand, on=['item_nm'])
            org_pdf_cand['sim'] = org_pdf_cand.apply(
                lambda x: combined_sequence_similarity(store_name, x['item_nm'])[0], axis=1
            ).round(5)
            
            # ëŒ€ë¦¬ì  ì½”ë“œ('D'ë¡œ ì‹œì‘) ìš°ì„  ê²€ìƒ‰
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold_for_store', 0.2)
            org_pdf_tmp = org_pdf_cand.query(
                "sim >= @similarity_threshold", engine='python'
            ).sort_values('sim', ascending=False)
            
            if org_pdf_tmp.empty:
                # ëŒ€ë¦¬ì ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰
                org_pdf_tmp = org_pdf_cand.query("sim >= @similarity_threshold").sort_values('sim', ascending=False)
            
            if not org_pdf_tmp.empty:
                # ìµœê³  ìˆœìœ„ ì¡°ì§ë“¤ì˜ ì •ë³´ ì¶”ì¶œ
                org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense', ascending=False)
                org_pdf_tmp = org_pdf_tmp.rename(columns={'item_id':'org_cd','item_nm':'org_nm'})
                org_info = org_pdf_tmp.query("rank == 1").groupby('org_nm')['org_cd'].apply(list).reset_index(name='org_cd').to_dict('records')
                return org_info
            else:
                return []
                
        except Exception as e:
            logger.error(f"ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []

    def _validate_extraction_result(self, result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'purpose', 'sales_script', 'product', 'channel', 'offer']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    if field == 'title':
                        result[field] = "ê´‘ê³  ë©”ì‹œì§€"
                    elif field == 'sales_script':
                        result[field] = ""
                    elif field == 'offer':
                        result[field] = {"type": "product", "value": []}
                    else:
                        result[field] = []

            # ì±„ë„ ì •ë³´ ê²€ì¦
            validated_channels = []
            for channel in result.get('channel', []):
                if isinstance(channel, dict) and channel.get('value'):
                    validated_channels.append(channel)
            
            result['channel'] = validated_channels
            
            # offer ì •ë³´ ê²€ì¦
            if not isinstance(result.get('offer'), dict):
                logger.warning("offer í•„ë“œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
                result['offer'] = {"type": "product", "value": []}
            elif 'type' not in result['offer'] or 'value' not in result['offer']:
                logger.warning("offer í•„ë“œì— type ë˜ëŠ” valueê°€ ì—†ìŒ, ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
                result['offer'] = {"type": "product", "value": result.get('product', [])}

            return result
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return result

    @log_performance
    def process_message(self, mms_msg: str) -> Dict[str, Any]:
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜)
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ MMS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë©”ì‹œì§€ ë‚´ìš©: {mms_msg[:200]}...")
            logger.info(f"ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # í˜„ì¬ ì„¤ì • ìƒíƒœ ë¡œê¹…
            logger.info("=== í˜„ì¬ ì¶”ì¶œê¸° ì„¤ì • ===")
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.offer_info_data_src}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: {self.product_info_extraction_mode}")
            logger.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ: {self.entity_extraction_mode}")
            logger.info(f"LLM ëª¨ë¸: {self.llm_model_name}")
            logger.info(f"ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì…ë ¥ ê²€ì¦
            msg = validate_text_input(mms_msg)
            
            # 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("=" * 30 + " 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ " + "=" * 30)
            
            # DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨ ì‹œì‘")
                logger.info(f"ìƒí’ˆ ë°ì´í„° ìƒíƒœ: {self.item_pdf_all.shape}")
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                required_columns = ['item_nm', 'item_id', 'item_nm_alias']
                missing_columns = [col for col in required_columns if col not in self.item_pdf_all.columns]
                if missing_columns:
                    logger.error(f"ğŸš¨ DB ëª¨ë“œì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                
                # ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    null_aliases = self.item_pdf_all['item_nm_alias'].isnull().sum()
                    total_aliases = len(self.item_pdf_all)
                    logger.info(f"DB ëª¨ë“œ ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ: {total_aliases - null_aliases}/{total_aliases} ìœ íš¨")
            
            entities_from_kiwi, cand_item_list, extra_item_pdf = self._extract_entities(msg)
            logger.info(f"ì¶”ì¶œëœ Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")
            logger.info(f"ì¶”ì¶œëœ í›„ë³´ ì—”í‹°í‹°: {cand_item_list}")
            logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
            
            # DB ëª¨ë“œì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ íŠ¹ë³„ ë¶„ì„
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
                # cand_item_listê°€ numpy ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•œ ê²€ì‚¬ ì‚¬ìš©
                if safe_check_empty(cand_item_list):
                    logger.error("ğŸš¨ DB ëª¨ë“œì—ì„œ í›„ë³´ ì—”í‹°í‹°ê°€ ì „í˜€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
                    logger.error("1. ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ìƒí’ˆì´ ì—†ìŒ")
                    logger.error("2. ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨")
                    logger.error("3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ")
                    logger.error("4. Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨")
            
            # 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜
            logger.info("=" * 30 + " 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ " + "=" * 30)
            pgm_info = self._classify_programs(msg)
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ê²°ê³¼ í‚¤: {list(pgm_info.keys())}")
            
            # 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            logger.info("=" * 30 + " 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± " + "=" * 30)
            rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_info['pgm_cand_info']}" if self.num_cand_pgms > 0 else ""
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ (ëª¨ë“œë³„ ì²˜ë¦¬)
            logger.info("=" * 30 + " 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ " + "=" * 30)
            product_element = None
            
            # cand_item_listê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ ì•ˆì „í•˜ê²Œ ê²€ì‚¬
            if not safe_check_empty(cand_item_list):
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°: {len(cand_item_list)}ê°œ")
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                # extra_item_pdf ìƒíƒœ í™•ì¸
                logger.info(f"extra_item_pdf í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    logger.info(f"extra_item_pdf ì»¬ëŸ¼ë“¤: {list(extra_item_pdf.columns)}")
                    logger.info(f"extra_item_pdf ìƒ˜í”Œ: {extra_item_pdf.head(2).to_dict('records')}")
                
                if self.product_info_extraction_mode == 'rag':
                    rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'llm':
                    # LLM ëª¨ë“œì—ë„ í›„ë³´ ëª©ë¡ ì œê³µí•˜ì—¬ ì¼ê´€ì„± í–¥ìƒ
                    rag_context += f"\n\n### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("LLM ëª¨ë“œ: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'nlp':
                    if not extra_item_pdf.empty and 'item_nm' in extra_item_pdf.columns:
                        product_df = extra_item_pdf.rename(columns={'item_nm': 'name'}).query(
                            "not name in @self.stop_item_names"
                        )[['name']]
                        product_df['action'] = 'ê¸°íƒ€'
                        product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
                        logger.info(f"NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ - {len(product_element) if product_element else 0}ê°œ")
                        if product_element:
                            logger.info(f"NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ ìƒ˜í”Œ: {product_element[:2]}")
                    else:
                        logger.warning("NLP ëª¨ë“œ: extra_item_pdfê°€ ë¹„ì–´ìˆê±°ë‚˜ item_nm ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            else:
                logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                logger.warning("ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                logger.warning("1. ìƒí’ˆ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                logger.warning("2. ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨") 
                logger.warning("3. ìœ ì‚¬ë„ ë§¤ì¹­ ì„ê³„ê°’ ë¬¸ì œ")

            # 5ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹¤í–‰
            logger.info("=" * 30 + " 5ë‹¨ê³„: LLM í˜¸ì¶œ " + "=" * 30)
            prompt = self._build_extraction_prompt(msg, rag_context, product_element)
            logger.info(f"êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€: {'í›„ë³´ ìƒí’ˆ' in rag_context}")
            
            # í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…/ë¯¸ë¦¬ë³´ê¸°ìš©)
            self._store_prompt_for_preview(prompt, "main_extraction")
            
            result_json_text = self._safe_llm_invoke(prompt)
            logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(result_json_text)} ë¬¸ì")
            logger.info(f"LLM ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {result_json_text[:500]}...")
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            logger.info("=" * 30 + " 6ë‹¨ê³„: JSON íŒŒì‹± " + "=" * 30)
            json_objects_list = extract_json_objects(result_json_text)
            logger.info(f"ì¶”ì¶œëœ JSON ê°ì²´ ìˆ˜: {len(json_objects_list)}ê°œ")
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ: {result_json_text}")
                return self._create_fallback_result(msg)
            
            json_objects = json_objects_list[-1]
            logger.info(f"íŒŒì‹±ëœ JSON ê°ì²´ í‚¤: {list(json_objects.keys())}")
            logger.info(f"íŒŒì‹±ëœ JSON ë‚´ìš©: {json_objects}")
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ë° ì²˜ë¦¬
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.error("ğŸš¨ LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤! ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                logger.error("ì¬ì‹œë„ ë˜ëŠ” fallback ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._create_fallback_result(msg)

            raw_result = copy.deepcopy(json_objects)
            
            # 7ë‹¨ê³„: ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±
            logger.info("=" * 30 + " 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„± " + "=" * 30)
            final_result = self._build_final_result(json_objects, msg, pgm_info, entities_from_kiwi)
            
            # 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
            logger.info("=" * 30 + " 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ " + "=" * 30)
            final_result = self._validate_extraction_result(final_result)

            # # DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ (ì„ íƒì )
            # # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
            # # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
            # dag_section = ""
            # if self.extract_entity_dag:
            #     logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
            #     try:
            #         dag_start_time = time.time()
            #         # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
            #         extract_dag_result = extract_dag(DAGParser(), msg, self.llm_model)
            #         dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
            #         dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
            #         dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
                    
            #         # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
            #         dag_filename = f'dag_{sha256_hash(msg)}'
            #         create_dag_diagram(dag, filename=dag_filename)
            #         dag_processing_time = time.time() - dag_start_time
                    
            #         logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")
            #         logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
            #         logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
            #         if dag_section:
            #             logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
            #         else:
            #             logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                        
            #     except Exception as e:
            #         logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            #         dag_section = ""

            # # ìµœì¢… ê²°ê³¼ì— DAG ì •ë³´ ì¶”ê°€ (ë¹„ì–´ìˆì„ ìˆ˜ë„ ìˆìŒ)
            # final_result['entity_dag'] = sorted([d for d in dag_section.split('\n') if d!=''])
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
            logger.info("=" * 60)
            logger.info("âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"ì œëª©: {final_result.get('title', 'N/A')}")
            logger.info(f"ëª©ì : {final_result.get('purpose', [])}")
            sales_script = final_result.get('sales_script', '')
            if sales_script:
                logger.info(f"íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script[:100]}..." if len(sales_script) > 100 else f"íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script}")
            logger.info(f"ìƒí’ˆ ìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info(f"ì±„ë„ ìˆ˜: {len(final_result.get('channel', []))}ê°œ")
            logger.info(f"í”„ë¡œê·¸ë¨ ìˆ˜: {len(final_result.get('pgm', []))}ê°œ")
            offer_info = final_result.get('offer', {})
            logger.info(f"ì˜¤í¼ íƒ€ì…: {offer_info.get('type', 'N/A')}")
            logger.info(f"ì˜¤í¼ í•­ëª© ìˆ˜: {len(offer_info.get('value', []))}ê°œ")

            actual_prompts = get_stored_prompts_from_thread()

            return {"extracted_result": final_result, "raw_result": raw_result, "prompts": actual_prompts}
            
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_fallback_result(mms_msg)
    
    @log_performance
    def extract_json_objects_only(self, mms_msg: str) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ì—ì„œ 7ë‹¨ê³„(ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±) ì „ì˜ json_objectsë§Œ ì¶”ì¶œ
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€
            
        Returns:
            Dict: LLMì´ ìƒì„±í•œ json_objects (ì—”í‹°í‹° ë§¤ì¹­ ì „)
        """
        try:
            msg = mms_msg.strip()
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì‹œì‘ - ë©”ì‹œì§€ ê¸¸ì´: {len(msg)}ì")
            
            # 1-4ë‹¨ê³„: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤
            pgm_info = self._prepare_program_classification(msg)
            
            # RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (product_info_extraction_modeê°€ 'rag'ì¸ ê²½ìš°)
            rag_context = ""
            if self.product_info_extraction_mode == 'rag':
                rag_context = self._prepare_rag_context(msg)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
            prompt = self._build_extraction_prompt(msg, pgm_info, rag_context)
            result_json_text = self._safe_llm_invoke(prompt)
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            json_objects_list = extract_json_objects(result_json_text)
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {}
            
            json_objects = json_objects_list[-1]
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.warning("LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return {}
            
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì™„ë£Œ - í‚¤: {list(json_objects.keys())}")
            return json_objects
            
        except Exception as e:
            logger.error(f"JSON ê°ì²´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _prepare_program_classification(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì¤€ë¹„ (_classify_programs ë©”ì†Œë“œì™€ ë™ì¼)"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _detect_schema_response(self, json_objects: Dict) -> bool:
        """LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ ê°ì§€"""
        try:
            # purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            purpose = json_objects.get('purpose', {})
            if isinstance(purpose, dict) and 'type' in purpose and purpose.get('type') == 'array':
                logger.warning("purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸  
            product = json_objects.get('product', {})
            if isinstance(product, dict) and 'type' in product and product.get('type') == 'array':
                logger.warning("product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            channel = json_objects.get('channel', {})
            if isinstance(channel, dict) and 'type' in channel and channel.get('type') == 'array':
                logger.warning("channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def convert_df_to_json_list(self, df: pd.DataFrame) -> List[Dict]:
        """
        DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜
        ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ: item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ëª¨ë“  item_name_in_msgë¥¼ ë°°ì—´ë¡œ ìˆ˜ì§‘
        
        Schema:
        {
            "item_nm": "ìƒí’ˆëª…",
            "item_id": ["ID1", "ID2"],
            "item_name_in_msg": ["ë©”ì‹œì§€ë‚´í‘œí˜„1", "ë©”ì‹œì§€ë‚´í‘œí˜„2"]
        }
        """
        result = []
        # item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
        grouped = df.groupby('item_nm')
        for item_nm, group in grouped:
            # ë©”ì¸ ì•„ì´í…œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            item_name_in_msg_raw = list(group['item_name_in_msg'].unique())
            item_dict = {
                'item_nm': item_nm,
                'item_id': list(group['item_id'].unique()),
                'item_name_in_msg': select_most_comprehensive(item_name_in_msg_raw)
            }
            result.append(item_dict)
        return result

    def _create_fallback_result(self, msg: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "sales_script": "",
            "product": [],
            "channel": [],
            "pgm": [],
            "offer": {"type": "product", "value": []},
            "entity_dag": []
        }

    def _build_final_result(self, json_objects: Dict, msg: str, pgm_info: Dict, entities_from_kiwi: List[str]) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        try:
            logger.info("=" * 80)
            logger.info("ğŸ” [PRODUCT DEBUG] _build_final_result ì‹œì‘")
            logger.info("=" * 80)
            
            final_result = json_objects.copy()
            
            # offer_object ì´ˆê¸°í™”
            offer_object = {}
            
            # ìƒí’ˆ ì •ë³´ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("ğŸ“‹ [STEP 1] product_items ì¶”ì¶œ")
            product_items = json_objects.get('product', [])
            logger.info(f"   - ì›ë³¸ product íƒ€ì…: {type(product_items)}")
            logger.info(f"   - ì›ë³¸ product ë‚´ìš©: {product_items}")
            
            if isinstance(product_items, dict):
                logger.info("   - productê°€ dict íƒ€ì… â†’ 'items' í‚¤ë¡œ ì ‘ê·¼")
                product_items = product_items.get('items', [])
                logger.info(f"   - items ì¶”ì¶œ í›„: {product_items}")
            
            logger.info(f"   âœ… ìµœì¢… product_items ê°œìˆ˜: {len(product_items)}ê°œ")
            logger.info(f"   âœ… ìµœì¢… product_items ë‚´ìš©: {product_items}")

            primary_llm_extracted_entities = [x.get('name', '') for x in product_items]
            logger.info(f"ğŸ“‹ [STEP 2] LLM ì¶”ì¶œ ì—”í‹°í‹°: {primary_llm_extracted_entities}")
            logger.info(f"ğŸ“‹ [STEP 2] Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")
            logger.info(f"ğŸ“‹ [STEP 2] entity_extraction_mode: {self.entity_extraction_mode}")

            # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if self.entity_extraction_mode == 'logic':
                logger.info("ğŸ” [STEP 3] ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ë§¤ì¹­ ì‹œì‘")
                # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
                cand_entities = list(set(entities_from_kiwi+[item.get('name', '') for item in product_items if item.get('name')]))
                logger.info(f"   - cand_entities: {cand_entities}")
                similarities_fuzzy = self.extract_entities_by_logic(cand_entities)
                logger.info(f"   âœ… similarities_fuzzy ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            else:
                logger.info("ğŸ” [STEP 3] LLM ê¸°ë°˜ ì—”í‹°í‹° ë§¤ì¹­ ì‹œì‘")
                # LLM ê¸°ë°˜: LLMì„ í†µí•œ ì—”í‹°í‹° ì¶”ì¶œ (ê¸°ë³¸ ëª¨ë¸ë“¤: ax=ax, cld=claude)
                default_llm_models = self._initialize_multiple_llm_models(['gen','ax'])
                logger.info(f"   - ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ìˆ˜: {len(default_llm_models)}ê°œ")
                similarities_fuzzy = self.extract_entities_by_llm(msg, llm_models=default_llm_models, external_cand_entities=entities_from_kiwi)
                logger.info(f"   âœ… similarities_fuzzy ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if not similarities_fuzzy.empty:
                logger.info(f"   ğŸ“Š similarities_fuzzy ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                logger.info(f"{similarities_fuzzy.head(3).to_dict('records')}")
            else:
                logger.warning("   âš ï¸ similarities_fuzzyê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

            if not similarities_fuzzy.empty:
                logger.info("ğŸ” [STEP 4] alias_pdf_rawì™€ merge ì‹œì‘")
                logger.info(f"   - alias_pdf_raw í¬ê¸°: {self.alias_pdf_raw.shape}")
                merged_df = similarities_fuzzy.merge(
                    self.alias_pdf_raw[['alias_1','type']].drop_duplicates(), 
                    left_on='item_name_in_msg', 
                    right_on='alias_1', 
                    how='left'
                )
                logger.info(f"   âœ… merged_df í¬ê¸°: {merged_df.shape if not merged_df.empty else 'ë¹„ì–´ìˆìŒ'}")
                if not merged_df.empty:
                    logger.info(f"   ğŸ“Š merged_df ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                    logger.info(f"{merged_df.head(3).to_dict('records')}")

                logger.info("ğŸ” [STEP 5] filtered_df ìƒì„± (expansion íƒ€ì… í•„í„°ë§)")
                filtered_df = merged_df[merged_df.apply(
                    lambda x: (
                        replace_special_chars_with_space(x['item_nm_alias']) in replace_special_chars_with_space(x['item_name_in_msg']) or 
                        replace_special_chars_with_space(x['item_name_in_msg']) in replace_special_chars_with_space(x['item_nm_alias'])
                    ) if x['type'] != 'expansion' else True, 
                    axis=1
                )]
                logger.info(f"   âœ… filtered_df í¬ê¸°: {filtered_df.shape if not filtered_df.empty else 'ë¹„ì–´ìˆìŒ'}")
                if not filtered_df.empty:
                    logger.info(f"   ğŸ“Š filtered_df ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
                    logger.info(f"{filtered_df.head(3).to_dict('records')}")

                # similarities_fuzzy = filtered_df[similarities_fuzzy.columns]

            # ìƒí’ˆ ì •ë³´ ë§¤í•‘
            logger.info("ğŸ” [STEP 6] ìƒí’ˆ ì •ë³´ ë§¤í•‘ ì‹œì‘")
            logger.info(f"   - similarities_fuzzy.empty: {similarities_fuzzy.empty}")
            
            if not similarities_fuzzy.empty:
                logger.info("   âœ… similarities_fuzzyê°€ ë¹„ì–´ìˆì§€ ì•ŠìŒ â†’ _map_products_with_similarity í˜¸ì¶œ")
                final_result['product'] = self._map_products_with_similarity(similarities_fuzzy, json_objects)
                logger.info(f"   âœ… ìµœì¢… product ê°œìˆ˜: {len(final_result['product'])}ê°œ")
                logger.info(f"   âœ… ìµœì¢… product ë‚´ìš©: {final_result['product']}")
            else:
                logger.warning("   âš ï¸ similarities_fuzzyê°€ ë¹„ì–´ìˆìŒ â†’ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (else ë¸Œëœì¹˜)")
                logger.info(f"   - product_items ê°œìˆ˜: {len(product_items)}ê°œ")
                logger.info(f"   - stop_item_names ê°œìˆ˜: {len(self.stop_item_names)}ê°œ")
                
                # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìƒˆ ìŠ¤í‚¤ë§ˆ + expected_action ë¦¬ìŠ¤íŠ¸)
                filtered_product_items = [
                    d for d in product_items 
                    if d.get('name') and d['name'] not in self.stop_item_names
                ]
                logger.info(f"   - í•„í„°ë§ í›„ product_items ê°œìˆ˜: {len(filtered_product_items)}ê°œ")
                logger.info(f"   - í•„í„°ë§ í›„ product_items: {filtered_product_items}")
                
                final_result['product'] = [
                    {
                        'item_nm': d.get('name', ''), 
                        'item_id': ['#'],
                        'item_name_in_msg': [d.get('name', '')],
                        'expected_action': [d.get('action', 'ê¸°íƒ€')]
                    } 
                    for d in filtered_product_items
                ]
                logger.info(f"   âœ… ìµœì¢… product ê°œìˆ˜: {len(final_result['product'])}ê°œ")
                logger.info(f"   âœ… ìµœì¢… product ë‚´ìš©: {final_result['product']}")

            # offer_objectì— product íƒ€ì…ìœ¼ë¡œ ì„¤ì •
            offer_object['type'] = 'product'
            offer_object['value'] = final_result['product']
            logger.info(f"ğŸ·ï¸  [STEP 7] offer_object ì´ˆê¸°í™”: type=product, value ê°œìˆ˜={len(offer_object['value'])}ê°œ")

            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
            final_result['pgm'] = self._map_program_classification(json_objects, pgm_info)
            
            # ì±„ë„ ì •ë³´ ì²˜ë¦¬ (offer_objectë„ í•¨ê»˜ ì „ë‹¬ ë° ë°˜í™˜)
            logger.info("ğŸ” [STEP 8] ì±„ë„ ì •ë³´ ì²˜ë¦¬ ë° offer_object ì—…ë°ì´íŠ¸")
            final_result['channel'], offer_object = self._extract_channels(json_objects, msg, offer_object)
            logger.info(f"   âœ… ìµœì¢… channel ê°œìˆ˜: {len(final_result['channel'])}ê°œ")
            logger.info(f"   âœ… ìµœì¢… offer_object type: {offer_object.get('type', 'N/A')}")
            logger.info(f"   âœ… ìµœì¢… offer_object value ê°œìˆ˜: {len(offer_object.get('value', []))}ê°œ")
            
            # offer í•„ë“œ ì¶”ê°€
            final_result['offer'] = offer_object
            logger.info(f"âœ… [STEP 9] final_resultì— offer í•„ë“œ ì¶”ê°€ ì™„ë£Œ")
            
            # entity_dag ì´ˆê¸°í™” (ë¹ˆ ë°°ì—´)
            final_result['entity_dag'] = []
            
            logger.info("=" * 80)
            logger.info("âœ… [PRODUCT DEBUG] _build_final_result ì™„ë£Œ")
            logger.info(f"   ìµœì¢… final_result['product'] ê°œìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info("=" * 80)

            return final_result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return json_objects

    def _map_program_classification(self, json_objects: Dict, pgm_info: Dict) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘"""
        try:
            if (self.num_cand_pgms > 0 and 
                'pgm' in json_objects and 
                isinstance(json_objects['pgm'], list) and
                not pgm_info.get('pgm_pdf_tmp', pd.DataFrame()).empty):
                
                pgm_json = pgm_info['pgm_pdf_tmp'][
                    pgm_info['pgm_pdf_tmp']['pgm_nm'].apply(
                        lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm'])
                    )
                ][['pgm_nm', 'pgm_id']].to_dict('records')
                
                return pgm_json
            
            return []
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

def process_message_with_dag(extractor, message: str, extract_dag: bool = False) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ìš©)
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ (í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨)
    """
    try:
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")

        # 1. ë©”ì¸ ì¶”ì¶œ
        result = extractor.process_message(message)
        dag_list = []
        
        if extract_dag:
            # ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë³€ê²½ (í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ë¥¼ ìœ„í•´)
            # ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ë©´ ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œê°€ ë¶„ë¦¬ë˜ì–´ í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ê°€ ì•ˆë¨
            logger.info("ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë©”ì¸ ì¶”ì¶œ ë° DAG ì¶”ì¶œ ìˆ˜í–‰")
            
            # 2. DAG ì¶”ì¶œ
            dag_result = make_entity_dag(message, extractor.llm_model)
            dag_list = sorted([d for d in dag_result['dag_section'].split('\n') if d!=''])

        extracted_result = result.get('extracted_result', {})
        extracted_result['entity_dag'] = dag_list
        result['extracted_result'] = extracted_result

        raw_result = result.get('raw_result', {})
        raw_result['entity_dag'] = dag_list
        result['raw_result'] = raw_result

        result['error'] = ""
        
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "extracted_result": {
                "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                "purpose": ["ì˜¤ë¥˜"],
                "sales_script": "",
                "product": [],
                "channel": [],
                "pgm": [],
                "offer": {"type": "product", "value": []},
                "entity_dag": []
            },
            "raw_result": {},
            "prompts": {},
            "error": str(e)
        }

def process_messages_batch(extractor, messages: List[str], extract_dag: bool = False, max_workers: int = None) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        messages: ì²˜ë¦¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
    
    Returns:
        list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if max_workers is None:
        max_workers = min(len(messages), os.cpu_count())
    
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€, {max_workers}ê°œ ì›Œì»¤")
    
    start_time = time.time()
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë©”ì‹œì§€ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_message = {
            executor.submit(process_message_with_dag, extractor, msg, extract_dag): msg 
            for msg in messages
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
        for i, future in enumerate(future_to_message):
            try:
                result = future.result()
                results.append(result)
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {i+1}/{len(messages)} ({((i+1)/len(messages)*100):.1f}%)")
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                results.append({
                    "extracted_result": {
                        "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                        "purpose": ["ì˜¤ë¥˜"],
                        "sales_script": "",
                        "product": [],
                        "channel": [],
                        "pgm": [],
                        "offer": {"type": "product", "value": []},
                        "entity_dag": []
                    },
                    "raw_result": {},
                    "prompts": {},
                    "error": str(e)
                })
    
    elapsed_time = time.time() - start_time
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ")
    logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(messages):.2f}ì´ˆ/ë©”ì‹œì§€")
    
    return results

def make_entity_dag(msg: str, llm_model, save_dag_image=True):

    # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
    # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
    extract_dag_result = {}
    logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
    try:
        dag_start_time = time.time()
        # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
        extract_dag_result = extract_dag(DAGParser(), msg, llm_model)
        dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
        dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
        dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
        
        # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
        dag_filename = ""
        if save_dag_image:
            dag_filename = f'dag_{sha256_hash(msg)}'
            create_dag_diagram(dag, filename=dag_filename)
            logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")

        extract_dag_result['dag_filename'] = dag_filename
        
        dag_processing_time = time.time() - dag_start_time
        
        logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
        logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
        if dag_section:
            logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
        else:
            logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        dag_section = ""

    return extract_dag_result


def get_stored_prompts_from_thread():
    """í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    import threading
    current_thread = threading.current_thread()
    
    if hasattr(current_thread, 'stored_prompts'):
        return current_thread.stored_prompts
    else:
        return {}

def save_result_to_mongodb_if_enabled(message: str, result: dict, args_or_data, extractor=None):
    """MongoDB ì €ì¥ì´ í™œì„±í™”ëœ ê²½ìš° ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
    
    Args:
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        result: ì²˜ë¦¬ ê²°ê³¼ (extracted_result, raw_result í¬í•¨)
        args_or_data: argparse.Namespace ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
    
    Returns:
        str: ì €ì¥ëœ ë¬¸ì„œ ID, ì‹¤íŒ¨ ì‹œ None
    """
    # args_or_dataê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Namespaceë¡œ ë³€í™˜
    if isinstance(args_or_data, dict):
        import argparse
        args = argparse.Namespace(**args_or_data)
    else:
        args = args_or_data
    
    # save_to_mongodb ì†ì„±ì´ ì—†ê±°ë‚˜ Falseì¸ ê²½ìš°
    if not getattr(args, 'save_to_mongodb', False):
        return None
        
    try:
        # MongoDB ì„í¬íŠ¸ ì‹œë„
        from mongodb_utils import save_to_mongodb
        
        # ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œì—ì„œ í”„ë¡¬í”„íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stored_prompts = result.get('prompts', get_stored_prompts_from_thread()) 
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´ êµ¬ì„±
        prompts_data = {}
        for key, prompt_data in stored_prompts.items():
            prompts_data[key] = {
                'title': prompt_data.get('title', f'{key} í”„ë¡¬í”„íŠ¸'),
                'description': prompt_data.get('description', f'{key} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'),
                'content': prompt_data.get('content', ''),
                'length': len(prompt_data.get('content', ''))
            }
        
        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not prompts_data:
            prompts_data = {
                'main_extraction_prompt': {
                    'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                    'description': 'MMS ë©”ì‹œì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ',
                    'content': 'ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                    'length': 0
                }
            }
        
        extraction_prompts = {
            'success': True,
            'prompts': prompts_data,
            'settings': {
                'llm_model': getattr(args, 'llm_model', 'unknown'),
                'offer_data_source': getattr(args, 'offer_data_source', getattr(args, 'offer_info_data_src', 'unknown')),
                'product_info_extraction_mode': getattr(args, 'product_info_extraction_mode', 'unknown'),
                'entity_matching_mode': getattr(args, 'entity_matching_mode', getattr(args, 'entity_extraction_mode', 'unknown')),
                'extract_entity_dag': getattr(args, 'extract_entity_dag', False)
            }
        }
        
        # ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDB í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        extraction_result = {
            'success': not bool(result.get('error')),
            'result': result.get('extracted_result', result.get('result', {})),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }

        raw_result_data = {
            'success': not bool(result.get('error')),
            'result': result.get('raw_result', {}),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }
        
        # MongoDBì— ì €ì¥
        user_id = getattr(args, 'user_id', 'DEFAULT_USER')
        saved_id = save_to_mongodb(message, extraction_result, raw_result_data, extraction_prompts, 
                                 user_id=user_id, message_id=None)
        
        if saved_id:
            print(f"ğŸ“„ ê²°ê³¼ê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {saved_id[:8]}...)")
            return saved_id
        else:
            print("âš ï¸ MongoDB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
            
    except ImportError:
        print("âŒ MongoDB ì €ì¥ì´ ìš”ì²­ë˜ì—ˆì§€ë§Œ mongodb_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

            
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ì¶”ì¶œê¸° ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --extract-entity-dag
    
    # ë°°ì¹˜ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ìŠ¤)
    python mms_extractor.py --batch-file messages.txt --max-workers 4 --extract-entity-dag
    
    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œë¡œ ë°°ì¹˜ ì²˜ë¦¬
    python mms_extractor.py --batch-file messages.txt --offer-data-source db --max-workers 8
    
    # MongoDBì— ê²°ê³¼ ì €ì¥
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --save-to-mongodb --extract-entity-dag
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° - ê°œì„ ëœ ë²„ì „')
    parser.add_argument('--message', type=str, help='í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€')
    parser.add_argument('--batch-file', type=str, help='ë°°ì¹˜ ì²˜ë¦¬í•  ë©”ì‹œì§€ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)')
    parser.add_argument('--max-workers', type=int, help='ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)')
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='local',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm', 'rag'], default='llm',
                       help='ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ (nlp: í˜•íƒœì†Œë¶„ì„, llm: LLM ê¸°ë°˜, rag: ê²€ìƒ‰ì¦ê°•ìƒì„±)')
    parser.add_argument('--entity-matching-mode', choices=['logic', 'llm'], default='llm',
                       help='ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œ (logic: ë¡œì§ ê¸°ë°˜, llm: LLM ê¸°ë°˜)')
    parser.add_argument('--llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='gen',
                       help='ì‚¬ìš©í•  LLM ëª¨ë¸ (gem: Gemma, ax: ax, cld: Claude, gen: Gemini, gpt: GPT)')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                       help='ë¡œê·¸ ë ˆë²¨ ì„¤ì •')
    parser.add_argument('--extract-entity-dag', action='store_true', default=False, help='Entity DAG extraction (default: False)')
    parser.add_argument('--save-to-mongodb', action='store_true', default=True, 
                       help='ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥ (mongodb_utils.py í•„ìš”)')
    parser.add_argument('--test-mongodb', action='store_true', default=False,
                       help='MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì¢…ë£Œ')

    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.test_mongodb:
        try:
            from mongodb_utils import test_mongodb_connection
        except ImportError:
            print("âŒ MongoDB ìœ í‹¸ë¦¬í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("mongodb_utils.py íŒŒì¼ê³¼ pymongo íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
        
        print("ğŸ”Œ MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        if test_mongodb_connection():
            print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
            exit(0)
        else:
            print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨!")
            print("MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
    
    try:
                # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        logger.info("MMS ì¶”ì¶œê¸° ì´ˆê¸°í™” ì¤‘...")
        extractor = MMSExtractor(
            offer_info_data_src=args.offer_data_source,
            product_info_extraction_mode=args.product_info_extraction_mode,
            entity_extraction_mode=args.entity_matching_mode,
            llm_model=args.llm_model,
            extract_entity_dag=args.extract_entity_dag
        )
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
        if args.batch_file:
            # ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ë“¤ ë¡œë“œ
            logger.info(f"ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ ë¡œë“œ: {args.batch_file}")
            try:
                with open(args.batch_file, 'r', encoding='utf-8') as f:
                    messages = [line.strip() for line in f if line.strip()]
                
                logger.info(f"ë¡œë“œëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = process_messages_batch(
                    extractor, 
                    messages, 
                    extract_dag=args.extract_entity_dag,
                    max_workers=args.max_workers
                )
                
                # MongoDB ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                if args.save_to_mongodb:
                    print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                    args.processing_mode = 'batch'
                    saved_count = 0
                    for i, result in enumerate(results):
                        if i < len(messages):  # ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                            saved_id = save_result_to_mongodb_if_enabled(messages[i], result, args, extractor)
                            if saved_id:
                                saved_count += 1
                    print(f"ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(results)}ê°œ")
                
                # ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥
                print("\n" + "="*50)
                print("ğŸ¯ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼")
                print("="*50)
                
                for i, result in enumerate(results):
                    extracted = result.get('extracted_result', {})
                    print(f"\n--- ë©”ì‹œì§€ {i+1} ---")
                    print(f"ì œëª©: {extracted.get('title', 'N/A')}")
                    sales_script = extracted.get('sales_script', '')
                    if sales_script:
                        print(f"íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script[:80]}..." if len(sales_script) > 80 else f"íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script}")
                    print(f"ìƒí’ˆ: {len(extracted.get('product', []))}ê°œ")
                    print(f"ì±„ë„: {len(extracted.get('channel', []))}ê°œ")
                    print(f"í”„ë¡œê·¸ë¨: {len(extracted.get('pgm', []))}ê°œ")
                    offer_info = extracted.get('offer', {})
                    print(f"ì˜¤í¼ íƒ€ì…: {offer_info.get('type', 'N/A')}")
                    print(f"ì˜¤í¼ í•­ëª©: {len(offer_info.get('value', []))}ê°œ")
                    if result.get('error'):
                        print(f"ì˜¤ë¥˜: {result['error']}")
                
                # ì „ì²´ ë°°ì¹˜ í†µê³„
                successful = len([r for r in results if not r.get('error') and r.get('extracted_result')])
                failed = len(results) - successful
                print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í†µê³„")
                print(f"âœ… ì„±ê³µ: {successful}ê°œ")
                print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {(successful/len(results)*100):.1f}%")
                
                # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
                output_file = f"batch_results_{int(time.time())}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, ensure_ascii=False)
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
                
            except FileNotFoundError:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch_file}")
                exit(1)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                exit(1)
        
        else:
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
            test_message = args.message if args.message else """
  message: '(ê´‘ê³ )[SKT] iPhone ì‹ ì œí’ˆ êµ¬ë§¤ í˜œíƒ ì•ˆë‚´ __#04 ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”._SKí…”ë ˆì½¤ì—ì„œ iPhone ì‹ ì œí’ˆ êµ¬ë§¤í•˜ë©´, ìµœëŒ€ 22ë§Œ ì› ìºì‹œë°± ì´ë²¤íŠ¸ì— ì°¸ì—¬í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.__í˜„ëŒ€ì¹´ë“œë¡œ ì• í”Œ í˜ì´ë„ ë” í¸ë¦¬í•˜ê²Œ ì´ìš©í•´ ë³´ì„¸ìš”.__â–¶ í˜„ëŒ€ì¹´ë“œ ë°”ë¡œ ê°€ê¸°: https://t-mms.kr/ais/#74_ _ì• í”Œ í˜ì´ í‹°ë¨¸ë‹ˆ ì¶©ì „ ì¿ í° 96ë§Œ ì›, ìƒŒí”„ë€ì‹œìŠ¤ì½” ì™•ë³µ í•­ê³µê¶Œ, ì• í”Œ ì•¡ì„¸ì„œë¦¬ íŒ©ê¹Œì§€!_Lucky 1717 ì´ë²¤íŠ¸ ì‘ëª¨í•˜ê³  ê²½í’ˆ ë‹¹ì²¨ì˜ í–‰ìš´ì„ ëˆ„ë ¤ ë³´ì„¸ìš”.__â–¶ ì´ë²¤íŠ¸ ìì„¸íˆ ë³´ê¸°: https://t-mms.kr/aiN/#74_ _â–  ë¬¸ì˜: SKT ê³ ê°ì„¼í„°(1558, ë¬´ë£Œ)__SKTì™€ í•¨ê»˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.__ë¬´ë£Œ ìˆ˜ì‹ ê±°ë¶€ 1504',


"""
            
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
            logger.info("ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘ (ë©€í‹°ìŠ¤ë ˆë“œ)")
            result = process_message_with_dag(extractor, test_message, args.extract_entity_dag)
                    
            # MongoDB ì €ì¥ (ë‹¨ì¼ ë©”ì‹œì§€)
            if args.save_to_mongodb:
                print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                args.processing_mode = 'single'
                saved_id = save_result_to_mongodb_if_enabled(test_message, result, args, extractor)
                if saved_id:
                    print("ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ!")

            
            extracted_result = result.get('extracted_result', {})
        
            print("\n" + "="*50)
            print("ğŸ¯ ìµœì¢… ì¶”ì¶œëœ ì •ë³´")
            print("="*50)
            print(json.dumps(extracted_result, indent=4, ensure_ascii=False))

            # ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ì¶œë ¥
            print("\n" + "="*50)
            print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ")
            print("="*50)
            print(f"âœ… ì œëª©: {extracted_result.get('title', 'N/A')}")
            print(f"âœ… ëª©ì : {len(extracted_result.get('purpose', []))}ê°œ")
            sales_script = extracted_result.get('sales_script', '')
            if sales_script:
                print(f"âœ… íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script[:100]}..." if len(sales_script) > 100 else f"âœ… íŒë§¤ ìŠ¤í¬ë¦½íŠ¸: {sales_script}")
            print(f"âœ… ìƒí’ˆ: {len(extracted_result.get('product', []))}ê°œ")
            print(f"âœ… ì±„ë„: {len(extracted_result.get('channel', []))}ê°œ")
            print(f"âœ… í”„ë¡œê·¸ë¨: {len(extracted_result.get('pgm', []))}ê°œ")
            offer_info = extracted_result.get('offer', {})
            print(f"âœ… ì˜¤í¼ íƒ€ì…: {offer_info.get('type', 'N/A')}")
            print(f"âœ… ì˜¤í¼ í•­ëª©: {len(offer_info.get('value', []))}ê°œ")
            if extracted_result.get('error'):
                print(f"âŒ ì˜¤ë¥˜: {extracted_result['error']}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        exit(1)


if __name__ == '__main__':
    main()
# %%
