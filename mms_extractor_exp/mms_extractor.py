# %%
"""
MMS ì¶”ì¶œê¸° (MMS Extractor) - AI ê¸°ë°˜ ê´‘ê³  í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œìŠ¤í…œ
================================================================

ğŸ“‹ ê°œìš”
-------
ì´ ëª¨ë“ˆì€ MMS(ë©€í‹°ë¯¸ë””ì–´ ë©”ì‹œì§€) ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ”
AI ê¸°ë°˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. LLM(Large Language Model)ì„ í™œìš©í•˜ì—¬ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ
ìƒí’ˆëª…, ì±„ë„ ì •ë³´, ê´‘ê³  ëª©ì , ì—”í‹°í‹° ê´€ê³„ ë“±ì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤.

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
-----------
1. **ì—”í‹°í‹° ì¶”ì¶œ**: ìƒí’ˆëª…, ë¸Œëœë“œëª…, ì„œë¹„ìŠ¤ëª… ë“± í•µì‹¬ ì—”í‹°í‹° ì‹ë³„
2. **ì±„ë„ ë¶„ì„**: URL, ì „í™”ë²ˆí˜¸, ì•± ë§í¬ ë“± ê³ ê° ì ‘ì  ì±„ë„ ì¶”ì¶œ
3. **ëª©ì  ë¶„ë¥˜**: ê´‘ê³ ì˜ ì£¼ìš” ëª©ì  ë° ì•¡ì…˜ íƒ€ì… ë¶„ì„
4. **í”„ë¡œê·¸ë¨ ë§¤ì¹­**: ì‚¬ì „ ì •ì˜ëœ í”„ë¡œê·¸ë¨ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”

ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­
--------------
- **ëª¨ë“ˆí™” ì„¤ê³„**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì™¸ë¶€ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ ìš©ì´ì„± ì¦ëŒ€
- **ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”**: LLM í˜¸ì¶œ ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ë¡œê¹… ë° ì‹¤í–‰ ì‹œê°„ ì¶”ì ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” ì§€ì›
- **ë°ì´í„° ê²€ì¦**: ì¶”ì¶œ ê²°ê³¼ì˜ í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œ
- **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©

ğŸ—ï¸ ì•„í‚¤í…ì²˜
-----------
- **MMSExtractor**: ë©”ì¸ ì¶”ì¶œ ì—”ì§„ í´ë˜ìŠ¤
- **DataManager**: ë°ì´í„° ë¡œë”© ë° ê´€ë¦¬ ë‹´ë‹¹
- **LLMProcessor**: LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
- **EntityMatcher**: ì—”í‹°í‹° ë§¤ì¹­ ë° ìœ ì‚¬ë„ ê³„ì‚°
- **PromptModule**: ì™¸ë¶€í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

âš™ï¸ ì„¤ì • ë° í™˜ê²½
--------------
- Python 3.8+
- LangChain, OpenAI, Anthropic API ì§€ì›
- Oracle Database ì—°ë™ (ì„ íƒì‚¬í•­)
- GPU ê°€ì† (CUDA ì§€ì› ì‹œ)

ğŸ“Š ì„±ëŠ¥ ì§€í‘œ
-----------
- í‰ê·  ì²˜ë¦¬ ì‹œê°„: ~30ì´ˆ/ë©”ì‹œì§€
- ì •í™•ë„: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
- ì²˜ë¦¬ëŸ‰: ~120 ë©”ì‹œì§€/ì‹œê°„ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

ì‘ì„±ì: MMS ë¶„ì„íŒ€
ìµœì¢… ìˆ˜ì •: 2024-09
ë²„ì „: 2.0.0
"""

from concurrent.futures import ThreadPoolExecutor
import time
import logging
import warnings
from functools import wraps
from typing import List, Tuple, Union, Dict, Any, Optional
from abc import ABC, abstractmethod
import traceback
import json
import re
import ast
import glob
import os
import copy
import pandas as pd
import numpy as np
from langchain.prompts import PromptTemplate

# joblibê³¼ multiprocessing ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="joblib")
warnings.filterwarnings("ignore", category=UserWarning, module="multiprocessing")
warnings.filterwarnings("ignore", message=".*resource_tracker.*")
warnings.filterwarnings("ignore", message=".*leaked.*")
import torch
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher
import difflib
from dotenv import load_dotenv
import cx_Oracle
from contextlib import contextmanager

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from rapidfuzz import fuzz, process
from kiwipiepy import Kiwi
from joblib import Parallel, delayed
from entity_dag_extractor import DAGParser, extract_dag

# í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from prompts import (
    build_extraction_prompt,
    enhance_prompt_for_retry,
    get_fallback_result,
    build_entity_extraction_prompt,
    DEFAULT_ENTITY_EXTRACTION_PROMPT,
    DETAILED_ENTITY_EXTRACTION_PROMPT,
    SIMPLE_ENTITY_EXTRACTION_PROMPT
    )

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸
from utils import (
    select_most_comprehensive,
    log_performance,
    safe_execute,
    validate_text_input,
    safe_check_empty,
    dataframe_to_markdown_prompt,
    extract_json_objects,
    preprocess_text,
    fuzzy_similarities,
    get_fuzzy_similarities,
    parallel_fuzzy_similarity,
    longest_common_subsequence_ratio,
    sequence_matcher_similarity,
    substring_aware_similarity,
    token_sequence_similarity,
    combined_sequence_similarity,
    calculate_seq_similarity,
    parallel_seq_similarity,
    load_sentence_transformer,
    Token,
    Sentence,
    filter_text_by_exc_patterns,
    filter_specific_terms,
    convert_df_to_json_list,
    create_dag_diagram,
    sha256_hash,
    replace_special_chars_with_space,
    extract_ngram_candidates
)

# ì„¤ì • ë° ì˜ì¡´ì„± ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
try:
    from config.settings import API_CONFIG, MODEL_CONFIG, PROCESSING_CONFIG, METADATA_CONFIG, EMBEDDING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ ì—¬ê¸°ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë¡œê¹… ì„¤ì • - api.pyì—ì„œ ì‹¤í–‰ë  ë•ŒëŠ” í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê¸°ë³¸ ì„¤ì • ì ìš©
logger = logging.getLogger(__name__)

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ë¡œê¹… ì„¤ì • (api.pyì—ì„œ ì„í¬íŠ¸ë  ë•ŒëŠ” api.pyì˜ ì„¤ì • ì‚¬ìš©)
if __name__ == '__main__':
    import sys
    from pathlib import Path
    
    # MongoDB ìœ í‹¸ë¦¬í‹°ëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__file__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / 'mms_extractor.log'),
            logging.StreamHandler()
        ]
    )

# pandas ì¶œë ¥ ì„¤ì •
pd.set_option('display.max_colwidth', 500)
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# ===== ì¶”ìƒ í´ë˜ìŠ¤ ë° ì „ëµ íŒ¨í„´ =====

class EntityExtractionStrategy(ABC):
    """ì—”í‹°í‹° ì¶”ì¶œ ì „ëµ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def extract(self, text: str, **kwargs) -> pd.DataFrame:
        """ì—”í‹°í‹° ì¶”ì¶œ ë©”ì†Œë“œ"""
        pass

class DataLoader(ABC):
    """ë°ì´í„° ë¡œë” ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def load_data(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë“œ ë©”ì†Œë“œ"""
        pass

# ===== ê°œì„ ëœ MMSExtractor í´ë˜ìŠ¤ =====

class MMSExtractor:
    """
    MMS ê´‘ê³  í…ìŠ¤íŠ¸ AI ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì¶”ì¶œ ì—”ì§„
    ================================================================
    
    ğŸ¨ ê°œìš”
    -------
    ì´ í´ë˜ìŠ¤ëŠ” MMS ê´‘ê³  í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.
    LLM(Large Language Model), ì„ë² ë”© ëª¨ë¸, NLP ê¸°ë²•ì„ ì¡°í•©í•˜ì—¬
    ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜•í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
    -----------
    1. **ë‹¤ë‹¨ê³„ ì—”í‹°í‹° ì¶”ì¶œ**: Kiwi NLP + ì„ë² ë”© ìœ ì‚¬ë„ + LLM ê¸°ë°˜ ì¶”ì¶œ
    2. **ì§€ëŠ¥í˜• í”„ë¡œê·¸ë¨ ë¶„ë¥˜**: ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ë§¤ì¹­
    3. **RAG ê¸°ë°˜ ì»¬í…ìŠ¤íŠ¸ ì¦ê°•**: ê´€ë ¨ ë°ì´í„°ë¥¼ í™œìš©í•œ ì •í™•ë„ í–¥ìƒ
    4. **ë‹¤ì¤‘ LLM ì§€ì›**: OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
    5. **DAG ìƒì„±**: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ì‹œê°í™”
    
    ğŸ“Š ì„±ëŠ¥ íŠ¹ì§•
    -----------
    - **ì •í™•ë„**: 85%+ (ìˆ˜ë™ ê²€ì¦ ê¸°ì¤€)
    - **ì²˜ë¦¬ ì†ë„**: í‰ê·  30ì´ˆ/ë©”ì‹œì§€
    - **í™•ì¥ì„±**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ìš©ì´
    - **ì•ˆì •ì„±**: ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    
    âš™ï¸ ì£¼ìš” ê°œì„ ì‚¬í•­
    --------------
    - **ì•„í‚¤í…ì²˜ ëª¨ë“ˆí™”**: ëŒ€í˜• ë©”ì†Œë“œë¥¼ ê¸°ëŠ¥ë³„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
    - **í”„ë¡¬í”„íŠ¸ ì™¸ë¶€í™”**: í•˜ë“œì½”ë”©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
    - **ë‹¤ì¸µ ì˜ˆì™¸ ì²˜ë¦¬**: LLM API ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì— ëŒ€í•œ robustí•œ ì—ëŸ¬ ë³µêµ¬
    - **ìƒì„¸ ë¡œê¹…**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…, ê°ì‚¬ ë¡œê·¸ë¥¼ ìœ„í•œ í¬ê´„ì  ë¡œê¹… ì‹œìŠ¤í…œ
    - **ë°ì´í„° ê²€ì¦**: ì…ë ¥/ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ê²€ì¦
    - **í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì†ŒìŠ¤**: CSV íŒŒì¼ê³¼ Oracle DBë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ë°ì´í„° ë¡œë”©
    
    ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    -----------
    ```python
    # ê¸°ë³¸ ì´ˆê¸°í™”
    extractor = MMSExtractor(
        llm_model='ax',
        entity_extraction_mode='llm',
        extract_entity_dag=True
    )
    
    # ë©”ì‹œì§€ ì²˜ë¦¬
    result = extractor.process_message("ìƒ˜í”Œ MMS í…ìŠ¤íŠ¸")
    
    # ê²°ê³¼ í™œìš©
    products = result['product']
    channels = result['channel']
    entity_dag = result.get('entity_dag', [])
    ```
    
    ğŸ’¼ ì˜ì¡´ì„±
    ---------
    - LangChain (LLM ì¸í„°í˜ì´ìŠ¤)
    - SentenceTransformers (ì„ë² ë”©)
    - KiwiPiePy (NLP)
    - cx_Oracle (ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™)
    """
    
    def __init__(self, model_path=None, data_dir=None, product_info_extraction_mode=None, 
                 entity_extraction_mode=None, offer_info_data_src='local', llm_model='ax', extract_entity_dag=False):
        """
        MMSExtractor ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        ì‹œìŠ¤í…œì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:
        - LLM ëª¨ë¸ ì„¤ì • ë° ì—°ê²°
        - ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        - NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™”
        - ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ (CSV/DB)
        - ê°ì¢… ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
        
        Args:
            model_path (str, optional): ì„ë² ë”© ëª¨ë¸ ê²½ë¡œ. ê¸°ë³¸ê°’: 'jhgan/ko-sroberta-multitask'
            data_dir (str, optional): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ. ê¸°ë³¸ê°’: './data/'
            product_info_extraction_mode (str, optional): ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ ('nlp' ë˜ëŠ” 'llm')
            entity_extraction_mode (str, optional): ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ ('nlp', 'llm', 'hybrid')
            offer_info_data_src (str, optional): ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ('local' ë˜ëŠ” 'db')
            llm_model (str, optional): ì‚¬ìš©í•  LLM ëª¨ë¸. ê¸°ë³¸ê°’: 'ax'
            extract_entity_dag (bool, optional): DAG ì¶”ì¶œ ì—¬ë¶€. ê¸°ë³¸ê°’: False
            
        Raises:
            Exception: ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜¤ë¥˜
            
        Example:
            >>> extractor = MMSExtractor(
            ...     llm_model='gpt-4',
            ...     entity_extraction_mode='hybrid',
            ...     extract_entity_dag=True
            ... )
        """
        logger.info("ğŸš€ MMSExtractor ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„¤ì • ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
            logger.info("âš™ï¸ ê¸°ë³¸ ì„¤ì • ì ìš© ì¤‘...")
            self._set_default_config(model_path, data_dir, product_info_extraction_mode, 
                                   entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag)
            
            # 2ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±)
            logger.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¤‘...")
            load_dotenv()
            
            # 3ë‹¨ê³„: ì£¼ìš” êµ¬ì„± ìš”ì†Œë“¤ ìˆœì°¨ ì´ˆê¸°í™”
            logger.info("ğŸ’» ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...")
            self._initialize_device()
            
            logger.info("ğŸ¤– LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self._initialize_llm()
            
            logger.info("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self._initialize_embedding_model()
            
            logger.info("ğŸ“ NLP ë„êµ¬ (Kiwi) ì´ˆê¸°í™” ì¤‘...")
            self._initialize_kiwi()
            
            logger.info("ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
            self._load_data()
            
            logger.info("âœ… MMSExtractor ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ MMSExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            raise

    def _set_default_config(self, model_path, data_dir, product_info_extraction_mode, 
                          entity_extraction_mode, offer_info_data_src, llm_model, extract_entity_dag):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì ìš©"""
        self.data_dir = data_dir if data_dir is not None else './data/'
        self.model_path = model_path if model_path is not None else getattr(EMBEDDING_CONFIG, 'ko_sbert_model_path', 'jhgan/ko-sroberta-multitask')
        self.offer_info_data_src = offer_info_data_src
        self.product_info_extraction_mode = product_info_extraction_mode if product_info_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'product_info_extraction_mode', 'nlp')
        self.entity_extraction_mode = entity_extraction_mode if entity_extraction_mode is not None else getattr(PROCESSING_CONFIG, 'entity_extraction_mode', 'llm')
        self.llm_model_name = llm_model
        self.num_cand_pgms = getattr(PROCESSING_CONFIG, 'num_candidate_programs', 5)
        self.extract_entity_dag = extract_entity_dag
        
        # DAG ì¶”ì¶œ ì„¤ì • ë¡œê¹…
        # extract_entity_dag: ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ DAG(Directed Acyclic Graph)ë¡œ ì¶”ì¶œ
        # Trueì¸ ê²½ìš° ì¶”ê°€ì ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹° ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ 
        # NetworkX + Graphvizë¥¼ í†µí•´ ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ì„ ìƒì„±
        if self.extract_entity_dag:
            logger.info("ğŸ¯ DAG ì¶”ì¶œ ëª¨ë“œ í™œì„±í™”ë¨")
        else:
            logger.info("ğŸ“‹ í‘œì¤€ ì¶”ì¶œ ëª¨ë“œ (DAG ë¹„í™œì„±í™”)")

    @log_performance
    def _initialize_device(self):
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì´ˆê¸°í™”"""
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        logger.info(f"Using device: {self.device}")

    @log_performance
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ëª¨ë¸ ì„¤ì • ë§¤í•‘
            model_mapping = {
                "gemma": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),
                "gem": getattr(MODEL_CONFIG, 'gemma_model', 'gemma-7b'),  # 'gem'ì€ 'gemma'ì˜ ì¤„ì„ë§
                "ax": getattr(MODEL_CONFIG, 'ax_model', 'ax-4'),
                "claude": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),
                "cld": getattr(MODEL_CONFIG, 'claude_model', 'claude-4'),  # 'cld'ëŠ” 'claude'ì˜ ì¤„ì„ë§
                "gemini": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),
                "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gemini-pro'),  # 'gen'ì€ 'gemini'ì˜ ì¤„ì„ë§
                "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'gpt-4')
            }
            
            model_name = model_mapping.get(self.llm_model_name, getattr(MODEL_CONFIG, 'llm_model', 'gemini-pro'))
            
            # LLM ëª¨ë¸ë³„ ì¼ê´€ì„± ì„¤ì •
            model_kwargs = {
                "temperature": 0.0,  # ì™„ì „ ê²°ì •ì  ì¶œë ¥ì„ ìœ„í•´ 0.0 ê³ ì •
                "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                "model": model_name,
                "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
            }
            
            # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¼ê´€ì„± ê°•í™”
            if 'gpt' in model_name.lower():
                model_kwargs["seed"] = 42  # ê³ ì • ì‹œë“œë¡œ ì¼ê´€ì„± ë³´ì¥
                
            self.llm_model = ChatOpenAI(**model_kwargs)
            
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.llm_model_name} ({model_name})")
            
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ë¹„í™œì„±í™” ì˜µì…˜ í™•ì¸
        if MODEL_CONFIG.disable_embedding:
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¹„í™œì„±í™” ëª¨ë“œ (DISABLE_EMBEDDING=true)")
            self.emb_model = None
            return
            
        try:
            self.emb_model = load_sentence_transformer(self.model_path, self.device)
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            logger.info("ê¸°ë³¸ ëª¨ë¸ë¡œ fallback ì‹œë„")
            try:
                self.emb_model = load_sentence_transformer('jhgan/ko-sroberta-multitask', self.device)
            except Exception as e2:
                logger.error(f"Fallback ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                logger.warning("ì„ë² ë”© ëª¨ë¸ ì—†ì´ ë™ì‘ ëª¨ë“œë¡œ ì „í™˜")
                self.emb_model = None

    def _initialize_multiple_llm_models(self, model_names: List[str]) -> List:
        """
        ë³µìˆ˜ì˜ LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ
        
        Args:
            model_names (List[str]): ì´ˆê¸°í™”í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['ax', 'gpt', 'gen'])
            
        Returns:
            List: ì´ˆê¸°í™”ëœ LLM ëª¨ë¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        llm_models = []
        
        # ëª¨ë¸ëª… ë§¤í•‘ (ê¸°ì¡´ LLM ì´ˆê¸°í™” ë¡œì§ê³¼ ë™ì¼)
        model_mapping = {
            "cld": getattr(MODEL_CONFIG, 'anthropic_model', 'amazon/anthropic/claude-sonnet-4-20250514'),
            "ax": getattr(MODEL_CONFIG, 'ax_model', 'skt/ax4'),
            "gpt": getattr(MODEL_CONFIG, 'gpt_model', 'azure/openai/gpt-4o-2024-08-06'),
            "gen": getattr(MODEL_CONFIG, 'gemini_model', 'gcp/gemini-2.5-flash')
        }
        
        for model_name in model_names:
            try:
                actual_model_name = model_mapping.get(model_name, model_name)
                
                # ëª¨ë¸ë³„ ì„¤ì • (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
                model_kwargs = {
                    "temperature": 0.0,
                    "openai_api_key": getattr(API_CONFIG, 'llm_api_key', os.getenv('OPENAI_API_KEY')),
                    "openai_api_base": getattr(API_CONFIG, 'llm_api_url', None),
                    "model": actual_model_name,
                    "max_tokens": getattr(MODEL_CONFIG, 'llm_max_tokens', 4000)
                }
                
                # GPT ëª¨ë¸ì˜ ê²½ìš° ì‹œë“œ ì„¤ì •
                if 'gpt' in actual_model_name.lower():
                    model_kwargs["seed"] = 42
                
                llm_model = ChatOpenAI(**model_kwargs)
                llm_models.append(llm_model)
                logger.info(f"âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {model_name} ({actual_model_name})")
                
            except Exception as e:
                logger.error(f"âŒ LLM ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {model_name} - {e}")
                continue
        
        return llm_models

    @log_performance
    def _initialize_kiwi(self):
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            self.kiwi = Kiwi()
            
            # ì œì™¸í•  í’ˆì‚¬ íƒœê·¸ íŒ¨í„´ë“¤
            self.exc_tag_patterns = [
                ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],
                ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'],
                ['VV', 'ETN'], ['SSO'], ['SSC'], ['SW'], ['SF'], ['SP'], 
                ['SS'], ['SE'], ['SO'], ['SB'], ['SH'], ['W_HASHTAG']
            ]
            logger.info("Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    @log_performance
    def _load_data(self):
        """í•„ìš”í•œ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ëª¨ë“œ: {self.offer_info_data_src}")
            
            # ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ (ë³„ì¹­ ê·œì¹™ ì ìš© í¬í•¨)
            logger.info("1ï¸âƒ£ ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì¤‘...")
            self._load_and_prepare_item_data()
            logger.info(f"ìƒí’ˆ ì •ë³´ ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì •ì§€ì–´ ë¡œë“œ
            logger.info("2ï¸âƒ£ ì •ì§€ì–´ ë¡œë“œ ì¤‘...")
            self._load_stop_words()
            logger.info(f"ë¡œë“œëœ ì •ì§€ì–´ ìˆ˜: {len(self.stop_item_names)}ê°œ")
            
            # Kiwiì— ìƒí’ˆëª… ë“±ë¡
            logger.info("3ï¸âƒ£ Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì¤‘...")
            self._register_items_to_kiwi()
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ
            logger.info("4ï¸âƒ£ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_program_data()
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì¡°ì§ ì •ë³´ ë¡œë“œ
            logger.info("5ï¸âƒ£ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
            self._load_organization_data()
            logger.info(f"ì¡°ì§ ì •ë³´ ë¡œë“œ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
            
            # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½
            logger.info("=" * 60)
            logger.info("ğŸ“‹ ë°ì´í„° ë¡œë”© ì™„ë£Œ - ìµœì¢… ìƒíƒœ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"âœ… ìƒí’ˆ ë°ì´í„°: {self.item_pdf_all.shape}")
            logger.info(f"âœ… í”„ë¡œê·¸ë¨ ë°ì´í„°: {self.pgm_pdf.shape}")
            logger.info(f"âœ… ì¡°ì§ ë°ì´í„°: {self.org_pdf.shape}")
            logger.info(f"âœ… ì •ì§€ì–´: {len(self.stop_item_names)}ê°œ")
            
            # ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒíƒœ ë¹„êµë¥¼ ìœ„í•œ ì¶”ê°€ ì •ë³´
            if hasattr(self, 'item_pdf_all') and not self.item_pdf_all.empty:
                logger.info("=== ìƒí’ˆ ë°ì´í„° ìƒì„¸ ì •ë³´ ===")
                if 'item_nm' in self.item_pdf_all.columns:
                    unique_items = self.item_pdf_all['item_nm'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆëª… ìˆ˜: {unique_items}ê°œ")
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    unique_aliases = self.item_pdf_all['item_nm_alias'].nunique()
                    logger.info(f"ê³ ìœ  ë³„ì¹­ ìˆ˜: {unique_aliases}ê°œ")
                if 'item_id' in self.item_pdf_all.columns:
                    unique_ids = self.item_pdf_all['item_id'].nunique()
                    logger.info(f"ê³ ìœ  ìƒí’ˆID ìˆ˜: {unique_ids}ê°œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    def _load_and_prepare_item_data(self):
        """ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ (ipynb ì½”ë“œ ê¸°ì¤€ìœ¼ë¡œ í†µí•©)"""
        try:
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            # ===== 1ë‹¨ê³„: ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì›ë³¸ ë°ì´í„° ë¡œë“œ =====
            if self.offer_info_data_src == "local":
                logger.info("ğŸ“ ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ")
                csv_path = getattr(METADATA_CONFIG, 'offer_data_path', './data/items.csv')
                item_pdf_raw = pd.read_csv(csv_path)
            elif self.offer_info_data_src == "db":
                logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ")
                with self._database_connection() as conn:
                    sql = "SELECT * FROM TCAM_RC_OFER_MST"
                    item_pdf_raw = pd.read_sql(sql, conn)
            
            logger.info(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {item_pdf_raw.shape}")
            
            # ===== 2ë‹¨ê³„: ê³µí†µ ì „ì²˜ë¦¬ (ë°ì´í„° ì†ŒìŠ¤ ë¬´ê´€) =====
            # ITEM_DESCë¥¼ strë¡œ ë³€í™˜
            item_pdf_raw['ITEM_DESC'] = item_pdf_raw['ITEM_DESC'].astype('str')
            
            # ë‹¨ë§ê¸°ì¸ ê²½ìš° ì„¤ëª…ì„ ìƒí’ˆëª…ìœ¼ë¡œ ì‚¬ìš©
            item_pdf_raw['ITEM_NM'] = item_pdf_raw.apply(
                lambda x: x['ITEM_DESC'] if x['ITEM_DMN']=='E' else x['ITEM_NM'], axis=1
            )
            
            # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë³€í™˜
            item_pdf_all = item_pdf_raw.rename(columns={c: c.lower() for c in item_pdf_raw.columns})
            logger.info(f"ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜ ì™„ë£Œ")
            
            # ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
            item_pdf_all['item_ctg'] = None
            item_pdf_all['item_emb_vec'] = None
            item_pdf_all['ofer_cd'] = item_pdf_all['item_id']
            item_pdf_all['oper_dt_hms'] = '20250101000000'
            
            # ì œì™¸í•  ë„ë©”ì¸ ì½”ë“œ í•„í„°ë§
            excluded_domains = getattr(PROCESSING_CONFIG, 'excluded_domain_codes_for_items', [])
            if excluded_domains:
                before_filter = len(item_pdf_all)
                item_pdf_all = item_pdf_all.query("item_dmn not in @excluded_domains")
                logger.info(f"ë„ë©”ì¸ í•„í„°ë§: {before_filter} -> {len(item_pdf_all)}")
            
            # ===== 3ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ë¡œë“œ ë° ì²˜ë¦¬ (ë°ì´í„° ì†ŒìŠ¤ ë¬´ê´€) =====
            logger.info("ğŸ”— ë³„ì¹­ ê·œì¹™ ë¡œë“œ ì¤‘...")
            self.alias_pdf_raw = pd.read_csv(getattr(METADATA_CONFIG, 'alias_rules_path', './data/alias_rules.csv'))
            alias_pdf = self.alias_pdf_raw.copy()
            alias_pdf['alias_1'] = alias_pdf['alias_1'].str.split("&&")
            alias_pdf['alias_2'] = alias_pdf['alias_2'].str.split("&&")
            alias_pdf = alias_pdf.explode('alias_1')
            alias_pdf = alias_pdf.explode('alias_2')
            
            # build íƒ€ì… ë³„ì¹­ í™•ì¥
            alias_list_ext = alias_pdf.query("type=='build'")[['alias_1','category','direction','type']].to_dict('records')
            for alias in alias_list_ext:
                adf = item_pdf_all.query(
                    "item_nm.str.contains(@alias['alias_1']) and item_dmn==@alias['category']"
                )[['item_nm','item_desc','item_dmn']].rename(
                    columns={'item_nm':'alias_2','item_desc':'description','item_dmn':'category'}
                ).drop_duplicates()
                adf['alias_1'] = alias['alias_1']
                adf['direction'] = alias['direction']
                adf['type'] = alias['type']
                adf = adf[alias_pdf.columns]
                alias_pdf = pd.concat([alias_pdf.query(f"alias_1!='{alias['alias_1']}'"), adf])
            
            alias_pdf = alias_pdf.drop_duplicates()
            
            # ì–‘ë°©í–¥(B) ë³„ì¹­ ì¶”ê°€
            alias_pdf = pd.concat([
                alias_pdf, 
                alias_pdf.query("direction=='B'").rename(
                    columns={'alias_1':'alias_2', 'alias_2':'alias_1'}
                )[alias_pdf.columns]
            ])
            
            alias_rule_set = list(zip(alias_pdf['alias_1'], alias_pdf['alias_2'], alias_pdf['type']))
            logger.info(f"ë³„ì¹­ ê·œì¹™ ìˆ˜: {len(alias_rule_set)}ê°œ")
            
            # ===== 4ë‹¨ê³„: ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© (ë³‘ë ¬ ì²˜ë¦¬) =====
            def apply_alias_rule_cascade_parallel(args_dict):
                """ë³„ì¹­ ê·œì¹™ì„ ì—°ì‡„ì ìœ¼ë¡œ ì ìš©"""
                item_nm = args_dict['item_nm']
                max_depth = args_dict['max_depth']
                
                processed = set()
                result_dict = {item_nm: '#' * len(item_nm)}
                to_process = [(item_nm, 0, frozenset())]
                
                while to_process:
                    current_item, depth, path_applied_rules = to_process.pop(0)
                    
                    if depth >= max_depth or current_item in processed:
                        continue
                    
                    processed.add(current_item)
                    
                    for r in alias_rule_set:
                        alias_from, alias_to, alias_type = r[0], r[1], r[2]
                        rule_key = (alias_from, alias_to, alias_type)
                        
                        if rule_key in path_applied_rules:
                            continue
                        
                        # íƒ€ì…ì— ë”°ë¥¸ ë§¤ì¹­
                        if alias_type == 'exact':
                            matched = (current_item == alias_from)
                        else:
                            matched = (alias_from in current_item)
                        
                        if matched:
                            new_item = alias_to.strip() if alias_type == 'exact' else current_item.replace(alias_from.strip(), alias_to.strip())
                            
                            if new_item not in result_dict:
                                result_dict[new_item] = alias_from.strip()
                                to_process.append((new_item, depth + 1, path_applied_rules | {rule_key}))
                
                item_nm_list = [{'item_nm': k, 'item_nm_alias': v} for k, v in result_dict.items()]
                adf = pd.DataFrame(item_nm_list)
                selected_alias = select_most_comprehensive(adf['item_nm_alias'].tolist())
                result_aliases = list(adf.query("item_nm_alias in @selected_alias")['item_nm'].unique())
                
                if item_nm not in result_aliases:
                    result_aliases.append(item_nm)
                
                return {'item_nm': item_nm, 'item_nm_alias': result_aliases}
            
            def parallel_alias_rule_cascade(texts, max_depth=5, n_jobs=None):
                """ë³‘ë ¬ ë³„ì¹­ ê·œì¹™ ì ìš©"""
                if n_jobs is None:
                    n_jobs = min(os.cpu_count()-1, 4)
                
                batches = [{"item_nm": text, "max_depth": max_depth} for text in texts]
                with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                    batch_results = parallel(delayed(apply_alias_rule_cascade_parallel)(args) for args in batches)
                
                return pd.DataFrame(batch_results)
            
            logger.info("ğŸ”„ ë³„ì¹­ ê·œì¹™ ì—°ì‡„ ì ìš© ì¤‘...")
            item_alias_pdf = parallel_alias_rule_cascade(item_pdf_all['item_nm'], max_depth=3)
            
            # ë³„ì¹­ ë³‘í•© ë° explode
            item_pdf_all = item_pdf_all.merge(item_alias_pdf, on='item_nm', how='left')
            before_explode = len(item_pdf_all)
            item_pdf_all = item_pdf_all.explode('item_nm_alias').drop_duplicates()
            logger.info(f"ë³„ì¹­ explode: {before_explode} -> {len(item_pdf_all)}")
            
            # ===== 5ë‹¨ê³„: ì‚¬ìš©ì ì •ì˜ ì—”í‹°í‹° ì¶”ê°€ =====
            user_defined_entity = ['AIA Vitality', 'ë¶€ìŠ¤íŠ¸ íŒŒí¬ ê±´ëŒ€ì…êµ¬', 'Boost Park ê±´ëŒ€ì…êµ¬']
            item_pdf_ext = pd.DataFrame([{
                'item_nm': e, 'item_id': e, 'item_desc': e, 'item_dmn': 'user_defined',
                'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e
            } for e in user_defined_entity])
            item_pdf_all = pd.concat([item_pdf_all, item_pdf_ext])
            
            # ===== 6ë‹¨ê³„: item_dmn_nm ì»¬ëŸ¼ ì¶”ê°€ =====
            item_dmn_map = pd.DataFrame([
                {"item_dmn": 'P', 'item_dmn_nm': 'ìš”ê¸ˆì œ ë° ê´€ë ¨ ìƒí’ˆ'},
                {"item_dmn": 'E', 'item_dmn_nm': 'ë‹¨ë§ê¸°'},
                {"item_dmn": 'S', 'item_dmn_nm': 'êµ¬ë… ìƒí’ˆ'},
                {"item_dmn": 'C', 'item_dmn_nm': 'ì¿ í°'},
                {"item_dmn": 'X', 'item_dmn_nm': 'ê°€ìƒ ìƒí’ˆ'}
            ])
            item_pdf_all = item_pdf_all.merge(item_dmn_map, on='item_dmn', how='left')
            item_pdf_all['item_dmn_nm'] = item_pdf_all['item_dmn_nm'].fillna('ê¸°íƒ€')
            
            # ===== 7ë‹¨ê³„: TEST í•„í„°ë§ =====
            before_test = len(item_pdf_all)
            item_pdf_all = item_pdf_all.query("not item_nm_alias.str.contains('TEST', case=False, na=False)")
            logger.info(f"TEST í•„í„°ë§: {before_test} -> {len(item_pdf_all)}")
            
            self.item_pdf_all = item_pdf_all
            
            # ìµœì¢… í™•ì¸
            logger.info(f"=== ìƒí’ˆ ì •ë³´ ì¤€ë¹„ ì™„ë£Œ ===")
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.item_pdf_all.columns)}")
            
            # ì¤‘ìš” ì»¬ëŸ¼ í™•ì¸
            critical_columns = ['item_nm', 'item_id', 'item_nm_alias']
            missing_columns = [col for col in critical_columns if col not in self.item_pdf_all.columns]
            if missing_columns:
                logger.error(f"ì¤‘ìš” ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            else:
                logger.info("âœ… ëª¨ë“  ì¤‘ìš” ì»¬ëŸ¼ ì¡´ì¬")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if not self.item_pdf_all.empty:
                logger.info(f"ìƒí’ˆëª… ìƒ˜í”Œ: {self.item_pdf_all['item_nm'].dropna().head(3).tolist()}")
                logger.info(f"ë³„ì¹­ ìƒ˜í”Œ: {self.item_pdf_all['item_nm_alias'].dropna().head(3).tolist()}")
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë¡œë“œ ë° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback
            self.item_pdf_all = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn', 'item_nm_alias'])
            logger.warning("ë¹ˆ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")

    def _get_database_connection(self):
        """Oracle ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘ ===")
            
            username = os.getenv("DB_USERNAME")
            password = os.getenv("DB_PASSWORD")
            host = os.getenv("DB_HOST")
            port = os.getenv("DB_PORT")
            service_name = os.getenv("DB_NAME")
            
            # ì—°ê²° ì •ë³´ ë¡œê¹… (ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹)
            logger.info(f"DB ì—°ê²° ì •ë³´:")
            logger.info(f"  - ì‚¬ìš©ìëª…: {username if username else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ë¹„ë°€ë²ˆí˜¸: {'*' * len(password) if password else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í˜¸ìŠ¤íŠ¸: {host if host else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - í¬íŠ¸: {port if port else '[ë¹„ì–´ìˆìŒ]'}")
            logger.info(f"  - ì„œë¹„ìŠ¤ëª…: {service_name if service_name else '[ë¹„ì–´ìˆìŒ]'}")
            
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            missing_vars = []
            if not username: missing_vars.append('DB_USERNAME')
            if not password: missing_vars.append('DB_PASSWORD')
            if not host: missing_vars.append('DB_HOST')
            if not port: missing_vars.append('DB_PORT')
            if not service_name: missing_vars.append('DB_NAME')
            
            if missing_vars:
                logger.error(f"ëˆ„ë½ëœ í™˜ê²½ ë³€ìˆ˜: {missing_vars}")
                logger.error("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë“¤ì„ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
                raise ValueError(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ëˆ„ë½: {missing_vars}")
            
            # DSN ìƒì„± ë° ë¡œê¹…
            logger.info(f"DSN ìƒì„± ì¤‘: {host}:{port}/{service_name}")
            dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
            logger.info(f"DSN ìƒì„± ì„±ê³µ: {dsn}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            conn = cx_Oracle.connect(user=username, password=password, dsn=dsn, encoding="UTF-8")
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            
            # LOB ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ outputtypehandler ì„¤ì •
            def output_type_handler(cursor, name, default_type, size, precision, scale):
                if default_type == cx_Oracle.CLOB:
                    return cursor.var(cx_Oracle.LONG_STRING, arraysize=cursor.arraysize)
                elif default_type == cx_Oracle.BLOB:
                    return cursor.var(cx_Oracle.LONG_BINARY, arraysize=cursor.arraysize)
            
            conn.outputtypehandler = output_type_handler
            
            # ì—°ê²° ì •ë³´ í™•ì¸
            logger.info(f"ì—°ê²°ëœ DB ë²„ì „: {conn.version}")
            
            return conn
            
        except cx_Oracle.DatabaseError as db_error:
            error_obj, = db_error.args
            logger.error(f"Oracle ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜:")
            logger.error(f"  - ì˜¤ë¥˜ ì½”ë“œ: {error_obj.code}")
            logger.error(f"  - ì˜¤ë¥˜ ë©”ì‹œì§€: {error_obj.message}")
            logger.error(f"  - ì „ì²´ ì˜¤ë¥˜: {db_error}")
            raise
        except ImportError as import_error:
            logger.error(f"cx_Oracle ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜: {import_error}")
            logger.error("ì½”ë§¨ë“œ: pip install cx_Oracle")
            raise
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise

    @contextmanager
    def _database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager"""
        conn = None
        start_time = time.time()
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° context manager ì‹œì‘")
            conn = self._get_database_connection()
            connection_time = time.time() - start_time
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ ({connection_time:.2f}ì´ˆ)")
            yield conn
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            raise
        finally:
            if conn:
                try:
                    conn.close()
                    total_time = time.time() - start_time
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
                except Exception as close_error:
                    logger.warning(f"ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {close_error}")
            else:
                logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _load_program_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì‹œì‘ ===")
            
            with self._database_connection() as conn:
                # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ì¿¼ë¦¬
                sql = """SELECT CMPGN_PGM_NUM pgm_id, CMPGN_PGM_NM pgm_nm, RMK clue_tag 
                         FROM TCAM_CMPGN_PGM_INFO
                         WHERE DEL_YN = 'N' 
                         AND APRV_OP_RSLT_CD = 'APPR'
                         AND EXPS_YN = 'Y'
                         AND CMPGN_PGM_NUM like '2025%' 
                         AND RMK is not null"""
                
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.pgm_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
                logger.info(f"DBì—ì„œ ë¡œë“œëœ í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ë“¤: {list(self.pgm_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.pgm_pdf.columns)
                self.pgm_pdf = self.pgm_pdf.rename(columns={c:c.lower() for c in self.pgm_pdf.columns})
                logger.info(f"í”„ë¡œê·¸ë¨ ì»¬ëŸ¼ëª… ë³€í™˜: {dict(zip(original_columns, self.pgm_pdf.columns))}")
                
                # LOB ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë°ì´í„° ê°•ì œ ë¡œë“œ
                if not self.pgm_pdf.empty:
                    try:
                        # DataFrameì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ê°•ì œ ë¡œë“œ
                        _ = self.pgm_pdf.values  # ëª¨ë“  ë°ì´í„° ì ‘ê·¼í•˜ì—¬ LOB ë¡œë“œ ìœ ë„
                        
                        # í”„ë¡œê·¸ë¨ ë°ì´í„° ìƒ˜í”Œ í™•ì¸
                        if 'pgm_nm' in self.pgm_pdf.columns:
                            sample_pgms = self.pgm_pdf['pgm_nm'].dropna().head(3).tolist()
                            logger.info(f"í”„ë¡œê·¸ë¨ëª… ìƒ˜í”Œ: {sample_pgms}")
                        
                        if 'clue_tag' in self.pgm_pdf.columns:
                            sample_clues = self.pgm_pdf['clue_tag'].dropna().head(3).tolist()
                            logger.info(f"í´ë£¨ íƒœê·¸ ìƒ˜í”Œ: {sample_clues}")
                            
                        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ")
                    except Exception as load_error:
                        logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ê°•ì œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {load_error}")
                        raise
                else:
                    logger.warning("ë¡œë“œëœ í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            raise

    def _load_stop_words(self):
        """ì •ì§€ì–´ ëª©ë¡ ë¡œë“œ"""
        try:
            self.stop_item_names = pd.read_csv(getattr(METADATA_CONFIG, 'stop_items_path', './data/stop_words.csv'))['stop_words'].to_list()
            logger.info(f"ì •ì§€ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.stop_item_names)}ê°œ")
        except Exception as e:
            logger.warning(f"ì •ì§€ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stop_item_names = []

    def _register_items_to_kiwi(self):
        """Kiwiì— ìƒí’ˆëª…ë“¤ì„ ê³ ìœ ëª…ì‚¬ë¡œ ë“±ë¡"""
        try:
            logger.info("=== Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì‹œì‘ ===")
            
            # ìƒí’ˆëª… ë³„ì¹­ ë°ì´í„° í™•ì¸
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                return
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë“±ë¡í•  ê³ ìœ  ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # nullì´ ì•„ë‹Œ ìœ íš¨í•œ ë³„ì¹­ë“¤ë§Œ í•„í„°ë§
            valid_aliases = [w for w in unique_aliases if isinstance(w, str) and len(w.strip()) > 0]
            logger.info(f"ìœ íš¨í•œ ë³„ì¹­ ìˆ˜: {len(valid_aliases)}ê°œ")
            
            if len(valid_aliases) > 0:
                sample_aliases = valid_aliases[:5]
                logger.info(f"ë“±ë¡í•  ë³„ì¹­ ìƒ˜í”Œ: {sample_aliases}")
            
            registered_count = 0
            failed_count = 0
            
            for w in valid_aliases:
                try:
                    self.kiwi.add_user_word(w, "NNP")
                    registered_count += 1
                except Exception as reg_error:
                    failed_count += 1
                    if failed_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨ë§Œ ë¡œê¹…
                        logger.warning(f"Kiwi ë“±ë¡ ì‹¤íŒ¨ - '{w}': {reg_error}")
            
            logger.info(f"Kiwiì— ìƒí’ˆëª… ë“±ë¡ ì™„ë£Œ: {registered_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"Kiwi ìƒí’ˆëª… ë“±ë¡ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

    def _load_program_data(self):
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
        try:
            logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë¡œë”© ì‹œì‘...")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                self.pgm_pdf = pd.read_csv(getattr(METADATA_CONFIG, 'pgm_info_path', './data/program_info.csv'))
                logger.info(f"ë¡œì»¬ íŒŒì¼ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                self._load_program_from_database()
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ë¡œë“œ: {len(self.pgm_pdf)}ê°œ")
            
            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ë² ë”© ìƒì„±
            if not self.pgm_pdf.empty:
                logger.info("í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì‹œì‘...")
                clue_texts = self.pgm_pdf[["pgm_nm","clue_tag"]].apply(
                    lambda x: preprocess_text(x['pgm_nm'].lower()) + " " + x['clue_tag'].lower(), axis=1
                ).tolist()
                
                if self.emb_model is not None:
                    self.clue_embeddings = self.emb_model.encode(
                        clue_texts, convert_to_tensor=True, show_progress_bar=False
                    )
                else:
                    logger.warning("ì„ë² ë”© ëª¨ë¸ì´ ì—†ì–´ ë¹ˆ tensor ì‚¬ìš©")
                    self.clue_embeddings = torch.empty((0, 768))
                
                logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.pgm_pdf)}ê°œ í”„ë¡œê·¸ë¨")
            else:
                logger.warning("í”„ë¡œê·¸ë¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.clue_embeddings = torch.tensor([])
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ë°ì´í„°ë¡œ fallback
            self.pgm_pdf = pd.DataFrame(columns=['pgm_nm', 'clue_tag', 'pgm_id'])
            self.clue_embeddings = torch.tensor([])

    def _load_organization_data(self):
        """ì¡°ì§/ë§¤ì¥ ì •ë³´ ë¡œë“œ"""
        try:
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: {self.offer_info_data_src}) ===")
            
            if self.offer_info_data_src == "local":
                # ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ë¡œë“œ
                logger.info("ë¡œì»¬ CSV íŒŒì¼ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                csv_path = getattr(METADATA_CONFIG, 'org_info_path', './data/org_info_all_250605.csv')
                logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
                
                org_pdf_raw = pd.read_csv(csv_path)
                logger.info(f"ë¡œì»¬ CSVì—ì„œ ë¡œë“œëœ ì›ë³¸ ì¡°ì§ ë°ì´í„° í¬ê¸°: {org_pdf_raw.shape}")
                logger.info(f"ë¡œì»¬ CSV ì›ë³¸ ì»¬ëŸ¼ë“¤: {list(org_pdf_raw.columns)}")
                
                # ITEM_DMN='R' ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§
                if 'ITEM_DMN' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("ITEM_DMN=='R'").copy()
                elif 'item_dmn' in org_pdf_raw.columns:
                    self.org_pdf = org_pdf_raw.query("item_dmn=='R'").copy()
                else:
                    logger.warning("ITEM_DMN/item_dmn ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    self.org_pdf = org_pdf_raw.copy()
                
                # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë¦¬ë„¤ì„
                self.org_pdf = self.org_pdf.rename(columns={c: c.lower() for c in self.org_pdf.columns})
                
                logger.info(f"ë¡œì»¬ ëª¨ë“œ: ITEM_DMN='R' í•„í„°ë§ í›„ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                
            elif self.offer_info_data_src == "db":
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ ì¤‘...")
                self._load_org_from_database()
            
            # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
            if not self.org_pdf.empty:
                sample_orgs = self.org_pdf.head(3).to_dict('records')
                logger.info(f"ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ (3ê°œ í–‰): {sample_orgs}")
            
            logger.info(f"=== ì¡°ì§ ì •ë³´ ë¡œë“œ ìµœì¢… ì™„ë£Œ: {len(self.org_pdf)}ê°œ ì¡°ì§ ===")
            logger.info(f"ìµœì¢… ì¡°ì§ ë°ì´í„° ìŠ¤í‚¤ë§ˆ: {list(self.org_pdf.columns)}")
            
            # ì¡°ì§ ë°ì´í„° ìµœì¢… ê²€ì¦
            if not self.org_pdf.empty:
                critical_org_columns = ['item_nm', 'item_id']
                missing_org_columns = [col for col in critical_org_columns if col not in self.org_pdf.columns]
                if missing_org_columns:
                    logger.error(f"ì¡°ì§ ë°ì´í„°ì—ì„œ ì¤‘ìš” ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_org_columns}")
                    logger.error("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    logger.info("ëª¨ë“  ì¤‘ìš” ì¡°ì§ ì»¬ëŸ¼ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¡°ì§/ë§¤ì¥ ì¶”ì¶œì´ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ì¡°ì§ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback (ì¡°ì§ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ë¹ˆ ì¡°ì§ DataFrameìœ¼ë¡œ fallback ì„¤ì •ë¨")
            logger.warning("ì´ë¡œ ì¸í•´ ì¡°ì§/ë§¤ì¥ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    def _load_org_from_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ì§ ì •ë³´ ë¡œë“œ (ITEM_DMN='R')"""
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„ ì¤‘...")
            
            with self._database_connection() as conn:
                sql = "SELECT * FROM TCAM_RC_OFER_MST WHERE ITEM_DMN='R'"
                logger.info(f"ì‹¤í–‰í•  SQL: {sql}")
                
                self.org_pdf = pd.read_sql(sql, conn)
                logger.info(f"DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„° í¬ê¸°: {self.org_pdf.shape}")
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ì†Œë¬¸ì ë³€í™˜
                original_columns = list(self.org_pdf.columns)
                logger.info(f"DB ì¡°ì§ ë°ì´í„° ì›ë³¸ ì»¬ëŸ¼ë“¤: {original_columns}")
                
                # ì¡°ì§ ë°ì´í„°ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ë§¤í•‘ (ë™ì¼í•œ í…Œì´ë¸”ì´ì§€ë§Œ ì‚¬ìš© ëª©ì ì´ ë‹¤ë¦„)
                column_mapping = {c: c.lower() for c in self.org_pdf.columns}
                
                # ì¡°ì§ ë°ì´í„°ëŠ” item í…Œì´ë¸”ê³¼ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
                # ITEM_NM -> item_nm, ITEM_ID -> item_id, ITEM_DESC -> item_desc ë“±
                
                self.org_pdf = self.org_pdf.rename(columns=column_mapping)
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ì»¬ëŸ¼ëª… ë§¤í•‘ ì™„ë£Œ: {dict(zip(original_columns, self.org_pdf.columns))}")
                logger.info(f"DB ëª¨ë“œ ì¡°ì§ ìµœì¢… ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                
                # ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ë° ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
                if not self.org_pdf.empty:
                    logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìµœì¢… í¬ê¸°: {self.org_pdf.shape}")
                    
                    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    required_columns = ['item_nm', 'item_id']
                    missing_columns = [col for col in required_columns if col not in self.org_pdf.columns]
                    if missing_columns:
                        logger.error(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                        logger.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤: {list(self.org_pdf.columns)}")
                    else:
                        logger.info("ëª¨ë“  í•„ìˆ˜ ì¡°ì§ ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    
                    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
                    if 'item_nm' in self.org_pdf.columns:
                        sample_orgs = self.org_pdf['item_nm'].dropna().head(5).tolist()
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ëª… ìƒ˜í”Œ: {sample_orgs}")
                    else:
                        logger.error("item_nm ì»¬ëŸ¼ì´ ì—†ì–´ ìƒ˜í”Œì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ í‘œì‹œ
                        sample_data = self.org_pdf.head(3).to_dict('records')
                        logger.info(f"DB ëª¨ë“œ ì¡°ì§ ë°ì´í„° ìƒ˜í”Œ: {sample_data}")
                else:
                    logger.warning("DBì—ì„œ ë¡œë“œëœ ì¡°ì§ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                
                logger.info(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.org_pdf)}ê°œ ì¡°ì§")
                
        except Exception as e:
            logger.error(f"DBì—ì„œ ì¡°ì§ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"DB ì¡°ì§ ë¡œë“œ ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            
            # ë¹ˆ DataFrameìœ¼ë¡œ fallback (ì¡°ì§ ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            self.org_pdf = pd.DataFrame(columns=['item_nm', 'item_id', 'item_desc', 'item_dmn'])
            logger.warning("ì¡°ì§ ë°ì´í„° DB ë¡œë“œ ì‹¤íŒ¨ë¡œ ë¹ˆ DataFrame ì‚¬ìš©")
            
            raise

    def _store_prompt_for_preview(self, prompt: str, prompt_type: str):
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ìš©ìœ¼ë¡œ ì €ì¥"""
        import threading
        current_thread = threading.current_thread()
        
        if not hasattr(current_thread, 'stored_prompts'):
            current_thread.stored_prompts = {}
        
        # í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ ì œëª©ê³¼ ì„¤ëª… ë§¤í•‘
        prompt_info = {
            "main_extraction": {
                'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                'description': 'ê´‘ê³  ë©”ì‹œì§€ì—ì„œ ì œëª©, ëª©ì , ìƒí’ˆ, ì±„ë„, í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            },
            "entity_extraction": {
                'title': 'ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸', 
                'description': 'ë©”ì‹œì§€ì—ì„œ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸'
            }
        }
        
        info = prompt_info.get(prompt_type, {
            'title': f'{prompt_type} í”„ë¡¬í”„íŠ¸',
            'description': f'{prompt_type} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'
        })
        
        prompt_key = f'{prompt_type}_prompt'
        prompt_data = {
            'title': info['title'],
            'description': info['description'],
            'content': prompt,
            'length': len(prompt)
        }
        
        current_thread.stored_prompts[prompt_key] = prompt_data
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        prompt_length = len(prompt)
        logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì €ì¥ë¨: {prompt_key}")
        logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length:,} ë¬¸ì")
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë§¤ìš° ê¸´ ê²½ìš° ê²½ê³ 
        if prompt_length > 20000:
            logger.warning(f"âš ï¸ ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸ê°€ ì €ì¥ë¨: {prompt_length:,} ë¬¸ì")
            logger.warning("ì´ëŠ” UI í‘œì‹œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë¶„ì„ (ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ì¸ ê²½ìš°)
            if 'entity' in prompt_key.lower():
                entity_section_start = prompt.find("## Candidate entities:")
                if entity_section_start > 0:
                    entity_section = prompt[entity_section_start:]
                    entity_lines = entity_section.split('\n')
                    entity_count = len([line for line in entity_lines if line.strip().startswith('-')])
                    logger.warning(f"ğŸ” í›„ë³´ ì—”í‹°í‹° ê°œìˆ˜: {entity_count}ê°œ")
        
        logger.info(f"ğŸ“ í˜„ì¬ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(current_thread.stored_prompts)}")
        logger.info(f"ğŸ“ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í‚¤ë“¤: {list(current_thread.stored_prompts.keys())}")

    def _safe_llm_invoke(self, prompt: str, max_retries: int = 3) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì†Œë“œ"""
        for attempt in range(max_retries):
            try:
                # LLM í˜¸ì¶œ
                response = self.llm_model.invoke(prompt)
                result_text = response.content if hasattr(response, 'content') else str(response)
                
                # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
                json_objects_list = extract_json_objects(result_text)
                if json_objects_list:
                    json_objects = json_objects_list[-1]
                    if self._detect_schema_response(json_objects):
                        logger.warning(f"ì‹œë„ {attempt + 1}: LLMì´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        
                        # ìŠ¤í‚¤ë§ˆ ì‘ë‹µì¸ ê²½ìš° ë” ê°•í•œ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì¬ì‹œë„
                        if attempt < max_retries - 1:
                            enhanced_prompt = self._enhance_prompt_for_retry(prompt)
                            response = self.llm_model.invoke(enhanced_prompt)
                            result_text = response.content if hasattr(response, 'content') else str(response)
                
                return result_text
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e}")
                    return self._fallback_extraction(prompt)
                else:
                    logger.warning(f"LLM í˜¸ì¶œ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return ""

    def _enhance_prompt_for_retry(self, original_prompt: str) -> str:
        """ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”"""
        return enhance_prompt_for_retry(original_prompt)

    def _fallback_extraction(self, prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ fallback ì¶”ì¶œ ë¡œì§"""
        logger.info("Fallback ì¶”ì¶œ ë¡œì§ ì‹¤í–‰")
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì—ì„œ fallback ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        fallback_result = get_fallback_result()
        
        return json.dumps(fallback_result, ensure_ascii=False)

    @log_performance
    def extract_entities_from_kiwi(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            logger.info("=== Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘ ===")
            mms_msg = validate_text_input(mms_msg)
            logger.info(f"ì²˜ë¦¬í•  ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # ìƒí’ˆ ë°ì´í„° ìƒíƒœ í™•ì¸
            if self.item_pdf_all.empty:
                logger.error("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë§¤ì¹­í•  ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # ë¬¸ì¥ ë¶„í•  ë° í•˜ìœ„ ë¬¸ì¥ ì²˜ë¦¬
            sentences = sum(self.kiwi.split_into_sents(
                re.split(r"_+", mms_msg), return_tokens=True, return_sub_sents=True
            ), [])
            
            sentences_all = []
            for sent in sentences:
                if sent.subs:
                    sentences_all.extend(sent.subs)
                else:
                    sentences_all.append(sent)
            
            logger.info(f"ë¶„í• ëœ ë¬¸ì¥ ìˆ˜: {len(sentences_all)}ê°œ")
            
            # ì œì™¸ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ í•„í„°ë§
            sentence_list = [
                filter_text_by_exc_patterns(sent, self.exc_tag_patterns) 
                for sent in sentences_all
            ]
            
            logger.info(f"í•„í„°ë§ëœ ë¬¸ì¥ë“¤: {sentence_list[:3]}...")  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…

            # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
            result_msg = self.kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)
            all_tokens = [(token.form, token.tag) for token in result_msg]
            logger.info(f"ì „ì²´ í† í° ìˆ˜: {len(all_tokens)}ê°œ")
            
            # NNP íƒœê·¸ í† í°ë“¤ë§Œ ì¶”ì¶œ
            nnp_tokens = [token.form for token in result_msg if token.tag == 'NNP']
            logger.info(f"NNP íƒœê·¸ í† í°ë“¤: {nnp_tokens}")
            
            entities_from_kiwi = [
                token.form for token in result_msg 
                if token.tag == 'NNP' and 
                   token.form not in self.stop_item_names + ['-'] and 
                   len(token.form) >= 2 and 
                   not token.form.lower() in self.stop_item_names
            ]
            entities_from_kiwi = [e for e in filter_specific_terms(entities_from_kiwi) if e in unique_aliases]
            
            logger.info(f"í•„í„°ë§ í›„ Kiwi ì¶”ì¶œ ì—”í‹°í‹°: {list(set(entities_from_kiwi))}")

            # í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ìœ ì‚¬ ìƒí’ˆëª… ì°¾ê¸°
            logger.info("í¼ì§€ ë§¤ì¹­ ì‹œì‘...")
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                sentence_list, 
                unique_aliases,
                threshold=getattr(PROCESSING_CONFIG, 'fuzzy_threshold', 0.5),
                text_col_nm='sent', 
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            logger.info(f"í¼ì§€ ë§¤ì¹­ ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if similarities_fuzzy.empty:
                logger.warning("í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©
                cand_item_list = list(entities_from_kiwi) if entities_from_kiwi else []
                logger.info(f"Kiwi ê¸°ë°˜ í›„ë³´ ì•„ì´í…œ: {cand_item_list}")
                
                if cand_item_list:
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                    logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
                else:
                    extra_item_pdf = pd.DataFrame()
                    logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                
                return cand_item_list, extra_item_pdf
            else:
                logger.info(f"í¼ì§€ ë§¤ì¹­ ì„±ê³µ: {len(similarities_fuzzy)}ê°œ ê²°ê³¼")
                if not similarities_fuzzy.empty:
                    sample_fuzzy = similarities_fuzzy.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                    logger.info(f"í¼ì§€ ë§¤ì¹­ ìƒ˜í”Œ: {sample_fuzzy}")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ í†µí•œ ì •ë°€ ë§¤ì¹­
            logger.info("ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")
            similarities_seq = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='sent',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )
            
            logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê²°ê³¼ í¬ê¸°: {similarities_seq.shape if not similarities_seq.empty else 'ë¹„ì–´ìˆìŒ'}")
            if not similarities_seq.empty:
                sample_seq = similarities_seq.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ìƒ˜í”Œ: {sample_seq}")
            
            # ì„ê³„ê°’ ì´ìƒì˜ í›„ë³´ ì•„ì´í…œë“¤ í•„í„°ë§
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold', 0.2)
            logger.info(f"ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
            
            cand_items = similarities_seq.query(
                "sim >= @similarity_threshold and "
                "item_nm_alias.str.contains('', case=False) and "
                "item_nm_alias not in @self.stop_item_names"
            )
            logger.info(f"ì„ê³„ê°’ í•„í„°ë§ í›„ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_items)}ê°œ")
            
            # Kiwiì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ ì¶”ê°€
            entities_from_kiwi_pdf = self.item_pdf_all.query("item_nm_alias in @entities_from_kiwi")[
                ['item_nm','item_nm_alias']
            ]
            entities_from_kiwi_pdf['sim'] = 1.0
            logger.info(f"Kiwi ì—”í‹°í‹° ë§¤ì¹­ ê²°ê³¼: {len(entities_from_kiwi_pdf)}ê°œ")

            # ê²°ê³¼ í†µí•© ë° ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            cand_item_pdf = pd.concat([cand_items, entities_from_kiwi_pdf])
            logger.info(f"í†µí•©ëœ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_item_pdf)}ê°œ")
            
            if not cand_item_pdf.empty:
                cand_item_array = cand_item_pdf.sort_values('sim', ascending=False).groupby([
                    "item_nm_alias"
                ])['sim'].max().reset_index(name='final_sim').sort_values(
                    'final_sim', ascending=False
                ).query("final_sim >= 0.2")['item_nm_alias'].unique()
                
                # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „ì„± ë³´ì¥
                cand_item_list = list(cand_item_array) if hasattr(cand_item_array, '__iter__') else []
                
                logger.info(f"ìµœì¢… í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                if cand_item_list:  # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¿¼ë¦¬ ì‹¤í–‰
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                else:
                    extra_item_pdf = pd.DataFrame()
                
                logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ DataFrame í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    sample_final = extra_item_pdf.head(3).to_dict('records')
                    logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ìƒ˜í”Œ: {sample_final}")
            else:
                logger.warning("í†µí•©ëœ í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                cand_item_list = []
                extra_item_pdf = pd.DataFrame()

            return entities_from_kiwi, cand_item_list, extra_item_pdf
            
        except Exception as e:
            logger.error(f"Kiwi ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜ - ë¹ˆ ë¦¬ìŠ¤íŠ¸ì™€ ë¹ˆ DataFrame
            return [], [], pd.DataFrame()

    def extract_entities_by_logic(self, cand_entities: List[str], threshold_for_fuzzy: float = 0.5) -> pd.DataFrame:
        """ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            if not cand_entities:
                return pd.DataFrame()
            
            # í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entities,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=threshold_for_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _calculate_combined_similarity(self, similarities_fuzzy: pd.DataFrame, weights: dict = None) -> pd.DataFrame:
        """s1, s2 ì •ê·œí™” ë°©ì‹ìœ¼ë¡œ ê°ê° ê³„ì‚° í›„ í•©ì‚°"""
        try:
            # s1 ì •ê·œí™”
            sim_s1 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s1',
                # weights=weights,
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s2',
                # weights=weights,
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s2'})
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            if not sim_s1.empty and not sim_s2.empty:
                try:
                    # ipynbì™€ ë™ì¼í•˜ê²Œ merge í›„ ì¿¼ë¦¬ ì¡°ê±´ ì ìš©
                    combined = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
                    # ipynbì™€ ë™ì¼í•œ í•„í„°ë§ ì¡°ê±´: (sim_s1>=0.4 and sim_s2>=0.4) or (sim_s1>=1.9 and sim_s2>=0.3) or (sim_s1>=0.3 and sim_s2>=0.9)
                    filtered = combined.query("(sim_s1>=0.4 and sim_s2>=0.4) or (sim_s1>=1.9 and sim_s2>=0.3) or (sim_s1>=0.3 and sim_s2>=0.9)")
                    # sim_s1ê³¼ sim_s2ë¥¼ ê°ê° í•©ì‚°í•œ í›„ ë”í•˜ê¸° (ipynbì™€ ë™ì¼)
                    combined = filtered.groupby(['item_name_in_msg', 'item_nm_alias']).agg({
                        'sim_s1': 'sum',
                        'sim_s2': 'sum'
                    }).reset_index()
                    combined['sim'] = combined['sim_s1'] + combined['sim_s2']
                except Exception as e:
                    logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    return pd.DataFrame()
                return combined
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    @log_performance
    def extract_entities_by_llm(self, msg_text: str, rank_limit: int = 200, llm_models: List = None, external_cand_entities: List[str] = []) -> pd.DataFrame:
        """
        LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ (ë³µìˆ˜ ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
        
        Args:
            msg_text (str): ë¶„ì„í•  ë©”ì‹œì§€ í…ìŠ¤íŠ¸
            rank_limit (int): ê²°ê³¼ì—ì„œ ë°˜í™˜í•  ìµœëŒ€ ìˆœìœ„
            llm_models (List, optional): ì‚¬ìš©í•  LLM ëª¨ë¸ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            
        Returns:
            pd.DataFrame: ì¶”ì¶œëœ ì—”í‹°í‹°ì™€ ìœ ì‚¬ë„ ì •ë³´
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ” [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì‹œì‘")
            logger.info(f"ğŸ“ ì…ë ¥ íŒŒë¼ë¯¸í„°:")
            logger.info(f"   - rank_limit: {rank_limit}")
            logger.info(f"   - external_cand_entities ì œê³µ ì—¬ë¶€: {external_cand_entities is not None}")
            if external_cand_entities is not None:
                logger.info(f"   - external_cand_entities ê°œìˆ˜: {len(external_cand_entities)}")
            
            msg_text = validate_text_input(msg_text)
            logger.info(f"ğŸ“„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(msg_text):,} ë¬¸ì")
            logger.info(f"ğŸ“„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {msg_text[:100]}..." if len(msg_text) > 100 else f"ğŸ“„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸: {msg_text}")
            
            # LLM ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            if llm_models is None:
                llm_models = [self.llm_model]
                logger.info(f"ğŸ¤– LLM ëª¨ë¸ ìë™ ì„ íƒ: ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (1ê°œ)")
            else:
                logger.info(f"ğŸ¤– LLM ëª¨ë¸ ì§€ì •ë¨: {len(llm_models)}ê°œ ëª¨ë¸ ì‚¬ìš©")
            
            for idx, model in enumerate(llm_models):
                model_name = getattr(model, 'model_name', 'Unknown')
                logger.info(f"   [{idx+1}] ëª¨ë¸: {model_name}")
            
            def get_entities_by_llm(args_dict):
                """ë‹¨ì¼ LLMìœ¼ë¡œ ì—”í‹°í‹° ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
                llm_model, msg_text = args_dict['llm_model'], args_dict['msg_text']
                model_name = getattr(llm_model, 'model_name', 'Unknown')
                
                try:
                    logger.info(f"   âš™ï¸  [{model_name}] ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘")
                    
                    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼
                    base_prompt = getattr(PROCESSING_CONFIG, 'entity_extraction_prompt', None)
                    if base_prompt is None:
                        base_prompt = DETAILED_ENTITY_EXTRACTION_PROMPT
                        logger.info(f"   ğŸ“‹ [{model_name}] ì—”í‹°í‹° ì¶”ì¶œì— prompts ë””ë ‰í† ë¦¬ì˜ DETAILED_ENTITY_EXTRACTION_PROMPT ì‚¬ìš©")
                    else:
                        logger.info(f"   ğŸ“‹ [{model_name}] ì—”í‹°í‹° ì¶”ì¶œì— settings.pyì˜ entity_extraction_prompt ì‚¬ìš©")
                    
                    # ë² ì´ìŠ¤ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸
                    base_prompt_length = len(base_prompt)
                    msg_length = len(msg_text)
                    logger.info(f"   ğŸ“ [{model_name}] ë² ì´ìŠ¤ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {base_prompt_length:,} ë¬¸ì")
                    logger.info(f"   ğŸ“ [{model_name}] ë©”ì‹œì§€ ê¸¸ì´: {msg_length:,} ë¬¸ì")
                    
                    # í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë¡œê¹… (ì „ì²´)
                    logger.info(f"   ğŸ“ [{model_name}] ë² ì´ìŠ¤ í”„ë¡¬í”„íŠ¸ ë‚´ìš© (ì „ì²´):")
                    logger.info(f"   {'-' * 75}")
                    for line in base_prompt.split('\n'):
                        logger.info(f"   {line}")
                    logger.info(f"   {'-' * 75}")
                    
                    # PromptTemplate ì‚¬ìš© (langchain ë°©ì‹)
                    zero_shot_prompt = PromptTemplate(
                        input_variables=["entity_extraction_prompt", "msg", "cand_entities"],
                        template="""
                        {entity_extraction_prompt}
                        
                        ## message:                
                        {msg}
                        """
                    )
                    
                    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‹¤ì œë¡œ LLMì— ì „ë‹¬ë˜ëŠ” í”„ë¡¬í”„íŠ¸)
                    final_prompt_for_llm = zero_shot_prompt.format(
                        entity_extraction_prompt=base_prompt,
                        msg=msg_text
                    )
                    final_prompt_length = len(final_prompt_for_llm)
                    logger.info(f"   ğŸ“ [{model_name}] ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {final_prompt_length:,} ë¬¸ì")
                    logger.info(f"   ğŸ“ [{model_name}] ìµœì¢… í”„ë¡¬í”„íŠ¸ ë‚´ìš© (ì „ì²´):")
                    logger.info(f"   {'-' * 75}")
                    for line in final_prompt_for_llm.split('\n'):
                        logger.info(f"   {line}")
                    logger.info(f"   {'-' * 75}")

                    logger.info(f"   ğŸš€ [{model_name}] LLM í˜¸ì¶œ ì‹œì‘...")
                    chain = zero_shot_prompt | llm_model
                    cand_entities = chain.invoke({
                        "entity_extraction_prompt": base_prompt, 
                        "msg": msg_text, 
                    }).content
                    logger.info(f"   âœ… [{model_name}] LLM í˜¸ì¶œ ì™„ë£Œ")
                    logger.info(f"   ğŸ“¥ [{model_name}] LLM ì‘ë‹µ ê¸¸ì´: {len(cand_entities):,} ë¬¸ì")
                    logger.info(f"   ğŸ“¥ [{model_name}] LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {cand_entities[:200]}..." if len(cand_entities) > 200 else f"   ğŸ“¥ [{model_name}] LLM ì‘ë‹µ: {cand_entities}")

                    # LLM ì‘ë‹µ íŒŒì‹± ë° ì •ë¦¬
                    logger.info(f"   ğŸ”§ [{model_name}] ì—”í‹°í‹° íŒŒì‹± ì‹œì‘...")
                    cand_entity_list_raw = [e.strip() for e in cand_entities.split(',') if e.strip()]
                    logger.info(f"   ğŸ“Š [{model_name}] ì½¤ë§ˆë¡œ ë¶„í•  í›„ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list_raw)}ê°œ")
                    
                    before_filter = len(cand_entity_list_raw)
                    cand_entity_list = [e for e in cand_entity_list_raw if e not in self.stop_item_names and len(e) >= 2]
                    after_filter = len(cand_entity_list)
                    filtered_count = before_filter - after_filter
                    
                    logger.info(f"   ğŸ¯ [{model_name}] í•„í„°ë§ ê²°ê³¼:")
                    logger.info(f"      - í•„í„°ë§ ì „: {before_filter}ê°œ")
                    logger.info(f"      - í•„í„°ë§ í›„: {after_filter}ê°œ (ì œê±°: {filtered_count}ê°œ)")
                    logger.info(f"      - ìµœì¢… ì—”í‹°í‹°: {cand_entity_list[:10]}..." if len(cand_entity_list) > 10 else f"      - ìµœì¢… ì—”í‹°í‹°: {cand_entity_list}")

                    return cand_entity_list
                    
                except Exception as e:
                    logger.error(f"   âŒ [{model_name}] LLM ëª¨ë¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    logger.error(f"   âŒ [{model_name}] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    return []
            
            # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ (ë””ë²„ê¹…ìš©) - ë³µìˆ˜ ëª¨ë¸ì´ì–´ë„ í”„ë¡¬í”„íŠ¸ëŠ” ë™ì¼í•˜ë¯€ë¡œ í•­ìƒ ì €ì¥
            logger.info("ğŸ“‹ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ ì¤‘...")
            base_prompt = getattr(PROCESSING_CONFIG, 'entity_extraction_prompt', None)
            if base_prompt is None:
                base_prompt = DETAILED_ENTITY_EXTRACTION_PROMPT
            preview_prompt = build_entity_extraction_prompt(msg_text, base_prompt)
            
            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸
            final_prompt_length = len(preview_prompt)
            logger.info(f"ğŸ“ ìµœì¢… ì—”í‹°í‹° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {final_prompt_length:,} ë¬¸ì")
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ë‚´ìš© (ì „ì²´):")
            logger.info("-" * 80)
            for line in preview_prompt.split('\n'):
                logger.info(f"   {line}")
            logger.info("-" * 80)
            
            self._store_prompt_for_preview(preview_prompt, "entity_extraction")
            logger.info("âœ… í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ ì™„ë£Œ")

            
            logger.info("ğŸ”„ LLM ì§ì ‘ ì¶”ì¶œ ëª¨ë“œ")
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ êµ¬ì„± (ë‹¨ì¼/ë³µìˆ˜ ëª¨ë¸ ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
            batches = []
            for llm_model in llm_models:
                batches.append({
                    "msg_text": msg_text, 
                    "llm_model": llm_model, 
                })
            
            logger.info(f"ğŸ”„ {len(llm_models)}ê°œ LLM ëª¨ë¸ë¡œ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘")
            logger.info(f"ğŸ”„ ë³‘ë ¬ ì‘ì—… ìˆ˜: {len(batches)}ê°œ ë°°ì¹˜")
            
            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
            n_jobs = min(3, len(llm_models))  # ìµœëŒ€ 3ê°œ ì‘ì—…ìœ¼ë¡œ ì œí•œ
            logger.info(f"âš™ï¸  ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: {n_jobs}ê°œ ì›Œì»¤ (threading ë°±ì—”ë“œ)")
            
            with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                batch_results = parallel(delayed(get_entities_by_llm)(args) for args in batches)
            
            logger.info(f"âœ… ëª¨ë“  LLM ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ëª¨ë¸ë³„ ê²°ê³¼:")
            for idx, (model, result) in enumerate(zip(llm_models, batch_results)):
                model_name = getattr(model, 'model_name', 'Unknown')
                logger.info(f"   [{idx+1}] {model_name}: {len(result)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ")
            
            # ëª¨ë“  ê²°ê³¼ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
            all_entities = sum(batch_results, [])
            if external_cand_entities is not None and len(external_cand_entities)>0:
                all_entities = list(set(all_entities+external_cand_entities))
            logger.info(f"ğŸ“Š ë³‘í•© ì „ ì´ ì—”í‹°í‹° ìˆ˜: {len(all_entities)}ê°œ")
            cand_entity_list = list(set(all_entities))
            cand_entity_list = list(set(sum([[c['text'] for c in extract_ngram_candidates(cand_entity, min_n=2, max_n=len(cand_entity.split())) if c['start_idx']<=0] if len(cand_entity.split())>=4 else [cand_entity] for cand_entity in cand_entity_list], [])))
            logger.info(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            logger.info(f"âœ… LLM ì¶”ì¶œ ì™„ë£Œ: {cand_entity_list[:20]}..." if len(cand_entity_list) > 20 else f"âœ… LLM ì¶”ì¶œ ì™„ë£Œ: {cand_entity_list}")

            if not cand_entity_list:
                logger.warning("âš ï¸  LLM ì¶”ì¶œì—ì„œ ìœ íš¨í•œ ì—”í‹°í‹°ë¥¼ ì°¾ì§€ ëª»í•¨")
                logger.info("=" * 80)
                return pd.DataFrame()
            
            # cand_entity_list = select_most_comprehensive(cand_entity_list)
            logger.info("ğŸ” ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹œì‘...")
            logger.info(f"   ì…ë ¥ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            cand_entities_sim = self._match_entities_with_products(cand_entity_list, rank_limit)
            logger.info(f"   ë§¤ì¹­ ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰")
            
            if cand_entities_sim.empty:
                logger.warning("âš ï¸  ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                logger.info("=" * 80)
                return pd.DataFrame()
            
            # [ë‹¨ê³„ 1] ë§¤ì¹­ ì™„ë£Œ ì§í›„ item_name_in_msg ë¡œê¹…
            logger.info(f"   [ë‹¨ê³„ 1] ë§¤ì¹­ ì™„ë£Œ ì§í›„ item_name_in_msg:")
            logger.info(f"      - ê³ ìœ  ê°œìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"      - ì „ì²´ ê°œìˆ˜: {len(cand_entities_sim)}ê°œ")
            item_name_list_1 = list(cand_entities_sim['item_name_in_msg'].unique())
            logger.info(f"      - ê³ ìœ  item_name_in_msg ëª©ë¡: {item_name_list_1}")
            
            logger.info(f"   ë§¤ì¹­ëœ ê³ ìœ  item_name_in_msg ìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"   ë§¤ì¹­ëœ ê³ ìœ  item_nm_alias ìˆ˜: {cand_entities_sim['item_nm_alias'].nunique()}ê°œ")

            # í›„ë³´ ì—”í‹°í‹°ë“¤ê³¼ ìƒí’ˆ DB ë§¤ì¹­
            logger.info("ğŸ” 2ë‹¨ê³„ LLM í•„í„°ë§ ì‹œì‘...")
            logger.info(f"   ì…ë ¥ ë©”ì‹œì§€ ì—”í‹°í‹° ìˆ˜: {len(cand_entities_sim['item_name_in_msg'].unique())}ê°œ")
            logger.info(f"   í›„ë³´ ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(cand_entities_sim['item_nm_alias'].unique())}ê°œ")
            
            # [ë‹¨ê³„ 2] 2ë‹¨ê³„ LLM í•„í„°ë§ ì‹œì‘ ì „ item_name_in_msg ë¡œê¹…
            logger.info(f"   [ë‹¨ê³„ 2] 2ë‹¨ê³„ LLM í•„í„°ë§ ì‹œì‘ ì „ item_name_in_msg:")
            logger.info(f"      - ê³ ìœ  ê°œìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"      - ì „ì²´ ê°œìˆ˜: {len(cand_entities_sim)}ê°œ")
            item_name_list_2 = list(cand_entities_sim['item_name_in_msg'].unique())
            logger.info(f"      - ê³ ìœ  item_name_in_msg ëª©ë¡: {item_name_list_2}")
            
            # SIMPLE_ENTITY_EXTRACTION_PROMPT ë¡œê¹…
            simple_prompt_length = len(SIMPLE_ENTITY_EXTRACTION_PROMPT)
            logger.info(f"   ğŸ“ SIMPLE_ENTITY_EXTRACTION_PROMPT ê¸¸ì´: {simple_prompt_length:,} ë¬¸ì")
            logger.info(f"   ğŸ“ SIMPLE_ENTITY_EXTRACTION_PROMPT ë‚´ìš© (ì „ì²´):")
            logger.info(f"   {'-' * 75}")
            for line in SIMPLE_ENTITY_EXTRACTION_PROMPT.split('\n'):
                logger.info(f"   {line}")
            logger.info(f"   {'-' * 75}")
            
            zero_shot_prompt = PromptTemplate(
            input_variables=["msg","entities_msg","cand_entities_voca"],
            template="""
            {entity_extraction_prompt}
            
            ## message:                
            {msg}

            ## entities in message:
            {entities_msg}

            ## candidate entities in vocabulary:
            {cand_entities_voca}

            """
            )
            
            # 2ë‹¨ê³„ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
            entities_msg_list = list(cand_entities_sim['item_name_in_msg'].unique())
            cand_entities_voca_list = list(cand_entities_sim['item_nm_alias'].unique())
            
            logger.info(f"   ğŸ“ ì…ë ¥ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ (ì²˜ìŒ 20ê°œ): {entities_msg_list[:20]}..." if len(entities_msg_list) > 20 else f"   ğŸ“ ì…ë ¥ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸: {entities_msg_list}")
            logger.info(f"   ğŸ“ í›„ë³´ ìƒí’ˆ ë³„ì¹­ ë¦¬ìŠ¤íŠ¸ (ì²˜ìŒ 20ê°œ): {cand_entities_voca_list[:20]}..." if len(cand_entities_voca_list) > 20 else f"   ğŸ“ í›„ë³´ ìƒí’ˆ ë³„ì¹­ ë¦¬ìŠ¤íŠ¸: {cand_entities_voca_list}")
            
            final_prompt_2nd = zero_shot_prompt.format(
                entity_extraction_prompt=SIMPLE_ENTITY_EXTRACTION_PROMPT,
                msg=msg_text,
                entities_msg=entities_msg_list,
                cand_entities_voca=cand_entities_voca_list
            )
            final_prompt_2nd_length = len(final_prompt_2nd)
            logger.info(f"   ğŸ“ 2ë‹¨ê³„ ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {final_prompt_2nd_length:,} ë¬¸ì")
            logger.info(f"   ğŸ“ 2ë‹¨ê³„ ìµœì¢… í”„ë¡¬í”„íŠ¸ ë‚´ìš© (ì „ì²´):")
            logger.info(f"   {'-' * 75}")
            for line in final_prompt_2nd.split('\n'):
                logger.info(f"   {line}")
            logger.info(f"   {'-' * 75}")
                        
            logger.info("ğŸš€ 2ë‹¨ê³„ LLM í˜¸ì¶œ ì‹œì‘...")
            chain = zero_shot_prompt | self.llm_model
            cand_entities = chain.invoke({"entity_extraction_prompt": SIMPLE_ENTITY_EXTRACTION_PROMPT, "msg": msg_text, "entities_msg":cand_entities_sim['item_name_in_msg'].unique(), "cand_entities_voca":cand_entities_sim['item_nm_alias'].unique()}).content
            logger.info("âœ… 2ë‹¨ê³„ LLM í˜¸ì¶œ ì™„ë£Œ")
            logger.info(f"ğŸ“¥ 2ë‹¨ê³„ LLM ì‘ë‹µ ê¸¸ì´: {len(cand_entities):,} ë¬¸ì")
            logger.info(f"ğŸ“¥ 2ë‹¨ê³„ LLM ì‘ë‹µ: {cand_entities}")

            logger.info("ğŸ”§ 2ë‹¨ê³„ ì—”í‹°í‹° íŒŒì‹± ì‹œì‘...")
            cand_entity_list = [e.strip() for e in cand_entities.split("\n")[-1].replace("ENTITY: ","").split(',') if e.strip()]
            logger.info(f"   íŒŒì‹± ì§í›„ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            
            before_filter = len(cand_entity_list)
            cand_entity_list = [e for e in cand_entity_list if e not in self.stop_item_names and len(e)>=2]
            after_filter = len(cand_entity_list)
            
            logger.info(f"   í•„í„°ë§ ê²°ê³¼:")
            logger.info(f"      - í•„í„°ë§ ì „: {before_filter}ê°œ")
            logger.info(f"      - í•„í„°ë§ í›„: {after_filter}ê°œ (ì œê±°: {before_filter - after_filter}ê°œ)")
            logger.info(f"   ìµœì¢… ì„ íƒëœ ì—”í‹°í‹°: {cand_entity_list}")

            logger.info(f"ğŸ” ìµœì¢… ì—”í‹°í‹°ë¡œ í•„í„°ë§ ì¤‘...")
            logger.info(f"   í•„í„°ë§ ì „ í–‰ ìˆ˜: {len(cand_entities_sim)}ê°œ")
            
            # [ë‹¨ê³„ 3] ìµœì¢… í•„í„°ë§ ì „ item_name_in_msg ë¡œê¹…
            logger.info(f"   [ë‹¨ê³„ 3] ìµœì¢… í•„í„°ë§ ì „ item_name_in_msg:")
            logger.info(f"      - ê³ ìœ  ê°œìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"      - ì „ì²´ ê°œìˆ˜: {len(cand_entities_sim)}ê°œ")
            item_name_list_3 = list(cand_entities_sim['item_name_in_msg'].unique())
            logger.info(f"      - ê³ ìœ  item_name_in_msg ëª©ë¡: {item_name_list_3}")
            
            cand_entities_sim = cand_entities_sim.query("item_nm_alias in @cand_entity_list")
            logger.info(f"   í•„í„°ë§ í›„ í–‰ ìˆ˜: {len(cand_entities_sim)}ê°œ")
            
            # [ë‹¨ê³„ 4] ìµœì¢… í•„í„°ë§ í›„ item_name_in_msg ë¡œê¹…
            logger.info(f"   [ë‹¨ê³„ 4] ìµœì¢… í•„í„°ë§ í›„ item_name_in_msg:")
            logger.info(f"      - ê³ ìœ  ê°œìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"      - ì „ì²´ ê°œìˆ˜: {len(cand_entities_sim)}ê°œ")
            if not cand_entities_sim.empty:
                item_name_list_4 = list(cand_entities_sim['item_name_in_msg'].unique())
                logger.info(f"      - ê³ ìœ  item_name_in_msg ëª©ë¡: {item_name_list_4}")
            else:
                logger.info(f"      - ê³ ìœ  item_name_in_msg ëª©ë¡: [] (ë¹„ì–´ìˆìŒ)")
            
            logger.info("=" * 80)
            logger.info("âœ… [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰ ë°˜í™˜")
            logger.info("=" * 80)

            return cand_entities_sim
            
        except Exception as e:
            logger.error("=" * 80)
            logger.error("âŒ [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì‹¤íŒ¨")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            logger.error("=" * 80)
            return pd.DataFrame()

    def _match_entities_with_products(self, cand_entity_list: List[str], rank_limit: int) -> pd.DataFrame:
        """í›„ë³´ ì—”í‹°í‹°ë“¤ì„ ìƒí’ˆ DBì™€ ë§¤ì¹­ (ipynb ë¡œì§ê³¼ ë™ì¼)"""
        try:
            logger.info("   ğŸ” [ë§¤ì¹­] í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ì‹œì‘...")
            logger.info(f"   ğŸ“ ì…ë ¥ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            logger.info(f"   ğŸ“ ìƒí’ˆ DB ë³„ì¹­ ìˆ˜: {len(self.item_pdf_all['item_nm_alias'].unique()):,}ê°œ")
            logger.info(f"   âš™ï¸  í¼ì§€ ìœ ì‚¬ë„ ì„ê³„ê°’: 0.6")
            logger.info(f"   âš™ï¸  n_jobs: 6, batch_size: 30")
            
            # í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ (ipynbì™€ ë™ì¼í•˜ê²Œ ì§ì ‘ í˜¸ì¶œ)
            similarities_fuzzy = parallel_fuzzy_similarity(
                cand_entity_list,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=0.6,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30
            )
            
            logger.info(f"   âœ… í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ì™„ë£Œ: {len(similarities_fuzzy)}ê°œ í–‰")
            
            if similarities_fuzzy.empty:
                logger.warning("   âš ï¸  í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            
            logger.info(f"   ğŸ“Š í¼ì§€ ë§¤ì¹­ ê³ ìœ  ì—”í‹°í‹° ìˆ˜: {similarities_fuzzy['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"   ğŸ“Š í¼ì§€ ë§¤ì¹­ ê³ ìœ  ë³„ì¹­ ìˆ˜: {similarities_fuzzy['item_nm_alias'].nunique()}ê°œ")
            
            # ì •ì§€ì–´ í•„í„°ë§
            logger.info("   ğŸ” [ë§¤ì¹­] ì •ì§€ì–´ í•„í„°ë§...")
            before_stopwords = len(similarities_fuzzy)
            similarities_fuzzy = similarities_fuzzy[
                ~similarities_fuzzy['item_nm_alias'].isin(self.stop_item_names)
            ]
            after_stopwords = len(similarities_fuzzy)
            logger.info(f"   ğŸ“Š ì •ì§€ì–´ í•„í„°ë§ ê²°ê³¼: {before_stopwords}ê°œ â†’ {after_stopwords}ê°œ (ì œê±°: {before_stopwords - after_stopwords}ê°œ)")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ë§¤ì¹­ (ipynbì™€ ë™ì¼í•˜ê²Œ ë‘ ë²ˆ í˜¸ì¶œ)
            logger.info("   ğŸ” [ë§¤ì¹­] ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (s1, s2 ê°ê°)...")
            logger.info(f"   âš™ï¸  ipynbì™€ ë™ì¼í•˜ê²Œ weights=None, n_jobs=6, batch_size=30 ì‚¬ìš©")
            
            # s1 ì •ê·œí™”
            sim_s1 = parallel_seq_similarity(
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30,
                # weights=None,  # ipynbì™€ ë™ì¼í•˜ê²Œ weights ì—†ìŒ
                normalizaton_value='s1'
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = parallel_seq_similarity(
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30,
                # weights=None,  # ipynbì™€ ë™ì¼í•˜ê²Œ weights ì—†ìŒ
                normalizaton_value='s2'
            ).rename(columns={'sim': 'sim_s2'})
            
            logger.info(f"   âœ… ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: sim_s1={len(sim_s1)}ê°œ, sim_s2={len(sim_s2)}ê°œ")
            
            # mergeë¡œ í•©ì¹˜ê¸° (ipynbì™€ ë™ì¼)
            logger.info("   ğŸ” [ë§¤ì¹­] sim_s1ê³¼ sim_s2 ë³‘í•© ì¤‘...")
            cand_entities_sim = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
            logger.info(f"   âœ… ë³‘í•© ì™„ë£Œ: {len(cand_entities_sim)}ê°œ í–‰")
            
            if cand_entities_sim.empty:
                logger.warning("   âš ï¸  ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            
            logger.info(f"   ğŸ“Š ìœ ì‚¬ë„ í†µê³„:")
            logger.info(f"      - sim_s1 ìµœì†Œ: {cand_entities_sim['sim_s1'].min():.4f}")
            logger.info(f"      - sim_s1 ìµœëŒ€: {cand_entities_sim['sim_s1'].max():.4f}")
            logger.info(f"      - sim_s2 ìµœì†Œ: {cand_entities_sim['sim_s2'].min():.4f}")
            logger.info(f"      - sim_s2 ìµœëŒ€: {cand_entities_sim['sim_s2'].max():.4f}")
            
            # ipynbì™€ ë™ì¼í•œ í•„í„°ë§ ì¡°ê±´ ì ìš©
            logger.info(f"   ğŸ” [ë§¤ì¹­] ì¿¼ë¦¬ ì¡°ê±´ í•„í„°ë§...")
            logger.info(f"   âš™ï¸  ì¡°ê±´: (sim_s1>=0.4 and sim_s2>=0.4) or (sim_s1>=1.9 and sim_s2>=0.3) or (sim_s1>=0.3 and sim_s2>=0.9)")
            before_query = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query("(sim_s1>=0.4 and sim_s2>=0.4) or (sim_s1>=1.9 and sim_s2>=0.3) or (sim_s1>=0.3 and sim_s2>=0.9)")
            after_query = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ì¿¼ë¦¬ í•„í„°ë§ ê²°ê³¼: {before_query}ê°œ â†’ {after_query}ê°œ (ì œê±°: {before_query - after_query}ê°œ)")

            # ipynbì™€ ë™ì¼í•˜ê²Œ groupbyë¡œ í•©ì‚°
            logger.info(f"   ğŸ” [ë§¤ì¹­] sim_s1ê³¼ sim_s2 í•©ì‚° ì¤‘...")
            cand_entities_sim = cand_entities_sim.groupby(['item_name_in_msg', 'item_nm_alias'])[['sim_s1', 'sim_s2']].apply(
                lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()
            ).to_frame('sim').reset_index()
            logger.info(f"   âœ… í•©ì‚° ì™„ë£Œ: {len(cand_entities_sim)}ê°œ í–‰")
            
            # ipynbì™€ ë™ì¼í•˜ê²Œ sim>=1.1 í•„í„°ë§
            logger.info(f"   ğŸ” [ë§¤ì¹­] ìœ ì‚¬ë„ í•„í„°ë§ (ì„ê³„ê°’: sim>=1.0)...")
            before_sim_filter = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query("sim >= 1.0").copy()
            after_sim_filter = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ìœ ì‚¬ë„ í•„í„°ë§ ê²°ê³¼: {before_sim_filter}ê°œ â†’ {after_sim_filter}ê°œ (ì œê±°: {before_sim_filter - after_sim_filter}ê°œ)")
            
            logger.info(f"   ğŸ“Š í•©ì‚° sim í†µê³„:")
            logger.info(f"      - ìµœì†Œ: {cand_entities_sim['sim'].min():.4f}")
            logger.info(f"      - ìµœëŒ€: {cand_entities_sim['sim'].max():.4f}")
            logger.info(f"      - í‰ê· : {cand_entities_sim['sim'].mean():.4f}")
            logger.info(f"      - ì¤‘ì•™ê°’: {cand_entities_sim['sim'].median():.4f}")

            # ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ
            logger.info(f"   ğŸ” [ë§¤ì¹­] ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ (rank_limit: {rank_limit})...")
            cand_entities_sim["rank"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(
                method='dense', ascending=False
            )
            before_rank_limit = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query(f"rank <= {rank_limit}").sort_values(
                ['item_name_in_msg', 'rank'], ascending=[True, True]
            )
            after_rank_limit = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ìˆœìœ„ ì œí•œ ê²°ê³¼: {before_rank_limit}ê°œ â†’ {after_rank_limit}ê°œ (ì œê±°: {before_rank_limit - after_rank_limit}ê°œ)")
            
            # ipynbì™€ ë™ì¼í•˜ê²Œ rank ì œí•œ í›„ item_dmn_nm ë³‘í•©
            logger.info(f"   ğŸ” [ë§¤ì¹­] item_dmn_nm ë³‘í•© ì¤‘...")
            if 'item_dmn_nm' in self.item_pdf_all.columns:
                cand_entities_sim = cand_entities_sim.merge(
                    self.item_pdf_all[['item_nm_alias', 'item_dmn_nm']].drop_duplicates(),
                    on='item_nm_alias',
                    how='left'
                )
                logger.info(f"   âœ… item_dmn_nm ë³‘í•© ì™„ë£Œ")
            else:
                logger.warning(f"   âš ï¸  item_dmn_nm ì»¬ëŸ¼ì´ ì—†ì–´ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                logger.warning(f"   âš ï¸  item_pdf_all ì»¬ëŸ¼ ëª©ë¡: {list(self.item_pdf_all.columns)}")
            
            logger.info(f"   âœ… [ë§¤ì¹­] ìµœì¢… ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰, {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ ê³ ìœ  ì—”í‹°í‹°")

            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"   âŒ [ë§¤ì¹­] ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            logger.error(f"   âŒ [ë§¤ì¹­] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return pd.DataFrame()

    def _extract_entities(self, mms_msg: str) -> Tuple[List[str], List[str], pd.DataFrame]:
        """ì—”í‹°í‹° ì¶”ì¶œ (Kiwi ë˜ëŠ” LLM ë°©ì‹)"""
        try:
            if self.entity_extraction_mode == 'logic':
                # Kiwi ê¸°ë°˜ ì¶”ì¶œ
                return self.extract_entities_from_kiwi(mms_msg)
            else:
                # LLM ê¸°ë°˜ ì¶”ì¶œì„ ìœ„í•´ ë¨¼ì € Kiwië¡œ ê¸°ë³¸ ì¶”ì¶œ
                entities_from_kiwi, cand_item_list, extra_item_pdf = self.extract_entities_from_kiwi(mms_msg)
                return entities_from_kiwi, cand_item_list, extra_item_pdf
                
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return [], [], pd.DataFrame()

    def _classify_programs(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _build_extraction_prompt(self, msg: str, rag_context: str, product_element: Optional[List[Dict]]) -> str:
        """ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì‚¬ìš©"""
        
        # ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©
        prompt = build_extraction_prompt(
            message=msg,
            rag_context=rag_context,
            product_element=product_element,
            product_info_extraction_mode=self.product_info_extraction_mode
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (LLM ëª¨ë“œì—ì„œë§Œ)
        if self.product_info_extraction_mode == 'llm':
            logger.debug(f"LLM ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í›„ë³´ ìƒí’ˆ ëª©ë¡ í¬í•¨ ì—¬ë¶€: {'ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡' in rag_context}")
            
        return prompt

    def _extract_channels(self, json_objects: Dict, msg: str) -> List[Dict]:
        """ì±„ë„ ì •ë³´ ì¶”ì¶œ ë° ë§¤ì¹­"""
        try:
            channel_tag = []
            channel_items = json_objects.get('channel', [])
            if isinstance(channel_items, dict):
                channel_items = channel_items.get('items', [])

            for d in channel_items:
                if d.get('type') == 'ëŒ€ë¦¬ì ' and d.get('value'):
                    # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
                    store_info = self._match_store_info(d['value'])
                    d['store_info'] = store_info
                else:
                    d['store_info'] = []
                channel_tag.append(d)

            return channel_tag
            
        except Exception as e:
            logger.error(f"ì±„ë„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _match_store_info(self, store_name: str) -> List[Dict]:
        """ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­"""
        try:
            # ëŒ€ë¦¬ì ëª…ìœ¼ë¡œ ì¡°ì§ ì •ë³´ ê²€ìƒ‰
            org_pdf_cand = safe_execute(
                parallel_fuzzy_similarity,
                [preprocess_text(store_name.lower())],
                self.org_pdf['item_nm'].unique(),
                threshold=0.5,
                text_col_nm='org_nm_in_msg',
                item_col_nm='item_nm',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )

            if org_pdf_cand.empty:
                return []

            org_pdf_cand = org_pdf_cand.drop('org_nm_in_msg', axis=1)
            org_pdf_cand = self.org_pdf.merge(org_pdf_cand, on=['item_nm'])
            org_pdf_cand['sim'] = org_pdf_cand.apply(
                lambda x: combined_sequence_similarity(store_name, x['item_nm'])[0], axis=1
            ).round(5)
            
            # ëŒ€ë¦¬ì  ì½”ë“œ('D'ë¡œ ì‹œì‘) ìš°ì„  ê²€ìƒ‰
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold_for_store', 0.2)
            org_pdf_tmp = org_pdf_cand.query(
                "sim >= @similarity_threshold", engine='python'
            ).sort_values('sim', ascending=False)
            
            if org_pdf_tmp.empty:
                # ëŒ€ë¦¬ì ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ê²€ìƒ‰
                org_pdf_tmp = org_pdf_cand.query("sim >= @similarity_threshold").sort_values('sim', ascending=False)
            
            if not org_pdf_tmp.empty:
                # ìµœê³  ìˆœìœ„ ì¡°ì§ë“¤ì˜ ì •ë³´ ì¶”ì¶œ
                org_pdf_tmp['rank'] = org_pdf_tmp['sim'].rank(method='dense', ascending=False)
                org_pdf_tmp = org_pdf_tmp.rename(columns={'item_id':'org_cd','item_nm':'org_nm'})
                org_info = org_pdf_tmp.query("rank == 1").groupby('org_nm')['org_cd'].apply(list).reset_index(name='org_cd').to_dict('records')
                return org_info
            else:
                return []
                
        except Exception as e:
            logger.error(f"ëŒ€ë¦¬ì  ì •ë³´ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []

    def _validate_extraction_result(self, result: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'purpose', 'product', 'channel']
            for field in required_fields:
                if field not in result:
                    logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                    result[field] = [] if field != 'title' else "ê´‘ê³  ë©”ì‹œì§€"

            # ìƒí’ˆëª… ê¸¸ì´ ê²€ì¦
            validated_products = []
            for product in result.get('product', []):
                if isinstance(product, dict):
                    item_name = product.get('item_name_in_msg', product.get('name', ''))
                    if len(item_name) >= 2 and item_name not in self.stop_item_names:
                        validated_products.append(product)
                    else:
                        logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìƒí’ˆëª… ì œì™¸: {item_name}")
            
            result['product'] = validated_products

            # ì±„ë„ ì •ë³´ ê²€ì¦
            validated_channels = []
            for channel in result.get('channel', []):
                if isinstance(channel, dict) and channel.get('value'):
                    validated_channels.append(channel)
            
            result['channel'] = validated_channels

            return result
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return result

    @log_performance
    def process_message(self, mms_msg: str) -> Dict[str, Any]:
        """
        MMS ë©”ì‹œì§€ ì „ì²´ ì²˜ë¦¬ (ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜)
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        
        Returns:
            dict: ì¶”ì¶œëœ ì •ë³´ê°€ ë‹´ê¸´ JSON êµ¬ì¡°
        """
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ MMS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
            logger.info("=" * 60)
            logger.info(f"ë©”ì‹œì§€ ë‚´ìš©: {mms_msg[:200]}...")
            logger.info(f"ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # í˜„ì¬ ì„¤ì • ìƒíƒœ ë¡œê¹…
            logger.info("=== í˜„ì¬ ì¶”ì¶œê¸° ì„¤ì • ===")
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤: {self.offer_info_data_src}")
            logger.info(f"ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: {self.product_info_extraction_mode}")
            logger.info(f"ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“œ: {self.entity_extraction_mode}")
            logger.info(f"LLM ëª¨ë¸: {self.llm_model_name}")
            logger.info(f"ìƒí’ˆ ë°ì´í„° í¬ê¸°: {self.item_pdf_all.shape}")
            logger.info(f"í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ê¸°: {self.pgm_pdf.shape}")
            
            # ì…ë ¥ ê²€ì¦
            msg = validate_text_input(mms_msg)
            
            # 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ
            logger.info("=" * 30 + " 1ë‹¨ê³„: ì—”í‹°í‹° ì¶”ì¶œ " + "=" * 30)
            
            # DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ íŠ¹ë³„ ì§„ë‹¨ ì‹œì‘")
                logger.info(f"ìƒí’ˆ ë°ì´í„° ìƒíƒœ: {self.item_pdf_all.shape}")
                
                # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                required_columns = ['item_nm', 'item_id', 'item_nm_alias']
                missing_columns = [col for col in required_columns if col not in self.item_pdf_all.columns]
                if missing_columns:
                    logger.error(f"ğŸš¨ DB ëª¨ë“œì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                
                # ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if 'item_nm_alias' in self.item_pdf_all.columns:
                    null_aliases = self.item_pdf_all['item_nm_alias'].isnull().sum()
                    total_aliases = len(self.item_pdf_all)
                    logger.info(f"DB ëª¨ë“œ ë³„ì¹­ ë°ì´í„° í’ˆì§ˆ: {total_aliases - null_aliases}/{total_aliases} ìœ íš¨")
            
            entities_from_kiwi, cand_item_list, extra_item_pdf = self._extract_entities(msg)
            logger.info(f"ì¶”ì¶œëœ Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")
            logger.info(f"ì¶”ì¶œëœ í›„ë³´ ì—”í‹°í‹°: {cand_item_list}")
            logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
            
            # DB ëª¨ë“œì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ íŠ¹ë³„ ë¶„ì„
            if self.offer_info_data_src == "db":
                logger.info("ğŸ” DB ëª¨ë“œ ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ ë¶„ì„")
                # cand_item_listê°€ numpy ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•œ ê²€ì‚¬ ì‚¬ìš©
                if safe_check_empty(cand_item_list):
                    logger.error("ğŸš¨ DB ëª¨ë“œì—ì„œ í›„ë³´ ì—”í‹°í‹°ê°€ ì „í˜€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                    logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
                    logger.error("1. ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ìƒí’ˆì´ ì—†ìŒ")
                    logger.error("2. ë³„ì¹­ ê·œì¹™ ì ìš© ì‹¤íŒ¨")
                    logger.error("3. ìœ ì‚¬ë„ ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ")
                    logger.error("4. Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨")
            
            # 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜
            logger.info("=" * 30 + " 2ë‹¨ê³„: í”„ë¡œê·¸ë¨ ë¶„ë¥˜ " + "=" * 30)
            pgm_info = self._classify_programs(msg)
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ê²°ê³¼ í‚¤: {list(pgm_info.keys())}")
            
            # 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            logger.info("=" * 30 + " 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± " + "=" * 30)
            rag_context = f"\n### ê´‘ê³  ë¶„ë¥˜ ê¸°ì¤€ ì •ë³´ ###\n\t{pgm_info['pgm_cand_info']}" if self.num_cand_pgms > 0 else ""
            logger.info(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ (ëª¨ë“œë³„ ì²˜ë¦¬)
            logger.info("=" * 30 + " 4ë‹¨ê³„: ì œí’ˆ ì •ë³´ ì¤€ë¹„ " + "=" * 30)
            product_element = None
            
            # cand_item_listê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ ì•ˆì „í•˜ê²Œ ê²€ì‚¬
            if not safe_check_empty(cand_item_list):
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ í¬ê¸°: {len(cand_item_list)}ê°œ")
                logger.info(f"í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                # extra_item_pdf ìƒíƒœ í™•ì¸
                logger.info(f"extra_item_pdf í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    logger.info(f"extra_item_pdf ì»¬ëŸ¼ë“¤: {list(extra_item_pdf.columns)}")
                    logger.info(f"extra_item_pdf ìƒ˜í”Œ: {extra_item_pdf.head(2).to_dict('records')}")
                
                if self.product_info_extraction_mode == 'rag':
                    rag_context += f"\n\n### í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("RAG ëª¨ë“œ: í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'llm':
                    # LLM ëª¨ë“œì—ë„ í›„ë³´ ëª©ë¡ ì œê³µí•˜ì—¬ ì¼ê´€ì„± í–¥ìƒ
                    rag_context += f"\n\n### ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ì´ë¦„ ëª©ë¡ ###\n\t{cand_item_list}"
                    logger.info("LLM ëª¨ë“œ: ì°¸ê³ ìš© í›„ë³´ ìƒí’ˆ ëª©ë¡ì„ RAG ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€")
                elif self.product_info_extraction_mode == 'nlp':
                    if not extra_item_pdf.empty and 'item_nm' in extra_item_pdf.columns:
                        product_df = extra_item_pdf.rename(columns={'item_nm': 'name'}).query(
                            "not name in @self.stop_item_names"
                        )[['name']]
                        product_df['action'] = 'ê¸°íƒ€'
                        product_element = product_df.to_dict(orient='records') if product_df.shape[0] > 0 else None
                        logger.info(f"NLP ëª¨ë“œ: ì œí’ˆ ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ - {len(product_element) if product_element else 0}ê°œ")
                        if product_element:
                            logger.info(f"NLP ëª¨ë“œ ì œí’ˆ ìš”ì†Œ ìƒ˜í”Œ: {product_element[:2]}")
                    else:
                        logger.warning("NLP ëª¨ë“œ: extra_item_pdfê°€ ë¹„ì–´ìˆê±°ë‚˜ item_nm ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            else:
                logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                logger.warning("ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                logger.warning("1. ìƒí’ˆ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                logger.warning("2. ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨") 
                logger.warning("3. ìœ ì‚¬ë„ ë§¤ì¹­ ì„ê³„ê°’ ë¬¸ì œ")

            # 5ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹¤í–‰
            logger.info("=" * 30 + " 5ë‹¨ê³„: LLM í˜¸ì¶œ " + "=" * 30)
            prompt = self._build_extraction_prompt(msg, rag_context, product_element)
            logger.info(f"êµ¬ì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€: {'í›„ë³´ ìƒí’ˆ' in rag_context}")
            
            # í”„ë¡¬í”„íŠ¸ ì €ì¥ (ë””ë²„ê¹…/ë¯¸ë¦¬ë³´ê¸°ìš©)
            self._store_prompt_for_preview(prompt, "main_extraction")
            
            result_json_text = self._safe_llm_invoke(prompt)
            logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(result_json_text)} ë¬¸ì")
            logger.info(f"LLM ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {result_json_text[:500]}...")
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            logger.info("=" * 30 + " 6ë‹¨ê³„: JSON íŒŒì‹± " + "=" * 30)
            json_objects_list = extract_json_objects(result_json_text)
            logger.info(f"ì¶”ì¶œëœ JSON ê°ì²´ ìˆ˜: {len(json_objects_list)}ê°œ")
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ: {result_json_text}")
                return self._create_fallback_result(msg)
            
            json_objects = json_objects_list[-1]
            logger.info(f"íŒŒì‹±ëœ JSON ê°ì²´ í‚¤: {list(json_objects.keys())}")
            logger.info(f"íŒŒì‹±ëœ JSON ë‚´ìš©: {json_objects}")
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ë° ì²˜ë¦¬
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.error("ğŸš¨ LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤! ì‹¤ì œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                logger.error("ì¬ì‹œë„ ë˜ëŠ” fallback ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._create_fallback_result(msg)

            raw_result = copy.deepcopy(json_objects)
            
            # 7ë‹¨ê³„: ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±
            logger.info("=" * 30 + " 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„± " + "=" * 30)
            final_result = self._build_final_result(json_objects, msg, pgm_info, entities_from_kiwi)
            
            # 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
            logger.info("=" * 30 + " 8ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ " + "=" * 30)
            final_result = self._validate_extraction_result(final_result)

            # # DAG ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ (ì„ íƒì )
            # # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
            # # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
            # dag_section = ""
            # if self.extract_entity_dag:
            #     logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
            #     try:
            #         dag_start_time = time.time()
            #         # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
            #         extract_dag_result = extract_dag(DAGParser(), msg, self.llm_model)
            #         dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
            #         dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
            #         dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
                    
            #         # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
            #         dag_filename = f'dag_{sha256_hash(msg)}'
            #         create_dag_diagram(dag, filename=dag_filename)
            #         dag_processing_time = time.time() - dag_start_time
                    
            #         logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")
            #         logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
            #         logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
            #         if dag_section:
            #             logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
            #         else:
            #             logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                        
            #     except Exception as e:
            #         logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            #         dag_section = ""

            # # ìµœì¢… ê²°ê³¼ì— DAG ì •ë³´ ì¶”ê°€ (ë¹„ì–´ìˆì„ ìˆ˜ë„ ìˆìŒ)
            # final_result['entity_dag'] = sorted([d for d in dag_section.split('\n') if d!=''])
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½ ë¡œê¹…
            logger.info("=" * 60)
            logger.info("âœ… ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 60)
            logger.info(f"ì œëª©: {final_result.get('title', 'N/A')}")
            logger.info(f"ëª©ì : {final_result.get('purpose', [])}")
            logger.info(f"ìƒí’ˆ ìˆ˜: {len(final_result.get('product', []))}ê°œ")
            logger.info(f"ì±„ë„ ìˆ˜: {len(final_result.get('channel', []))}ê°œ")
            logger.info(f"í”„ë¡œê·¸ë¨ ìˆ˜: {len(final_result.get('pgm', []))}ê°œ")

            actual_prompts = get_stored_prompts_from_thread()

            return {"extracted_result": final_result, "raw_result": raw_result, "prompts": actual_prompts}
            
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_fallback_result(mms_msg)
    
    @log_performance
    def extract_json_objects_only(self, mms_msg: str) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ì—ì„œ 7ë‹¨ê³„(ì—”í‹°í‹° ë§¤ì¹­ ë° ìµœì¢… ê²°ê³¼ êµ¬ì„±) ì „ì˜ json_objectsë§Œ ì¶”ì¶œ
        
        Args:
            mms_msg: ì²˜ë¦¬í•  MMS ë©”ì‹œì§€
            
        Returns:
            Dict: LLMì´ ìƒì„±í•œ json_objects (ì—”í‹°í‹° ë§¤ì¹­ ì „)
        """
        try:
            msg = mms_msg.strip()
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì‹œì‘ - ë©”ì‹œì§€ ê¸¸ì´: {len(msg)}ì")
            
            # 1-4ë‹¨ê³„: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤
            pgm_info = self._prepare_program_classification(msg)
            
            # RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (product_info_extraction_modeê°€ 'rag'ì¸ ê²½ìš°)
            rag_context = ""
            if self.product_info_extraction_mode == 'rag':
                rag_context = self._prepare_rag_context(msg)
            
            # 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
            prompt = self._build_extraction_prompt(msg, pgm_info, rag_context)
            result_json_text = self._safe_llm_invoke(prompt)
            
            # 6ë‹¨ê³„: JSON íŒŒì‹±
            json_objects_list = extract_json_objects(result_json_text)
            
            if not json_objects_list:
                logger.warning("LLMì´ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {}
            
            json_objects = json_objects_list[-1]
            
            # ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€
            is_schema_response = self._detect_schema_response(json_objects)
            if is_schema_response:
                logger.warning("LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return {}
            
            logger.info(f"JSON ê°ì²´ ì¶”ì¶œ ì™„ë£Œ - í‚¤: {list(json_objects.keys())}")
            return json_objects
            
        except Exception as e:
            logger.error(f"JSON ê°ì²´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _prepare_program_classification(self, mms_msg: str) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì¤€ë¹„ (_classify_programs ë©”ì†Œë“œì™€ ë™ì¼)"""
        try:
            if self.emb_model is None or self.clue_embeddings.numel() == 0:
                return {"pgm_cand_info": "", "similarities": []}
            
            # ë©”ì‹œì§€ ì„ë² ë”© ë° í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ìœ ì‚¬ë„ ê³„ì‚°
            mms_embedding = self.emb_model.encode([mms_msg.lower()], convert_to_tensor=True, show_progress_bar=False)
            similarities = torch.nn.functional.cosine_similarity(mms_embedding, self.clue_embeddings, dim=1).cpu().numpy()
            
            # ìƒìœ„ í›„ë³´ í”„ë¡œê·¸ë¨ë“¤ ì„ ë³„
            pgm_pdf_tmp = self.pgm_pdf.copy()
            pgm_pdf_tmp['sim'] = similarities
            pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)
            
            pgm_cand_info = "\n\t".join(
                pgm_pdf_tmp.iloc[:self.num_cand_pgms][['pgm_nm','clue_tag']].apply(
                    lambda x: re.sub(r'\[.*?\]', '', x['pgm_nm']) + " : " + x['clue_tag'], axis=1
                ).to_list()
            )
            
            return {
                "pgm_cand_info": pgm_cand_info,
                "similarities": similarities,
                "pgm_pdf_tmp": pgm_pdf_tmp
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {"pgm_cand_info": "", "similarities": [], "pgm_pdf_tmp": pd.DataFrame()}

    def _detect_schema_response(self, json_objects: Dict) -> bool:
        """LLMì´ ìŠ¤í‚¤ë§ˆ ì •ì˜ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ ê°ì§€"""
        try:
            # purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            purpose = json_objects.get('purpose', {})
            if isinstance(purpose, dict) and 'type' in purpose and purpose.get('type') == 'array':
                logger.warning("purpose í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸  
            product = json_objects.get('product', {})
            if isinstance(product, dict) and 'type' in product and product.get('type') == 'array':
                logger.warning("product í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
            
            # channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ì¸ì§€ í™•ì¸
            channel = json_objects.get('channel', {})
            if isinstance(channel, dict) and 'type' in channel and channel.get('type') == 'array':
                logger.warning("channel í•„ë“œê°€ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¡œ ê°ì§€ë¨")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì‘ë‹µ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def convert_df_to_json_list(self, df: pd.DataFrame) -> List[Dict]:
        """
        DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜
        ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ: item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ëª¨ë“  item_name_in_msgë¥¼ ë°°ì—´ë¡œ ìˆ˜ì§‘
        
        Schema:
        {
            "item_nm": "ìƒí’ˆëª…",
            "item_id": ["ID1", "ID2"],
            "item_name_in_msg": ["ë©”ì‹œì§€ë‚´í‘œí˜„1", "ë©”ì‹œì§€ë‚´í‘œí˜„2"]
        }
        """
        result = []
        # item_nm ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
        grouped = df.groupby('item_nm')
        for item_nm, group in grouped:
            # ë©”ì¸ ì•„ì´í…œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            item_name_in_msg_raw = list(group['item_name_in_msg'].unique())
            item_dict = {
                'item_nm': item_nm,
                'item_id': list(group['item_id'].unique()),
                'item_name_in_msg': select_most_comprehensive(item_name_in_msg_raw)
            }
            result.append(item_dict)
        return result

    def _create_fallback_result(self, msg: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²°ê³¼ ìƒì„±"""
        return {
            "title": "ê´‘ê³  ë©”ì‹œì§€",
            "purpose": ["ì •ë³´ ì œê³µ"],
            "product": [],
            "channel": [],
            "pgm": []
        }

    def _build_final_result(self, json_objects: Dict, msg: str, pgm_info: Dict, entities_from_kiwi: List[str]) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ êµ¬ì„±"""
        try:
            final_result = json_objects.copy()
            
            # ìƒí’ˆ ì •ë³´ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            product_items = json_objects.get('product', [])
            if isinstance(product_items, dict):
                product_items = product_items.get('items', [])

            primary_llm_extracted_entities = [x.get('name', '') for x in product_items]
            logger.info(f"Primary LLM ì¶”ì¶œ ì—”í‹°í‹°: {primary_llm_extracted_entities}")
            logger.info(f"Kiwi ì—”í‹°í‹°: {entities_from_kiwi}")

            # ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            if self.entity_extraction_mode == 'logic':
                # ë¡œì§ ê¸°ë°˜: í¼ì§€ + ì‹œí€€ìŠ¤ ìœ ì‚¬ë„
                cand_entities = list(set(entities_from_kiwi+[item.get('name', '') for item in product_items if item.get('name')]))
                similarities_fuzzy = self.extract_entities_by_logic(cand_entities)
            else:
                # LLM ê¸°ë°˜: LLMì„ í†µí•œ ì—”í‹°í‹° ì¶”ì¶œ (ê¸°ë³¸ ëª¨ë¸ë“¤: ax=ax, cld=claude)
                default_llm_models = self._initialize_multiple_llm_models(['ax','gen'])
                similarities_fuzzy = self.extract_entities_by_llm(msg, llm_models=default_llm_models, external_cand_entities=entities_from_kiwi)

            # similarities_fuzzy = similarities_fuzzy[similarities_fuzzy.apply(lambda x: (x['item_nm_alias'].replace(' ', '').lower() in x['item_name_in_msg'].replace(' ', '').lower() or x['item_name_in_msg'].replace(' ', '').lower() in x['item_nm_alias'].replace(' ', '').lower()) , axis=1)]
            merged_df = similarities_fuzzy.merge(
                self.alias_pdf_raw[['alias_1','type']].drop_duplicates(), 
                left_on='item_name_in_msg', 
                right_on='alias_1', 
                how='left'
            )

            filtered_df = merged_df[merged_df.apply(
                lambda x: (
                    replace_special_chars_with_space(x['item_nm_alias']) in replace_special_chars_with_space(x['item_name_in_msg']) or 
                    replace_special_chars_with_space(x['item_name_in_msg']) in replace_special_chars_with_space(x['item_nm_alias'])
                ) if x['type'] != 'expansion' else True, 
                axis=1
            )]

            # similarities_fuzzy = filtered_df[similarities_fuzzy.columns]

            # ìƒí’ˆ ì •ë³´ ë§¤í•‘
            if not similarities_fuzzy.empty:
                final_result['product'] = self._map_products_with_similarity(similarities_fuzzy, json_objects)
            else:
                # ìœ ì‚¬ë„ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìƒˆ ìŠ¤í‚¤ë§ˆ + expected_action ë¦¬ìŠ¤íŠ¸)
                final_result['product'] = [
                    {
                        'item_nm': d.get('name', ''), 
                        'item_id': ['#'],
                        'item_name_in_msg': [d.get('name', '')],
                        'expected_action': [d.get('action', 'ê¸°íƒ€')]
                    } 
                    for d in product_items 
                    if d.get('name') and d['name'] not in self.stop_item_names
                ]

            # í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘
            final_result['pgm'] = self._map_program_classification(json_objects, pgm_info)
            
            # ì±„ë„ ì •ë³´ ì²˜ë¦¬
            final_result['channel'] = self._extract_channels(json_objects, msg)

            return final_result
            
        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return json_objects

    def _map_products_with_similarity(self, similarities_fuzzy: pd.DataFrame, json_objects: Dict = None) -> List[Dict]:
        """ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ë§¤í•‘"""
        try:
            # ë†’ì€ ìœ ì‚¬ë„ ì•„ì´í…œë“¤ í•„í„°ë§
            high_sim_threshold = getattr(PROCESSING_CONFIG, 'high_similarity_threshold', 1.5)
            high_sim_items = similarities_fuzzy.query('sim >= @high_sim_threshold')['item_nm_alias'].unique()
            filtered_similarities = similarities_fuzzy[
                (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &
                (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &
                (~similarities_fuzzy['item_name_in_msg'].isin(self.stop_item_names))
            ]
            
            # ìƒí’ˆ ì •ë³´ì™€ ë§¤í•‘í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„± (ìƒˆ ìŠ¤í‚¤ë§ˆ + expected_action)
            product_tag = self.convert_df_to_json_list(
                self.item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])
            )
            
            # Add expected_action to each product
            if json_objects:
                action_mapping = self._create_action_mapping(json_objects)
                for product in product_tag:
                    item_names_in_msg = product.get('item_name_in_msg', [])
                    # ë°°ì—´ì˜ ê° í•­ëª©ì— ëŒ€í•´ ëª¨ë“  action ì°¾ê¸° (ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘, ì¤‘ë³µ ì œê±°)
                    found_actions = []
                    for item_name in item_names_in_msg:
                        if item_name in action_mapping:
                            found_actions.append(action_mapping[item_name])
                    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
                    product['expected_action'] = list(dict.fromkeys(found_actions)) if found_actions else ['ê¸°íƒ€']
            
            return product_tag
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì •ë³´ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

    def _create_action_mapping(self, json_objects: Dict) -> Dict[str, str]:
        """LLM ì‘ë‹µì—ì„œ ìƒí’ˆëª…-ì•¡ì…˜ ë§¤í•‘ ìƒì„±"""
        try:
            action_mapping = {}
            product_data = json_objects.get('product', [])
            
            if isinstance(product_data, list):
                # ì •ìƒì ì¸ ë°°ì—´ êµ¬ì¡°
                for item in product_data:
                    if isinstance(item, dict) and 'name' in item and 'action' in item:
                        action_mapping[item['name']] = item['action']
            elif isinstance(product_data, dict):
                # ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ë˜ëŠ” ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                if 'items' in product_data:
                    # ìŠ¤í‚¤ë§ˆ êµ¬ì¡°: {"items": [...]}
                    items = product_data.get('items', [])
                    for item in items:
                        if isinstance(item, dict) and 'name' in item and 'action' in item:
                            action_mapping[item['name']] = item['action']
                elif 'type' in product_data and product_data.get('type') == 'array':
                    # ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡°ëŠ” ê±´ë„ˆë›°ê¸°
                    logger.debug("ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡° ê°ì§€ë¨, ì•¡ì…˜ ë§¤í•‘ ê±´ë„ˆë›°ê¸°")
                else:
                    # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ì²˜ë¦¬
                    if 'name' in product_data and 'action' in product_data:
                        action_mapping[product_data['name']] = product_data['action']
            
            logger.debug(f"ìƒì„±ëœ ì•¡ì…˜ ë§¤í•‘: {action_mapping}")
            return action_mapping
            
        except Exception as e:
            logger.error(f"ì•¡ì…˜ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _map_program_classification(self, json_objects: Dict, pgm_info: Dict) -> List[Dict]:
        """í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ì •ë³´ ë§¤í•‘"""
        try:
            if (self.num_cand_pgms > 0 and 
                'pgm' in json_objects and 
                isinstance(json_objects['pgm'], list) and
                not pgm_info.get('pgm_pdf_tmp', pd.DataFrame()).empty):
                
                pgm_json = pgm_info['pgm_pdf_tmp'][
                    pgm_info['pgm_pdf_tmp']['pgm_nm'].apply(
                        lambda x: re.sub(r'\[.*?\]', '', x) in ' '.join(json_objects['pgm'])
                    )
                ][['pgm_nm', 'pgm_id']].to_dict('records')
                
                return pgm_json
            
            return []
            
        except Exception as e:
            logger.error(f"í”„ë¡œê·¸ë¨ ë¶„ë¥˜ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            return []

def process_message_with_dag(extractor, message: str, extract_dag: bool = False) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ìš©)
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
    
    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ (í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨)
    """
    try:
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")

        # 1. ë©”ì¸ ì¶”ì¶œ
        result = extractor.process_message(message)
        dag_list = []
        
        if extract_dag:
            # ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë³€ê²½ (í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ë¥¼ ìœ„í•´)
            # ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ë©´ ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œê°€ ë¶„ë¦¬ë˜ì–´ í”„ë¡¬í”„íŠ¸ ìº¡ì²˜ê°€ ì•ˆë¨
            logger.info("ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë©”ì¸ ì¶”ì¶œ ë° DAG ì¶”ì¶œ ìˆ˜í–‰")
            
            # 2. DAG ì¶”ì¶œ
            dag_result = make_entity_dag(message, extractor.llm_model)
            dag_list = sorted([d for d in dag_result['dag_section'].split('\n') if d!=''])

        extracted_result = result.get('extracted_result', {})
        extracted_result['entity_dag'] = dag_list
        result['extracted_result'] = extracted_result

        raw_result = result.get('raw_result', {})
        raw_result['entity_dag'] = dag_list
        result['raw_result'] = raw_result

        result['error'] = ""
        
        logger.info(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
        return result
        
    except Exception as e:
        logger.error(f"ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
            "purpose": ["ì˜¤ë¥˜"],
            "product": [],
            "channel": [],
            "pgm": [],
            "entity_dag": [],
            "error": str(e)
        }

def process_messages_batch(extractor, messages: List[str], extract_dag: bool = False, max_workers: int = None) -> List[Dict[str, Any]]:
    """
    ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤
        messages: ì²˜ë¦¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        extract_dag: DAG ì¶”ì¶œ ì—¬ë¶€
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜)
    
    Returns:
        list: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if max_workers is None:
        max_workers = min(len(messages), os.cpu_count())
    
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(messages)}ê°œ ë©”ì‹œì§€, {max_workers}ê°œ ì›Œì»¤")
    
    start_time = time.time()
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ë©”ì‹œì§€ì— ëŒ€í•´ ì‘ì—… ì œì¶œ
        future_to_message = {
            executor.submit(process_message_with_dag, extractor, msg, extract_dag): msg 
            for msg in messages
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
        for i, future in enumerate(future_to_message):
            try:
                result = future.result()
                results.append(result)
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ : {i+1}/{len(messages)} ({((i+1)/len(messages)*100):.1f}%)")
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                results.append({
                    "title": "ì²˜ë¦¬ ì‹¤íŒ¨",
                    "purpose": ["ì˜¤ë¥˜"],
                    "product": [],
                    "channel": [],
                    "pgm": [],
                    "entity_dag": [],
                    "error": str(e)
                })
    
    elapsed_time = time.time() - start_time
    logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ")
    logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(messages):.2f}ì´ˆ/ë©”ì‹œì§€")
    
    return results

def make_entity_dag(msg: str, llm_model, save_dag_image=True):

    # ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ë¥¼ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„ë¡œ ì¶”ì¶œ
    # ì˜ˆ: (ê³ ê°:ê°€ì…) -[í•˜ë©´]-> (í˜œíƒ:ìˆ˜ë ¹) -[í†µí•´]-> (ë§Œì¡±ë„:í–¥ìƒ)
    extract_dag_result = {}
    logger.info("=" * 30 + " DAG ì¶”ì¶œ ì‹œì‘ " + "=" * 30)
    try:
        dag_start_time = time.time()
        # DAG ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ (entity_dag_extractor.py)
        extract_dag_result = extract_dag(DAGParser(), msg, llm_model)
        dag_raw = extract_dag_result['dag_raw']      # LLM ì›ë³¸ ì‘ë‹µ
        dag_section = extract_dag_result['dag_section']  # íŒŒì‹±ëœ DAG í…ìŠ¤íŠ¸
        dag = extract_dag_result['dag']             # NetworkX ê·¸ë˜í”„ ê°ì²´
        
        # ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± (utils.py)
        dag_filename = ""
        if save_dag_image:
            dag_filename = f'dag_{sha256_hash(msg)}'
            create_dag_diagram(dag, filename=dag_filename)
            logger.info(f"âœ… DAG ì¶”ì¶œ ì™„ë£Œ: {dag_filename}")

        extract_dag_result['dag_filename'] = dag_filename
        
        dag_processing_time = time.time() - dag_start_time
        
        logger.info(f"ğŸ•’ DAG ì²˜ë¦¬ ì‹œê°„: {dag_processing_time:.3f}ì´ˆ")
        logger.info(f"ğŸ“ DAG ì„¹ì…˜ ê¸¸ì´: {len(dag_section)}ì")
        if dag_section:
            logger.info(f"ğŸ“„ DAG ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {dag_section[:200]}...")
        else:
            logger.warning("âš ï¸ DAG ì„¹ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"âŒ DAG ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        dag_section = ""

    return extract_dag_result


def get_stored_prompts_from_thread():
    """í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    import threading
    current_thread = threading.current_thread()
    
    if hasattr(current_thread, 'stored_prompts'):
        return current_thread.stored_prompts
    else:
        return {}

def save_result_to_mongodb_if_enabled(message: str, result: dict, args_or_data, extractor=None):
    """MongoDB ì €ì¥ì´ í™œì„±í™”ëœ ê²½ìš° ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜
    
    Args:
        message: ì²˜ë¦¬í•  ë©”ì‹œì§€
        result: ì²˜ë¦¬ ê²°ê³¼ (extracted_result, raw_result í¬í•¨)
        args_or_data: argparse.Namespace ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬
        extractor: MMSExtractor ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
    
    Returns:
        str: ì €ì¥ëœ ë¬¸ì„œ ID, ì‹¤íŒ¨ ì‹œ None
    """
    # args_or_dataê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Namespaceë¡œ ë³€í™˜
    if isinstance(args_or_data, dict):
        import argparse
        args = argparse.Namespace(**args_or_data)
    else:
        args = args_or_data
    
    # save_to_mongodb ì†ì„±ì´ ì—†ê±°ë‚˜ Falseì¸ ê²½ìš°
    if not getattr(args, 'save_to_mongodb', False):
        return None
        
    try:
        # MongoDB ì„í¬íŠ¸ ì‹œë„
        from mongodb_utils import save_to_mongodb
        
        # ìŠ¤ë ˆë“œ ë¡œì»¬ ì €ì¥ì†Œì—ì„œ í”„ë¡¬í”„íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stored_prompts = result.get('prompts', get_stored_prompts_from_thread()) 
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´ êµ¬ì„±
        prompts_data = {}
        for key, prompt_data in stored_prompts.items():
            prompts_data[key] = {
                'title': prompt_data.get('title', f'{key} í”„ë¡¬í”„íŠ¸'),
                'description': prompt_data.get('description', f'{key} ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸'),
                'content': prompt_data.get('content', ''),
                'length': len(prompt_data.get('content', ''))
            }
        
        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not prompts_data:
            prompts_data = {
                'main_extraction_prompt': {
                    'title': 'ë©”ì¸ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸',
                    'description': 'MMS ë©”ì‹œì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ',
                    'content': 'ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                    'length': 0
                }
            }
        
        extraction_prompts = {
            'success': True,
            'prompts': prompts_data,
            'settings': {
                'llm_model': getattr(args, 'llm_model', 'unknown'),
                'offer_data_source': getattr(args, 'offer_data_source', getattr(args, 'offer_info_data_src', 'unknown')),
                'product_info_extraction_mode': getattr(args, 'product_info_extraction_mode', 'unknown'),
                'entity_matching_mode': getattr(args, 'entity_matching_mode', getattr(args, 'entity_extraction_mode', 'unknown')),
                'extract_entity_dag': getattr(args, 'extract_entity_dag', False)
            }
        }
        
        # ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDB í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        extraction_result = {
            'success': not bool(result.get('error')),
            'result': result.get('extracted_result', result.get('result', {})),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }

        raw_result_data = {
            'success': not bool(result.get('error')),
            'result': result.get('raw_result', {}),
            'metadata': {
                'processing_time_seconds': result.get('processing_time', 0),
                'processing_mode': getattr(args, 'processing_mode', 'single'),
                'model_used': getattr(args, 'llm_model', 'unknown')
            }
        }
        
        # MongoDBì— ì €ì¥
        user_id = getattr(args, 'user_id', 'DEFAULT_USER')
        saved_id = save_to_mongodb(message, extraction_result, raw_result_data, extraction_prompts, 
                                 user_id=user_id, message_id=None)
        
        if saved_id:
            print(f"ğŸ“„ ê²°ê³¼ê°€ MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {saved_id[:8]}...)")
            return saved_id
        else:
            print("âš ï¸ MongoDB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
            
    except ImportError:
        print("âŒ MongoDB ì €ì¥ì´ ìš”ì²­ë˜ì—ˆì§€ë§Œ mongodb_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

            
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def main():
    """
    ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì‹¤í–‰í•  ë•Œì˜ ë©”ì¸ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì˜µì…˜ì„ í†µí•´ ì¶”ì¶œê¸° ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --extract-entity-dag
    
    # ë°°ì¹˜ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ìŠ¤)
    python mms_extractor.py --batch-file messages.txt --max-workers 4 --extract-entity-dag
    
    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œë¡œ ë°°ì¹˜ ì²˜ë¦¬
    python mms_extractor.py --batch-file messages.txt --offer-data-source db --max-workers 8
    
    # MongoDBì— ê²°ê³¼ ì €ì¥
    python mms_extractor.py --message "ê´‘ê³  ë©”ì‹œì§€" --save-to-mongodb --extract-entity-dag
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='MMS ê´‘ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° - ê°œì„ ëœ ë²„ì „')
    parser.add_argument('--message', type=str, help='í…ŒìŠ¤íŠ¸í•  ë©”ì‹œì§€')
    parser.add_argument('--batch-file', type=str, help='ë°°ì¹˜ ì²˜ë¦¬í•  ë©”ì‹œì§€ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)')
    parser.add_argument('--max-workers', type=int, help='ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)')
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='local',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm', 'rag'], default='llm',
                       help='ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ëª¨ë“œ (nlp: í˜•íƒœì†Œë¶„ì„, llm: LLM ê¸°ë°˜, rag: ê²€ìƒ‰ì¦ê°•ìƒì„±)')
    parser.add_argument('--entity-matching-mode', choices=['logic', 'llm'], default='llm',
                       help='ì—”í‹°í‹° ë§¤ì¹­ ëª¨ë“œ (logic: ë¡œì§ ê¸°ë°˜, llm: LLM ê¸°ë°˜)')
    parser.add_argument('--llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='gen',
                       help='ì‚¬ìš©í•  LLM ëª¨ë¸ (gem: Gemma, ax: ax, cld: Claude, gen: Gemini, gpt: GPT)')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                       help='ë¡œê·¸ ë ˆë²¨ ì„¤ì •')
    parser.add_argument('--extract-entity-dag', action='store_true', default=False, help='Entity DAG extraction (default: False)')
    parser.add_argument('--save-to-mongodb', action='store_true', default=True, 
                       help='ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥ (mongodb_utils.py í•„ìš”)')
    parser.add_argument('--test-mongodb', action='store_true', default=False,
                       help='MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì¢…ë£Œ')

    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    
    # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.test_mongodb:
        try:
            from mongodb_utils import test_mongodb_connection
        except ImportError:
            print("âŒ MongoDB ìœ í‹¸ë¦¬í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("mongodb_utils.py íŒŒì¼ê³¼ pymongo íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
        
        print("ğŸ”Œ MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        if test_mongodb_connection():
            print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
            exit(0)
        else:
            print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨!")
            print("MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
    
    try:
                # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        logger.info("MMS ì¶”ì¶œê¸° ì´ˆê¸°í™” ì¤‘...")
        extractor = MMSExtractor(
            offer_info_data_src=args.offer_data_source,
            product_info_extraction_mode=args.product_info_extraction_mode,
            entity_extraction_mode=args.entity_matching_mode,
            llm_model=args.llm_model,
            extract_entity_dag=args.extract_entity_dag
        )
        
        # ë°°ì¹˜ ì²˜ë¦¬ ë˜ëŠ” ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
        if args.batch_file:
            # ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ë“¤ ë¡œë“œ
            logger.info(f"ë°°ì¹˜ íŒŒì¼ì—ì„œ ë©”ì‹œì§€ ë¡œë“œ: {args.batch_file}")
            try:
                with open(args.batch_file, 'r', encoding='utf-8') as f:
                    messages = [line.strip() for line in f if line.strip()]
                
                logger.info(f"ë¡œë“œëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = process_messages_batch(
                    extractor, 
                    messages, 
                    extract_dag=args.extract_entity_dag,
                    max_workers=args.max_workers
                )
                
                # MongoDB ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                if args.save_to_mongodb:
                    print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                    args.processing_mode = 'batch'
                    saved_count = 0
                    for i, result in enumerate(results):
                        if i < len(messages):  # ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                            saved_id = save_result_to_mongodb_if_enabled(messages[i], result, args, extractor)
                            if saved_id:
                                saved_count += 1
                    print(f"ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(results)}ê°œ")
                
                # ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥
                print("\n" + "="*50)
                print("ğŸ¯ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼")
                print("="*50)
                
                for i, result in enumerate(results):
                    print(f"\n--- ë©”ì‹œì§€ {i+1} ---")
                    print(f"ì œëª©: {result.get('title', 'N/A')}")
                    print(f"ìƒí’ˆ: {len(result.get('product', []))}ê°œ")
                    if result.get('error'):
                        print(f"ì˜¤ë¥˜: {result['error']}")
                
                # ì „ì²´ ë°°ì¹˜ í†µê³„
                successful = len([r for r in results if not r.get('error')])
                failed = len(results) - successful
                print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ í†µê³„")
                print(f"âœ… ì„±ê³µ: {successful}ê°œ")
                print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {(successful/len(results)*100):.1f}%")
                
                # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
                output_file = f"batch_results_{int(time.time())}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, ensure_ascii=False)
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
                
            except FileNotFoundError:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch_file}")
                exit(1)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                exit(1)
        
        else:
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
            test_message = args.message if args.message else """
  message: '[SKT] Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œ êµ¬ë…ë£Œ ë³€ê²½ ì•ˆë‚´__ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”._2025ë…„ 12ì›” 1ì¼(ì›”)ë¶€í„° Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œ êµ¬ë…ë£Œê°€ ë³€ê²½ë©ë‹ˆë‹¤.__ìš”ê¸ˆì œ í˜œíƒìœ¼ë¡œ Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œë¥¼ ì¶”ê°€ ìš”ê¸ˆ ì—†ì´ ì´ìš© ì¤‘ì¸ ê²½ìš°, ë³„ë„ ì•ˆë‚´ ì „ê¹Œì§€ ê¸°ì¡´ êµ¬ë…ë£Œë¡œ ë™ì¼í•˜ê²Œ ì¦ê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.__ì•„ì§ ê°€ì…í•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ì•„ë˜ URLì„ í†µí•´ ê°€ì… ê°€ëŠ¥í•©ë‹ˆë‹¤.__â–¶ ê°€ì…í•˜ê¸°: https://m.sktuniverse.co.kr/product/detail?prdId=PR00000501__â–  ë³€ê²½ ë‚´ìš©_- ëŒ€ìƒ: Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œ_- ë³€ê²½ì¼: 2025ë…„ 12ì›” 1ì¼(ì›”)_- ë‚´ìš©: ì›” êµ¬ë…ë£Œ ë³€ê²½(5,500ì› â†’ 7,000ì›)_* 2025ë…„ 11ì›” 30ì¼(ì¼)ê¹Œì§€ Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œì™€ í• ì¸ ëŒ€ìƒ ìš”ê¸ˆì œ ëª¨ë‘ ê°€ì… ì‹œ, ë³„ë„ ì•ˆë‚´ ì „ê¹Œì§€ ê¸°ì¡´ êµ¬ë…ë£Œë¡œ ê³„ì† ì´ìš© ê°€ëŠ¥__â–  ìœ ì˜ ì‚¬í•­_- êµ¬ë…ë£Œ ë³€ê²½ í›„ì—ë„ <T ìš°ì£¼ Netflix> ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œ í• ì¸ ìš”ê¸ˆì œ í˜œíƒì€ ê¸°ì¡´ê³¼ ë™ì¼(5,500ì› í• ì¸)í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤._* ëŒ€ìƒ ìš”ê¸ˆì œ: 5GX í”„ë¼ì„(ë„·í”Œë¦­ìŠ¤), 0 ì²­ë…„ 89(ë„·í”Œë¦­ìŠ¤), ë‹¤ì´ë ‰íŠ¸5G 62(ë„·í”Œë¦­ìŠ¤), 0 ì²­ë…„ ë‹¤ì´ë ‰íŠ¸ 62(ë„·í”Œë¦­ìŠ¤)_- 2025ë…„ 12ì›” 1ì¼(ì›”)ë¶€í„° í• ì¸ ëŒ€ìƒ ìš”ê¸ˆì œ ë˜ëŠ” Netflix ê´‘ê³ í˜• ìŠ¤íƒ ë‹¤ë“œ ìƒí’ˆ ì‹ ê·œê°€ì… ì‹œ ë³€ê²½ëœ êµ¬ë…ë£Œë¡œ ê²°ì œë©ë‹ˆë‹¤._ - Wavveì™€ ê²°í•©ëœ <T ìš°ì£¼íŒ¨ìŠ¤ Netflix>ì— ê°€ì…í•œ ê²½ìš°, 2025ë…„ 12ì›” 1ì¼(ì›”)ë¶€í„° ê°€ê²©ì´ ì¸ìƒë©ë‹ˆë‹¤.__â–  ë¬¸ì˜: SKT ê³ ê°ì„¼í„°(114)__SKTì™€ í•¨ê»˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.',


"""
            
            # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë“œ)
            logger.info("ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘ (ë©€í‹°ìŠ¤ë ˆë“œ)")
            result = process_message_with_dag(extractor, test_message, args.extract_entity_dag)
                    
            # MongoDB ì €ì¥ (ë‹¨ì¼ ë©”ì‹œì§€)
            if args.save_to_mongodb:
                print("\nğŸ“„ MongoDB ì €ì¥ ì¤‘...")
                args.processing_mode = 'single'
                saved_id = save_result_to_mongodb_if_enabled(test_message, result, args, extractor)
                if saved_id:
                    print("ğŸ“„ MongoDB ì €ì¥ ì™„ë£Œ!")

            
            extracted_result = result.get('extracted_result', {})
        
            print("\n" + "="*50)
            print("ğŸ¯ ìµœì¢… ì¶”ì¶œëœ ì •ë³´")
            print("="*50)
            print(json.dumps(extracted_result, indent=4, ensure_ascii=False))

            # ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ì¶œë ¥
            print("\n" + "="*50)
            print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ")
            print("="*50)
            print(f"âœ… ì œëª©: {extracted_result.get('title', 'N/A')}")
            print(f"âœ… ëª©ì : {len(extracted_result.get('purpose', []))}ê°œ")
            print(f"âœ… ìƒí’ˆ: {len(extracted_result.get('product', []))}ê°œ")
            print(f"âœ… ì±„ë„: {len(extracted_result.get('channel', []))}ê°œ")
            print(f"âœ… í”„ë¡œê·¸ë¨: {len(extracted_result.get('pgm', []))}ê°œ")
            if extracted_result.get('error'):
                print(f"âŒ ì˜¤ë¥˜: {extracted_result['error']}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        exit(1)


if __name__ == '__main__':
    main()
# %%
