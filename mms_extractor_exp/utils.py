"""
MMS Extractor ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ
================================

ì´ ëª¨ë“ˆì€ MMS ì¶”ì¶œê¸°ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ë°ì½”ë ˆì´í„° ë° ì•ˆì „ ì‹¤í–‰ í•¨ìˆ˜ë“¤
- í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° JSON ë³µêµ¬ í•¨ìˆ˜ë“¤
- ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤
- í˜•íƒœì†Œ ë¶„ì„ ê´€ë ¨ í´ë˜ìŠ¤ë“¤

ì‘ì„±ì: MMS ë¶„ì„íŒ€
ë²„ì „: 2.0.0
"""

import time
import logging
import re
import ast
import json
import os
import hashlib
import textwrap
from functools import wraps
from typing import List, Dict, Any
import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
import difflib
from rapidfuzz import fuzz
from joblib import Parallel, delayed
import networkx as nx
from graphviz import Digraph

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ===== ë°ì½”ë ˆì´í„° ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def log_performance(func):
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ì„ ë¡œê¹…í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            elapsed = time.time() - start_time
            logger.info(f"{func.__name__} ì‹¤í–‰ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
            return result
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"{func.__name__} ì‹¤í–‰ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ): {e}")
            raise
    return wrapper

def safe_execute(func, *args, default_return=None, max_retries=2, **kwargs):
    """
    ì•ˆì „í•œ í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, API í˜¸ì¶œ ì‹¤íŒ¨ ë“±ì˜ ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€í•´
    ì§€ìˆ˜ ë°±ì˜¤í”„(exponential backoff)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    
    Args:
        func: ì‹¤í–‰í•  í•¨ìˆ˜
        *args: í•¨ìˆ˜ì— ì „ë‹¬í•  ìœ„ì¹˜ ì¸ìˆ˜
        default_return: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë°˜í™˜í•  ê¸°ë³¸ê°’
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (default: 2)
        **kwargs: í•¨ìˆ˜ì— ì „ë‹¬í•  í‚¤ì›Œë“œ ì¸ìˆ˜
        
    Returns:
        í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” default_return
        
    Example:
        result = safe_execute(api_call, data, default_return={}, max_retries=3)
    """
    for attempt in range(max_retries + 1):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries:
                # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… ë° ê¸°ë³¸ê°’ ë°˜í™˜
                logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
                return default_return
            else:
                # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„: 1ì´ˆ, 2ì´ˆ, 4ì´ˆ, 8ì´ˆ... (ì§€ìˆ˜ ë°±ì˜¤í”„)
                logger.warning(f"{func.__name__} ì¬ì‹œë„ {attempt + 1}/{max_retries}: {e}")
                time.sleep(2 ** attempt)
    return default_return

def validate_text_input(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ ê²€ì¦ ë° ì •ë¦¬ í•¨ìˆ˜
    
    MMS í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì „ì— ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ê³ 
    ì²˜ë¦¬ì— ì í•©í•œ í˜•íƒœë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ê²€ì¦í•  ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        
    Raises:
        ValueError: ë¹„ì–´ìˆê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì˜ ì…ë ¥
        
    Example:
        clean_text = validate_text_input("  [SKí…”ë ˆì½¤] í˜œíƒ ì•ˆë‚´  ")
    """
    # íƒ€ì… ê²€ì¦: ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
    if not isinstance(text, str):
        raise ValueError(f"í…ìŠ¤íŠ¸ ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(text)}")
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬
    if not text:
        raise ValueError("ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # ìµœëŒ€ ê¸¸ì´ ì œí•œ (LLM í† í° ì œí•œ ë° ì„±ëŠ¥ ê³ ë ¤)
    if len(text) > 10000:
        logger.warning(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(text)} ë¬¸ì). ì²˜ìŒ 10000ìë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        text = text[:10000]
    
    return text

def safe_check_empty(obj) -> bool:
    """ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°ì²´ê°€ ë¹„ì–´ìˆëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸"""
    try:
        if hasattr(obj, '__len__'):
            return len(obj) == 0
        elif hasattr(obj, 'size'):  # numpy ë°°ì—´
            return obj.size == 0
        elif hasattr(obj, 'empty'):  # pandas DataFrame/Series
            return obj.empty
        else:
            return not bool(obj)
    except (ValueError, TypeError):
        # numpy ë°°ì—´ì˜ truth value ì—ëŸ¬ ë“±ì„ ì²˜ë¦¬
        try:
            return getattr(obj, 'size', 1) == 0
        except:
            return True  # ì•ˆì „ì„ ìœ„í•´ ë¹„ì–´ìˆë‹¤ê³  ê°€ì •

# ===== ì›ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìœ ì§€) =====

def dataframe_to_markdown_prompt(df, max_rows=None):
    """DataFrameì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
    if max_rows is not None and len(df) > max_rows:
        display_df = df.head(max_rows)
        truncation_note = f"\n[Note: Only showing first {max_rows} of {len(df)} rows]"
    else:
        display_df = df
        truncation_note = ""
    df_markdown = display_df.to_markdown()
    prompt = f"\n\n    {df_markdown}\n    {truncation_note}\n\n    "
    return prompt

def escape_quotes_in_value(value):
    """ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„í•˜ê±°ë‚˜ ì œê±°"""
    value = value.strip()
    if not value:
        return value
    
    # ê°’ì´ ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
    if value[0] in ['"', "'"]:
        quote_char = value[0]
        # ë‹«ëŠ” ë”°ì˜´í‘œ ì°¾ê¸°
        end_idx = -1
        for i in range(1, len(value)):
            if value[i] == quote_char:
                end_idx = i
                break
        
        if end_idx > 0:
            # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë¶€ë¶„ê³¼ ë‚˜ë¨¸ì§€ ë¶„ë¦¬
            quoted_part = value[1:end_idx]
            remaining = value[end_idx+1:].strip()
            
            # ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ í•©ì¹˜ê¸° (ë”°ì˜´í‘œ ë‚´ë¶€ì˜ ë‚´ìš©ìœ¼ë¡œ í†µí•©)
            if remaining:
                combined = quoted_part + ' ' + remaining
                return quote_char + combined + quote_char
            else:
                return quote_char + quoted_part + quote_char
        else:
            # ë‹«ëŠ” ë”°ì˜´í‘œê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
            return quote_char + value[1:] + quote_char
    
    # ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
    return '"' + value + '"'

def split_key_value(text):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ ì²« ë²ˆì§¸ ì½œë¡ ì„ ê¸°ì¤€ìœ¼ë¡œ í‚¤-ê°’ ë¶„ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
    in_quote = False
    quote_char = ''
    escape_next = False
    
    for i, char in enumerate(text):
        if escape_next:
            escape_next = False
            continue
            
        if char == '\\':
            escape_next = True
            continue
            
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
        elif char == ':' and not in_quote:
            return text[:i].strip(), text[i+1:].strip()
    
    return text.strip(), ''

def split_outside_quotes(text, delimiter=','):
    """ë”°ì˜´í‘œ ì™¸ë¶€ì˜ êµ¬ë¶„ìë¡œë§Œ í…ìŠ¤íŠ¸ ë¶„ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
    parts = []
    current = []
    in_quote = False
    quote_char = ''
    escape_next = False
    
    for char in text:
        if escape_next:
            current.append(char)
            escape_next = False
            continue
            
        if char == '\\':
            current.append(char)
            escape_next = True
            continue
            
        if char in ['"', "'"]:
            if in_quote:
                if char == quote_char:
                    in_quote = False
                    quote_char = ''
            else:
                in_quote = True
                quote_char = char
            current.append(char)
        elif char == delimiter and not in_quote:
            if current:
                parts.append(''.join(current).strip())
                current = []
        else:
            current.append(char)
    
    if current:
        parts.append(''.join(current).strip())
    
    return parts

def clean_ill_structured_json(text):
    """ì˜ëª» êµ¬ì¡°í™”ëœ JSON í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
    # ì¤‘ê´„í˜¸ ì œê±°
    text = text.strip()
    if text.startswith('{'):
        text = text[1:]
    if text.endswith('}'):
        text = text[:-1]
    
    parts = split_outside_quotes(text.strip(), delimiter=',')
    cleaned_parts = []
    
    for part in parts:
        if not part.strip():
            continue
            
        key, value = split_key_value(part)
        
        if not value:
            continue
        
        # í‚¤ ì •ë¦¬ (ë”°ì˜´í‘œ ì¶”ê°€)
        key_clean = key.strip()
        if not (key_clean.startswith('"') and key_clean.endswith('"')):
            if key_clean.startswith('"') or key_clean.startswith("'"):
                key_clean = key_clean[1:]
            if key_clean.endswith('"') or key_clean.endswith("'"):
                key_clean = key_clean[:-1]
            key_clean = '"' + key_clean + '"'
        
        # ê°’ ì •ë¦¬ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        value_clean = escape_quotes_in_value(value)
        
        cleaned_parts.append(f"{key_clean}: {value_clean}")
    
    return '{' + ', '.join(cleaned_parts) + '}'

def repair_json(broken_json):
    """ì†ìƒëœ JSON ë¬¸ìì—´ ë³µêµ¬ (ê°œì„ ëœ ë²„ì „)"""
    json_str = broken_json.strip()
    
    # í›„í–‰ ì‰¼í‘œ ì œê±°
    json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
    
    # ì—°ì†ëœ ì‰¼í‘œ ì œê±°
    json_str = re.sub(r',\s*,', ',', json_str)
    
    return json_str

def extract_json_objects(text):
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    pattern = r'(\{(?:[^{}]|(?:\{(?:[^{}]|(?:\{[^{}]*\}))*\}))*\})'
    result = []
    
    for match in re.finditer(pattern, text):
        potential_json = match.group(0)
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        attempts = [
            lambda: json.loads(potential_json),
            lambda: json.loads(repair_json(potential_json)),
            lambda: json.loads(clean_ill_structured_json(repair_json(potential_json))),
            lambda: ast.literal_eval(clean_ill_structured_json(repair_json(potential_json))),
        ]
        
        for attempt in attempts:
            try:
                json_obj = attempt()
                if isinstance(json_obj, dict):
                    result.append(json_obj)
                    break
            except (json.JSONDecodeError, SyntaxError, ValueError, TypeError):
                continue
    
    return result

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ê·œí™”)"""
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ===== ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ë“¤ (ì›ë³¸ ìœ ì§€) =====

def fuzzy_similarities(text, entities):
    """í¼ì§€ ë§¤ì¹­ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    results = []
    for entity in entities:
        scores = {
            'ratio': fuzz.ratio(text, entity) / 100,
            'partial_ratio': fuzz.partial_ratio(text, entity) / 100,
            'token_sort_ratio': fuzz.token_sort_ratio(text, entity) / 100,
            'token_set_ratio': fuzz.token_set_ratio(text, entity) / 100
        }
        max_score = max(scores.values())
        results.append((entity, max_score))
    return results

def get_fuzzy_similarities(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜"""
    text = args_dict['text']
    entities = args_dict['entities']
    threshold = args_dict['threshold']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    
    text_processed = preprocess_text(text)
    similarities = fuzzy_similarities(text_processed, entities)
    
    filtered_results = [
        {
            text_col_nm: text,
            item_col_nm: entity, 
            "sim": score
        } 
        for entity, score in similarities 
        if score >= threshold
    ]
    return filtered_results

def parallel_fuzzy_similarity(texts, entities, threshold=0.5, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, len(entities) // (n_jobs * 2))
    
    batches = []
    for text in texts:
        for i in range(0, len(entities), batch_size):
            batch = entities[i:i + batch_size]
            batches.append({"text": text, "entities": batch, "threshold": threshold, "text_col_nm": text_col_nm, "item_col_nm": item_col_nm})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(get_fuzzy_similarities)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def longest_common_subsequence_ratio(s1, s2, normalizaton_value):
    """ìµœì¥ ê³µí†µ ë¶€ë¶„ìˆ˜ì—´ ë¹„ìœ¨ ê³„ì‚°"""
    def lcs_length(x, y):
        m, n = len(x), len(y)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if x[i-1] == y[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_len = lcs_length(s1, s2)
    if normalizaton_value == 'max':
        max_len = max(len(s1), len(s2))
        return lcs_len / max_len if max_len > 0 else 1.0
    elif normalizaton_value == 'min':
        min_len = min(len(s1), len(s2))
        return lcs_len / min_len if min_len > 0 else 1.0
    elif normalizaton_value == 's1':
        return lcs_len / len(s1) if len(s1) > 0 else 1.0
    elif normalizaton_value == 's2':
        return lcs_len / len(s2) if len(s2) > 0 else 1.0
    else:
        raise ValueError(f"Invalid normalization value: {normalizaton_value}")

def sequence_matcher_similarity(s1, s2, normalizaton_value):
    """SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    matcher = difflib.SequenceMatcher(None, s1, s2)
    matches = sum(triple.size for triple in matcher.get_matching_blocks())
    
    normalization_length = min(len(s1), len(s2))
    if normalizaton_value == 'max':
        normalization_length = max(len(s1), len(s2))
    elif normalizaton_value == 's1':
        normalization_length = len(s1)
    elif normalizaton_value == 's2':
        normalization_length = len(s2)
        
    if normalization_length == 0: 
        return 0.0
    
    return matches / normalization_length

def substring_aware_similarity(s1, s2, normalizaton_value):
    """ë¶€ë¶„ë¬¸ìì—´ ê´€ê³„ë¥¼ ê³ ë ¤í•œ ìœ ì‚¬ë„ ê³„ì‚°"""
    if s1 in s2 or s2 in s1:
        shorter = min(s1, s2, key=len)
        longer = max(s1, s2, key=len)
        base_score = len(shorter) / len(longer)
        return min(0.95 + base_score * 0.05, 1.0)
    return longest_common_subsequence_ratio(s1, s2, normalizaton_value)

def token_sequence_similarity(s1, s2, normalizaton_value, separator_pattern=r'[\s_\-]+'):
    """í† í° ì‹œí€€ìŠ¤ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    tokens1 = [t for t in re.split(separator_pattern, s1.strip()) if t]
    tokens2 = [t for t in re.split(separator_pattern, s2.strip()) if t]
    
    if not tokens1 or not tokens2:
        return 0.0
    
    def token_lcs_length(t1, t2):
        m, n = len(t1), len(t2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if t1[i-1] == t2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]
    
    lcs_tokens = token_lcs_length(tokens1, tokens2)
    normalization_tokens = max(len(tokens1), len(tokens2))
    if normalizaton_value == 'min':
        normalization_tokens = min(len(tokens1), len(tokens2))
    elif normalizaton_value == 's1':
        normalization_tokens = len(tokens1)
    elif normalizaton_value == 's2':
        normalization_tokens = len(tokens2)
    
    return lcs_tokens / normalization_tokens  

def replace_special_chars_with_space(text):
    """
    ë¬¸ìì—´ì—ì„œ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text (str): ë³€í™˜í•  ë¬¸ìì—´
        
    Returns:
        str: íŠ¹ìˆ˜ ë¬¸ìê°€ ê³µë°±ìœ¼ë¡œ ë³€í™˜ëœ ë¬¸ìì—´
    """
    # ì˜ë¬¸ì, ìˆ«ì, í•œê¸€ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    return re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', ' ', text)
 
def combined_sequence_similarity(s1, s2, weights=None, normalizaton_value='max'):
    """ì—¬ëŸ¬ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ì„ ê²°í•©í•œ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°"""

    s1 = replace_special_chars_with_space(s1)
    s2 = replace_special_chars_with_space(s2)
    
    if weights is None:
        weights = {'substring': 0.1, 'sequence_matcher': 0.7, 'token_sequence': 0.2}
    
    similarities = {
        'substring': substring_aware_similarity(s1, s2, normalizaton_value),
        'sequence_matcher': sequence_matcher_similarity(s1, s2, normalizaton_value),
        'token_sequence': token_sequence_similarity(s1, s2, normalizaton_value)
    }
    
    return sum(similarities[key] * weights[key] for key in weights), similarities

def calculate_seq_similarity(args_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    sent_item_batch = args_dict['sent_item_batch']
    text_col_nm = args_dict['text_col_nm']
    item_col_nm = args_dict['item_col_nm']
    normalizaton_value = args_dict['normalizaton_value']
    weights = args_dict.get('weights', None)
    results = []
    for sent_item in sent_item_batch:
        sent = sent_item[text_col_nm]
        item = sent_item[item_col_nm]
        try:
            sent_processed = preprocess_text(sent.lower())
            item_processed = preprocess_text(item.lower())
            similarity = combined_sequence_similarity(sent_processed, item_processed, weights=weights, normalizaton_value=normalizaton_value)[0]
            results.append({text_col_nm:sent, item_col_nm:item, "sim":similarity})
        except Exception as e:
            logger.error(f"Error processing {item}: {e}")
            results.append({text_col_nm:sent, item_col_nm:item, "sim":0.0})
    
    return results

def parallel_seq_similarity(sent_item_pdf, text_col_nm='sent', item_col_nm='item_nm_alias', n_jobs=None, batch_size=None, weights=None, normalizaton_value='s2'):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°"""
    if n_jobs is None:
        n_jobs = min(os.cpu_count()-1, 8)
    if batch_size is None:
        batch_size = max(1, sent_item_pdf.shape[0] // (n_jobs * 2))
    
    batches = []
    for i in range(0, sent_item_pdf.shape[0], batch_size):
        batch = sent_item_pdf.iloc[i:i + batch_size].to_dict(orient='records')
        batches.append({"sent_item_batch": batch, 'text_col_nm': text_col_nm, 'item_col_nm': item_col_nm, 'weights': weights, 'normalizaton_value': normalizaton_value})
    
    with Parallel(n_jobs=n_jobs) as parallel:
        batch_results = parallel(delayed(calculate_seq_similarity)(args) for args in batches)
    
    return pd.DataFrame(sum(batch_results, []))

def load_sentence_transformer(model_path, device=None):
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Loading model from {model_path}...")
    model = SentenceTransformer(model_path).to(device)
    logger.info(f"Model loaded on {device}")
    return model

# ===== Kiwi í˜•íƒœì†Œ ë¶„ì„ ê´€ë ¨ í´ë˜ìŠ¤ë“¤ (ì›ë³¸ ìœ ì§€) =====

class Token:
    """í˜•íƒœì†Œ ë¶„ì„ í† í° í´ë˜ìŠ¤"""
    def __init__(self, form, tag, start, length):
        self.form = form      # í† í° í˜•íƒœ
        self.tag = tag        # í’ˆì‚¬ íƒœê·¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.len = length     # ê¸¸ì´

class Sentence:
    """í˜•íƒœì†Œ ë¶„ì„ ë¬¸ì¥ í´ë˜ìŠ¤"""
    def __init__(self, text, start, end, tokens, subs=None):
        self.text = text      # ë¬¸ì¥ í…ìŠ¤íŠ¸
        self.start = start    # ì‹œì‘ ìœ„ì¹˜
        self.end = end        # ë ìœ„ì¹˜
        self.tokens = tokens  # í† í° ë¦¬ìŠ¤íŠ¸
        self.subs = subs or []  # í•˜ìœ„ ë¬¸ì¥ë“¤

def filter_text_by_exc_patterns(sentence, exc_tag_patterns):
    """ì œì™¸í•  í’ˆì‚¬ íŒ¨í„´ì— ë”°ë¼ í…ìŠ¤íŠ¸ í•„í„°ë§"""
    # ê°œë³„ íƒœê·¸ì™€ ì‹œí€€ìŠ¤ íŒ¨í„´ ë¶„ë¦¬
    individual_tags = set()
    sequences = []
    
    for pattern in exc_tag_patterns:
        if isinstance(pattern, list):
            if len(pattern) == 1:
                individual_tags.add(pattern[0])
            else:
                sequences.append(pattern)
        else:
            individual_tags.add(pattern)
    
    # ì œì™¸í•  í† í° ì¸ë±ìŠ¤ ìˆ˜ì§‘
    tokens_to_exclude = set()
    
    # ê°œë³„ íƒœê·¸ ë§¤ì¹­ í™•ì¸
    for i, token in enumerate(sentence.tokens):
        if token.tag in individual_tags:
            tokens_to_exclude.add(i)
    
    # ì‹œí€€ìŠ¤ íŒ¨í„´ ë§¤ì¹­ í™•ì¸
    for sequence in sequences:
        seq_len = len(sequence)
        for i in range(len(sentence.tokens) - seq_len + 1):
            if all(sentence.tokens[i + j].tag == sequence[j] for j in range(seq_len)):
                for j in range(seq_len):
                    tokens_to_exclude.add(i + j)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì œì™¸í•  í† í° ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
    result_chars = list(sentence.text)
    for i, token in enumerate(sentence.tokens):
        if i in tokens_to_exclude:
            start_pos = token.start - sentence.start
            end_pos = start_pos + token.len
            for j in range(start_pos, end_pos):
                if j < len(result_chars) and result_chars[j] != ' ':
                    result_chars[j] = ' '
    
    filtered_text = ''.join(result_chars)
    return re.sub(r'\s+', ' ', filtered_text)

def filter_specific_terms(strings: List[str]) -> List[str]:
    """ì¤‘ë³µë˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì— ìˆëŠ” ìš©ì–´ë“¤ í•„í„°ë§"""
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)
    
    filtered = []
    for s in unique_strings:
        if not any(s in other for other in filtered):
            filtered.append(s)
    
    return filtered

def convert_df_to_json_list(df):
    """DataFrameì„ íŠ¹ì • JSON êµ¬ì¡°ë¡œ ë³€í™˜"""
    result = []
    grouped = df.groupby('item_name_in_msg')
    
    for item_name_in_msg, group in grouped:
        item_dict = {
            'item_name_in_msg': item_name_in_msg,
            'item_in_voca': []
        }
        
        item_nm_groups = group.groupby('item_nm')
        for item_nm, item_group in item_nm_groups:
            item_ids = list(item_group['item_id'].unique())
            voca_item = {
                'item_nm': item_nm,
                'item_id': item_ids
            }
            item_dict['item_in_voca'].append(voca_item)
        result.append(item_dict)
    
    return result

# ===== DAG ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def sha256_hash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ SHA256 í•´ì‹œê°’ì„ ë°˜í™˜"""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]

def format_node_label(text, wrap_method='record'):
    """
    Format node label based on wrapping method
    
    Returns:
    --------
    tuple: (label, node_attributes_dict)
    """
    
    if wrap_method == 'html_table':
        # Method 1: HTML Table (Best for auto-wrapping)
        clean_text = text.replace('_', '_ ')
        label = f'<<TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="4">' \
                f'<TR><TD BALIGN="CENTER">{clean_text}</TD></TR></TABLE>>'
        
        node_attrs = {
            'shape': 'box',
            'style': 'rounded,filled',
            'width': '2.0',
            'height': '0.8',
            'fixedsize': 'false',
            'margin': '0.1,0.1'
        }
        
    elif wrap_method == 'record':
        # Method 2: Record shape (Good for structured text)
        clean_text = text.replace('_', '|')
        label = f'{{{clean_text}}}'
        
        node_attrs = {
            'shape': 'record',
            'style': 'rounded,filled',
            'fixedsize': 'false',
            'margin': '0.2,0.1'
        }
        
    elif wrap_method == 'manual_wrap':
        # Method 3: Manual text wrapping
        if ':' in text:
            parts = text.split(':')
            if len(parts) == 2:
                part1 = parts[0]
                part2 = parts[1]
                
                if len(part1) > 10:
                    part1 = '\\n'.join(textwrap.wrap(part1, width=10))
                if len(part2) > 10:
                    part2 = '\\n'.join(textwrap.wrap(part2, width=10))
                
                label = f'{part1}:\\n{part2}'
        else:
            if len(text) > 12:
                wrapped = textwrap.wrap(text, width=12)
                label = '\\n'.join(wrapped)
            else:
                label = text
        
        node_attrs = {
            'shape': 'box',
            'style': 'rounded,filled',
            'fixedsize': 'false',
            'margin': '0.3,0.2'
        }
        
    elif wrap_method == 'fixedsize_false':
        # Method 4: Let Graphviz auto-size
        label = text.replace(':', ': ')
        
        node_attrs = {
            'shape': 'box',
            'style': 'rounded,filled',
            'fixedsize': 'false',
            'margin': '0.3,0.2'
        }
        
    else:  # default
        label = text
        node_attrs = {
            'shape': 'box',
            'style': 'rounded,filled',
            'width': '1.5',
            'height': '0.8',
            'fixedsize': 'true'
        }
    
    return label, node_attrs

def create_dag_diagram(dag: nx.DiGraph, filename: str = "dag", save_dir: str = "dag_images", wrap_method: str = 'record', **kwargs):
    """
    DAG ì‹œê°í™” ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± í•¨ìˆ˜
    
    NetworkX ê·¸ë˜í”„ë¥¼ Graphvizë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        dag: NetworkX DiGraph ê°ì²´
        filename: ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
        save_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬
        wrap_method: í…ìŠ¤íŠ¸ ë˜í•‘ ë°©ë²• ('html_table', 'record', 'manual_wrap', 'fixedsize_false')
        **kwargs: Graphviz ìŠ¤íƒ€ì¼ë§ íŒŒë¼ë¯¸í„°
        
    Returns:
        str or None: ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
    """
    
    logger.info(f"ğŸ¨ DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹œì‘ - íŒŒì¼ëª…: {filename}")
    logger.info(f"ğŸ“Š ì…ë ¥ ê·¸ë˜í”„ - ë…¸ë“œ ìˆ˜: {dag.number_of_nodes()}, ì—£ì§€ ìˆ˜: {dag.number_of_edges()}")
    
    # Graphviz ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ì„¤ì • (PATH ë¬¸ì œ í•´ê²°)
    import os as os_module
    graphviz_path = '/usr/local/bin'
    if graphviz_path not in os_module.environ.get('PATH', ''):
        os_module.environ['PATH'] = f"{graphviz_path}:{os_module.environ.get('PATH', '')}"
        logger.info(f"âœ… Graphviz PATH ì¶”ê°€: {graphviz_path}")
    
    try:
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(save_dir, exist_ok=True)
        
        # Step 1: ì—°ê²°ëœ ë…¸ë“œë§Œ í•„í„°ë§
        connected_nodes = set()
        for edge in dag.edges():
            connected_nodes.add(edge[0])
            connected_nodes.add(edge[1])
        
        if not connected_nodes:
            logger.warning("âŒ ê·¸ë˜í”„ì—ì„œ ì—°ê²°ëœ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # ì—°ê²°ëœ ë…¸ë“œë§Œìœ¼ë¡œ ì„œë¸Œê·¸ë˜í”„ ìƒì„±
        G_connected = dag.subgraph(connected_nodes).copy()
        
        # Step 2: Graphviz ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'engine': 'dot',
            'format': 'png',
            'graph_attr': {
                'rankdir': 'LR',
                'size': '10,4',
                'dpi': '300',
                'bgcolor': 'white',
                'fontname': 'Arial',
                'fontsize': '11',
                'pad': '0.3',
                'ranksep': '1.2',
                'nodesep': '0.8',
                'splines': 'false',
                'concentrate': 'false',
                'ordering': 'out',
                'minlen': '1',
                'overlap': 'false'
            },
            'node_attr': {
                'shape': 'record',
                'style': 'rounded,filled',
                'fontname': 'Arial',
                'fontsize': '11',
                'fontcolor': 'black',
                'penwidth': '2',
                'fixedsize': 'false'
            },
            'edge_attr': {
                'fontname': 'Arial',
                'fontsize': '12',
                'color': 'darkblue',
                'arrowsize': '1.0',
                'arrowhead': 'normal',
                'penwidth': '3',
                'fontcolor': 'darkred',
                'minlen': '1',
                'len': '1.0'
            }
        }
        
        # Update with user parameters
        params = {**default_params, **kwargs}
        
        # Create Graphviz Digraph
        dot = Digraph(name=filename, engine=params['engine'], format=params.get('format', 'png'))
        
        # Set attributes
        for key, value in params['graph_attr'].items():
            dot.graph_attr[key] = str(value)
        for key, value in params['node_attr'].items():
            dot.node_attr[key] = str(value)
        for key, value in params['edge_attr'].items():
            dot.edge_attr[key] = str(value)
        
        # Process nodes
        for node in G_connected.nodes(data=True):
            node_id = str(node[0]).replace(':', '_')
            
            # Determine colors based on node position
            in_degree = G_connected.in_degree(node[0])
            out_degree = G_connected.out_degree(node[0])
            
            if in_degree == 0:
                fillcolor, color = '#90EE90', '#228B22'  # Green for start nodes
            elif out_degree == 0:
                fillcolor, color = '#FFB6C1', '#DC143C'  # Pink for end nodes
            else:
                fillcolor, color = '#87CEEB', '#4682B4'  # Blue for middle nodes
            
            # Apply wrapping method
            label, node_attrs = format_node_label(node_id, wrap_method)
            
            dot.node(node_id, label=label, fillcolor=fillcolor, color=color, **node_attrs)
        
        # Add edges
        for edge in G_connected.edges(data=True):
            source = str(edge[0]).replace(':', '_')
            target = str(edge[1]).replace(':', '_')
            edge_data = edge[2] if len(edge) > 2 else {}
            
            # Edge attributes
            edge_attrs = {}
            if 'label' in edge_data or 'relation' in edge_data:
                edge_attrs['label'] = f' {edge_data["relation"]} ' if 'relation' in edge_data else f' {edge_data["label"]} '
                edge_attrs['fontsize'] = '12'
                edge_attrs['fontcolor'] = 'darkred'
            
            dot.edge(source, target, **edge_attrs)
        
        # Render
        logger.info("ğŸ–¼ï¸ DAG ì´ë¯¸ì§€ ë Œë”ë§ ì¤‘...")
        output_path = dot.render(filename, directory=save_dir, cleanup=True)
        logger.info(f"âœ… DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ DAG ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def select_most_comprehensive(strings):
    """
    Select the most comprehensive string from a list of overlapping strings.
    Returns the longest string that contains other strings as substrings.
    
    Args:
        strings: List of strings to filter
        
    Returns:
        List of most comprehensive strings (usually one, but could be multiple if no containment)
    """
    if not strings:
        return []
    
    # Remove duplicates and sort by length (longest first)
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)
    
    result = []
    
    for current in unique_strings:
        # Check if current string contains any of the strings already in result
        is_contained = any(current in existing for existing in result)
        
        # Check if current string contains other strings not yet in result
        contains_others = any(other in current for other in unique_strings if other != current and other not in result)
        
        # If current is not contained by existing results and either:
        # 1. It contains other strings, or 
        # 2. No strings contain each other (keep all unique)
        if not is_contained:
            # Remove any strings from result that are contained in current
            result = [r for r in result if r not in current]
            result.append(current)
    
    return result

def extract_ngram_candidates(text, min_n=1, max_n=5):
    """
    ìƒí’ˆëª… ë§¤ì¹­ì„ ìœ„í•œ n-gram í›„ë³´ ì¶”ì¶œ
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        min_n: ìµœì†Œ ë‹¨ì–´ ìˆ˜
        max_n: ìµœëŒ€ ë‹¨ì–´ ìˆ˜
        
    Returns:
        list of dict: [{'ngram': [...], 'text': '...', 'start': 0, 'end': 2}, ...]
    """
    words = text.split()
    candidates = []
    
    for n in range(min_n, min(max_n + 1, len(words) + 1)):
        for i in range(len(words) - n + 1):
            window = words[i:i + n]
            candidates.append({
                'ngram': window,
                'text': ' '.join(window),
                'start_idx': i,
                'end_idx': i + n - 1,
                'n': n
            })
    
    return candidates

