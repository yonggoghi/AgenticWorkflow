#!/usr/bin/env python3
"""
Batch Processing Script for MMS Message Extraction with Parallel Processing
==========================================================================

This script performs batch processing of MMS messages using the MMSExtractor with
support for parallel processing to improve performance. It reads messages from a CSV file,
processes them in batches using multiprocessing, and saves results.

Features:
- Batch processing with MMSExtractor
- Parallel processing support (multiprocessing)
- DAG extraction with parallel processing
- Performance monitoring and metrics
- Result storage with timestamps
- Automatic update of processing status
- Configurable worker count

Usage:
    # Basic batch processing (parallel)
    python batch.py --batch-size 10
    
    # Custom worker count
    python batch.py --batch-size 50 --max-workers 8 --output-file results_batch.csv
    
    # With DAG extraction (parallel processing of main + DAG)
    python batch.py --batch-size 20 --extract-entity-dag --max-workers 4
    
    # Sequential processing (disable multiprocessing)
    python batch.py --batch-size 10 --disable-multiprocessing
    
    # Full configuration
    python batch.py --batch-size 100 --max-workers 16 --extract-entity-dag \
                     --llm-model ax --entity-extraction-mode llm
"""

import os
import sys
import argparse
import pandas as pd
import json
from datetime import datetime
import logging
from pathlib import Path

# Add current directory to path for imports
# Add parent directory to path to allow imports from core
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import METADATA_CONFIG
from core.mms_extractor import MMSExtractor, process_message_worker, process_messages_batch, save_result_to_mongodb_if_enabled

# MongoDB ìœ í‹¸ë¦¬í‹°ëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
# from utils.mongodb_utils import save_to_mongodb # (í•„ìš”ì‹œ ì‚¬ìš©)
import multiprocessing
import time

# Configure logging - í†µí•© ë¡œê·¸ íŒŒì¼ ì‚¬ìš©
from pathlib import Path

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = Path(__file__).parent.parent / 'logs'
log_dir.mkdir(exist_ok=True)

# ë°°ì¹˜ ì²˜ë¦¬ ì „ìš© ë¡œê¹… ì„¤ì •
import logging.handlers

root_logger = logging.getLogger()
if not root_logger.handlers:
    # ë°°ì¹˜ ì²˜ë¦¬ìš© íŒŒì¼ í•¸ë“¤ëŸ¬ (íšŒì „ ë¡œê·¸ - 20MBì”© ìµœëŒ€ 5ê°œ íŒŒì¼, ì¥ê¸° ë³´ì¡´)
    batch_file_handler = logging.handlers.RotatingFileHandler(
        log_dir / 'batch_processing.log',
        maxBytes=20*1024*1024,  # 20MB (ë°°ì¹˜ ë¡œê·¸ëŠ” ë§ì€ ì–‘)
        backupCount=5,          # ê°ì‚¬ ëª©ì ìœ¼ë¡œ ì¥ê¸° ë³´ì¡´
        encoding='utf-8'
    )
    
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    batch_file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    logging.basicConfig(
        level=logging.INFO,
        handlers=[batch_file_handler, console_handler]
    )
logger = logging.getLogger(__name__)


# save_result_to_mongodb_if_enabled í•¨ìˆ˜ëŠ” mms_extractor.pyì—ì„œ ì„í¬íŠ¸ë¨


class BatchProcessor:
    """
    Batch processor for MMS message extraction
    """
    
    def __init__(self, result_file_path="./results/batch_results.csv", max_workers=None, enable_multiprocessing=True, save_to_mongodb=False, save_results_enabled=False):
        """
        Initialize batch processor
        
        Args:
            result_file_path: Path to store batch processing results
            max_workers: Maximum number of worker processes/threads (default: CPU count)
            enable_multiprocessing: Whether to use multiprocessing for batch processing
            save_to_mongodb: Whether to save results to MongoDB
            save_results_enabled: Whether to save results to CSV file
        """
        self.result_file_path = result_file_path
        self.extractor = None
        self.mms_pdf = None
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.enable_multiprocessing = enable_multiprocessing
        self.save_to_mongodb = save_to_mongodb
        self.save_results_enabled = save_results_enabled
        self.extract_entity_dag = True
        
    def initialize_extractor(self, **extractor_kwargs):
        """
        Initialize the MMS extractor with given parameters
        
        Args:
            **extractor_kwargs: Keyword arguments for MMSExtractor initialization
        """
        logger.info("Initializing MMS Extractor...")
        try:
            self.extract_entity_dag = extractor_kwargs.get('extract_entity_dag', True)
            self.extractor = MMSExtractor(**extractor_kwargs)
            logger.info(f"MMS Extractor initialized successfully (DAG ì¶”ì¶œ: {'ON' if self.extract_entity_dag else 'OFF'})")
        except Exception as e:
            logger.error(f"Failed to initialize MMS Extractor: {str(e)}")
            raise
    
    def load_mms_data(self):
        """
        Load MMS PDF dataframe and prepare it for processing
        """
        logger.info(f"Loading MMS data from: {METADATA_CONFIG.mms_msg_path}")
        
        try:
            # Load the CSV file
            self.mms_pdf = pd.read_csv(METADATA_CONFIG.mms_msg_path)
            
            # Handle different column name formats
            # If 'mms_phrs' exists but 'msg' doesn't, rename it
            if 'mms_phrs' in self.mms_pdf.columns and 'msg' not in self.mms_pdf.columns:
                self.mms_pdf['msg'] = self.mms_pdf['mms_phrs']
                logger.info("Renamed 'mms_phrs' column to 'msg'")
            elif 'msg' not in self.mms_pdf.columns:
                raise ValueError("CSV file must have either 'msg' or 'mms_phrs' column")
            
            # Filter out rows with empty or NaN messages before converting to string
            original_count = len(self.mms_pdf)
            self.mms_pdf = self.mms_pdf[self.mms_pdf['msg'].notna() & (self.mms_pdf['msg'].astype(str).str.strip() != '')]
            filtered_count = original_count - len(self.mms_pdf)
            if filtered_count > 0:
                logger.info(f"Filtered out {filtered_count} empty messages")
            
            self.mms_pdf = self.mms_pdf.astype('str')
            
            # Add msg_id column if it doesn't exist
            if 'msg_id' not in self.mms_pdf.columns:
                self.mms_pdf['msg_id'] = self.mms_pdf.index.astype(str)
            
            logger.info(f"Loaded {len(self.mms_pdf)} messages")
            
        except Exception as e:
            logger.error(f"Failed to load MMS data: {str(e)}")
            raise
    
    def get_processed_msg_ids(self):
        """
        Load processed message IDs from the result file
        
        Returns:
            set: Set of processed message IDs
        """
        processed_ids = set()
        
        if os.path.exists(self.result_file_path):
            try:
                results_df = pd.read_csv(self.result_file_path)
                if 'msg_id' in results_df.columns:
                    processed_ids = set(results_df['msg_id'].astype(str))
                    logger.info(f"Found {len(processed_ids)} previously processed messages")
                else:
                    logger.warning("No msg_id column found in results file")
            except Exception as e:
                logger.error(f"Failed to load processed message IDs: {str(e)}")
        else:
            logger.info("No existing results file found - all messages are unprocessed")
            
        return processed_ids
    
    def sample_unprocessed_messages(self, batch_size):
        """
        Sample M random unprocessed messages based on result file
        
        Args:
            batch_size: Number of messages to sample
            
        Returns:
            pandas.DataFrame: Sampled messages that haven't been processed yet
        """
        # Get processed message IDs from result file
        processed_ids = self.get_processed_msg_ids()
        
        # Filter out already processed messages
        unprocessed = self.mms_pdf[~self.mms_pdf['msg_id'].isin(processed_ids)].copy()
        
        if len(unprocessed) == 0:
            logger.warning("No unprocessed messages found")
            return pd.DataFrame()
        
        # Sample random messages
        sample_size = min(batch_size, len(unprocessed))
        sampled_messages = unprocessed.sample(n=sample_size, random_state=None)
        
        logger.info(f"Total messages: {len(self.mms_pdf)}")
        logger.info(f"Previously processed: {len(processed_ids)}")
        logger.info(f"Unprocessed messages: {len(unprocessed)}")
        logger.info(f"Sampled {len(sampled_messages)} messages for processing")
        return sampled_messages
    
    def process_messages(self, sampled_messages):
        """
        Process sampled messages using MMSExtractor with parallel processing support
        
        Args:
            sampled_messages: DataFrame with messages to process
            
        Returns:
            list: List of processing results
        """
        if self.extractor is None:
            raise ValueError("Extractor not initialized. Call initialize_extractor() first.")
        
        processing_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        messages_list = []
        
        # Convert DataFrame to list of messages
        for idx, row in sampled_messages.iterrows():
            msg = row.get('msg', '')
            # message_id: DataFrameì— message_id ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ msg_id ì‚¬ìš©, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ idx ì‚¬ìš©
            # rowëŠ” Seriesì´ë¯€ë¡œ .get() ì‚¬ìš©, ì—†ìœ¼ë©´ None ë°˜í™˜
            message_id = row.get('message_id')
            if message_id is None or pd.isna(message_id):
                message_id = row.get('msg_id')
            if message_id is None or pd.isna(message_id):
                message_id = str(idx)
            else:
                message_id = str(message_id)
            
            # Skip empty messages (safety check)
            if not msg or msg.strip() == '' or msg == 'nan':
                logger.warning(f"Skipping empty message with ID: {message_id}")
                continue
                
            messages_list.append({'msg': msg, 'message_id': message_id})
        
        if self.enable_multiprocessing and len(messages_list) > 1:
            logger.info(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œë¡œ {len(messages_list)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘ (ì›Œì»¤: {self.max_workers}ê°œ)")
            results = self._process_messages_parallel(messages_list, processing_time)
        else:
            logger.info(f"âš¡ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œë¡œ {len(messages_list)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
            results = self._process_messages_sequential(messages_list, processing_time)
        
        return results
    
    def _process_messages_parallel(self, messages_list, processing_time):
        """
        Process messages in parallel using multiprocessing
        
        Args:
            messages_list: List of message dictionaries
            processing_time: Processing timestamp
            
        Returns:
            list: List of processing results
        """
        start_time = time.time()
        
        # ë©”ì‹œì§€ ë¬¸ìì—´ë§Œ ì¶”ì¶œ
        message_texts = [msg['msg'] for msg in messages_list]
        
        try:
            # process_messages_batch í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
            batch_result = process_messages_batch(
                self.extractor,
                messages_list,
                extract_dag=self.extract_entity_dag,
                max_workers=self.max_workers
            )
            
            results = []
            for i, msg_info in enumerate(messages_list):
                message_id = msg_info['message_id']
                msg = msg_info['msg']
                
                if i < len(batch_result):
                    extraction_result = batch_result[i]

                    # ë””ë²„ê¹…ìš© ì¶œë ¥ (í•„ìš”ì‹œì—ë§Œ í™œì„±í™”)
                    # print("=" * 50 + " extraction_result " + "=" * 50)
                    # print(f"Type: {type(extraction_result)}")
                    # print(f"Content: {extraction_result}")
                    # print("=" * 50)
                    
                    # Create result record
                    result_record = {
                        'message_id': message_id,
                        'message': msg,
                        'extraction_result': json.dumps(extraction_result, ensure_ascii=False),
                        'processed_at': processing_time,
                        'title': extraction_result.get('title', ''),
                        'purpose': json.dumps(extraction_result.get('purpose', []), ensure_ascii=False),
                        'product_count': len(extraction_result.get('product', [])),
                        'channel_count': len(extraction_result.get('channel', [])),
                        'pgm': json.dumps(extraction_result.get('pgm', []), ensure_ascii=False)
                    }
                    
                    # MongoDB ì €ì¥ (ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬)
                    if self.save_to_mongodb:
                        # LLM ëª¨ë¸ ì´ë¦„ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                        llm_model_name = getattr(self.extractor, 'llm_model', 'unknown')
                        if hasattr(llm_model_name, 'model_name'):
                            llm_model_name = llm_model_name.model_name
                        elif hasattr(llm_model_name, '__class__'):
                            llm_model_name = llm_model_name.__class__.__name__
                        
                        # MongoDB ì €ì¥ì„ ìœ„í•œ args ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
                        args_data = {
                            'save_to_mongodb': self.save_to_mongodb,
                            'llm_model': str(llm_model_name),
                            'offer_data_source': getattr(self.extractor, 'offer_info_data_src', 'unknown'),
                            'product_info_extraction_mode': getattr(self.extractor, 'product_info_extraction_mode', 'unknown'),
                            'entity_matching_mode': getattr(self.extractor, 'entity_extraction_mode', 'unknown'),
                            'extract_entity_dag': getattr(self.extractor, 'extract_entity_dag', False),
                            'user_id': 'BATCH_USER',
                            'processing_mode': 'batch'
                        }
                        save_result_to_mongodb_if_enabled(msg, extraction_result, args_data, self.extractor)
                    
                    # DAG ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…
                    if self.extract_entity_dag and 'entity_dag' in extraction_result:
                        dag_items = extraction_result['entity_dag']
                        if dag_items and len(dag_items) > 0:
                            logger.info(f"âœ… ë©”ì‹œì§€ {message_id} DAG ì¶”ì¶œ ì„±ê³µ - {len(dag_items)}ê°œ ê´€ê³„")
                        else:
                            logger.warning(f"âš ï¸ ë©”ì‹œì§€ {message_id} DAG ì¶”ì¶œ ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                    
                    results.append(result_record)
                    
                    # ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
                    is_error = self._is_error_result(result_record['extraction_result'])
                    logger.debug(f"ë©”ì‹œì§€ {message_id} ì—ëŸ¬ íŒë‹¨ ê²°ê³¼: {is_error}")
                    
                    if is_error:
                        logger.error(f"âŒ ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹¤íŒ¨")
                    else:
                        logger.info(f"âœ… ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì„±ê³µ")
                else:
                    # ì²˜ë¦¬ ì‹¤íŒ¨í•œ ê²½ìš° (ë°°ì¹˜ ê²°ê³¼ì— í•´ë‹¹ ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš°)
                    error_record = {
                        'message_id': message_id,
                        'message': msg,
                        'extraction_result': json.dumps({'error': 'Processing failed - no result returned'}, ensure_ascii=False),
                        'processed_at': processing_time,
                        'title': '',
                        'purpose': '[]',
                        'product_count': 0,
                        'channel_count': 0,
                        'pgm': '[]'
                    }
                    results.append(error_record)
                    logger.error(f"âŒ ë©”ì‹œì§€ {message_id} ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨ - ë°°ì¹˜ ê²°ê³¼ ì—†ìŒ")
            
            elapsed_time = time.time() - start_time
            logger.info(f"ğŸ¯ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ ì†Œìš”")
            return results
            
        except Exception as e:
            logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {str(e)}")
            return self._process_messages_sequential(messages_list, processing_time)
    
    def _process_messages_sequential(self, messages_list, processing_time):
        """
        Process messages sequentially (fallback method)
        
        Args:
            messages_list: List of message dictionaries
            processing_time: Processing timestamp
            
        Returns:
            list: List of processing results
        """
        results = []
        start_time = time.time()
        
        for i, msg_info in enumerate(messages_list, 1):
            msg = msg_info['msg']
            message_id = msg_info['message_id']
            
            try:
                logger.info(f"ì²˜ë¦¬ ì¤‘ ({i}/{len(messages_list)}): {message_id} - {msg[:50]}...")
                
                # DAG ì¶”ì¶œì´ í™œì„±í™”ëœ ê²½ìš° ë³‘ë ¬ë¡œ ì²˜ë¦¬
                if self.extract_entity_dag:
                    extraction_result = process_message_worker(
                        self.extractor, 
                        msg, 
                        extract_dag=True,
                        message_id=message_id
                    )
                else:
                    extraction_result = self.extractor.process_message(msg, message_id=message_id)
                
                # Create result record
                result_record = {
                    'message_id': message_id,
                    'message': msg,
                    'extraction_result': json.dumps(extraction_result, ensure_ascii=False),
                    'processed_at': processing_time,
                    'title': extraction_result.get('title', ''),
                    'purpose': json.dumps(extraction_result.get('purpose', []), ensure_ascii=False),
                    'product_count': len(extraction_result.get('product', [])),
                    'channel_count': len(extraction_result.get('channel', [])),
                    'pgm': json.dumps(extraction_result.get('pgm', []), ensure_ascii=False)
                }
                
                # MongoDB ì €ì¥ (ë°°ì¹˜ ìˆœì°¨ ì²˜ë¦¬)
                if self.save_to_mongodb:
                    # LLM ëª¨ë¸ ì´ë¦„ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                    llm_model_name = getattr(self.extractor, 'llm_model', 'unknown')
                    if hasattr(llm_model_name, 'model_name'):
                        llm_model_name = llm_model_name.model_name
                    elif hasattr(llm_model_name, '__class__'):
                        llm_model_name = llm_model_name.__class__.__name__
                    
                    # MongoDB ì €ì¥ì„ ìœ„í•œ args ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
                    args_data = {
                        'save_to_mongodb': self.save_to_mongodb,
                        'llm_model': str(llm_model_name),
                        'offer_data_source': getattr(self.extractor, 'offer_info_data_src', 'unknown'),
                        'product_info_extraction_mode': getattr(self.extractor, 'product_info_extraction_mode', 'unknown'),
                        'entity_matching_mode': getattr(self.extractor, 'entity_extraction_mode', 'unknown'),
                        'extract_entity_dag': getattr(self.extractor, 'extract_entity_dag', True),
                        'user_id': 'BATCH_USER',
                        'processing_mode': 'batch'
                    }
                    save_result_to_mongodb_if_enabled(msg, extraction_result, args_data, self.extractor)
                
                results.append(result_record)
                
                # DAG ì¶”ì¶œ ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…
                if self.extract_entity_dag and 'entity_dag' in extraction_result:
                    dag_items = extraction_result['entity_dag']
                    if dag_items and len(dag_items) > 0:
                        logger.info(f"âœ… ë©”ì‹œì§€ {message_id} DAG ì¶”ì¶œ ì„±ê³µ - {len(dag_items)}ê°œ ê´€ê³„")
                    else:
                        logger.warning(f"âš ï¸ ë©”ì‹œì§€ {message_id} DAG ì¶”ì¶œ ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                
                # ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
                is_error = self._is_error_result(result_record['extraction_result'])
                logger.debug(f"ë©”ì‹œì§€ {message_id} ì—ëŸ¬ íŒë‹¨ ê²°ê³¼: {is_error}")
                
                if is_error:
                    logger.error(f"âŒ ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹¤íŒ¨")
                else:
                    logger.info(f"âœ… ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì„±ê³µ")
                
            except Exception as e:
                logger.error(f"âŒ ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                # Add error record
                error_record = {
                    'message_id': message_id,
                    'message': msg,
                    'extraction_result': json.dumps({'error': str(e)}, ensure_ascii=False),
                    'processed_at': processing_time,
                    'title': '',
                    'purpose': '[]',
                    'product_count': 0,
                    'channel_count': 0,
                    'pgm': '[]'
                }
                results.append(error_record)
        
        elapsed_time = time.time() - start_time
        logger.info(f"âš¡ ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ë©”ì‹œì§€, {elapsed_time:.2f}ì´ˆ ì†Œìš”")
        return results
    
    def save_results(self, results):
        """
        Save processing results to CSV file (append if exists, create if new)
        
        Args:
            results: List of result dictionaries
            
        Returns:
            str: Path to the results file
        """
        if not results:
            logger.warning("No results to save")
            return self.result_file_path
        
        results_df = pd.DataFrame(results)
        
        # results ë””ë ‰í† ë¦¬ ìƒì„±
        result_dir = os.path.dirname(self.result_file_path)
        if result_dir:
            os.makedirs(result_dir, exist_ok=True)
        
        # Check if result file exists
        if os.path.exists(self.result_file_path):
            logger.info(f"Appending {len(results_df)} results to existing file: {self.result_file_path}")
            # Load existing results and append
            existing_df = pd.read_csv(self.result_file_path)
            combined_df = pd.concat([existing_df, results_df], ignore_index=True)
            combined_df.to_csv(self.result_file_path, index=False)
        else:
            logger.info(f"Creating new results file: {self.result_file_path}")
            results_df.to_csv(self.result_file_path, index=False)
        
        logger.info(f"Results saved to: {self.result_file_path}")
        return self.result_file_path
    
    def _is_error_result(self, extraction_result_str):
        """
        Check if extraction result contains an error
        
        Args:
            extraction_result_str: JSON string of extraction result
            
        Returns:
            bool: True if result contains error, False otherwise
        """
        try:
            if isinstance(extraction_result_str, str):
                result_dict = json.loads(extraction_result_str)
                # process_messages_batchëŠ” error í‚¤ê°€ ìˆì–´ë„ ì‹¤ì œ ì—ëŸ¬ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
                # error í‚¤ê°€ ìˆê³  ì‹¤ì œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                if 'error' in result_dict:
                    error_msg = result_dict['error']
                    # ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ Noneì¸ ê²½ìš°ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                    if error_msg and str(error_msg).strip():
                        logger.debug(f"ì‹¤ì œ ì—ëŸ¬ ë°œê²¬: {error_msg}")
                        return True
                    else:
                        logger.debug(f"error í‚¤ëŠ” ìˆì§€ë§Œ ë¹ˆ ê°’: '{error_msg}' - ì„±ê³µìœ¼ë¡œ ê°„ì£¼")
                        return False
                return False
            elif isinstance(extraction_result_str, dict):
                if 'error' in extraction_result_str:
                    error_msg = extraction_result_str['error']
                    if error_msg and str(error_msg).strip():
                        return True
                    else:
                        return False
                return False
            else:
                logger.debug(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: {type(extraction_result_str)}")
                return False
        except (json.JSONDecodeError, TypeError) as e:
            logger.debug(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì›ë³¸: {extraction_result_str}")
            return False
    
    def log_processing_summary(self, processed_msg_ids):
        """
        Log processing summary for processed message IDs
        
        Args:
            processed_msg_ids: List of message IDs that were processed
        """
        if processed_msg_ids:
            logger.info(f"Successfully processed {len(processed_msg_ids)} messages")
            logger.info(f"Processed message IDs: {processed_msg_ids[:10]}{'...' if len(processed_msg_ids) > 10 else ''}")
        else:
            logger.warning("No messages were successfully processed")
    
    def run_batch(self, batch_size, **extractor_kwargs):
        """
        Run complete batch processing pipeline with performance monitoring
        
        Args:
            batch_size: Number of messages to process
            **extractor_kwargs: Keyword arguments for MMSExtractor
            
        Returns:
            dict: Processing summary with performance metrics
        """
        overall_start_time = time.time()
        
        try:
            # Initialize components
            self.load_mms_data()
            self.initialize_extractor(**extractor_kwargs)
            
            # Sample and process messages
            sampled_messages = self.sample_unprocessed_messages(batch_size)
            
            if sampled_messages.empty:
                return {
                    'status': 'completed',
                    'processed_count': 0,
                    'message': 'No unprocessed messages found',
                    'processing_mode': 'N/A',
                    'max_workers': self.max_workers,
                    'dag_extraction': self.extract_entity_dag
                }
            
            # Process messages with timing
            processing_start_time = time.time()
            results = self.process_messages(sampled_messages)
            processing_time = time.time() - processing_start_time
            
            # Save results and log summary (ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°ë§Œ)
            if self.save_results_enabled:
                self.save_results(results)
            else:
                logger.info("ğŸ’¾ ë°°ì¹˜ ê²°ê³¼ CSV íŒŒì¼ ì €ì¥ ìƒëµ (--save-results ì˜µì…˜ìœ¼ë¡œ í™œì„±í™” ê°€ëŠ¥)")
            
            processed_msg_ids = [r['message_id'] for r in results if not self._is_error_result(r.get('extraction_result', ''))]
            self.log_processing_summary(processed_msg_ids)
            
            total_time = time.time() - overall_start_time
            processing_mode = "ë³‘ë ¬ ì²˜ë¦¬" if (self.enable_multiprocessing and len(results) > 1) else "ìˆœì°¨ ì²˜ë¦¬"
            
            # Performance metrics
            avg_time_per_message = processing_time / len(results) if results else 0
            throughput = len(results) / processing_time if processing_time > 0 else 0
            
            success_rate = len(processed_msg_ids) / len(results) * 100 if results else 0
            
            logger.info("="*50)
            logger.info("ğŸ¯ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ìš”ì•½")
            logger.info("="*50)
            logger.info(f"ì²˜ë¦¬ ëª¨ë“œ: {processing_mode}")
            logger.info(f"ì›Œì»¤ ìˆ˜: {self.max_workers}")
            logger.info(f"DAG ì¶”ì¶œ: {'ON' if self.extract_entity_dag else 'OFF'}")
            logger.info(f"ì´ ì²˜ë¦¬ ë©”ì‹œì§€: {len(results)}ê°œ")
            logger.info(f"ì„±ê³µ: {len(processed_msg_ids)}ê°œ")
            logger.info(f"ì‹¤íŒ¨: {len(results) - len(processed_msg_ids)}ê°œ")
            logger.info(f"ì„±ê³µë¥ : {success_rate:.2f}%")
            logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            logger.info(f"ë©”ì‹œì§€ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_message:.2f}ì´ˆ")
            logger.info(f"ì²˜ë¦¬ëŸ‰: {throughput:.2f} ë©”ì‹œì§€/ì´ˆ")
            logger.info("="*50)
            
            return {
                'status': 'completed',
                'processed_count': len(results),
                'successful_count': len(processed_msg_ids),
                'failed_count': len(results) - len(processed_msg_ids),
                'results_file': self.result_file_path,
                'processing_mode': processing_mode,
                'max_workers': self.max_workers,
                'dag_extraction': self.extract_entity_dag,
                'total_time_seconds': round(total_time, 2),
                'processing_time_seconds': round(processing_time, 2),
                'avg_time_per_message': round(avg_time_per_message, 2),
                'throughput_messages_per_second': round(throughput, 2),
                'success_rate': round(len(processed_msg_ids) / len(results) * 100, 2) if results else 0
            }
            
        except Exception as e:
            logger.error(f"Batch processing failed: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e),
                'processed_count': 0,
                'processing_mode': 'N/A',
                'max_workers': self.max_workers,
                'dag_extraction': self.extract_entity_dag
            }


def main():
    """
    Main function for command-line execution
    """
    parser = argparse.ArgumentParser(description='Batch MMS Message Processing')
    
    # Batch processing arguments
    parser.add_argument('--batch-size', '-b', type=int, default=10,
                       help='Number of messages to process (default: 10)')
    parser.add_argument('--output-file', '-o', type=str, default='./results/batch_results.csv',
                       help='Output CSV file for results (default: results/batch_results.csv)')
    parser.add_argument('--save-results', action='store_true', default=False,
                       help='ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (results/ ë””ë ‰í† ë¦¬ì— ì €ì¥)')
    
    # Parallel processing arguments
    parser.add_argument('--max-workers', '-w', type=int, default=None,
                       help='Maximum number of worker processes (default: CPU count)')
    parser.add_argument('--disable-multiprocessing', action='store_true', default=False,
                       help='Disable multiprocessing and use sequential processing')
    
    # MMSExtractor arguments
    parser.add_argument('--offer-data-source', choices=['local', 'db'], default='db',
                       help='ë°ì´í„° ì†ŒìŠ¤ (local: CSV íŒŒì¼, db: ë°ì´í„°ë² ì´ìŠ¤)')
    parser.add_argument('--product-info-extraction-mode', choices=['nlp', 'llm', 'rag'], default='llm',
                       help='Product information extraction mode (default: llm)')
    parser.add_argument('--entity-extraction-mode', choices=['logic', 'llm'], default='llm',
                       help='Entity extraction mode (default: llm)')
    parser.add_argument('--llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='ax',
                       help='ë©”ì¸ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  LLM ëª¨ë¸ (default: ax)')
    parser.add_argument('--entity-llm-model', choices=['gem', 'ax', 'cld', 'gen', 'gpt'], default='ax',
                       help='ì—”í‹°í‹° ì¶”ì¶œì— ì‚¬ìš©í•  LLM ëª¨ë¸ (default: ax)')
    parser.add_argument('--entity-extraction-context-mode', choices=['dag', 'pairing', 'none', 'ont'], default='dag',
                       help='ì—”í‹°í‹° ì¶”ì¶œ ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œ (dag: DAG ì»¨í…ìŠ¤íŠ¸, pairing: PAIRING ì»¨í…ìŠ¤íŠ¸, none: ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ, ont: ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¶”ì¶œ, default: dag)')
    parser.add_argument('--extract-entity-dag', action='store_true', default=False, 
                       help='ì—”í‹°í‹° DAG ì¶”ì¶œ í™œì„±í™” - ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ì¶”ì¶œí•˜ê³  ì‹œê°í™” (default: False)')
    
    # Logging arguments
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO',
                       help='ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUG: ìƒì„¸, INFO: ì¼ë°˜, WARNING: ê²½ê³ , ERROR: ì˜¤ë¥˜ë§Œ)')
    
    # MongoDB arguments
    parser.add_argument('--save-to-mongodb', action='store_true', default=True,
                       help='ì¶”ì¶œ ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥ (utils/mongodb_utils.py í•„ìš”)')
    parser.add_argument('--test-mongodb', action='store_true', default=False,
                       help='MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ê³  ì¢…ë£Œ')

    args = parser.parse_args()
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì • - ë£¨íŠ¸ ë¡œê±°ì™€ ëª¨ë“  í•¸ë“¤ëŸ¬ì— ì ìš©
    log_level = getattr(logging, args.log_level)
    root_logger.setLevel(log_level)
    for handler in root_logger.handlers:
        handler.setLevel(log_level)
    
    logger.info(f"ë¡œê·¸ ë ˆë²¨ ì„¤ì •: {args.log_level}")
    
    # MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°
    if args.test_mongodb:
        try:
            from utils.mongodb_utils import test_mongodb_connection
        except ImportError:
            print("âŒ MongoDB ìœ í‹¸ë¦¬í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("utils/mongodb_utils.py íŒŒì¼ê³¼ pymongo íŒ¨í‚¤ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        
        print("ğŸ”Œ MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        if test_mongodb_connection():
            print("âœ… MongoDB ì—°ê²° ì„±ê³µ!")
            sys.exit(0)
        else:
            print("âŒ MongoDB ì—°ê²° ì‹¤íŒ¨!")
            print("MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
    
    # ì¶”ì¶œê¸° ì„¤ì • ì¤€ë¹„
    # extract_entity_dag: Trueì¸ ê²½ìš° ê° ë©”ì‹œì§€ë§ˆë‹¤ DAG ì¶”ì¶œ ë° ì´ë¯¸ì§€ ìƒì„± ìˆ˜í–‰
    extractor_kwargs = {
        'offer_info_data_src': args.offer_data_source,
        'product_info_extraction_mode': args.product_info_extraction_mode,
        'entity_extraction_mode': args.entity_extraction_mode,
        'llm_model': args.llm_model,
        'entity_llm_model': args.entity_llm_model,
        'extract_entity_dag': args.extract_entity_dag,  # DAG ì¶”ì¶œ ì—¬ë¶€
        'entity_extraction_context_mode': args.entity_extraction_context_mode  # ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œ
    }
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers = args.max_workers or multiprocessing.cpu_count()
    enable_multiprocessing = not args.disable_multiprocessing
    
    logger.info("="*50)
    logger.info("ğŸš€ Starting Batch MMS Processing")
    logger.info("="*50)
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"ì¶œë ¥ íŒŒì¼: {args.output_file}")
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬: {'ON' if enable_multiprocessing else 'OFF'}")
    logger.info(f"ìµœëŒ€ ì›Œì»¤ ìˆ˜: {max_workers}")
    logger.info(f"ì¶”ì¶œê¸° ì„¤ì •: {extractor_kwargs}")
    if args.extract_entity_dag:
        logger.info("ğŸ¯ DAG ì¶”ì¶œ ëª¨ë“œ í™œì„±í™”ë¨")
    if args.save_to_mongodb:
        logger.info("ğŸ“„ MongoDB ì €ì¥ ëª¨ë“œ í™œì„±í™”ë¨")
    logger.info("="*50)
    
    # Run batch processing
    processor = BatchProcessor(
        result_file_path=args.output_file,
        max_workers=max_workers,
        enable_multiprocessing=enable_multiprocessing,
        save_to_mongodb=args.save_to_mongodb,
        save_results_enabled=args.save_results
    )
    summary = processor.run_batch(args.batch_size, **extractor_kwargs)
    
    # Print summary
    logger.info("="*50)
    logger.info("Batch Processing Summary")
    logger.info("="*50)
    for key, value in summary.items():
        logger.info(f"{key}: {value}")
    logger.info("="*50)
    
    return summary


if __name__ == '__main__':
    main()