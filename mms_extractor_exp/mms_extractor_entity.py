# %%
"""
MMS Extractor - ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ ëª¨ë“ˆ
========================================

ì´ ëª¨ë“ˆì€ MMSExtractorì˜ ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Mixin íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ MMSExtractor í´ë˜ìŠ¤ì— í†µí•©ë©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
- LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ
- ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­
- ìœ ì‚¬ë„ ê³„ì‚° ë° í•„í„°ë§
"""

import logging
import traceback
import re
from typing import List, Tuple, Dict
import pandas as pd
from langchain_core.prompts import PromptTemplate
from joblib import Parallel, delayed

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì„í¬íŠ¸
from utils import (
    log_performance,
    validate_text_input,
    safe_execute,
    parallel_fuzzy_similarity,
    parallel_seq_similarity,
    filter_text_by_exc_patterns,
    filter_specific_terms,
    extract_ngram_candidates
)

# í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
from prompts import (
    HYBRID_DAG_EXTRACTION_PROMPT,
    SIMPLE_ENTITY_EXTRACTION_PROMPT
)

# ì„¤ì • ì„í¬íŠ¸
try:
    from config.settings import PROCESSING_CONFIG
except ImportError:
    logging.warning("ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class MMSExtractorEntityMixin:
    """
    MMS Extractor ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ Mixin
    
    ì´ í´ë˜ìŠ¤ëŠ” MMSExtractorì˜ ì—”í‹°í‹° ì¶”ì¶œ ë° ë§¤ì¹­ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    @log_performance
    def extract_entities_from_kiwi(self, mms_msg: str) -> Tuple[List[str], pd.DataFrame]:
        """Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            logger.info("=== Kiwi ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘ ===")
            mms_msg = validate_text_input(mms_msg)
            logger.info(f"ì²˜ë¦¬í•  ë©”ì‹œì§€ ê¸¸ì´: {len(mms_msg)} ë¬¸ì")
            
            # ìƒí’ˆ ë°ì´í„° ìƒíƒœ í™•ì¸
            if self.item_pdf_all.empty:
                logger.error("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            if 'item_nm_alias' not in self.item_pdf_all.columns:
                logger.error("item_nm_alias ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì—”í‹°í‹° ì¶”ì¶œ ë¶ˆê°€")
                return [], pd.DataFrame()
            
            unique_aliases = self.item_pdf_all['item_nm_alias'].unique()
            logger.info(f"ë§¤ì¹­í•  ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(unique_aliases)}ê°œ")
            
            # ë¬¸ì¥ ë¶„í•  ë° í•˜ìœ„ ë¬¸ì¥ ì²˜ë¦¬
            sentences = sum(self.kiwi.split_into_sents(
                re.split(r"_+", mms_msg), return_tokens=True, return_sub_sents=True
            ), [])
            
            sentences_all = []
            for sent in sentences:
                if sent.subs:
                    sentences_all.extend(sent.subs)
                else:
                    sentences_all.append(sent)
            
            logger.info(f"ë¶„í• ëœ ë¬¸ì¥ ìˆ˜: {len(sentences_all)}ê°œ")
            
            # ì œì™¸ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¬¸ì¥ í•„í„°ë§
            sentence_list = [
                filter_text_by_exc_patterns(sent, self.exc_tag_patterns) 
                for sent in sentences_all
            ]
            
            logger.info(f"í•„í„°ë§ëœ ë¬¸ì¥ë“¤: {sentence_list[:3]}...")

            # í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
            result_msg = self.kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)
            all_tokens = [(token.form, token.tag) for token in result_msg]
            logger.info(f"ì „ì²´ í† í° ìˆ˜: {len(all_tokens)}ê°œ")
            
            # NNP íƒœê·¸ í† í°ë“¤ë§Œ ì¶”ì¶œ
            nnp_tokens = [token.form for token in result_msg if token.tag == 'NNP']
            logger.info(f"NNP íƒœê·¸ í† í°ë“¤: {nnp_tokens}")
            
            entities_from_kiwi = [
                token.form for token in result_msg 
                if token.tag == 'NNP' and 
                   token.form not in self.stop_item_names + ['-'] and 
                   len(token.form) >= 2 and 
                   not token.form.lower() in self.stop_item_names
            ]
            entities_from_kiwi = [e for e in filter_specific_terms(entities_from_kiwi) if e in unique_aliases]
            
            logger.info(f"í•„í„°ë§ í›„ Kiwi ì¶”ì¶œ ì—”í‹°í‹°: {list(set(entities_from_kiwi))}")

            # í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ìœ ì‚¬ ìƒí’ˆëª… ì°¾ê¸°
            logger.info("í¼ì§€ ë§¤ì¹­ ì‹œì‘...")
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                sentence_list, 
                unique_aliases,
                threshold=getattr(PROCESSING_CONFIG, 'fuzzy_threshold', 0.5),
                text_col_nm='sent', 
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            logger.info(f"í¼ì§€ ë§¤ì¹­ ê²°ê³¼ í¬ê¸°: {similarities_fuzzy.shape if not similarities_fuzzy.empty else 'ë¹„ì–´ìˆìŒ'}")
            
            if similarities_fuzzy.empty:
                logger.warning("í¼ì§€ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Kiwi ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                cand_item_list = list(entities_from_kiwi) if entities_from_kiwi else []
                logger.info(f"Kiwi ê¸°ë°˜ í›„ë³´ ì•„ì´í…œ: {cand_item_list}")
                
                if cand_item_list:
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                    logger.info(f"ë§¤ì¹­ëœ ìƒí’ˆ ì •ë³´: {extra_item_pdf.shape}")
                else:
                    extra_item_pdf = pd.DataFrame()
                    logger.warning("í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                
                return cand_item_list, extra_item_pdf
            else:
                logger.info(f"í¼ì§€ ë§¤ì¹­ ì„±ê³µ: {len(similarities_fuzzy)}ê°œ ê²°ê³¼")
                if not similarities_fuzzy.empty:
                    sample_fuzzy = similarities_fuzzy.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                    logger.info(f"í¼ì§€ ë§¤ì¹­ ìƒ˜í”Œ: {sample_fuzzy}")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ë¥¼ í†µí•œ ì •ë°€ ë§¤ì¹­
            logger.info("ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")
            similarities_seq = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='sent',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=getattr(PROCESSING_CONFIG, 'batch_size', 100),
                default_return=pd.DataFrame()
            )
            
            logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê²°ê³¼ í¬ê¸°: {similarities_seq.shape if not similarities_seq.empty else 'ë¹„ì–´ìˆìŒ'}")
            if not similarities_seq.empty:
                sample_seq = similarities_seq.head(3)[['sent', 'item_nm_alias', 'sim']].to_dict('records')
                logger.info(f"ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ìƒ˜í”Œ: {sample_seq}")
            
            # ì„ê³„ê°’ ì´ìƒì˜ í›„ë³´ ì•„ì´í…œë“¤ í•„í„°ë§
            similarity_threshold = getattr(PROCESSING_CONFIG, 'similarity_threshold', 0.2)
            logger.info(f"ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
            
            cand_items = similarities_seq.query(
                "sim >= @similarity_threshold and "
                "item_nm_alias.str.contains('', case=False) and "
                "item_nm_alias not in @self.stop_item_names"
            )
            logger.info(f"ì„ê³„ê°’ í•„í„°ë§ í›„ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_items)}ê°œ")
            
            # Kiwiì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ë“¤ ì¶”ê°€
            entities_from_kiwi_pdf = self.item_pdf_all.query("item_nm_alias in @entities_from_kiwi")[
                ['item_nm','item_nm_alias']
            ]
            entities_from_kiwi_pdf['sim'] = 1.0
            logger.info(f"Kiwi ì—”í‹°í‹° ë§¤ì¹­ ê²°ê³¼: {len(entities_from_kiwi_pdf)}ê°œ")

            # ê²°ê³¼ í†µí•© ë° ìµœì¢… í›„ë³´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            cand_item_pdf = pd.concat([cand_items, entities_from_kiwi_pdf])
            logger.info(f"í†µí•©ëœ í›„ë³´ ì•„ì´í…œ ìˆ˜: {len(cand_item_pdf)}ê°œ")
            
            if not cand_item_pdf.empty:
                cand_item_array = cand_item_pdf.sort_values('sim', ascending=False).groupby([
                    "item_nm_alias"
                ])['sim'].max().reset_index(name='final_sim').sort_values(
                    'final_sim', ascending=False
                ).query("final_sim >= 0.2")['item_nm_alias'].unique()
                
                # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „ì„± ë³´ì¥
                cand_item_list = list(cand_item_array) if hasattr(cand_item_array, '__iter__') else []
                
                logger.info(f"ìµœì¢… í›„ë³´ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸: {cand_item_list}")
                
                if cand_item_list:
                    extra_item_pdf = self.item_pdf_all.query("item_nm_alias in @cand_item_list")[
                        ['item_nm','item_nm_alias','item_id']
                    ].groupby(["item_nm"])['item_id'].apply(list).reset_index()
                else:
                    extra_item_pdf = pd.DataFrame()
                
                logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ DataFrame í¬ê¸°: {extra_item_pdf.shape}")
                if not extra_item_pdf.empty:
                    sample_final = extra_item_pdf.head(3).to_dict('records')
                    logger.info(f"ìµœì¢… ìƒí’ˆ ì •ë³´ ìƒ˜í”Œ: {sample_final}")
            else:
                logger.warning("í†µí•©ëœ í›„ë³´ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤!")
                cand_item_list = []
                extra_item_pdf = pd.DataFrame()

            return entities_from_kiwi, cand_item_list, extra_item_pdf
            
        except Exception as e:
            logger.error(f"Kiwi ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return [], [], pd.DataFrame()

    def extract_entities_by_logic(self, cand_entities: List[str], threshold_for_fuzzy: float = 0.5) -> pd.DataFrame:
        """ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            if not cand_entities:
                return pd.DataFrame()
            
            # í¼ì§€ ìœ ì‚¬ë„ ê³„ì‚°
            similarities_fuzzy = safe_execute(
                parallel_fuzzy_similarity,
                cand_entities,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=threshold_for_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                default_return=pd.DataFrame()
            )
            
            if similarities_fuzzy.empty:
                return pd.DataFrame()
            
            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚°
            cand_entities_sim = self._calculate_combined_similarity(similarities_fuzzy)
            
            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"ë¡œì§ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _calculate_combined_similarity(self, similarities_fuzzy: pd.DataFrame, weights: dict = None) -> pd.DataFrame:
        """s1, s2 ì •ê·œí™” ë°©ì‹ìœ¼ë¡œ ê°ê° ê³„ì‚° í›„ í•©ì‚°"""
        try:
            # s1 ì •ê·œí™”
            sim_s1 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s1',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = safe_execute(
                parallel_seq_similarity,
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=getattr(PROCESSING_CONFIG, 'n_jobs', 4),
                batch_size=30,
                normalizaton_value='s2',
                default_return=pd.DataFrame()
            ).rename(columns={'sim': 'sim_s2'})
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            if not sim_s1.empty and not sim_s2.empty:
                try:
                    combined = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
                    filtered = combined.query("(sim_s1>=@PROCESSING_CONFIG.combined_similarity_threshold and sim_s2>=@PROCESSING_CONFIG.combined_similarity_threshold)")
                    if filtered.empty:
                        logger.warning("ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                        return pd.DataFrame()
                    combined = filtered.groupby(['item_name_in_msg', 'item_nm_alias']).agg({
                        'sim_s1': 'sum',
                        'sim_s2': 'sum'
                    }).reset_index()
                    combined['sim'] = combined['sim_s1'] + combined['sim_s2']
                except Exception as e:
                    logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    return pd.DataFrame()
                return combined
            else:
                logger.warning("ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ê²°í•© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _parse_entity_response(self, response: str) -> List[str]:
        """
        LLM ì‘ë‹µì—ì„œ ì—”í‹°í‹°ë¥¼ ê²¬ê³ í•˜ê²Œ íŒŒì‹±
        
        ì—¬ëŸ¬ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ LLM ì‘ë‹µ í˜•ì‹ì„ ì²˜ë¦¬
        """
        try:
            # Strategy 1: ENTITY: ë¼ì¸ì„ ì°¾ì•„ì„œ ì •í™•í•˜ê²Œ ì¶”ì¶œ
            lines = response.split('\n')
            for line in lines:
                line_stripped = line.strip()
                line_upper = line_stripped.upper()
                
                if line_upper.startswith('REASON:'):
                    continue
                
                if line_upper.startswith('ENTITY:'):
                    entity_part = line_stripped[line_upper.find('ENTITY:') + 7:].strip()
                    
                    if not entity_part or entity_part.lower() in ['none', 'empty', 'ì—†ìŒ', 'null']:
                        logger.debug("ENTITY ì„¹ì…˜ì´ ë¹„ì–´ìˆìŒ (ì •ìƒ)")
                        return []
                    
                    if len(entity_part) > 200:
                        logger.warning(f"ENTITY ê°’ì´ ë„ˆë¬´ ê¹€ ({len(entity_part)}ì) - ì„¤ëª… ë¬¸ì¥ìœ¼ë¡œ íŒë‹¨")
                        continue
                    
                    entities = [e.strip() for e in entity_part.split(',') if e.strip()]
                    
                    valid_entities = []
                    for entity in entities:
                        if len(entity) > 100:
                            logger.debug(f"ì—”í‹°í‹°ê°€ ë„ˆë¬´ ê¹€ ({len(entity)}ì): {entity[:50]}...")
                            continue
                        if entity.startswith('"') and not entity.endswith('"'):
                            logger.debug(f"ë¶ˆì™„ì „í•œ ë”°ì˜´í‘œ êµ¬ì¡°: {entity[:50]}...")
                            continue
                        valid_entities.append(entity)
                    
                    if valid_entities:
                        logger.debug(f"íŒŒì‹±ëœ ì—”í‹°í‹°: {valid_entities}")
                        return valid_entities
            
            # Strategy 2: ENTITY: íŒ¨í„´ì„ ì •ê·œì‹ìœ¼ë¡œ ì°¾ê¸°
            entity_pattern = r'ENTITY:\s*([^\n]*?)(?:\n|$)'
            entity_matches = list(re.finditer(entity_pattern, response, re.IGNORECASE))
            
            if entity_matches:
                last_match = entity_matches[-1]
                entity_text = last_match.group(1).strip()
                
                if entity_text and entity_text.lower() not in ['none', 'empty', 'ì—†ìŒ', 'null']:
                    if len(entity_text) <= 200:
                        entities = [e.strip() for e in entity_text.split(',') 
                                   if e.strip() and len(e.strip()) <= 100]
                        if entities:
                            logger.debug(f"ì •ê·œì‹ìœ¼ë¡œ íŒŒì‹±ëœ ì—”í‹°í‹°: {entities}")
                            return entities
            
            # Strategy 3: ENTITY: í‚¤ì›Œë“œ ì—†ì´ ì—”í‹°í‹°ë§Œ ë°˜í™˜ëœ ê²½ìš°
            for line in reversed(lines):
                line_stripped = line.strip()
                if not line_stripped:
                    continue
                
                if line_stripped.upper().startswith('REASON:'):
                    continue
                
                if len(line_stripped) > 200:
                    continue
                
                if ',' in line_stripped:
                    entities = [e.strip() for e in line_stripped.split(',') 
                               if e.strip() and len(e.strip()) <= 100]
                    if entities:
                        if all(len(e) <= 100 for e in entities):
                            logger.debug(f"í‚¤ì›Œë“œ ì—†ì´ íŒŒì‹±ëœ ì—”í‹°í‹°: {entities}")
                            return entities
                elif len(line_stripped) <= 100:
                    logger.debug(f"ë‹¨ì¼ ì—”í‹°í‹°: [{line_stripped}]")
                    return [line_stripped]
            
            # Strategy 4: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logger.debug(f"ì—”í‹°í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‘ë‹µ: {response[:100]}...")
            return []
            
        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_optimal_batch_size(self, msg_text: str, base_size: int = 50) -> int:
        """ë©”ì‹œì§€ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        msg_length = len(msg_text)
        
        if msg_length < 500:
            return min(base_size * 2, 100)
        elif msg_length < 1000:
            return base_size
        else:
            return max(base_size // 2, 25)
    
    @log_performance
    def extract_entities_by_llm(self, msg_text: str, rank_limit: int = 50, llm_models: List = None, external_cand_entities: List[str] = []) -> pd.DataFrame:
        """
        LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ (ë³µìˆ˜ ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ” [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì‹œì‘")
            logger.info(f"ğŸ“ ì…ë ¥ íŒŒë¼ë¯¸í„°:")
            logger.info(f"   - rank_limit: {rank_limit}")
            logger.info(f"   - external_cand_entities ì œê³µ ì—¬ë¶€: {external_cand_entities is not None}")
            if external_cand_entities is not None:
                logger.info(f"   - external_cand_entities ê°œìˆ˜: {len(external_cand_entities)}")
            
            msg_text = validate_text_input(msg_text)
            logger.info(f"ğŸ“„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(msg_text):,} ë¬¸ì")
            
            # LLM ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            if llm_models is None:
                llm_models = [self.llm_model]
                logger.info(f"ğŸ¤– LLM ëª¨ë¸ ìë™ ì„ íƒ: ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (1ê°œ)")
            else:
                logger.info(f"ğŸ¤– LLM ëª¨ë¸ ì§€ì •ë¨: {len(llm_models)}ê°œ ëª¨ë¸ ì‚¬ìš©")
            
            for idx, model in enumerate(llm_models):
                model_name = getattr(model, 'model_name', 'Unknown')
                logger.info(f"   [{idx+1}] ëª¨ë¸: {model_name}")
            
            def get_entities_and_dag_by_llm(args_dict):
                """ë‹¨ì¼ LLMìœ¼ë¡œ ì—”í‹°í‹°ì™€ DAG ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
                llm_model, prompt = args_dict['llm_model'], args_dict['prompt']
                extract_dag = args_dict.get('extract_dag', True)  # ê¸°ë³¸ê°’ì€ True (í•˜ìœ„ í˜¸í™˜ì„±)
                model_name = getattr(llm_model, 'model_name', 'Unknown')
                
                try:
                    logger.info(f"ğŸ”„ [{model_name}] LLM í˜¸ì¶œ ì‹œì‘")
                    logger.info(f"   ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt):,} ë¬¸ì")
                    logger.info(f"   ğŸ“„ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì): {prompt[:200]}...")
                    
                    # PromptTemplate ì‚¬ìš©
                    zero_shot_prompt = PromptTemplate(
                        input_variables=["prompt"],
                        template="{prompt}"
                    )
                    
                    logger.info(f"   ğŸš€ [{model_name}] LLM API í˜¸ì¶œ ì¤‘...")
                    # LLM í˜¸ì¶œ
                    chain = zero_shot_prompt | llm_model
                    response = chain.invoke({"prompt": prompt}).content
                    
                    logger.info(f"   âœ… [{model_name}] LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                    logger.info(f"   ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response):,} ë¬¸ì")
                    logger.info(f"   ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 300ì): {response[:300]}...")
                    
                    # ê²¬ê³ í•œ ì‘ë‹µ íŒŒì‹± ì‚¬ìš©
                    logger.info(f"   ğŸ” [{model_name}] ì—”í‹°í‹° íŒŒì‹± ì‹œì‘...")
                    cand_entity_list_raw = self._parse_entity_response(response)
                    logger.info(f"   ğŸ“Š [{model_name}] íŒŒì‹±ëœ ì›ë³¸ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list_raw)}ê°œ")
                    if cand_entity_list_raw:
                        logger.info(f"   ğŸ“ [{model_name}] ì›ë³¸ ì—”í‹°í‹°: {cand_entity_list_raw}")
                    
                    # ì •ì§€ì–´ ë° ê¸¸ì´ í•„í„°ë§
                    cand_entity_list = [e for e in cand_entity_list_raw if e not in self.stop_item_names and len(e) >= 2]
                    logger.info(f"   ğŸ“Š [{model_name}] í•„í„°ë§ í›„ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
                    if cand_entity_list:
                        logger.info(f"   ğŸ“ [{model_name}] í•„í„°ë§ëœ ì—”í‹°í‹°: {cand_entity_list}")
                    else:
                        logger.warning(f"   âš ï¸ [{model_name}] í•„í„°ë§ í›„ ìœ íš¨í•œ ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                    
                    # DAG ì„¹ì…˜ ì¶”ì¶œ (extract_dagê°€ Trueì¼ ë•Œë§Œ)
                    dag_text = ""
                    if extract_dag:
                        logger.info(f"   ğŸ” [{model_name}] DAG ì„¹ì…˜ ì¶”ì¶œ ì‹œì‘...")
                        dag_match = re.search(r'DAG:\s*(.*)', response, re.DOTALL | re.IGNORECASE)
                        if dag_match:
                            dag_text = dag_match.group(1).strip()
                            logger.info(f"   âœ… [{model_name}] DAG ì¶”ì¶œ ì„±ê³µ")
                            logger.info(f"   ğŸ“ DAG í…ìŠ¤íŠ¸ ê¸¸ì´: {len(dag_text):,} ë¬¸ì")
                            logger.info(f"   ğŸ“„ DAG ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì): {dag_text[:200]}...")
                        else:
                            logger.warning(f"   âš ï¸ [{model_name}] DAG ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            logger.info(f"   ğŸ’¡ ì‘ë‹µì— 'DAG:' í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸: {'DAG:' in response.upper()}")
                    
                    logger.info(f"   âœ… [{model_name}] ì²˜ë¦¬ ì™„ë£Œ - ì—”í‹°í‹°: {len(cand_entity_list)}ê°œ, DAG: {'ìˆìŒ' if dag_text else 'ì—†ìŒ'}")
                    return {"entities": cand_entity_list, "dag_text": dag_text}
                    
                except Exception as e:
                    logger.error(f"   âŒ [{model_name}] LLM ëª¨ë¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    logger.error(f"   âŒ [{model_name}] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                    logger.error(f"   âŒ [{model_name}] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    return {"entities": [], "dag_text": ""}
            
            def get_entities_only_by_llm(args_dict):
                """get_entities_and_dag_by_llmì˜ ë˜í¼ (ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜)"""
                result = get_entities_and_dag_by_llm(args_dict)
                return result['entities']
            
            # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥
            logger.info("ğŸ“‹ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ ì¤‘...")
            preview_prompt = f"""
            {HYBRID_DAG_EXTRACTION_PROMPT}

            ## message:                
            {msg_text}
            """
            self._store_prompt_for_preview(preview_prompt, "entity_extraction")
            logger.info("âœ… í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ ì™„ë£Œ")

            
            logger.info("ğŸ”„ 1ë‹¨ê³„ LLM ì¶”ì¶œ - ë©”ì‹œì§€ì—ì„œ ì§ì ‘ ì—”í‹°í‹° ë° DAG ì¶”ì¶œ")
            # 1ë‹¨ê³„: ê° LLM ëª¨ë¸ë¡œ ë©”ì‹œì§€ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
            batches = []
            for llm_model in llm_models:
                prompt = f"""
                {HYBRID_DAG_EXTRACTION_PROMPT}

                ## message:                
                {msg_text}
                """
                batches.append({"prompt": prompt, "llm_model": llm_model, "extract_dag": True})  # 1ë‹¨ê³„ëŠ” DAG ì¶”ì¶œ í•„ìš”
            
            logger.info(f"ğŸ”„ {len(llm_models)}ê°œ LLM ëª¨ë¸ë¡œ 1ë‹¨ê³„ ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘")
            
            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
            n_jobs = min(len(batches), 3)
            logger.info(f"âš™ï¸  ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: {n_jobs}ê°œ ì›Œì»¤ (threading ë°±ì—”ë“œ)")
            
            with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                batch_results_dicts = parallel(delayed(get_entities_and_dag_by_llm)(args) for args in batches)
            
            logger.info(f"âœ… ëª¨ë“  LLM ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ê²°ê³¼ ë¶„ë¦¬ ë° ìˆ˜ì§‘
            all_entities = []
            all_dags = []
            
            for idx, (model, result_dict) in enumerate(zip(llm_models, batch_results_dicts)):
                model_name = getattr(model, 'model_name', 'Unknown')
                entities = result_dict['entities']
                dag_text = result_dict['dag_text']
                
                logger.info(f"   [{idx+1}] {model_name}: {len(entities)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ")
                all_entities.extend(entities)
                if dag_text:
                    all_dags.append(dag_text)
            
            # DAG ì»¨í…ìŠ¤íŠ¸ ë³‘í•©
            combined_dag_context = "\n".join(all_dags)
            if combined_dag_context:
                logger.info(f"   ğŸ“ ìº¡ì²˜ëœ DAG ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(combined_dag_context)}ì")
            
            # ì™¸ë¶€ ì—”í‹°í‹° ì¶”ê°€ ë° ì¤‘ë³µ ì œê±°
            if external_cand_entities is not None and len(external_cand_entities)>0:
                all_entities.extend(external_cand_entities)
            
            logger.info(f"ğŸ“Š ë³‘í•© ì „ ì´ ì—”í‹°í‹° ìˆ˜: {len(all_entities)}ê°œ")
            cand_entity_list = list(set(all_entities))
            
            # N-gram í™•ì¥
            cand_entity_list = list(set(sum([[c['text'] for c in extract_ngram_candidates(cand_entity, min_n=2, max_n=len(cand_entity.split())) if c['start_idx']<=0] if len(cand_entity.split())>=4 else [cand_entity] for cand_entity in cand_entity_list], [])))
            
            logger.info(f"ğŸ“Š ì¤‘ë³µ ì œê±° ë° í™•ì¥ í›„ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            logger.info(f"âœ… LLM ì¶”ì¶œ ì™„ë£Œ: {cand_entity_list[:20]}..." if len(cand_entity_list) > 20 else f"âœ… LLM ì¶”ì¶œ ì™„ë£Œ: {cand_entity_list}")

            if not cand_entity_list:
                logger.warning("âš ï¸  LLM ì¶”ì¶œì—ì„œ ìœ íš¨í•œ ì—”í‹°í‹°ë¥¼ ì°¾ì§€ ëª»í•¨")
                logger.info("=" * 80)
                return pd.DataFrame()
            
            logger.info("ğŸ” ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹œì‘...")
            logger.info(f"   ì…ë ¥ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            cand_entities_sim = self._match_entities_with_products(cand_entity_list, rank_limit)
            logger.info(f"   ë§¤ì¹­ ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰")
            
            if cand_entities_sim.empty:
                logger.warning("âš ï¸  ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                logger.info("=" * 80)
                return pd.DataFrame()
            
            logger.info(f"   ë§¤ì¹­ëœ ê³ ìœ  item_name_in_msg ìˆ˜: {cand_entities_sim['item_name_in_msg'].nunique()}ê°œ")
            logger.info(f"   ë§¤ì¹­ëœ ê³ ìœ  item_nm_alias ìˆ˜: {cand_entities_sim['item_nm_alias'].nunique()}ê°œ")

            # í›„ë³´ ì—”í‹°í‹°ë“¤ê³¼ ìƒí’ˆ DB ë§¤ì¹­
            logger.info("ğŸ” 2ë‹¨ê³„ LLM í•„í„°ë§ ì‹œì‘ (ë™ì  ë°°ì¹˜ í¬ê¸° + DAG ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)...")
            logger.info(f"   ì…ë ¥ ë©”ì‹œì§€ ì—”í‹°í‹° ìˆ˜: {len(cand_entities_sim['item_name_in_msg'].unique())}ê°œ")
            logger.info(f"   í›„ë³´ ìƒí’ˆ ë³„ì¹­ ìˆ˜: {len(cand_entities_sim['item_nm_alias'].unique())}ê°œ")
            
            # entities_in_message ì¶”ì¶œ
            entities_in_message = cand_entities_sim['item_name_in_msg'].unique()
            
            # 2ë‹¨ê³„: ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            optimal_batch_size = self._calculate_optimal_batch_size(msg_text, base_size=10)
            logger.info(f"   ğŸ“ ë©”ì‹œì§€ ê¸¸ì´ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}ê°œ")
            
            # cand_entities_voca_allì„ ë™ì  ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• í•´ì„œ ë³‘ë ¬ ì²˜ë¦¬
            cand_entities_voca_all = cand_entities_sim['item_nm_alias'].unique()
            logger.info(f"   ì´ í›„ë³´ ìƒí’ˆ ë³„ì¹­: {len(cand_entities_voca_all)}ê°œ")
            
            # 2ë‹¨ê³„ í•„í„°ë§ì—ëŠ” ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
            second_stage_llm = llm_models[0] if llm_models else self.llm_model
            
            batches = []
            for i in range(0, len(cand_entities_voca_all), optimal_batch_size):
                cand_entities_voca = cand_entities_voca_all[i:i+optimal_batch_size]
                prompt = f"""
                {SIMPLE_ENTITY_EXTRACTION_PROMPT}
                
                ## message:                
                {msg_text}

                ## DAG Context (User Action Paths):
                {combined_dag_context}

                ## entities in message:
                {entities_in_message}

                ## candidate entities in vocabulary:
                {cand_entities_voca}
                """
                batches.append({"prompt": prompt, "llm_model": second_stage_llm, "extract_dag": False})  # 2ë‹¨ê³„ëŠ” DAG ì¶”ì¶œ ë¶ˆí•„ìš”
            
            logger.info(f"ğŸ”„ 2ë‹¨ê³„ LLM í•„í„°ë§: {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (ë°°ì¹˜ë‹¹ ~{optimal_batch_size}ê°œ)")
            
            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰
            n_jobs = min(len(batches), 3)
            logger.info(f"âš™ï¸  ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: {n_jobs}ê°œ ì›Œì»¤ (threading ë°±ì—”ë“œ)")
            
            with Parallel(n_jobs=n_jobs, backend='threading') as parallel:
                batch_results = parallel(delayed(get_entities_only_by_llm)(args) for args in batches)
            
            # ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
            logger.info(f"ğŸ“Š ë°°ì¹˜ë³„ ê²°ê³¼ ìš”ì•½:")
            for idx, batch_result in enumerate(batch_results):
                logger.info(f"   ë°°ì¹˜ {idx+1}: {len(batch_result)}ê°œ ì—”í‹°í‹°")
            
            cand_entity_list = list(set(sum(batch_results, [])))
            
            logger.info(f"âœ… 2ë‹¨ê³„ LLM í•„í„°ë§ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìµœì¢… ì„ íƒëœ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            logger.info(f"ğŸ“Š ìµœì¢… ì„ íƒëœ ì—”í‹°í‹°: {cand_entity_list}")

            logger.info(f"ğŸ” ìµœì¢… ì—”í‹°í‹°ë¡œ í•„í„°ë§ ì¤‘...")
            logger.info(f"   í•„í„°ë§ ì „ í–‰ ìˆ˜: {len(cand_entities_sim)}ê°œ")
            
            cand_entities_sim = cand_entities_sim.query("item_nm_alias in @cand_entity_list")
            logger.info(f"   í•„í„°ë§ í›„ í–‰ ìˆ˜: {len(cand_entities_sim)}ê°œ")
            
            logger.info("=" * 80)
            logger.info("âœ… [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰ ë°˜í™˜")
            logger.info("=" * 80)

            return cand_entities_sim
            
        except Exception as e:
            logger.error("=" * 80)
            logger.error("âŒ [LLM ì—”í‹°í‹° ì¶”ì¶œ] í•¨ìˆ˜ ì‹¤íŒ¨")
            logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            logger.error("=" * 80)
            return pd.DataFrame()

    def _match_entities_with_products(self, cand_entity_list: List[str], rank_limit: int) -> pd.DataFrame:
        """í›„ë³´ ì—”í‹°í‹°ë“¤ì„ ìƒí’ˆ DBì™€ ë§¤ì¹­"""
        try:
            logger.info("   ğŸ” [ë§¤ì¹­] í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ì‹œì‘...")
            logger.info(f"   ğŸ“ ì…ë ¥ ì—”í‹°í‹° ìˆ˜: {len(cand_entity_list)}ê°œ")
            logger.info(f"   ğŸ“ ìƒí’ˆ DB ë³„ì¹­ ìˆ˜: {len(self.item_pdf_all['item_nm_alias'].unique()):,}ê°œ")
            
            # í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­
            similarities_fuzzy = parallel_fuzzy_similarity(
                cand_entity_list,
                self.item_pdf_all['item_nm_alias'].unique(),
                threshold=0.6,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30
            )
            
            logger.info(f"   âœ… í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ì™„ë£Œ: {len(similarities_fuzzy)}ê°œ í–‰")
            
            if similarities_fuzzy.empty:
                logger.warning("   âš ï¸  í¼ì§€ ìœ ì‚¬ë„ ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            
            # ì •ì§€ì–´ í•„í„°ë§
            before_stopwords = len(similarities_fuzzy)
            similarities_fuzzy = similarities_fuzzy[
                ~similarities_fuzzy['item_nm_alias'].isin(self.stop_item_names)
            ]
            after_stopwords = len(similarities_fuzzy)
            logger.info(f"   ğŸ“Š ì •ì§€ì–´ í•„í„°ë§ ê²°ê³¼: {before_stopwords}ê°œ â†’ {after_stopwords}ê°œ")

            # ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ë§¤ì¹­
            logger.info("   ğŸ” [ë§¤ì¹­] ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (s1, s2 ê°ê°)...")
            
            # s1 ì •ê·œí™”
            sim_s1 = parallel_seq_similarity(
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30,
                normalizaton_value='s1'
            ).rename(columns={'sim': 'sim_s1'})
            
            # s2 ì •ê·œí™”
            sim_s2 = parallel_seq_similarity(
                sent_item_pdf=similarities_fuzzy,
                text_col_nm='item_name_in_msg',
                item_col_nm='item_nm_alias',
                n_jobs=6,
                batch_size=30,
                normalizaton_value='s2'
            ).rename(columns={'sim': 'sim_s2'})
            
            logger.info(f"   âœ… ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: sim_s1={len(sim_s1)}ê°œ, sim_s2={len(sim_s2)}ê°œ")
            
            # mergeë¡œ í•©ì¹˜ê¸°
            cand_entities_sim = sim_s1.merge(sim_s2, on=['item_name_in_msg', 'item_nm_alias'])
            logger.info(f"   âœ… ë³‘í•© ì™„ë£Œ: {len(cand_entities_sim)}ê°œ í–‰")
            
            if cand_entities_sim.empty:
                logger.warning("   âš ï¸  ì‹œí€€ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            
            # í•„í„°ë§ ì¡°ê±´ ì ìš©
            before_query = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query("(sim_s1>=@PROCESSING_CONFIG.combined_similarity_threshold and sim_s2>=@PROCESSING_CONFIG.combined_similarity_threshold)")
            after_query = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ì¿¼ë¦¬ í•„í„°ë§ ê²°ê³¼: {before_query}ê°œ â†’ {after_query}ê°œ")

            # groupbyë¡œ í•©ì‚°
            cand_entities_sim = cand_entities_sim.groupby(['item_name_in_msg', 'item_nm_alias'])[['sim_s1', 'sim_s2']].apply(
                lambda x: x['sim_s1'].sum() + x['sim_s2'].sum()
            )
            if cand_entities_sim.empty:
                logger.warning("í•©ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            
            cand_entities_sim = cand_entities_sim.reset_index(name='sim')
            logger.info(f"   âœ… í•©ì‚° ì™„ë£Œ: {len(cand_entities_sim)}ê°œ í–‰")
            
            # sim>=1.0 í•„í„°ë§
            before_sim_filter = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query("sim >= @PROCESSING_CONFIG.high_similarity_threshold").copy()
            if cand_entities_sim.empty:
                logger.warning("í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return pd.DataFrame()
            after_sim_filter = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ìœ ì‚¬ë„ í•„í„°ë§ ê²°ê³¼: {before_sim_filter}ê°œ â†’ {after_sim_filter}ê°œ")

            # ìˆœìœ„ ë§¤ê¸°ê¸° ë° ê²°ê³¼ ì œí•œ
            cand_entities_sim["rank"] = cand_entities_sim.groupby('item_name_in_msg')['sim'].rank(
                method='dense', ascending=False
            )
            before_rank_limit = len(cand_entities_sim)
            cand_entities_sim = cand_entities_sim.query(f"rank <= {rank_limit}").sort_values(
                ['item_name_in_msg', 'rank'], ascending=[True, True]
            )
            after_rank_limit = len(cand_entities_sim)
            logger.info(f"   ğŸ“Š ìˆœìœ„ ì œí•œ ê²°ê³¼: {before_rank_limit}ê°œ â†’ {after_rank_limit}ê°œ")
            
            # item_dmn_nm ë³‘í•©
            if 'item_dmn_nm' in self.item_pdf_all.columns:
                cand_entities_sim = cand_entities_sim.merge(
                    self.item_pdf_all[['item_nm_alias', 'item_dmn_nm']].drop_duplicates(),
                    on='item_nm_alias',
                    how='left'
                )
                logger.info(f"   âœ… item_dmn_nm ë³‘í•© ì™„ë£Œ")
            
            logger.info(f"   âœ… [ë§¤ì¹­] ìµœì¢… ê²°ê³¼: {len(cand_entities_sim)}ê°œ í–‰")

            return cand_entities_sim
            
        except Exception as e:
            logger.error(f"   âŒ [ë§¤ì¹­] ì—”í‹°í‹°-ìƒí’ˆ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            logger.error(f"   âŒ [ë§¤ì¹­] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return pd.DataFrame()

    def _map_products_with_similarity(self, similarities_fuzzy: pd.DataFrame, json_objects: Dict = None) -> List[Dict]:
        """ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ë§¤í•‘"""
        try:
            logger.info("ğŸ” [_map_products_with_similarity] ì‹œì‘")
            logger.info(f"   - ì…ë ¥ similarities_fuzzy í¬ê¸°: {similarities_fuzzy.shape}")
            
            # ë†’ì€ ìœ ì‚¬ë„ ì•„ì´í…œë“¤ í•„í„°ë§
            high_sim_threshold = getattr(PROCESSING_CONFIG, 'high_similarity_threshold', 1.0)
            logger.info(f"   - high_sim_threshold: {high_sim_threshold}")
            
            high_sim_items = similarities_fuzzy.query('sim >= @high_sim_threshold')['item_nm_alias'].unique()
            logger.info(f"   - high_sim_items ê°œìˆ˜: {len(high_sim_items)}ê°œ")
            
            before_filter = len(similarities_fuzzy)
            filtered_similarities = similarities_fuzzy[
                (similarities_fuzzy['item_nm_alias'].isin(high_sim_items)) &
                (~similarities_fuzzy['item_nm_alias'].str.contains('test', case=False)) &
                (~similarities_fuzzy['item_name_in_msg'].isin(self.stop_item_names))
            ]
            after_filter = len(filtered_similarities)
            logger.info(f"   - í•„í„°ë§: {before_filter}ê°œ â†’ {after_filter}ê°œ")
            
            if filtered_similarities.empty:
                logger.warning("   âš ï¸ filtered_similaritiesê°€ ë¹„ì–´ìˆìŒ â†’ ë¹ˆ ë°°ì—´ ë°˜í™˜")
                return []
            
            # ìƒí’ˆ ì •ë³´ì™€ ë§¤í•‘í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
            merged_items = self.item_pdf_all.merge(filtered_similarities, on=['item_nm_alias'])
            logger.info(f"   - merged_items í¬ê¸°: {merged_items.shape}")
            
            if merged_items.empty:
                logger.warning("   âš ï¸ merged_itemsê°€ ë¹„ì–´ìˆìŒ â†’ ë¹ˆ ë°°ì—´ ë°˜í™˜")
                return []
            
            product_tag = self.convert_df_to_json_list(merged_items)
            logger.info(f"   âœ… product_tag ê°œìˆ˜: {len(product_tag)}ê°œ")
            
            # Add expected_action to each product
            if json_objects:
                logger.info("   ğŸ” expected_action ì¶”ê°€ ì‹œì‘")
                action_mapping = self._create_action_mapping(json_objects)
                
                for product in product_tag:
                    item_names_in_msg = product.get('item_name_in_msg', [])
                    found_actions = []
                    for item_name in item_names_in_msg:
                        if item_name in action_mapping:
                            found_actions.append(action_mapping[item_name])
                    product['expected_action'] = list(dict.fromkeys(found_actions)) if found_actions else ['ê¸°íƒ€']
                
                logger.info(f"   âœ… expected_action ì¶”ê°€ ì™„ë£Œ")
            
            logger.info(f"âœ… [_map_products_with_similarity] ì™„ë£Œ - ë°˜í™˜: {len(product_tag)}ê°œ")
            return product_tag
            
        except Exception as e:
            logger.error(f"âŒ [_map_products_with_similarity] ì‹¤íŒ¨: {e}")
            logger.error(f"   ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return []

    def _create_action_mapping(self, json_objects: Dict) -> Dict[str, str]:
        """LLM ì‘ë‹µì—ì„œ ìƒí’ˆëª…-ì•¡ì…˜ ë§¤í•‘ ìƒì„±"""
        try:
            action_mapping = {}
            product_data = json_objects.get('product', [])
            
            if isinstance(product_data, list):
                for item in product_data:
                    if isinstance(item, dict) and 'name' in item and 'action' in item:
                        action_mapping[item['name']] = item['action']
            elif isinstance(product_data, dict):
                if 'items' in product_data:
                    items = product_data.get('items', [])
                    for item in items:
                        if isinstance(item, dict) and 'name' in item and 'action' in item:
                            action_mapping[item['name']] = item['action']
                elif 'type' in product_data and product_data.get('type') == 'array':
                    logger.debug("ìŠ¤í‚¤ë§ˆ ì •ì˜ êµ¬ì¡° ê°ì§€ë¨, ì•¡ì…˜ ë§¤í•‘ ê±´ë„ˆë›°ê¸°")
                else:
                    if 'name' in product_data and 'action' in product_data:
                        action_mapping[product_data['name']] = product_data['action']
            
            logger.debug(f"ìƒì„±ëœ ì•¡ì…˜ ë§¤í•‘: {action_mapping}")
            return action_mapping
            
        except Exception as e:
            logger.error(f"ì•¡ì…˜ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
