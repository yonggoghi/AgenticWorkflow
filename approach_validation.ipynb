{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import ast\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Union, Dict, Any\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from kiwipiepy import Kiwi\n",
        "from rapidfuzz import fuzz, process\n",
        "from pygments import highlight\n",
        "from pygments.lexers import JsonLexer\n",
        "from pygments.formatters import HtmlFormatter\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "# API Configuration\n",
        "try:\n",
        "    import config\n",
        "    LLM_API_KEY = config.CUSTOM_API_KEY\n",
        "    LLM_API_URL = \"https://api.platform.a15t.com/v1\"\n",
        "except ImportError:\n",
        "    LLM_API_KEY = \"sktax-z2GВxrBfqЗР71Хh7RNIst3xchZdoOjyixfyHlIzGttXuz\"\n",
        "    LLM_API_URL = \"https://apigw.sktax.chat/api/v1/serving/openai-api/v1\"\n",
        "\n",
        "# Initialize OpenAI Clients\n",
        "client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_API_URL)\n",
        "client_2 = OpenAI(\n",
        "    api_key=\"sktax-z2GВxrBfqЗР71Хh7RNIst3xchZdoOjyixfyHlIzGttXuz\",\n",
        "    base_url=\"https://apigw.sktax.chat/api/v1/serving/openai-api/v1\"\n",
        ")\n",
        "\n",
        "# Initialize Language Models\n",
        "def initialize_llm(model: str = \"skt/claude-3-7-sonnet-20250219\", max_tokens: int = 100) -> ChatOpenAI:\n",
        "    \"\"\"Initialize a language model with specified parameters.\"\"\"\n",
        "    return ChatOpenAI(\n",
        "        temperature=0,\n",
        "        openai_api_key=LLM_API_KEY,\n",
        "        openai_api_base=LLM_API_URL,\n",
        "        model=model,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "llm_cld37 = initialize_llm()\n",
        "llm_gem3 = initialize_llm(model=\"skt/gemma3-12b-it\")\n",
        "llm_chat = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model=\"gpt-4o\",\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    max_tokens=2000\n",
        ")\n",
        "llm_cld40 = ChatAnthropic(\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=3000\n",
        ")\n",
        "\n",
        "# Data Loading\n",
        "mms_pdf = pd.read_csv(\"./data/mms_data_250408.csv\")\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm'] + \"\\n\" + mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\", \"mms_phrs\", \"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "mms_pdf = mms_pdf.reset_index().astype(str)\n",
        "\n",
        "item_pdf_raw = pd.read_csv(\"./data/item_info_all_250527.csv\")\n",
        "item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm', 'item_id'])[\n",
        "    ['item_nm', 'item_id', 'item_desc', 'domain', 'start_dt', 'end_dt', 'rank']\n",
        "].copy()\n",
        "\n",
        "# Alias Rules and Entity Extension\n",
        "alia_rule_set = list(zip(\n",
        "    pd.read_csv(\"./data/alias_rules.csv\")['alias_1'],\n",
        "    pd.read_csv(\"./data/alias_rules.csv\")['alias_2']\n",
        "))\n",
        "\n",
        "def apply_alias_rule(item_nm: str) -> List[str]:\n",
        "    \"\"\"Apply alias rules to generate alternative item names.\"\"\"\n",
        "    item_nm_list = [item_nm]\n",
        "    for alias_1, alias_2 in alia_rule_set:\n",
        "        if alias_1 in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(alias_1, alias_2))\n",
        "        if alias_2 in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(alias_2, alias_1))\n",
        "    return item_nm_list\n",
        "\n",
        "item_pdf_all['item_nm_alias'] = item_pdf_all['item_nm'].apply(apply_alias_rule)\n",
        "item_pdf_all = item_pdf_all.explode('item_nm_alias')\n",
        "\n",
        "# User-Defined Entities\n",
        "user_defined_entity = ['AIA Vitality', '부스트 파크 건대입구', 'Boost Park �대입구']\n",
        "item_pdf_ext = pd.DataFrame([\n",
        "    {'item_nm': e, 'item_id': e, 'item_desc': e, 'domain': 'user_defined', \n",
        "     'start_dt': 20250101, 'end_dt': 99991231, 'rank': 1, 'item_nm_alias': e}\n",
        "    for e in user_defined_entity\n",
        "])\n",
        "item_pdf_all = pd.concat([item_pdf_all, item_pdf_ext])\n",
        "\n",
        "# Entity List for Fuzzy Matching\n",
        "entity_list_for_fuzzy = [\n",
        "    (row['item_nm'], {\n",
        "        'item_id': row['item_id'], 'description': row['item_desc'], 'domain': row['domain'],\n",
        "        'start_dt': row['start_dt'], 'end_dt': row['end_dt'], 'rank': 1, 'item_nm_alias': row['item_nm_alias']\n",
        "    })\n",
        "    for row in item_pdf_all.to_dict('records')\n",
        "]\n",
        "\n",
        "# Stop Words and Sentence Transformer\n",
        "stop_item_names = pd.read_csv(\"./data/stop_words.csv\")['stop_words'].to_list()\n",
        "model = SentenceTransformer('jhgan/ko-sbert-nli')\n",
        "pgm_pdf = pd.read_csv(\"./data/pgm_tag_ext_250516.csv\")\n",
        "clue_embeddings = model.encode(\n",
        "    pgm_pdf[[\"pgm_nm\", \"clue_tag\"]].apply(\n",
        "        lambda x: preprocess_text(x['pgm_nm'].lower()) + \" \" + x['clue_tag'].lower(), axis=1\n",
        "    ).tolist(),\n",
        "    convert_to_tensor=True\n",
        ")\n",
        "\n",
        "# Text Processing Utilities\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Preprocess text by removing special characters and normalizing spaces.\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean text while preserving JSON structure and Korean characters.\"\"\"\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\", \"'\": \"SQUOTE_TOKEN\", \"{\": \"OCURLY_TOKEN\", \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\", \"]\": \"CSQUARE_TOKEN\", \":\": \"COLON_TOKEN\", \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    allowed_chars = (\n",
        "        r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF'\n",
        "        r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F'\n",
        "        r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    )\n",
        "    text = re.sub(allowed_chars, '', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    return text\n",
        "\n",
        "def repair_json(broken_json: str) -> str:\n",
        "    \"\"\"Repair ill-formed JSON strings.\"\"\"\n",
        "    json_str = re.sub(r':\\s*([a-zA-Z0-9_]+)(\\s*[,}])', r': \"\\1\"\\2', broken_json)\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1 \"\\2\":', json_str)\n",
        "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text: str) -> List[Dict]:\n",
        "    \"\"\"Extract valid JSON objects from text.\"\"\"\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except (json.JSONDecodeError, SyntaxError):\n",
        "            pass\n",
        "    return result\n",
        "\n",
        "# Entity Matching with Kiwi\n",
        "kiwi = Kiwi()\n",
        "kiwi_raw = Kiwi()\n",
        "kiwi_raw.space_tolerance = 2\n",
        "stop_item_names = list(set(stop_item_names + [x.lower() for x in stop_item_names]))\n",
        "entity_list_for_kiwi = list(item_pdf_all['item_nm_alias'].unique())\n",
        "for w in entity_list_for_kiwi:\n",
        "    kiwi.add_user_word(w, \"NNP\")\n",
        "for w in stop_item_names:\n",
        "    kiwi.add_user_word(w, \"NNG\")\n",
        "\n",
        "tags_to_exclude = ['W_SERIAL', 'W_URL', 'JKO', 'SSO', 'SSC', 'SW', 'SF', 'SP', 'SS', 'SE', 'SO', 'SB', 'SH']\n",
        "exc_tag_patterns = [\n",
        "    ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],\n",
        "    ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'], ['VV', 'ETN']\n",
        "] + [[t] for t in tags_to_exclude]\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, form: str, tag: str, start: int, len: int):\n",
        "        self.form = form\n",
        "        self.tag = tag\n",
        "        self.start = start\n",
        "        self.len = len\n",
        "    \n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Token(form='{self.form}', tag='{self.tag}', start={self.start}, len={self.len})\"\n",
        "\n",
        "edf = item_pdf_all.copy()\n",
        "edf['token_entity'] = edf.apply(\n",
        "    lambda x: [d[0] for d in kiwi_raw.tokenize(x['item_nm_alias'], normalize_coda=True, z_coda=False, split_complex=False) \n",
        "               if d[1] not in tags_to_exclude], axis=1\n",
        ")\n",
        "edf['char_entity'] = edf.apply(lambda x: list(x['item_nm_alias'].lower().replace(' ', '')), axis=1)\n",
        "\n",
        "# Korean Entity Matching\n",
        "class KoreanEntityMatcher:\n",
        "    def __init__(self, min_similarity: int = 70, ngram_size: int = 2, min_entity_length: int = 2, token_similarity: bool = True):\n",
        "        self.min_similarity = min_similarity\n",
        "        self.ngram_size = ngram_size\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.token_similarity = token_similarity\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        self.ngram_index = {}\n",
        "        self.normalized_entities = {}\n",
        "\n",
        "    def build_from_list(self, entities: List[Union[str, Tuple[str, Dict]]]):\n",
        "        \"\"\"Build entity index from a list of entities.\"\"\"\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        for i, entity in enumerate(entities):\n",
        "            if isinstance(entity, tuple) and len(entity) == 2:\n",
        "                entity_name, data = entity\n",
        "                self.entities.append(entity_name)\n",
        "                self.entity_data[entity_name] = data\n",
        "            else:\n",
        "                self.entities.append(entity)\n",
        "                self.entity_data[entity] = {'id': i, 'entity': entity}\n",
        "        \n",
        "        for entity in self.entities:\n",
        "            normalized = self._normalize_text(entity)\n",
        "            self.normalized_entities[normalized] = entity\n",
        "        self._build_ngram_index(n=self.ngram_size)\n",
        "\n",
        "    def _normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalize text by converting to lowercase and unifying spaces.\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into Korean, English, and numeric tokens.\"\"\"\n",
        "        return re.findall(r'[가-힣]+|[a-z0-9]+', self._normalize_text(text))\n",
        "\n",
        "    def _build_ngram_index(self, n: int):\n",
        "        \"\"\"Build n-gram index optimized for Korean characters.\"\"\"\n",
        "        for entity in self.entities:\n",
        "            if len(entity) < self.min_entity_length:\n",
        "                continue\n",
        "            normalized_entity = self._normalize_text(entity)\n",
        "            entity_chars = list(normalized_entity)\n",
        "            ngrams = [''.join(entity_chars[i:i+n]) for i in range(len(entity_chars) - n + 1)]\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram_index:\n",
        "                    self.ngram_index[ngram] = set()\n",
        "                self.ngram_index[ngram].add(entity)\n",
        "            \n",
        "            tokens = self._tokenize(normalized_entity)\n",
        "            for token in tokens:\n",
        "                if len(token) >= n:\n",
        "                    token_key = f\"TOKEN:{token}\"\n",
        "                    if token_key not in self.ngram_index:\n",
        "                        self.ngram_index[token_key] = set()\n",
        "                    self.ngram_index[token_key].add(entity)\n",
        "\n",
        "    def _get_candidates(self, text: str, n: int = None) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get candidate entities based on n-gram overlap.\"\"\"\n",
        "        if n is None:\n",
        "            n = self.ngram_size\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        if normalized_text in self.normalized_entities:\n",
        "            return [(self.normalized_entities[normalized_text], float('inf'))]\n",
        "        \n",
        "        text_chars = list(normalized_text)\n",
        "        text_ngrams = set(''.join(text_chars[i:i+n]) for i in range(len(text_chars) - n + 1))\n",
        "        tokens = self._tokenize(normalized_text)\n",
        "        for token in tokens:\n",
        "            if len(token) >= n:\n",
        "                text_ngrams.add(f\"TOKEN:{token}\")\n",
        "        \n",
        "        candidates = set()\n",
        "        for ngram in text_ngrams:\n",
        "            if ngram in self.ngram_index:\n",
        "                candidates.update(self.ngram_index[ngram])\n",
        "        \n",
        "        candidate_scores = {}\n",
        "        for candidate in candidates:\n",
        "            candidate_normalized = self._normalize_text(candidate)\n",
        "            candidate_chars = list(candidate_normalized)\n",
        "            candidate_ngrams = set(''.join(candidate_chars[i:i+n]) for i in range(len(candidate_chars) - n + 1))\n",
        "            candidate_tokens = self._tokenize(candidate_normalized)\n",
        "            for token in candidate_tokens:\n",
        "                if len(token) >= n:\n",
        "                    candidate_ngrams.add(f\"TOKEN:{token}\")\n",
        "            \n",
        "            overlap = len(candidate_ngrams.intersection(text_ngrams))\n",
        "            token_bonus = 0\n",
        "            if self.token_similarity:\n",
        "                query_tokens = set(tokens)\n",
        "                cand_tokens = set(candidate_tokens)\n",
        "                if query_tokens and cand_tokens:\n",
        "                    common = query_tokens.intersection(cand_tokens)\n",
        "                    token_bonus = len(common) * 2\n",
        "            candidate_scores[candidate] = overlap + token_bonus\n",
        "        \n",
        "        return sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def _calculate_similarity(self, text: str, entity: str) -> float:\n",
        "        \"\"\"Calculate similarity between text and entity using multiple metrics.\"\"\"\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        normalized_entity = self._normalize_text(entity)\n",
        "        \n",
        "        if normalized_text == normalized_entity:\n",
        "            return 100\n",
        "        \n",
        "        ratio_score = fuzz.ratio(normalized_text, normalized_entity)\n",
        "        partial_score = 0\n",
        "        if normalized_text in normalized_entity:\n",
        "            partial_score = (len(normalized_text) / len(normalized_entity)) * 100\n",
        "        elif normalized_entity in normalized_text:\n",
        "            partial_score = (len(normalized_entity) / len(normalized_text)) * 100\n",
        "        \n",
        "        token_score = 0\n",
        "        if self.token_similarity:\n",
        "            text_tokens = set(self._tokenize(normalized_text))\n",
        "            entity_tokens = set(self._tokenize(normalized_entity))\n",
        "            if text_tokens and entity_tokens:\n",
        "                common_tokens = text_tokens.intersection(entity_tokens)\n",
        "                all_tokens = text_tokens.union(entity_tokens)\n",
        "                token_score = (len(common_tokens) / len(all_tokens)) * 100 if all_tokens else 0\n",
        "        \n",
        "        token_sort_score = fuzz.token_sort_ratio(normalized_text, normalized_entity)\n",
        "        token_set_score = fuzz.token_set_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        return (\n",
        "            ratio_score * 0.3 +\n",
        "            partial_score * 0.1 +\n",
        "            token_score * 0.2 +\n",
        "            token_sort_score * 0.2 +\n",
        "            token_set_score * 0.2\n",
        "        )\n",
        "\n",
        "    def find_entities(self, text: str, max_candidates_per_span: int = 10) -> List[Dict]:\n",
        "        \"\"\"Find entity matches in text using fuzzy matching.\"\"\"\n",
        "        potential_spans = self._extract_korean_spans(text)\n",
        "        matches = []\n",
        "        \n",
        "        for span_text, start, end in potential_spans:\n",
        "            if len(span_text.strip()) < self.min_entity_length:\n",
        "                continue\n",
        "            candidates = self._get_candidates(span_text)\n",
        "            if not candidates:\n",
        "                continue\n",
        "            top_candidates = [c[0] for c in candidates[:max_candidates_per_span]]\n",
        "            \n",
        "            for entity in top_candidates:\n",
        "                score = self._calculate_similarity(span_text, entity)\n",
        "                if score >= self.min_similarity:\n",
        "                    matches.append({\n",
        "                        'text': span_text,\n",
        "                        'matched_entity': entity,\n",
        "                        'score': score,\n",
        "                        'start': start,\n",
        "                        'end': end,\n",
        "                        'data': self.entity_data.get(entity, {})\n",
        "                    })\n",
        "        \n",
        "        matches.sort(key=lambda x: (x['start'], -x['score']))\n",
        "        return self._resolve_overlapping_matches(matches)\n",
        "\n",
        "    def _extract_korean_spans(self, text: str) -> List[Tuple[str, int, int]]:\n",
        "        \"\"\"Extract potential entity spans from mixed Korean and English text.\"\"\"\n",
        "        spans = []\n",
        "        min_len = self.min_entity_length\n",
        "        patterns = [\n",
        "            r'[a-zA-Z]+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*',\n",
        "            r'[a-zA-Z]+\\s+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*',\n",
        "            r'[a-zA-Z]+[가-힣]+(?:[0-9]+)?',\n",
        "            r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+',\n",
        "            r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+\\s+[가-힣]+',\n",
        "            r'[a-zA-Z가-힣]+(?:\\s+[a-zA-Z가-힣]+){1,3}',\n",
        "            r'\\d+\\s+[a-zA-Z]+',\n",
        "            r'[a-zA-Z가-힣0-9]+(?:\\s+[a-zA-Z가-힣0-9]+)*'\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            for match in re.finditer(pattern, text):\n",
        "                if len(match.group(0)) >= min_len:\n",
        "                    spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        for span in re.split(r'[,\\.!?;:\"\\'…\\(\\)\\[\\]\\{\\}\\s_/]+', text):\n",
        "            if span and len(span) >= min_len:\n",
        "                span_pos = text.find(span)\n",
        "                if span_pos != -1:\n",
        "                    spans.append((span, span_pos, span_pos + len(span)))\n",
        "        \n",
        "        return spans\n",
        "\n",
        "    def _resolve_overlapping_matches(self, matches: List[Dict], high_score_threshold: int = 50, overlap_tolerance: float = 0.5) -> List[Dict]:\n",
        "        \"\"\"Resolve overlapping matches by keeping the best match.\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        sorted_matches = sorted(matches, key=lambda x: (-x['score'], x['end'] - x['start']))\n",
        "        final_matches = []\n",
        "        \n",
        "        for current_match in sorted_matches:\n",
        "            current_score = current_match['score']\n",
        "            current_start, current_end = current_match['start'], current_match['end']\n",
        "            current_range = set(range(current_start, current_end))\n",
        "            current_len = len(current_range)\n",
        "            current_match['overlap_ratio'] = 0.0\n",
        "            \n",
        "            if current_score >= high_score_threshold:\n",
        "                is_too_similar = False\n",
        "                for existing_match in final_matches:\n",
        "                    if existing_match['score'] < high_score_threshold:\n",
        "                        continue\n",
        "                    existing_range = set(range(existing_match['start'], existing_match['end']))\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    \n",
        "                    if current_overlap_ratio > overlap_tolerance and current_match['matched_entity'] == existing_match['matched_entity']:\n",
        "                        is_too_similar = True\n",
        "                        break\n",
        "                if not is_too_similar:\n",
        "                    final_matches.append(current_match)\n",
        "            else:\n",
        "                should_add = True\n",
        "                for existing_match in final_matches:\n",
        "                    existing_range = set(range(existing_match['start'], existing_match['end']))\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    if current_overlap_ratio > (1 - overlap_tolerance):\n",
        "                        should_add = False\n",
        "                        break\n",
        "                if should_add:\n",
        "                    final_matches.append(current_match)\n",
        "        \n",
        "        return sorted(final_matches, key=lambda x: x['start'])\n",
        "\n",
        "def find_entities_in_text(\n",
        "    text: str,\n",
        "    entity_list: List[Union[str, Tuple[str, Dict]]],\n",
        "    min_similarity: int = 70,\n",
        "    ngram_size: int = 3,\n",
        "    min_entity_length: int = 2,\n",
        "    token_similarity: bool = True,\n",
        "    high_score_threshold: int = 50,\n",
        "    overlap_tolerance: float = 0.5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Find entity matches in text using fuzzy matching.\"\"\"\n",
        "    matcher = KoreanEntityMatcher(\n",
        "        min_similarity=min_similarity,\n",
        "        ngram_size=ngram_size,\n",
        "        min_entity_length=min_entity_length,\n",
        "        token_similarity=token_similarity\n",
        "    )\n",
        "    matcher.build_from_list(entity_list)\n",
        "    matches = matcher.find_entities(text)\n",
        "    return matcher._resolve_overlapping_matches(\n",
        "        matches, high_score_threshold=high_score_threshold, overlap_tolerance=overlap_tolerance\n",
        "    )\n",
        "\n",
        "def highlight_entities(text: str, matches: List[Dict]) -> str:\n",
        "    \"\"\"Highlight matched entities in text.\"\"\"\n",
        "    marked_text = text\n",
        "    offset = 0\n",
        "    for match in sorted(matches, key=lambda x: x['start'], reverse=True):\n",
        "        start = match['start'] + offset\n",
        "        end = match['end'] + offset\n",
        "        entity = match['matched_entity']\n",
        "        score = match['score']\n",
        "        marked_text = marked_text[:start] + f\"[{marked_text[start:end]}→{entity} ({score:.1f}%)]\" + marked_text[end:]\n",
        "        offset += len(f\"[→{entity} ({score:.1f}%)]\") + 2\n",
        "    return marked_text\n",
        "\n",
        "# Advanced Similarity Analysis\n",
        "def advanced_sequential_similarity(str1: str, str2: str, metrics: List[str] = None, visualize: bool = False) -> Dict[str, float]:\n",
        "    \"\"\"Calculate multiple character-level similarity metrics between two strings.\"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "    results = {}\n",
        "    \n",
        "    if not str1 or not str2:\n",
        "        return {metric: 0.0 for metric in metrics}\n",
        "    \n",
        "    s1, s2 = str1.lower(), str2.lower()\n",
        "    \n",
        "    if 'ngram' in metrics:\n",
        "        ngram_scores = {}\n",
        "        for window in range(min(len(s1), len(s2), 2), min(5, max(len(s1), len(s2)) + 1)):\n",
        "            if len(s1) < window or len(s2) < window:\n",
        "                ngram_scores[f'window_{window}'] = 0.0\n",
        "                continue\n",
        "            ngrams1 = [s1[i:i+window] for i in range(len(s1) - window + 1)]\n",
        "            ngrams2 = [s2[i:i+window] for i in range(len(s2) - window + 1)]\n",
        "            matches = sum(1 for ng in ngrams1 if ng in ngrams2)\n",
        "            max_possible = max(len(ngrams1), len(ngrams2))\n",
        "            score = matches / max_possible if max_possible > 0 else 0.0\n",
        "            ngram_scores[f'window_{window}'] = score\n",
        "        results['ngram'] = max(ngram_scores.values())\n",
        "        results['ngram_details'] = ngram_scores\n",
        "    \n",
        "    if 'lcs' in metrics:\n",
        "        def longest_common_substring(s1: str, s2: str) -> int:\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            max_length = 0\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                        max_length = max(max_length, dp[i][j])\n",
        "            return max_length\n",
        "        \n",
        "        lcs_length = longest_common_substring(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['lcs'] = lcs_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    if 'subsequence' in metrics:\n",
        "        def longest_common_subsequence(s1: str, s2: str) -> int:\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "            return dp[m][n]\n",
        "        \n",
        "        subseq_length = longest_common_subsequence(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['subsequence'] = subseq_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    if 'difflib' in metrics:\n",
        "        sm = SequenceMatcher(None, s1, s2)\n",
        "        results['difflib'] = sm.ratio()\n",
        "    \n",
        "    if visualize:\n",
        "        try:\n",
        "            sm = SequenceMatcher(None, s1, s2)\n",
        "            matches = sm.get_matching_blocks()\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            ax.barh(0, len(s1), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            ax.barh(1, len(s2), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            \n",
        "            for match in matches:\n",
        "                i, j, size = match\n",
        "                if size > 0:\n",
        "                    ax.barh(0, size, height=0.4, left=i, color='green', alpha=0.5)\n",
        "                    ax.barh(1, size, height=0.4, left=j, color='green', alpha=0.5)\n",
        "                    ax.plot([i + size/2, j + size/2], [0.2, 0.8], 'k-', alpha=0.3)\n",
        "            \n",
        "            for i, c in enumerate(s1):\n",
        "                ax.text(i + 0.5, 0, c, ha='center', va='center')\n",
        "            for i, c in enumerate(s2):\n",
        "                ax.text(i + 0.5, 1, c, ha='center', va='center')\n",
        "            \n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_yticklabels(['String 1', 'String 2'])\n",
        "            ax.set_xlabel('Character Position')\n",
        "            ax.set_title('Character-Level String Comparison')\n",
        "            ax.grid(False)\n",
        "            plt.tight_layout()\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "    \n",
        "    metrics_to_average = [m for m in results.keys() if not m.endswith('_details')]\n",
        "    results['overall'] = sum(results[m] for m in metrics_to_average) / len(metrics_to_average)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_prd = {\n",
        "    \"title\": {\n",
        "        \"type\": \"string\", \n",
        "        'description': '광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성'\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array',\n",
        "        'items': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "            'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름'},\n",
        "            'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'pgm':{\n",
        "        'type': 'array', \n",
        "        'description': '아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공'\n",
        "    },\n",
        "'required': ['purpose', 'product', 'channel', 'pgm'], \n",
        "'objectType': 'object'}\n",
        "\n",
        "schema_prd_cot = {\n",
        "\"reasoning\": {\n",
        "    \"type\": \"object\",\n",
        "    \"description\": \"단계별 분석 과정 (최종 JSON에는 포함하지 않음)\",\n",
        "    \"properties\": {\n",
        "    \"step1_purpose_analysis\": \"광고 목적 분석 과정\",\n",
        "    \"step2_product_identification\": \"상품 식별 및 도메인 매칭 과정\", \n",
        "    \"step3_channel_extraction\": \"채널 정보 추출 과정\",\n",
        "    \"step4_pgm_classification\": \"프로그램 분류 과정\"\n",
        "    }\n",
        "},\n",
        "\"title\": {\n",
        "    \"type\": \"string\",\n",
        "    \"description\": \"광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명\"\n",
        "},\n",
        "\"purpose\": {\n",
        "    \"type\": \"array\",\n",
        "    \"description\": \"STEP 1에서 분석한 광고의 주요 목적 (복수 가능). [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]에서 선택\"\n",
        "},\n",
        "\"product\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\n",
        "        \"type\": \"string\", \n",
        "        \"description\": \"STEP 2에서 식별한 제품/서비스 이름\"\n",
        "        },\n",
        "        \"action\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"STEP 2-3에서 결정한 고객 기대 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택\"\n",
        "        },\n",
        "        \"domain\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"매칭된 상품 후보의 도메인 정보 (참고용, 최종 JSON에는 제외)\"\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "},\n",
        "\"channel\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"type\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택\"\n",
        "        },\n",
        "        \"value\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보\"\n",
        "        },\n",
        "        \"action\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"\n",
        "        },\n",
        "        \"benefit\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"해당 채널 이용 시 특별 혜택\"\n",
        "        },\n",
        "        \"store_code\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "\"pgm\": {\n",
        "    \"type\": \"array\",\n",
        "    \"description\": \"STEP 4에서 선택한 프로그램 분류 (적합도 순 2개)\"\n",
        "}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_result_list = []\n",
        "interim_result_list = []\n",
        "\n",
        "for mms_msg in mms_pdf.sample(100)['msg'].tolist():\n",
        "\n",
        "    final_result_dict = {}\n",
        "    interim_result_dict = {}\n",
        "\n",
        "    print(mms_msg)\n",
        "\n",
        "    mms_embedding = model.encode([mms_msg.lower()], convert_to_tensor=True)\n",
        "\n",
        "    similarities = torch.nn.functional.cosine_similarity(\n",
        "        mms_embedding,  \n",
        "        clue_embeddings,  \n",
        "        dim=1 \n",
        "    ).cpu().numpy()\n",
        "\n",
        "    pgm_pdf_tmp = pgm_pdf.copy()\n",
        "    pgm_pdf_tmp['sim'] = similarities\n",
        "\n",
        "    pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)\n",
        "\n",
        "    def filter_specific_terms(strings: List[str]) -> List[str]:\n",
        "        unique_strings = list(set(strings))  # 중복 제거\n",
        "        unique_strings.sort(key=len, reverse=True)  # 길이 기준 내림차순 정렬\n",
        "\n",
        "        filtered = []\n",
        "        for s in unique_strings:\n",
        "            if not any(s in other for other in filtered):\n",
        "                filtered.append(s)\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def sliding_window_with_step(data, window_size, step=1):\n",
        "        \"\"\"Sliding window with configurable step size.\"\"\"\n",
        "        return [data[i:i + window_size] for i in range(0, len(data) - window_size + 1, step)]\n",
        "\n",
        "    # tdf = pd.DataFrame([{'form_text':d[0],'tag_text':d[1],'start_text':d[2],'end_text':d[2]+d[3]} for d in kiwi_raw.analyze(mms_msg)[0][0]])\n",
        "    result_msg_raw = kiwi_raw.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "    token_list_msg = [d for d in result_msg_raw \n",
        "                    if d[1] not in tags_to_exclude\n",
        "                    ]\n",
        "\n",
        "    result_msg = kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "    entities_from_kiwi = []\n",
        "    for token in result_msg:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "        if token.tag == 'NNP' and token.form not in stop_item_names+['-'] and len(token.form)>=2 and not token.form.lower() in stop_item_names:  # 고유명사인 경우\n",
        "        # if token.tag == 'NNG' and token.form in stop_item_names_ext:  # 고유명사인 경우\n",
        "            entities_from_kiwi.append(token.form)\n",
        "\n",
        "    from typing import List\n",
        "    # 결과\n",
        "    entities_from_kiwi = filter_specific_terms(entities_from_kiwi)\n",
        "\n",
        "    # print(\"추출된 개체명:\", list(set(entities_from_kiwi)))\n",
        "\n",
        "    ngram_list_msg = []\n",
        "    for w_size in range(1,5):\n",
        "        windows = sliding_window_with_step(token_list_msg, w_size, step=1)\n",
        "        windows_new = []\n",
        "        for w in windows:\n",
        "            tag_str = ','.join([t.tag for t in w])\n",
        "            flag = True\n",
        "            for et in exc_tag_patterns:\n",
        "                if ','.join(et) in tag_str:\n",
        "                    flag = False\n",
        "                    # print(w)\n",
        "                    break\n",
        "        \n",
        "            if flag:\n",
        "                windows_new.append([[d.form for d in w], [d.tag for d in w]])\n",
        "\n",
        "        ngram_list_msg.extend(windows_new)\n",
        "\n",
        "    # 패턴에 해당하는 토큰 인덱스 찾기\n",
        "    def find_pattern_indices(tokens, patterns):\n",
        "        indices_to_exclude = set()\n",
        "        \n",
        "        # 단일 태그 패턴 먼저 체크\n",
        "        for i in range(len(tokens)):\n",
        "            for pattern in patterns:\n",
        "                if len(pattern) == 1 and tokens[i].tag == pattern[0]:\n",
        "                    indices_to_exclude.add(i)\n",
        "        \n",
        "        # 연속된 패턴 검사\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i in indices_to_exclude:\n",
        "                i += 1\n",
        "                continue\n",
        "                \n",
        "            for pattern in patterns:\n",
        "                if len(pattern) > 1:  # 두 개 이상의 태그로 구성된 패턴\n",
        "                    if i + len(pattern) <= len(tokens):  # 패턴 길이만큼 토큰이 남아있는지 확인\n",
        "                        match = True\n",
        "                        for j in range(len(pattern)):\n",
        "                            if tokens[i+j].tag != pattern[j]:\n",
        "                                match = False\n",
        "                                break\n",
        "                        \n",
        "                        if match:  # 패턴이 일치하면 해당 토큰들의 인덱스를 모두 추가\n",
        "                            for j in range(len(pattern)):\n",
        "                                indices_to_exclude.add(i+j)\n",
        "            i += 1\n",
        "                        \n",
        "        return indices_to_exclude\n",
        "\n",
        "    # 패턴에 해당하지 않는 토큰만 필터링\n",
        "    def filter_tokens_by_patterns(tokens, patterns):\n",
        "        indices_to_exclude = find_pattern_indices(tokens, patterns)\n",
        "        return [tokens[i] for i in range(len(tokens)) if i not in indices_to_exclude]\n",
        "\n",
        "    # 제외된 토큰 없이 텍스트 재구성 - 단순 연결 방식\n",
        "    def reconstruct_text_without_spaces(tokens):\n",
        "        # 토큰들을 원래 시작 위치에 따라 정렬\n",
        "        sorted_tokens = sorted(tokens, key=lambda token: token.start)\n",
        "        \n",
        "        result = []\n",
        "        for token in sorted_tokens:\n",
        "            result.append(token.form)\n",
        "        \n",
        "        # 토큰들을 공백 하나로 구분하여 결합\n",
        "        return ' '.join(result)\n",
        "\n",
        "    # 더 자연스러운 텍스트 재구성 - 원본 위치 기반 보존, 제외된 토큰은 건너뜀\n",
        "    def reconstruct_text_preserved_positions(original_tokens, filtered_tokens):\n",
        "        # 원본 토큰의 위치와 형태를 기록할 사전 생성\n",
        "        token_map = {}\n",
        "        for i, token in enumerate(original_tokens):\n",
        "            token_map[(token.start, token.len)] = (i, token.form)\n",
        "        \n",
        "        # 필터링된 토큰의 인덱스 찾기\n",
        "        filtered_indices = set()\n",
        "        for token in filtered_tokens:\n",
        "            key = (token.start, token.len)\n",
        "            if key in token_map:\n",
        "                filtered_indices.add(token_map[key][0])\n",
        "        \n",
        "        # 원본 순서대로 필터링된 토큰만 선택\n",
        "        result = []\n",
        "        for i, token in enumerate(original_tokens):\n",
        "            if i in filtered_indices:\n",
        "                result.append(token.form)\n",
        "        \n",
        "        return ' '.join(result)\n",
        "\n",
        "    # 결과 출력\n",
        "    filtered_tokens = filter_tokens_by_patterns(result_msg_raw, exc_tag_patterns)\n",
        "    msg_text_filtered = reconstruct_text_preserved_positions(result_msg_raw, filtered_tokens)\n",
        "\n",
        "    msg_text_filtered\n",
        "\n",
        "    ngram_list_msg_filtered = []\n",
        "    for w_size in range(2,4):\n",
        "        windows = sliding_window_with_step(list(msg_text_filtered.lower().replace(' ', '')), w_size, step=1)\n",
        "        ngram_list_msg_filtered.extend(windows)\n",
        "\n",
        "    col_for_form_tmp_ent = 'char_entity'\n",
        "    col_for_form_tmp_msg = 'char_msg'\n",
        "\n",
        "    edf['form_tmp'] = edf[col_for_form_tmp_ent].apply(lambda x: [' '.join(s) for s in sliding_window_with_step(x, 2, step=1)])\n",
        "\n",
        "    tdf = pd.DataFrame(ngram_list_msg).rename(columns={0:'token_txt', 1:'token_tag'})\n",
        "    tdf['token_key'] = tdf.apply(lambda x: ''.join(x['token_txt'])+''.join(x['token_tag']), axis=1)\n",
        "    tdf = tdf.drop_duplicates(['token_key']).drop(['token_key'], axis=1)\n",
        "    tdf['char_msg'] = tdf.apply(lambda x: list((\" \".join(x['token_txt'])).lower().replace(' ', '')), axis=1)\n",
        "\n",
        "    tdf['form_tmp'] = tdf[col_for_form_tmp_msg].apply(lambda x: [' '.join(s) for s in sliding_window_with_step(x, 2, step=1)])\n",
        "    tdf['token_txt_str'] = tdf['token_txt'].str.join(',')\n",
        "    tdf['token_tag_str'] = tdf['token_tag'].str.join(',')\n",
        "\n",
        "    # tdf['txt'] = tdf.apply(lambda x: ' '.join(x['token_txt']), axis=1) \n",
        "\n",
        "    fdf = edf.explode('form_tmp').merge(tdf.explode('form_tmp'), on='form_tmp').drop(['form_tmp'], axis=1)\n",
        "\n",
        "    fdf = fdf.query(\"item_nm_alias.str.lower() not in @stop_item_names and token_txt_str.replace(',','').str.lower() not in @stop_item_names\").drop_duplicates(['item_nm','item_nm_alias','item_id','token_txt_str','token_tag_str'])\n",
        "\n",
        "    def ngram_jaccard_similarity(list1, list2, n=2):\n",
        "        \"\"\"Calculate similarity using Jaccard similarity of n-grams.\"\"\"\n",
        "        # Generate n-grams for both lists\n",
        "        def get_ngrams(lst, n):\n",
        "            return [tuple(lst[i:i+n]) for i in range(len(lst)-n+1)]\n",
        "        \n",
        "        # Handle edge cases\n",
        "        if len(list1) < n or len(list2) < n:\n",
        "            if list1 == list2:\n",
        "                return 1.0\n",
        "            else:\n",
        "                return 0.0\n",
        "        \n",
        "        # Generate n-grams and calculate Jaccard similarity\n",
        "        ngrams1 = set(get_ngrams(list1, n))\n",
        "        ngrams2 = set(get_ngrams(list2, n))\n",
        "        \n",
        "        intersection = ngrams1.intersection(ngrams2)\n",
        "        union = ngrams1.union(ngrams2)\n",
        "        \n",
        "        return len(intersection) / len(union) if union else 0\n",
        "\n",
        "    def needleman_wunsch_similarity(list1, list2, match_score=1, mismatch_penalty=1, gap_penalty=1):\n",
        "        \"\"\"Global sequence alignment with Needleman-Wunsch algorithm.\"\"\"\n",
        "        m, n = len(list1), len(list2)\n",
        "        \n",
        "        # Initialize score matrix\n",
        "        score = np.zeros((m+1, n+1))\n",
        "        \n",
        "        # Initialize first row and column with gap penalties\n",
        "        for i in range(m+1):\n",
        "            score[i][0] = -i * gap_penalty\n",
        "        for j in range(n+1):\n",
        "            score[0][j] = -j * gap_penalty\n",
        "        \n",
        "        # Fill the score matrix\n",
        "        for i in range(1, m+1):\n",
        "            for j in range(1, n+1):\n",
        "                match = score[i-1][j-1] + (match_score if list1[i-1] == list2[j-1] else -mismatch_penalty)\n",
        "                delete = score[i-1][j] - gap_penalty\n",
        "                insert = score[i][j-1] - gap_penalty\n",
        "                score[i][j] = max(match, delete, insert)\n",
        "        \n",
        "        # Calculate similarity score\n",
        "        max_possible_score = min(m, n) * match_score\n",
        "        alignment_score = score[m][n]\n",
        "        \n",
        "        # Normalize to 0-1 range\n",
        "        min_possible_score = -max(m, n) * max(gap_penalty, mismatch_penalty)\n",
        "        normalized_score = (alignment_score - min_possible_score) / (max_possible_score - min_possible_score)\n",
        "        \n",
        "        return normalized_score\n",
        "\n",
        "    fdf['sim_score_token'] = fdf.apply(lambda row: needleman_wunsch_similarity(row['token_txt'], row['token_entity']), axis=1)\n",
        "    fdf['sim_score_char'] = fdf.apply(lambda row: advanced_sequential_similarity((''.join(row['char_msg'])), (''.join(row['char_entity'])),metrics='difflib')['difflib'], axis=1)\n",
        "\n",
        "    entity_list = [e.replace(' ', '').lower() for e in list(edf['item_nm_alias'].unique())]\n",
        "    entities_from_kiwi_rev = [e.replace(' ', '').lower() for e in entities_from_kiwi]\n",
        "\n",
        "    kdf = fdf.query(\"item_nm_alias in @entities_from_kiwi_rev or token_txt_str.str.replace(',',' ').str.lower() in @entities_from_kiwi_rev or token_txt_str.str.replace(',','').str.lower() in @entities_from_kiwi_rev\").copy()\n",
        "    kdf = kdf.query(\"(sim_score_token>=0.75 and sim_score_char>=0.75) or sim_score_char>=1\").query(\"item_nm_alias.str.replace(',','').str.lower() in @entity_list or item_nm_alias.str.replace(' ','').str.lower() in @entity_list\")\n",
        "    kdf['rank'] = kdf.groupby(['token_txt_str'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "    kdf = kdf.query(\"rank<=1\")[['item_nm','item_nm_alias','item_id','token_txt_str','domain','sim_score_token','sim_score_char']].drop_duplicates()\n",
        "    # kdf = kdf.query(\"rank<=1\")\n",
        "\n",
        "    # kdf = kdf.groupby('item_nm_alias', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 2), random_state=42), include_groups=False)\n",
        "\n",
        "    tags_to_exclude_final = ['SN']\n",
        "    filtering_condition = [\n",
        "    \"\"\"not token_tag_str in @tags_to_exclude_final\"\"\"\n",
        "    ,\"\"\"and token_txt_str.str.len()>=2\"\"\"\n",
        "    ,\"\"\"and not token_txt_str in @stop_item_names\"\"\"\n",
        "    ,\"\"\"and not token_txt_str.str.replace(',','').str.lower() in @stop_item_names\"\"\"\n",
        "    ,\"\"\"and not item_nm_alias in @stop_item_names\"\"\"\n",
        "    ]\n",
        "\n",
        "    sdf = (\n",
        "        fdf\n",
        "        .query(\"item_nm_alias.str.lower() not in @stop_item_names\")\n",
        "        .query(\"(sim_score_token>=0.7 and sim_score_char>=0.8) or (sim_score_token>=0.1 and sim_score_char>=0.9)\")\n",
        "        # .query(\"item_nm_alias.str.contains('에이닷', case=False)\")\n",
        "        .query(' '.join(filtering_condition))\n",
        "        .sort_values('sim_score_char', ascending=False)\n",
        "        [['item_nm_alias','item_id','token_txt','token_txt_str','sim_score_token','sim_score_char','domain']]\n",
        "    ).copy()\n",
        "\n",
        "    sdf['rank_e'] = sdf.groupby(['item_nm_alias'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "    sdf['rank_t'] = sdf.groupby(['token_txt_str'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "    sdf = sdf.query(\"rank_t<=1 and rank_e<=1\")[['item_nm_alias','item_id','token_txt_str','domain']].drop_duplicates()\n",
        "\n",
        "    # sdf = sdf.groupby('item_nm_alias', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 2), random_state=42), include_groups=False)\n",
        "\n",
        "    if pd.concat([kdf,sdf]).shape[0]<1:\n",
        "        continue\n",
        "    \n",
        "    product_df = pd.concat([kdf,sdf]).drop_duplicates(['item_id','item_nm','item_nm_alias','domain']).groupby([\"item_nm\",\"item_nm_alias\",\"item_id\",\"domain\"])['token_txt_str'].apply(list).reset_index(name='item_name_in_message').rename(columns={'item_nm':'item_name_in_voca'}).sort_values('item_name_in_voca')\n",
        "\n",
        "    product_df['item_name_in_message'] = product_df['item_name_in_message'].apply(lambda x: \",\".join(list(set([w.replace(',',' ') for w in x]))))\n",
        "\n",
        "    product_df[['item_name_in_message','item_name_in_voca','item_id','domain']]#.query(\"item_name_in_voca.str.contains('netflix', case=False)\").drop_duplicates(['item_name_in_voca']).sort_values('item_id')\n",
        "\n",
        "    ### Entity-Assisted\n",
        "\n",
        "    # product_info = (\",\\n\".join(product_df.apply(lambda x: f'\"item_name_in_msg\":\"{x['item_name_in_msg']}\", \"item_name_in_voca\":\"{x['item_name_in_voca']}\", \"item_id\":\"{x['item_id']}:, \"action\":고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택', axis=1).tolist()))\n",
        "    # product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "    product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "    # product_df = product_df.drop_duplicates(['item_name_in_message','item_name_in_voca'])\n",
        "    # product_df = product_df.merge(product_df.groupby('item_name_in_message')['item_id'].size().reset_index(name='count').sort_values('count', ascending=False), on='item_name_in_message', how='left').query('count<=3')\n",
        "    product_df = product_df[['item_name_in_voca','item_id','domain']].drop_duplicates()\n",
        "    product_df['action'] = '고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택'\n",
        "\n",
        "    # product_element = product_df.to_dict(orient='records') if product_df.shape[0]>0 else schema_prd['product']\n",
        "\n",
        "    # pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "    # rag_context = f\"\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "    # schema_prd_ent = {\n",
        "    #     \"title\": {\n",
        "    #         \"type\": \"string\", \n",
        "    #         'description': '광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성'\n",
        "    #     },\n",
        "    #     \"purpose\": {\n",
        "    #         \"type\": \"array\", \n",
        "    #         'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    #     },\n",
        "    #     \"product\": \n",
        "    #         product_element\n",
        "    #     ,\n",
        "    #     'channel': {\n",
        "    #         'type': 'array', \n",
        "    #         'items': {\n",
        "    #             'type': 'object', \n",
        "    #             'properties': {\n",
        "    #                 'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "    #                 'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "    #                 'action': {'type': 'string', 'description': '채널 목적: [방문, 접속, 가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "    #                 # 'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "    #                 'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "    #             }\n",
        "    #         },\n",
        "    #     },\n",
        "    #     'pgm':{\n",
        "    #         'type': 'array', \n",
        "    #         'description': '아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공'\n",
        "    #     }\n",
        "    # }\n",
        "\n",
        "    # # Improved extraction guidance\n",
        "    # extraction_guide = \"\"\"\n",
        "    # ### 분석 목표 ###\n",
        "    # * Schema의 Product 태그 내에 action을 추출하세요.\n",
        "    # * Schema내 action 항목 외 태그 정보는 원본 그대로 두세요.\n",
        "\n",
        "    # ### 고려사항 ###\n",
        "    # * 상품 정보에 있는 항목을 임의로 변형하거나 누락시키지 마세요.\n",
        "    # * 광고 분류 기준 정보는 pgm_nm : clue_tag 로 구성\n",
        "\n",
        "    # ### JSON 응답 형식 ###\n",
        "    # 응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "    # \"\"\"\n",
        "\n",
        "    # # Create the system message with clear JSON output requirements\n",
        "    # user_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "    # ### 분석 대상 광고 메세지 ###\n",
        "    # {mms_msg}\n",
        "\n",
        "    # ### 결과 Schema ###\n",
        "    # {json.dumps(schema_prd_ent, indent=2, ensure_ascii=False)}\n",
        "\n",
        "    # {extraction_guide}\n",
        "\n",
        "    # {rag_context}\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "    # try:\n",
        "    #     # # Use OpenAI's ChatCompletion with the current API format\n",
        "    #     # response = client.chat.completions.create(\n",
        "    #     #     model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "    #     # # model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "    #     #     messages = [\n",
        "    #     #         {\"role\": \"user\", \"content\": user_message},\n",
        "    #     #     ],\n",
        "    #     #     temperature=0.0,\n",
        "    #     #     max_tokens=4000,\n",
        "    #     #     top_p=0.95,  # Reduces randomness\n",
        "    #     #     frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "    #     #     presence_penalty=0.0,\n",
        "    #     #     response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    #     # )\n",
        "        \n",
        "    #     # # Extract the JSON from the response\n",
        "    #     # result_json_text = response.choices[0].message.content\n",
        "    #     # json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    #     llm_result = llm_gem3.invoke(user_message, max_tokens=4000)\n",
        "    #     json_objects = extract_json_objects(llm_result.content)[0]\n",
        "\n",
        "    #     pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "\n",
        "    #     final_result = json_objects.copy()\n",
        "    #     final_result['pgm'] = pgm_json\n",
        "\n",
        "    #     final_result_dict['ent'] = final_result\n",
        "\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error with API call: {e}\")\n",
        "\n",
        "    # print(json.dumps(final_json, indent=4, ensure_ascii=False))\n",
        "\n",
        "    ### LLM-only\n",
        "\n",
        "    extraction_guide = \"\"\"\n",
        "    ### 분석 시 고려사항 ###\n",
        "    * 하나의 광고에 여러 상품이 포함될 수 있으며, 각 상품별로 별도 객체 생성\n",
        "    * 재현율이 높도록 모든 상품을 선택\n",
        "    * 상품 후보 정보는 상품 이름 (도메인) 형식으로 제공\n",
        "    * 광고 분류 기준 정보는 pgm_nm : clue_tag 로 구성\n",
        "\n",
        "    ### 분석 목표 ###\n",
        "    * 텍스트 매칭 기법으로 만들어진 상품 후보 정보가 제공되면 이를 확인하여 참고하라.\n",
        "    * 제공된 상품 이름이 적합하지 않으면 무시하고, 목록에 없어도 적합한 상품이 있으면 추출하세요.\n",
        "\n",
        "    ### JSON 응답 형식 ###\n",
        "    응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "    \"\"\"\n",
        "\n",
        "    # product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "    product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "    rag_context = f\"### 상품 후보 정보 ###\\n\\t{product_info}\" if product_df.shape[0]>0 else \"\"\n",
        "\n",
        "    pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "    rag_context += f\"\\n\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "    # Create the system message with clear JSON output requirements\n",
        "    user_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "    ### 분석 대상 광고 메세지 ###\n",
        "    {mms_msg}\n",
        "\n",
        "    ### 결과 Schema ###\n",
        "    {json.dumps(schema_prd, indent=2, ensure_ascii=False)}\n",
        "\n",
        "    {extraction_guide}\n",
        "\n",
        "    {rag_context}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Use OpenAI's ChatCompletion with the current API format\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "        #   model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": user_message},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=4000,\n",
        "            top_p=0.95,  # Reduces randomness\n",
        "            frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "            presence_penalty=0.0,\n",
        "            response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "        )\n",
        "        \n",
        "        # Extract the JSON from the response\n",
        "        result_json_text = response.choices[0].message.content\n",
        "        json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "        interim_result_dict['ax'] = json_objects\n",
        "\n",
        "        llm_result = llm_gem3.invoke(user_message, max_tokens=4000)\n",
        "        json_objects = extract_json_objects(llm_result.content)[0]\n",
        "\n",
        "        interim_result_dict['gem'] = json_objects\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with API call: {e}\")\n",
        "        # print(f\"Error with API call: {e}\")\n",
        "\n",
        "    # matches = []\n",
        "    # for item_name_message in json_objects['product']:\n",
        "    #     matches.extend(find_entities_in_text(\n",
        "    #         item_name_message['name'], \n",
        "    #         entity_list_for_fuzzy, \n",
        "    #         min_similarity=50,\n",
        "    #         high_score_threshold=50,\n",
        "    #         overlap_tolerance=0.5\n",
        "    #     ))\n",
        "\n",
        "    # mdf = pd.DataFrame(matches)\n",
        "    # if len(matches)>0:\n",
        "    #     mdf = mdf.query(\"text.str.lower() not in @stop_item_names and matched_entity.str.lower() not in @stop_item_names\")\n",
        "\n",
        "    # if mdf.shape[0]>0:\n",
        "    #     mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    #     mdf['domain'] = mdf['data'].apply(lambda x: x['domain'])\n",
        "    #     mdf = mdf.query(\"not matched_entity.str.contains('test', case=False)\").drop_duplicates(['item_id','domain'])\n",
        "\n",
        "    #     mdf = mdf.merge(mdf.groupby(['text','start'])['end'].max().reset_index(name='end'), on=['text', 'start', 'end'])\n",
        "\n",
        "    #     mdf['rank'] = mdf['data'].apply(lambda x: x['rank'])\n",
        "    #     mdf['re_rank'] = mdf.groupby('text')['score'].rank(ascending=False)\n",
        "    #     mdf = mdf.query(\"re_rank<=2\")\n",
        "\n",
        "    #     mdf = mdf.merge(pd.DataFrame(json_objects['product']).rename(columns={'name':'text'}), on='text', how='left')\n",
        "\n",
        "    #     product_tag = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','domain']].drop_duplicates().to_dict(orient='records')\n",
        "\n",
        "    #     final_result = {\n",
        "    #         \"title\":json_objects['title'],\n",
        "    #         \"purpose\":json_objects['purpose'],\n",
        "    #         \"product\":product_tag,\n",
        "    #         \"channel\":json_objects['channel'],\n",
        "    #         \"pgm\":json_objects['pgm']\n",
        "    #     }\n",
        "\n",
        "    # else:\n",
        "    #     final_result = json_objects\n",
        "    #     final_result['product'] = [{'item_name_in_message':d['name'], 'item_name_in_voca':d['name'], 'item_id': '#', 'domain': '#'} for d in final_result['product']]\n",
        "\n",
        "    # if num_cand_pgms>0:\n",
        "    #     pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    #     final_result['pgm'] = pgm_json\n",
        "\n",
        "    # # print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "    # final_result_dict['llm'] = final_result\n",
        "\n",
        "    ### cld 40\n",
        "\n",
        "    try:\n",
        "        response = llm_cld40.invoke([\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "        ])\n",
        "        \n",
        "        # Extract the JSON from the response\n",
        "        result_json_text = response.content\n",
        "        json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "        interim_result_dict['c40'] = json_objects\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with API call: {e}\")\n",
        "\n",
        "    # matches = []\n",
        "    # for item_name_message in json_objects['product']:\n",
        "    #     matches.extend(find_entities_in_text(\n",
        "    #         item_name_message['name'], \n",
        "    #         entity_list_for_fuzzy, \n",
        "    #         min_similarity=50,\n",
        "    #         high_score_threshold=50,\n",
        "    #         overlap_tolerance=0.5\n",
        "    #     ))\n",
        "\n",
        "    # mdf = pd.DataFrame(matches)\n",
        "    # if len(matches)>0:\n",
        "    #     mdf = mdf.query(\"text.str.lower() not in @stop_item_names and matched_entity.str.lower() not in @stop_item_names\")\n",
        "\n",
        "    # if mdf.shape[0]>0:\n",
        "    #     mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    #     mdf['domain'] = mdf['data'].apply(lambda x: x['domain'])\n",
        "    #     mdf = mdf.query(\"not matched_entity.str.contains('test', case=False)\").drop_duplicates(['item_id','domain'])\n",
        "\n",
        "    #     mdf = mdf.merge(mdf.groupby(['text','start'])['end'].max().reset_index(name='end'), on=['text', 'start', 'end'])\n",
        "\n",
        "    #     mdf['rank'] = mdf['data'].apply(lambda x: x['rank'])\n",
        "    #     mdf['re_rank'] = mdf.groupby('text')['score'].rank(ascending=False)\n",
        "    #     mdf = mdf.query(\"re_rank<=2\")\n",
        "\n",
        "    #     mdf = mdf.merge(pd.DataFrame(json_objects['product']).rename(columns={'name':'text'}), on='text', how='left')\n",
        "\n",
        "    #     product_tag = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','domain']].drop_duplicates().to_dict(orient='records')\n",
        "\n",
        "    #     final_result = {\n",
        "    #         \"title\":json_objects['title'],\n",
        "    #         \"purpose\":json_objects['purpose'],\n",
        "    #         \"product\":product_tag,\n",
        "    #         \"channel\":json_objects['channel'],\n",
        "    #         \"pgm\":json_objects['pgm']\n",
        "    #     }\n",
        "\n",
        "    # else:\n",
        "    #     final_result = json_objects\n",
        "    #     final_result['product'] = [{'item_name_in_message':d['name'], 'item_name_in_voca':d['name'], 'item_id': '#', 'domain': '#'} for d in final_result['product']]\n",
        "\n",
        "    # if num_cand_pgms>0:\n",
        "    #     pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    #     final_result['pgm'] = pgm_json\n",
        "\n",
        "    # # print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "    # final_result_dict['c40'] = final_result\n",
        "\n",
        "    ### LLM-COT\n",
        "\n",
        "    # extraction_guide = \"\"\"\n",
        "    # ## 분석 지침\n",
        "    # 1. **재현율 우선**: 광고에서 언급된 모든 상품을 누락 없이 추출\n",
        "    # 2. **도메인 활용**: 상품 후보의 도메인 정보를 적극 활용하여 정확한 매칭 수행\n",
        "    # 3. **목적 기반 추론**: 광고 목적을 명확히 파악한 후 다른 요소들을 일관성 있게 분석\n",
        "    # 4. **채널 완전성**: 모든 접촉 채널을 누락 없이 추출하고 각각의 역할과 혜택을 명확히 식별\n",
        "    # 5. **컨텍스트 고려**: 제공된 상품 후보가 부적합하면 무시하고, 누락된 중요 상품이 있으면 추가\n",
        "    # 6. **매장 코드 정확성**: tworldfriends.co.kr URL에서 정확한 패턴 매칭을 통해 매장 코드 추출\n",
        "\n",
        "    # ## JSON 응답 형식\n",
        "    # - reasoning 섹션은 분석 과정 설명용이며 최종 JSON에는 포함하지 않음\n",
        "    # - 순수한 JSON 형식으로만 응답\n",
        "    # - 시작과 끝은 '{'와 '}'\n",
        "    # - 추가 텍스트나 설명 없이 JSON만 제공\n",
        "    # \"\"\"\n",
        "\n",
        "    # # product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "    # product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "    # rag_context = f\"### 상품 후보 정보 ###\\n\\t{product_info}\" if product_df.shape[0]>0 else \"\"\n",
        "\n",
        "    # pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "    # rag_context += f\"\\n\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "    # # Create the system message with clear JSON output requirements\n",
        "    # user_message = f\"\"\"당당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. **단계별 사고 과정(Chain of Thought)**을 통해 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요.\n",
        "\n",
        "    # ## 분석 단계 (Chain of Thought)\n",
        "\n",
        "    # ### STEP 1: 광고 목적(Purpose) 분석\n",
        "    # 먼저 광고 메시지 전체를 읽고 광고의 주요 목적을 파악하세요.\n",
        "\n",
        "    # ### STEP 2: 상품(Product) 식별 및 도메인 매칭\n",
        "    # 파악된 목적을 바탕으로 다음 과정을 거쳐 상품을 식별하세요:\n",
        "\n",
        "    # **2-1. 광고 메시지에서 언급된 모든 상품/서비스 추출**\n",
        "    # - 직접적으로 언급된 상품명을 모두 나열\n",
        "    # - 묵시적으로 언급된 서비스나 혜택도 포함\n",
        "\n",
        "    # **2-2. RAG Context의 상품 후보 정보와 도메인 매칭**\n",
        "    # - 각 추출된 상품을 상품 후보 정보와 비교\n",
        "    # - 도메인 정보(product, subscription_service 등)를 고려하여 가장 적합한 매칭 수행\n",
        "    # - 상품 후보에 없어도 광고에서 중요하게 다뤄지는 상품이 있다면 추가\n",
        "\n",
        "    # **2-3. 각 상품별 고객 행동(Action) 결정**\n",
        "    # - STEP 1에서 파악한 목적과 연결하여 각 상품에 대한 기대 행동 결정\n",
        "    # - 행동 후보: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타]\n",
        "\n",
        "    # ### STEP 3: 채널(Channel) 및 연락처 정보 추출\n",
        "    # 광고 메시지에서 고객이 접촉할 수 있는 모든 채널을 식별하고 분석하세요:\n",
        "\n",
        "    # **3-1. 채널 유형별 식별**\n",
        "    # - **URL**: 웹사이트 링크, 프로모션 페이지, 랜딩 페이지 등\n",
        "    # - **전화번호**: 고객센터, 상담 전화, 수신거부 번호 등\n",
        "    # - **앱**: 모바일 앱, 웹앱 등의 애플리케이션\n",
        "    # - **대리점**: 매장, 지점, 서비스센터 등\n",
        "\n",
        "    # **3-2. 각 채널별 세부 정보 분석**\n",
        "    # - **value**: 정확한 URL, 전화번호, 앱명, 대리점명 추출\n",
        "    # - **action**: 해당 채널의 주요 목적 파악 [가입, 추가 정보, 문의, 수신, 수신 거부]\n",
        "    # - **benefit**: 채널 이용 시 제공되는 특별 혜택이나 무료 서비스 등\n",
        "    # - **store_code**: tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴 추출\n",
        "\n",
        "    # **3-3. 채널 우선순위 및 역할 분석**\n",
        "    # - 주요 행동 유도 채널 vs 보조 정보 제공 채널 구분\n",
        "    # - 각 채널이 STEP 1에서 파악한 목적과 어떻게 연결되는지 분석\n",
        "\n",
        "    # ### STEP 4: 프로그램 분류(PGM) 결정\n",
        "    # - 광고 분류 기준 정보의 키워드와 메시지 내용 매칭\n",
        "    # - 적합도 순서대로 2개 선택\n",
        "\n",
        "    # ### 분석 대상 광고 메세지 ###\n",
        "    # {mms_msg}\n",
        "\n",
        "    # {rag_context}\n",
        "\n",
        "    # ### 결과 Schema ###\n",
        "    # {json.dumps(schema_prd_cot, indent=2, ensure_ascii=False)}\n",
        "\n",
        "    # {extraction_guide}\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "    # try:\n",
        "    #     # Use OpenAI's ChatCompletion with the current API format\n",
        "    #     response = client.chat.completions.create(\n",
        "    #         # model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "    #       model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "    #         messages = [\n",
        "    #             {\"role\": \"user\", \"content\": user_message},\n",
        "    #         ],\n",
        "    #         temperature=0.0,\n",
        "    #         max_tokens=4000,\n",
        "    #         top_p=0.95,  # Reduces randomness\n",
        "    #         frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "    #         presence_penalty=0.0,\n",
        "    #         response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    #     )\n",
        "        \n",
        "    #     # Extract the JSON from the response\n",
        "    #     result_json_text = response.choices[0].message.content\n",
        "    #     json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    #     interim_result_dict['cot'] = json_objects\n",
        "                    \n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error with API call: {e}\")\n",
        "\n",
        "    # matches = []\n",
        "    # for item_name_message in json_objects['product']:\n",
        "    #     matches.extend(find_entities_in_text(\n",
        "    #         item_name_message['name'], \n",
        "    #         entity_list_for_fuzzy, \n",
        "    #         min_similarity=50,\n",
        "    #         high_score_threshold=50,\n",
        "    #         overlap_tolerance=0.5\n",
        "    #     ))\n",
        "\n",
        "    # mdf = pd.DataFrame(matches)\n",
        "    # if len(matches)>0:\n",
        "    #     mdf = mdf.query(\"text.str.lower() not in @stop_item_names and matched_entity.str.lower() not in @stop_item_names\")\n",
        "\n",
        "    # if mdf.shape[0]>0:\n",
        "    #     mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    #     mdf['domain'] = mdf['data'].apply(lambda x: x['domain'])\n",
        "    #     mdf = mdf.query(\"not matched_entity.str.contains('test', case=False)\").drop_duplicates(['item_id','domain'])\n",
        "\n",
        "    #     mdf = mdf.merge(mdf.groupby(['text','start'])['end'].max().reset_index(name='end'), on=['text', 'start', 'end'])\n",
        "\n",
        "    #     mdf['rank'] = mdf['data'].apply(lambda x: x['rank'])\n",
        "    #     mdf['re_rank'] = mdf.groupby('text')['score'].rank(ascending=False)\n",
        "    #     mdf = mdf.query(\"re_rank<=2\")\n",
        "\n",
        "    #     mdf = mdf.merge(pd.DataFrame(json_objects['product']).rename(columns={'name':'text'}), on='text', how='left')\n",
        "\n",
        "    #     product_tag = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','domain']].drop_duplicates().to_dict(orient='records')\n",
        "\n",
        "    #     final_result = {\n",
        "    #         \"title\":json_objects['title'],\n",
        "    #         \"purpose\":json_objects['purpose'],\n",
        "    #         \"product\":product_tag,\n",
        "    #         \"channel\":json_objects['channel'],\n",
        "    #         \"pgm\":json_objects['pgm']\n",
        "    #     }\n",
        "\n",
        "    # else:\n",
        "    #     final_result = json_objects\n",
        "    #     final_result['product'] = [{'item_name_in_message':d['name'], 'item_name_in_voca':d['name'], 'item_id': '#', 'domain': '#'} for d in final_result['product']]\n",
        "\n",
        "    # if num_cand_pgms>0:\n",
        "    #     pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    #     final_result['pgm'] = pgm_json\n",
        "\n",
        "    # # print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "    # final_result_dict['cot'] = final_result\n",
        "\n",
        "    final_result_list.append(final_result_dict)\n",
        "    interim_result_list.append(interim_result_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "def calculate_list_similarity(list1, list2):\n",
        "    \"\"\"Calculate Jaccard similarity between two lists\"\"\"\n",
        "    if isinstance(list1, dict):\n",
        "        list1 = [str(item) for item in list1.values()]\n",
        "    if isinstance(list2, dict):\n",
        "        list2 = [str(item) for item in list2.values()]\n",
        "    # Ensure lists contain strings\n",
        "    list1 = [str(item) for item in list1]\n",
        "    list2 = [str(item) for item in list2]\n",
        "    # Convert lists to sets for comparison\n",
        "    set1 = set(sorted(set(list1)))\n",
        "    set2 = set(sorted(set(list2)))\n",
        "    # Calculate Jaccard similarity\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "def calculate_text_similarity(text1, text2):\n",
        "    \"\"\"Calculate text similarity using SequenceMatcher\"\"\"\n",
        "    return SequenceMatcher(None, str(text1), str(text2)).ratio()\n",
        "def calculate_product_similarity(prod1, prod2):\n",
        "    \"\"\"Calculate similarity between product dictionaries with detailed structure\"\"\"\n",
        "    if not isinstance(prod1, dict) or not isinstance(prod2, dict):\n",
        "        return 0.0\n",
        "    # Calculate similarity for each field\n",
        "    item_name_message_sim = calculate_text_similarity(\n",
        "        prod1.get('item_name_in_message', '#'),\n",
        "        prod2.get('item_name_in_message', '&')\n",
        "    )\n",
        "    item_name_voca_sim = calculate_text_similarity(\n",
        "        prod1.get('item_name_in_voca', '#'),\n",
        "        prod2.get('item_name_in_voca', '&')\n",
        "    )\n",
        "    item_id_sim = calculate_text_similarity(\n",
        "        prod1.get('item_id', '#'),\n",
        "        prod2.get('item_id', '&')\n",
        "    )\n",
        "    domain_sim = calculate_text_similarity(\n",
        "        prod1.get('domain', '#'),\n",
        "        prod2.get('domain', '&')\n",
        "    )\n",
        "    name_sim = calculate_text_similarity(\n",
        "        prod1.get('name', '#'),\n",
        "        prod2.get('name', '&')\n",
        "    )\n",
        "    action_sim = calculate_text_similarity(\n",
        "        prod1.get('action', '#'),\n",
        "        prod2.get('action', '&')\n",
        "    )\n",
        "    # print(item_name_message_sim, item_name_voca_sim, item_id_sim, domain_sim, name_sim, action_sim)\n",
        "    # Weighted average - item_id and domain are more distinctive\n",
        "    similarity = (\n",
        "        item_name_message_sim +\n",
        "        item_name_voca_sim +\n",
        "        item_id_sim +\n",
        "        domain_sim +\n",
        "        name_sim +\n",
        "        action_sim\n",
        "    )/len(prod1.keys())\n",
        "    return similarity\n",
        "def calculate_channel_similarity(chan1, chan2):\n",
        "    \"\"\"Calculate similarity between channel dictionaries\"\"\"\n",
        "    if not isinstance(chan1, dict) or not isinstance(chan2, dict):\n",
        "        return 0.0\n",
        "    type_sim = calculate_text_similarity(chan1.get('type', ''), chan2.get('type', ''))\n",
        "    value_sim = calculate_text_similarity(chan1.get('value', ''), chan2.get('value', ''))\n",
        "    action_sim = calculate_text_similarity(chan1.get('action', ''), chan2.get('action', ''))\n",
        "    return (type_sim + value_sim + action_sim) / 3\n",
        "def calculate_pgm_similarity(pgm1, pgm2):\n",
        "    \"\"\"Calculate similarity between program dictionaries\"\"\"\n",
        "    if isinstance(pgm1, dict) and isinstance(pgm2, dict):\n",
        "        pgm_nm_sim = calculate_text_similarity(pgm1.get('pgm_nm', ''), pgm2.get('pgm_nm', ''))\n",
        "        pgm_id_sim = calculate_text_similarity(pgm1.get('pgm_id', ''), pgm2.get('pgm_id', ''))\n",
        "        pgm_sim = pgm_nm_sim * 0.4 + pgm_id_sim * 0.6\n",
        "    else:\n",
        "        pgm_sim = 0.0\n",
        "    # pgm_id is more distinctive, so give it higher weight\n",
        "    return pgm_sim\n",
        "def calculate_products_list_similarity(products1, products2):\n",
        "    \"\"\"Calculate similarity between two lists of product dictionaries\"\"\"\n",
        "    if not products1 or not products2:\n",
        "        return 0.0\n",
        "    # For each product in list1, find best match in list2\n",
        "    similarities = []\n",
        "    for p1 in products1:\n",
        "        best_match = 0.0\n",
        "        for p2 in products2:\n",
        "            similarity = calculate_product_similarity(p1, p2)\n",
        "            best_match = max(best_match, similarity)\n",
        "        similarities.append(best_match)\n",
        "    # Also check reverse direction to handle different list sizes\n",
        "    reverse_similarities = []\n",
        "    for p2 in products2:\n",
        "        best_match = 0.0\n",
        "        for p1 in products1:\n",
        "            similarity = calculate_product_similarity(p1, p2)\n",
        "            best_match = max(best_match, similarity)\n",
        "        reverse_similarities.append(best_match)\n",
        "    # Take average of both directions\n",
        "    forward_avg = sum(similarities) / len(similarities)\n",
        "    reverse_avg = sum(reverse_similarities) / len(reverse_similarities)\n",
        "    return (forward_avg + reverse_avg) / 2\n",
        "def calculate_channels_list_similarity(channels1, channels2):\n",
        "    \"\"\"Calculate similarity between two lists of channel dictionaries\"\"\"\n",
        "    if not channels1 or not channels2:\n",
        "        return 0.0\n",
        "    similarities = []\n",
        "    for c1 in channels1:\n",
        "        best_match = 0.0\n",
        "        for c2 in channels2:\n",
        "            similarity = calculate_channel_similarity(c1, c2)\n",
        "            best_match = max(best_match, similarity)\n",
        "        similarities.append(best_match)\n",
        "    return sum(similarities) / len(similarities)\n",
        "def calculate_pgms_list_similarity(pgms1, pgms2):\n",
        "    \"\"\"Calculate similarity between two lists of program dictionaries\"\"\"\n",
        "    if not pgms1 or not pgms2:\n",
        "        return 0.0\n",
        "    if isinstance(pgms1, list) and isinstance(pgms2, list):\n",
        "        # print(pgms1, pgms2)\n",
        "        pgm_sim = calculate_list_similarity(pgms1, pgms2)\n",
        "        return pgm_sim\n",
        "    # For each pgm in list1, find best match in list2\n",
        "    similarities = []\n",
        "    for p1 in pgms1:\n",
        "        best_match = 0.0\n",
        "        for p2 in pgms2:\n",
        "            similarity = calculate_pgm_similarity(p1, p2)\n",
        "            best_match = max(best_match, similarity)\n",
        "        similarities.append(best_match)\n",
        "    # Also check reverse direction\n",
        "    reverse_similarities = []\n",
        "    for p2 in pgms2:\n",
        "        best_match = 0.0\n",
        "        for p1 in pgms1:\n",
        "            similarity = calculate_pgm_similarity(p1, p2)\n",
        "            best_match = max(best_match, similarity)\n",
        "        reverse_similarities.append(best_match)\n",
        "    # Take average of both directions\n",
        "    forward_avg = sum(similarities) / len(similarities)\n",
        "    reverse_avg = sum(reverse_similarities) / len(reverse_similarities)\n",
        "    return (forward_avg + reverse_avg) / 2\n",
        "def calculate_dictionary_similarity(dict1, dict2):\n",
        "    \"\"\"\n",
        "    Calculate similarity between two dictionaries with generalized structure:\n",
        "    {\n",
        "        'title': str,\n",
        "        'purpose': [list of strings],\n",
        "        'product': [list of product dicts],\n",
        "        'channel': [list of channel dicts],\n",
        "        'pgm': [list of program dicts]\n",
        "    }\n",
        "    \"\"\"\n",
        "    if not isinstance(dict1, dict) or not isinstance(dict2, dict):\n",
        "        return {'overall_similarity': 0.0, 'error': 'Both inputs must be dictionaries'}\n",
        "    # Calculate title similarity\n",
        "    title_similarity = calculate_text_similarity(\n",
        "        dict1.get('title', ''),\n",
        "        dict2.get('title', '')\n",
        "    )\n",
        "    # Calculate purpose similarity (list of strings)\n",
        "    purpose_similarity = calculate_list_similarity(\n",
        "        dict1.get('purpose', []),\n",
        "        dict2.get('purpose', [])\n",
        "    )\n",
        "    # Calculate product similarity (list of product dicts)\n",
        "    product_similarity = calculate_products_list_similarity(\n",
        "        dict1.get('product', []),\n",
        "        dict2.get('product', [])\n",
        "    )\n",
        "    # Calculate channel similarity (list of channel dicts)\n",
        "    channel_similarity = calculate_channels_list_similarity(\n",
        "        dict1.get('channel', []),\n",
        "        dict2.get('channel', [])\n",
        "    )\n",
        "    # Calculate pgm similarity (list of program dicts)\n",
        "    pgm_similarity = calculate_pgms_list_similarity(\n",
        "        dict1.get('pgm', []),\n",
        "        dict2.get('pgm', [])\n",
        "    )\n",
        "    # Calculate overall similarity (weighted average)\n",
        "    # Adjusted weights to reflect importance of each component\n",
        "    overall_similarity = (\n",
        "        title_similarity * 0.2 +\n",
        "        purpose_similarity * 0.15 +\n",
        "        product_similarity * 0.35 +\n",
        "        channel_similarity * 0.15 +\n",
        "        pgm_similarity * 0.15\n",
        "    )\n",
        "    return {\n",
        "        'overall_similarity': overall_similarity,\n",
        "        'title_similarity': title_similarity,\n",
        "        'purpose_similarity': purpose_similarity,\n",
        "        'product_similarity': product_similarity,\n",
        "        'channel_similarity': channel_similarity,\n",
        "        'pgm_similarity': pgm_similarity\n",
        "    }\n",
        "def get_detailed_product_comparison(dict1, dict2):\n",
        "    \"\"\"\n",
        "    Get detailed comparison of products between two dictionaries\n",
        "    \"\"\"\n",
        "    products1 = dict1.get('product', [])\n",
        "    products2 = dict2.get('product', [])\n",
        "    detailed_comparison = []\n",
        "    for i, p1 in enumerate(products1):\n",
        "        best_match = {'similarity': 0.0, 'match_index': -1, 'match_product': None}\n",
        "        for j, p2 in enumerate(products2):\n",
        "            similarity = calculate_product_similarity(p1, p2)\n",
        "            if similarity > best_match['similarity']:\n",
        "                best_match = {\n",
        "                    'similarity': similarity,\n",
        "                    'match_index': j,\n",
        "                    'match_product': p2\n",
        "                }\n",
        "        detailed_comparison.append({\n",
        "            'product1_index': i,\n",
        "            'product1': p1,\n",
        "            'best_match': best_match\n",
        "        })\n",
        "    return detailed_comparison\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dictionaries with the new schema\n",
        "    dict1 = {\n",
        "        'title': 'POOQ 콘텐츠 팩 출시 기념 혜택',\n",
        "        'purpose': ['상품 가입 유도'],\n",
        "        'product': [\n",
        "            {'item_name_in_message': 'POOQ', 'item_name_in_voca': 'PooQ 팩', 'item_id': 'T000009330', 'domain': 'product'},\n",
        "            {'item_name_in_message': 'POOQ 콘텐츠 팩', 'item_name_in_voca': 'FLO 콘텐츠 팩', 'item_id': 'PR00000217', 'domain': 'subscription_service'}\n",
        "        ],\n",
        "        'channel': [{'type': 'URL', 'value': 'http://t-mms.kr/t.do?m=#61&u=http://m2.tworld.co.kr/jsp/op.jsp?p=w1026', 'action': '가입'}],\n",
        "        'pgm': [\n",
        "            {'pgm_nm': '[마케팅_Sales]상품및부가서비스가입유도_구독'},\n",
        "            {'pgm_nm': '[마케팅_Sales]타사회선(가망)_win-back'}\n",
        "        ]\n",
        "    }\n",
        "    dict2 = {\n",
        "        'title': 'POOQ 콘텐츠 팩 특별 혜택',\n",
        "        'purpose': ['상품 가입 유도', '프로모션'],\n",
        "        'product': [\n",
        "            {'item_name_in_message': 'POOQ', 'item_name_in_voca': 'PooQ 팩', 'item_id': 'T000009330', 'domain': 'product'},\n",
        "            {'item_name_in_message': 'POOQ 콘텐츠 팩', 'item_name_in_voca': 'FLO 콘텐츠 팩 플러스', 'item_id': 'PR00000218', 'domain': 'subscription_service'}\n",
        "        ],\n",
        "        'channel': [{'type': 'URL', 'value': 'http://different-url.com', 'action': '가입'}],\n",
        "        'pgm': [\n",
        "            {'pgm_nm': '[마케팅_Sales]상품및부가서비스가입유도_구독'}\n",
        "        ]\n",
        "    }\n",
        "    # Calculate similarity\n",
        "    result = calculate_dictionary_similarity(dict1, dict2)\n",
        "    print(\"Similarity Results:\")\n",
        "    for key, value in result.items():\n",
        "        print(f\"{key}: {value:.3f}\")\n",
        "    print(\"\\nDetailed Product Comparison:\")\n",
        "    detailed = get_detailed_product_comparison(dict1, dict2)\n",
        "    for comparison in detailed:\n",
        "        print(f\"Product {comparison['product1_index']}: {comparison['product1']['item_name_in_message']}\")\n",
        "        print(f\"  Best match (similarity: {comparison['best_match']['similarity']:.3f}): {comparison['best_match']['match_product']['item_name_in_message'] if comparison['best_match']['match_product'] else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interim_result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax_result_list = []\n",
        "gem_result_list = []\n",
        "for interim_result_dict in interim_result_list:\n",
        "    try:\n",
        "        if len(interim_result_dict['c40'])<1 or len(interim_result_dict['ax'])<1 or len(interim_result_dict['gem'])<1: continue\n",
        "        ax_result_list.append(calculate_dictionary_similarity(interim_result_dict['c40'], interim_result_dict['ax']))\n",
        "        gem_result_list.append(calculate_dictionary_similarity(interim_result_dict['c40'], interim_result_dict['gem']))\n",
        "    except Exception as e:\n",
        "        pass\n",
        "len(ax_result_list), len(gem_result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame(ax_result_list).mean().to_frame().rename(columns={0:'a.x-3'}).merge(pd.DataFrame(gem_result_list).mean().to_frame().rename(columns={0:'gemma-3'}), left_index=True, right_index=True).round(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interim_result_dict['c40']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interim_result_dict['cot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calculate_dictionary_similarity(interim_result_dict['c40'], interim_result_dict['cot'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ko_core_news_sm\")\n",
        "\n",
        "msg_text_list = [\"\"\"\n",
        "    광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "    광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    광고 제목:통화 부가서비스를 패키지로 저렴하게!\n",
        "    광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ', \n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    (광고)[SKT] 1월 0 day 혜택 안내_ _[1월 20일(토) 혜택]_만 13~34세 고객이라면 _CU에서 핫바 1,000원에 구매 하세요!_(선착순 1만 명 증정)_▶ 자세히 보기 : http://t-mms.kr/t.do?m=#61&s=24264&a=&u=https://bit.ly/3H2OHSs__■ 에이닷 X T 멤버십 구독캘린더 이벤트_0 day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요! _알림 설정하면 추첨을 통해 [스타벅스 카페 라떼tall 모바일쿠폰]을 드립니다. _▶ 이벤트 참여하기 : https://bit.ly/3RVSojv_ _■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    '[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    광고 제목:[SK텔레콤] T건강습관 X AIA Vitality, 우리 가족의 든든한 보험!\n",
        "    광고 내용:(광고)[SKT] 가족의 든든한 보험 (무배당)AIA Vitality 베스트핏 보장보험 안내  고객님, 안녕하세요. 4인 가족 표준생계비, 준비하고 계시나요? (무배당)AIA Vitality 베스트핏 보장보험(디지털 전용)으로 최대 20% 보험료 할인과 가족의 든든한 보험 보장까지 누려 보세요.   ▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/36oWjgX  ■ AIA Vitality  혜택 - 매달 리워드 최대 12,000원 - 등급 업그레이드 시 특별 리워드 - T건강습관 제휴 할인 최대 40% ※ 제휴사별 할인 조건과 주간 미션 달성 혜택 등 자세한 내용은 AIA Vitality 사이트에서 확인하세요. ※ 이 광고는 AIA생명의 광고이며 SK텔레콤은 모집 행위를 하지 않습니다.  - 보험료 납입 기간 중 피보험자가 장해분류표 중 동일한 재해 또는 재해 이외의 동일한 원인으로 여러 신체 부위의 장해지급률을 더하여 50% 이상인 장해 상태가 된 경우 차회 이후의 보험료 납입 면제 - 사망보험금은 계약일(부활일/효력회복일)로부터 2년 안에 자살한 경우 보장하지 않음 - 일부 특약 갱신 시 보험료 인상 가능 - 기존 계약 해지 후 신계약 체결 시 보험인수 거절, 보험료 인상, 보장 내용 변경 가능 - 해약 환급금(또는 만기 시 보험금이나 사고보험금)에 기타 지급금을 합해 5천만 원까지(본 보험 회사 모든 상품 합산) 예금자 보호 - 계약 체결 전 상품 설명서 및 약관 참조 - 월 보험료 5,500원(부가세 포함)  * 생명보험협회 심의필 제2020-03026호(2020-09-22) COM-2020-09-32426  ■문의: 청약 관련(1600-0880)  무료 수신거부 1504    \n",
        "    \"\"\"\n",
        "    ]\n",
        "\n",
        "message_idx = 0\n",
        "\n",
        "longer_text = msg_text_list[message_idx]\n",
        "\n",
        "doc = nlp(longer_text)\n",
        "\n",
        "print(\"=== Alternative 1: Extract All Nouns and Proper Nouns ===\")\n",
        "nouns = []\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_space and not token.is_punct:\n",
        "        nouns.append(f\"{token.text} ({token.pos_})\")\n",
        "\n",
        "for noun in nouns:\n",
        "    print(f\"  {noun}\")\n",
        "\n",
        "print(\"\\n=== Alternative 2: Group Adjacent Nouns ===\")\n",
        "def extract_noun_groups(doc):\n",
        "    noun_groups = []\n",
        "    current_group = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            current_group.append(token.text)\n",
        "        else:\n",
        "            if current_group:\n",
        "                noun_groups.append(\" \".join(current_group))\n",
        "                current_group = []\n",
        "    \n",
        "    # Don't forget the last group\n",
        "    if current_group:\n",
        "        noun_groups.append(\" \".join(current_group))\n",
        "    \n",
        "    return noun_groups\n",
        "\n",
        "noun_groups = extract_noun_groups(doc)\n",
        "for group in noun_groups:\n",
        "    print(f\"  {group}\")\n",
        "\n",
        "print(\"\\n=== Alternative 3: Extract Compound Nouns (연속된 명사) ===\")\n",
        "def extract_compound_nouns(doc):\n",
        "    compounds = []\n",
        "    current_compound = []\n",
        "    \n",
        "    for token in doc:\n",
        "        # Include nouns, proper nouns, and some particles that connect nouns\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"] or (token.pos_ == \"ADP\" and token.text in [\"의\", \"에서\"]):\n",
        "            current_compound.append(token.text)\n",
        "        else:\n",
        "            if len(current_compound) > 1:  # Only keep compounds with multiple parts\n",
        "                compounds.append(\"\".join(current_compound))\n",
        "            current_compound = []\n",
        "    \n",
        "    if len(current_compound) > 1:\n",
        "        compounds.append(\"\".join(current_compound))\n",
        "    \n",
        "    return compounds\n",
        "\n",
        "compounds = extract_compound_nouns(doc)\n",
        "for compound in compounds:\n",
        "    print(f\"  {compound}\")\n",
        "\n",
        "print(\"\\n=== Alternative 4: Named Entities (Most Reliable) ===\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"  {ent.text} ({ent.label_})\")\n",
        "\n",
        "print(\"\\n=== Alternative 5: Custom Korean Noun Phrase Pattern ===\")\n",
        "# This is a simple heuristic for Korean noun phrases\n",
        "def korean_noun_phrases(doc):\n",
        "    phrases = []\n",
        "    i = 0\n",
        "    while i < len(doc):\n",
        "        if doc[i].pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            phrase = [doc[i].text]\n",
        "            j = i + 1\n",
        "            \n",
        "            # Look ahead for particles and more nouns\n",
        "            while j < len(doc):\n",
        "                if doc[j].pos_ == \"ADP\" and doc[j].text in [\"의\", \"에서\", \"에게\", \"로\", \"으로\"]:\n",
        "                    phrase.append(doc[j].text)\n",
        "                    j += 1\n",
        "                elif doc[j].pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "                    phrase.append(doc[j].text)\n",
        "                    j += 1\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            if len(phrase) > 1:\n",
        "                phrases.append(\"\".join(phrase))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    \n",
        "    return phrases\n",
        "\n",
        "korean_phrases = korean_noun_phrases(doc)\n",
        "for phrase in korean_phrases:\n",
        "    print(f\"  {phrase}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
