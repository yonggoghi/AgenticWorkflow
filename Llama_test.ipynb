{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "from langgraph.prebuilt import ToolExecutor\n",
        "from langgraph.graph import Graph, StateGraph\n",
        "from operator import itemgetter\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Initialize Llama model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-v0.3\"; model_name_to_load  = \"Mistral\"\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"; model_name_to_load  = \"Llama2\"  # or your preferred Llama 3 checkpoint\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"; model_name_to_load  = \"Llama3\"\n",
        "if \"Llama-3\" in model_name:\n",
        "\tacc_token = \"${HUGGINGFACE_TOKEN}\" # for 1lama3\n",
        "elif \"Llama-2\" in model_name:\n",
        "\tacc_token = \"${HUGGINGFACE_TOKEN}\" # for 11ama2\n",
        "\n",
        "login (token=acc_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    # device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Save the model and tokenizer\n",
        "def save_model(model, tokenizer, output_dir: str):\n",
        "    \"\"\"Save both model and tokenizer to the specified directory\"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Save model\n",
        "        model.save_pretrained(output_dir)\n",
        "        \n",
        "        # Save tokenizer\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        \n",
        "        print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {str(e)}\")\n",
        "\n",
        "# Usage\n",
        "output_directory = \"model/\"+model_name_to_load\n",
        "save_model(model, tokenizer, output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_saved_model(model_path: str):\n",
        "    \"\"\"Load a saved model and tokenizer\"\"\"\n",
        "    try:\n",
        "        # Load model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        \n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        \n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "model, tokenizer = load_saved_model(f\"model/{model_name_to_load}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=\"microsoft/phi-4\",\n",
        "#     model_kwargs={\"torch_dtype\": \"auto\"},\n",
        "#     device_map=\"cpu\",\n",
        "# )\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medieval knight and must provide explanations to modern people.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How should I explain the Internet?\"},\n",
        "]\n",
        "\n",
        "outputs = pipeline(messages, max_new_tokens=30)\n",
        "print(outputs[0][\"generated_text\"][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Optional, Any, Callable, List, Dict\n",
        "from functools import wraps\n",
        "from langgraph.graph import StateGraph, END\n",
        "import json\n",
        "import transformers\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"microsoft/phi-4\",\n",
        "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "\n",
        "def generate_llm_response(prompt: str, max_length: int = 50) -> str:\n",
        "    \"\"\"Generate response using Llama model\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a intellient agent which can help with various tasks.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(messages, max_new_tokens=max_length)\n",
        "    response = (outputs[0][\"generated_text\"])\n",
        "\n",
        "    return response\n",
        "\n",
        "# Tool Definitions\n",
        "class ToolResult(TypedDict):\n",
        "    \"\"\"Type definition for tool results\"\"\"\n",
        "    error: Optional[str]\n",
        "    result: Optional[Any]\n",
        "\n",
        "def tool(func: Callable) -> Callable:\n",
        "    \"\"\"Decorator to mark and process tool functions\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs) -> ToolResult:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            return {'result': None, 'error': f\"{func.__name__} failed: {str(e)}\"}\n",
        "    wrapper.is_tool = True\n",
        "    wrapper.name = func.__name__\n",
        "    wrapper.description = func.__doc__\n",
        "    return wrapper\n",
        "\n",
        "@tool\n",
        "def calculator(input_text: str) -> ToolResult:\n",
        "    \"\"\"Calculate the result of a mathematical expression.\"\"\"\n",
        "    try:\n",
        "        # Clean the input\n",
        "        expression = input_text.strip()\n",
        "        if not expression:\n",
        "            return {'result': None, 'error': \"No expression provided\"}\n",
        "            \n",
        "        # Only allow basic arithmetic operations and numbers\n",
        "        import re\n",
        "        if not re.match(r'^[\\d\\s+\\-*/.()]+$', expression):\n",
        "            return {'result': None, 'error': \"Invalid characters in expression\"}\n",
        "        \n",
        "        # Additional safety checks\n",
        "        if '__' in expression or 'eval' in expression or 'exec' in expression:\n",
        "            return {'result': None, 'error': \"Invalid expression\"}\n",
        "            \n",
        "        # Evaluate the expression\n",
        "        result = eval(expression)\n",
        "        return {'result': str(result), 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Calculation failed: {str(e)}\"}\n",
        "\n",
        "@tool\n",
        "def wiki_search(query: str) -> ToolResult:\n",
        "    \"\"\"Search for information about a topic.\"\"\"\n",
        "    try:\n",
        "        # For demonstration, we'll use the LLM to generate a response about the topic\n",
        "        prompt = f\"Provide a brief, factual summary about: {query}\"\n",
        "        result = generate_llm_response(prompt)\n",
        "        return {'result': result, 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Search failed: {str(e)}\"}\n",
        "\n",
        "# Define available tools\n",
        "tools = {\n",
        "    \"calculator\": calculator,\n",
        "    \"wiki_search\": wiki_search\n",
        "}\n",
        "\n",
        "# State and Message Types\n",
        "class MessageBase(TypedDict):\n",
        "    content: str\n",
        "    type: str\n",
        "\n",
        "class HumanMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class AIMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class ToolMessage(MessageBase):\n",
        "    tool: str\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: List[MessageBase]\n",
        "    next_step: str\n",
        "\n",
        "def observe(state: dict) -> dict:\n",
        "    \"\"\"Process current state and determine next steps using LLM.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Look at the last message\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    # Use LLM to determine the next step\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    prompt = f\"\"\"Given the user message: '{content}'\n",
        "    Determine which tool to use:\n",
        "    - 'calculator' for mathematical calculations\n",
        "    - 'wiki_search' for information queries\n",
        "    - '' if no tool is needed\n",
        "    Reply with just the tool name.\"\"\"\n",
        "    \n",
        "    next_step = generate_llm_response(prompt).strip().lower()\n",
        "    \n",
        "    # Validate the response\n",
        "    if next_step not in tools and next_step != \"\":\n",
        "        next_step = \"\"\n",
        "        \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def think(state: dict) -> dict:\n",
        "    \"\"\"Plan the next action based on the current state using LLM.\"\"\"\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step:\n",
        "        return state\n",
        "\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Get the last message content\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    \n",
        "    # Use LLM to generate a response about the planned action\n",
        "    prompt = f\"\"\"Given the user message: '{content}'\n",
        "    And the selected tool: '{next_step}'\n",
        "    Generate a brief, natural response about how you'll help with this task.\"\"\"\n",
        "    \n",
        "    ai_response = generate_llm_response(prompt)\n",
        "    \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def act(state: dict) -> dict:\n",
        "    \"\"\"Execute the planned action and return the next steps.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step or next_step not in tools:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    # Get the original request from human message\n",
        "    human_messages = [msg for msg in messages if msg.get(\"type\") == \"human\"]\n",
        "    if not human_messages:\n",
        "        return {\"messages\": messages}\n",
        "    \n",
        "    content = human_messages[-1].get(\"content\", \"\")\n",
        "    \n",
        "    # Extract the actual expression for calculator\n",
        "    if next_step == \"calculator\":\n",
        "        # Use LLM to extract the mathematical expression\n",
        "        prompt = f\"\"\"Extract only the mathematical expression from: '{content}'\n",
        "        Return only the numbers and operators (+,-,*,/,()).\"\"\"\n",
        "        \n",
        "        expression = generate_llm_response(prompt).strip()\n",
        "        content = expression\n",
        "    \n",
        "    # Execute the tool\n",
        "    result = tools[next_step](content)\n",
        "    \n",
        "    # Add the tool result to messages\n",
        "    messages.append({\n",
        "        \"content\": json.dumps(result),\n",
        "        \"type\": \"tool\",\n",
        "        \"tool\": next_step\n",
        "    })\n",
        "    \n",
        "    # Use LLM to generate a natural response about the result\n",
        "    prompt = f\"\"\"Given the tool result: {json.dumps(result)}\n",
        "    Generate a natural, helpful response explaining the result.\"\"\"\n",
        "    \n",
        "    ai_response = generate_llm_response(prompt)\n",
        "    \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": \"\"  # Clear the next step since we've handled it\n",
        "    }\n",
        "\n",
        "def should_continue(state: dict) -> bool:\n",
        "    \"\"\"Determine if the workflow should continue.\"\"\"\n",
        "    return bool(state.get(\"next_step\")) or any(\n",
        "        msg.get(\"type\") == \"human\" \n",
        "        for msg in state.get(\"messages\", [])[-1:]\n",
        "    )\n",
        "\n",
        "# Create the workflow\n",
        "def build_workflow():\n",
        "    workflow = StateGraph(State)\n",
        "    \n",
        "    workflow.add_node(\"observe\", observe)\n",
        "    workflow.add_node(\"think\", think)\n",
        "    workflow.add_node(\"act\", act)\n",
        "    \n",
        "    workflow.add_edge(\"observe\", \"think\")\n",
        "    workflow.add_edge(\"think\", \"act\")\n",
        "    \n",
        "    workflow.set_entry_point(\"observe\")\n",
        "    \n",
        "    workflow.add_conditional_edges(\n",
        "        \"act\",\n",
        "        should_continue,\n",
        "        {\n",
        "            True: \"observe\",\n",
        "            False: END\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Create the agent\n",
        "agent = build_workflow()\n",
        "\n",
        "# Example usage\n",
        "initial_state = {\n",
        "    \"messages\": [{\n",
        "        \"content\": \"What is the square root of 16?\",\n",
        "        \"type\": \"human\"\n",
        "    }],\n",
        "    \"next_step\": \"\"\n",
        "}\n",
        "\n",
        "# Execute the agent\n",
        "result = agent.invoke(initial_state)\n",
        "print(\"\\nFinal state:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    print(f\"\\n{msg['type'].upper()}: {msg['content']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Optional, Any, Callable, List, Dict\n",
        "from functools import wraps\n",
        "from langgraph.graph import StateGraph, END\n",
        "import json\n",
        "\n",
        "# Tool Definitions\n",
        "class ToolResult(TypedDict):\n",
        "    \"\"\"Type definition for tool results\"\"\"\n",
        "    error: Optional[str]\n",
        "    result: Optional[Any]\n",
        "\n",
        "def tool(func: Callable) -> Callable:\n",
        "    \"\"\"Decorator to mark and process tool functions\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs) -> ToolResult:\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            return {'result': None, 'error': f\"{func.__name__} failed: {str(e)}\"}\n",
        "    wrapper.is_tool = True\n",
        "    wrapper.name = func.__name__\n",
        "    wrapper.description = func.__doc__\n",
        "    return wrapper\n",
        "\n",
        "@tool\n",
        "def calculator(input_text: str) -> ToolResult:\n",
        "    \"\"\"Calculate the result of a mathematical expression.\"\"\"\n",
        "    try:\n",
        "        # Clean the input\n",
        "        expression = input_text.strip()\n",
        "        if not expression:\n",
        "            return {'result': None, 'error': \"No expression provided\"}\n",
        "            \n",
        "        # Only allow basic arithmetic operations and numbers\n",
        "        import re\n",
        "        if not re.match(r'^[\\d\\s+\\-*/.()]+$', expression):\n",
        "            return {'result': None, 'error': \"Invalid characters in expression\"}\n",
        "        \n",
        "        # Additional safety checks\n",
        "        if '__' in expression or 'eval' in expression or 'exec' in expression:\n",
        "            return {'result': None, 'error': \"Invalid expression\"}\n",
        "            \n",
        "        # Evaluate the expression\n",
        "        result = eval(expression)\n",
        "        return {'result': str(result), 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Calculation failed: {str(e)}\"}\n",
        "\n",
        "@tool\n",
        "def wiki_search(query: str) -> ToolResult:\n",
        "    \"\"\"Search for information about a topic.\"\"\"\n",
        "    try:\n",
        "        # Simulated search result\n",
        "        result = f\"Found information about: {query}\"\n",
        "        return {'result': result, 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Search failed: {str(e)}\"}\n",
        "\n",
        "# Define available tools\n",
        "tools = {\n",
        "    \"calculator\": calculator,\n",
        "    \"wiki_search\": wiki_search\n",
        "}\n",
        "\n",
        "# State and Message Types\n",
        "class MessageBase(TypedDict):\n",
        "    content: str\n",
        "    type: str\n",
        "\n",
        "class HumanMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class AIMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class ToolMessage(MessageBase):\n",
        "    tool: str\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: List[MessageBase]\n",
        "    next_step: str\n",
        "\n",
        "def observe(state: dict) -> dict:\n",
        "    \"\"\"Process current state and determine next steps.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Look at the last message\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    # Determine next step based on message content\n",
        "    content = last_message.get(\"content\", \"\").lower()\n",
        "    next_step = \"\"\n",
        "    \n",
        "    if \"calculate\" in content:\n",
        "        next_step = \"calculator\"\n",
        "    elif \"search\" in content or \"find\" in content:\n",
        "        next_step = \"wiki_search\"\n",
        "        \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def think(state: dict) -> dict:\n",
        "    \"\"\"Plan the next action based on the current state.\"\"\"\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step:\n",
        "        return state\n",
        "\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Get the last message content\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    \n",
        "    # Add AI message indicating the planned action\n",
        "    messages.append({\n",
        "        \"content\": f\"I'll help you with the {next_step} task.\",\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def act(state: dict) -> dict:\n",
        "    \"\"\"Execute the planned action and return the next steps.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step or next_step not in tools:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    # Get the original request from human message\n",
        "    human_messages = [msg for msg in messages if msg.get(\"type\") == \"human\"]\n",
        "    if not human_messages:\n",
        "        return {\"messages\": messages}\n",
        "    \n",
        "    content = human_messages[-1].get(\"content\", \"\")\n",
        "    \n",
        "    # Extract the actual expression for calculator\n",
        "    if next_step == \"calculator\":\n",
        "        # Extract everything after \"calculate\" or just numbers and operators\n",
        "        import re\n",
        "        if \"calculate\" in content.lower():\n",
        "            expression = content.lower().split(\"calculate\")[-1].strip()\n",
        "        else:\n",
        "            # Remove everything except numbers, operators, and whitespace\n",
        "            expression = ' '.join(re.findall(r'[\\d+\\-*/() ]+', content))\n",
        "        content = expression.strip()\n",
        "    \n",
        "    # Execute the tool\n",
        "    result = tools[next_step](content)\n",
        "    \n",
        "    # Add the tool result to messages\n",
        "    messages.append({\n",
        "        \"content\": json.dumps(result),\n",
        "        \"type\": \"tool\",\n",
        "        \"tool\": next_step\n",
        "    })\n",
        "    \n",
        "    # Add an AI message interpreting the result\n",
        "    if result.get(\"error\") is None:\n",
        "        ai_response = f\"The {next_step} returned: {result.get('result')}\"\n",
        "    else:\n",
        "        ai_response = f\"Sorry, there was an error: {result.get('error')}\"\n",
        "        \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": \"\"  # Clear the next step since we've handled it\n",
        "    }\n",
        "\n",
        "def should_continue(state: dict) -> bool:\n",
        "    \"\"\"Determine if the workflow should continue.\"\"\"\n",
        "    return bool(state.get(\"next_step\")) or any(\n",
        "        msg.get(\"type\") == \"human\" \n",
        "        for msg in state.get(\"messages\", [])[-1:]\n",
        "    )\n",
        "\n",
        "# Create the workflow\n",
        "def build_workflow():\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(State)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"observe\", observe)\n",
        "    workflow.add_node(\"think\", think)\n",
        "    workflow.add_node(\"act\", act)\n",
        "    \n",
        "    # Define edges\n",
        "    workflow.add_edge(\"observe\", \"think\")\n",
        "    workflow.add_edge(\"think\", \"act\")\n",
        "    \n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"observe\")\n",
        "    \n",
        "    # Add conditional edges for continuation or completion\n",
        "    workflow.add_conditional_edges(\n",
        "        \"act\",\n",
        "        should_continue,\n",
        "        {\n",
        "            True: \"observe\",  # Continue to observe if there's more to do\n",
        "            False: END   # End the workflow if we're done\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "@tool\n",
        "def wiki_search(query: str) -> ToolResult:\n",
        "    \"\"\"Search for information about a topic.\"\"\"\n",
        "    try:\n",
        "        # Simulated search result\n",
        "        result = f\"Found information about: {query}\"\n",
        "        return {'result': result, 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Search failed: {str(e)}\"}\n",
        "\n",
        "# Define available tools\n",
        "tools = {\n",
        "    \"calculator\": calculator,\n",
        "    \"wiki_search\": wiki_search\n",
        "}\n",
        "\n",
        "# State and Message Types\n",
        "class MessageBase(TypedDict):\n",
        "    content: str\n",
        "    type: str\n",
        "\n",
        "class HumanMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class AIMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class ToolMessage(MessageBase):\n",
        "    tool: str\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: List[MessageBase]\n",
        "    next_step: str\n",
        "\n",
        "def observe(state: dict) -> dict:\n",
        "    \"\"\"Process current state and determine next steps.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Look at the last message\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    # Determine next step based on message content\n",
        "    content = last_message.get(\"content\", \"\").lower()\n",
        "    next_step = \"\"\n",
        "    \n",
        "    if \"calculate\" in content:\n",
        "        next_step = \"calculator\"\n",
        "    elif \"search\" in content or \"find\" in content:\n",
        "        next_step = \"wiki_search\"\n",
        "        \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def think(state: dict) -> dict:\n",
        "    \"\"\"Plan the next action based on the current state.\"\"\"\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step:\n",
        "        return state\n",
        "\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "\n",
        "    # Get the last message content\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    \n",
        "    # Add AI message indicating the planned action\n",
        "    messages.append({\n",
        "        \"content\": f\"I'll help you with the {next_step} task.\",\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step\n",
        "    }\n",
        "\n",
        "def act(state: dict) -> dict:\n",
        "    \"\"\"Execute the planned action and return the next steps.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    if not next_step or next_step not in tools:\n",
        "        return {\"messages\": messages}\n",
        "\n",
        "    # Get the original request from human message\n",
        "    human_messages = [msg for msg in messages if msg.get(\"type\") == \"human\"]\n",
        "    if not human_messages:\n",
        "        return {\"messages\": messages}\n",
        "    \n",
        "    content = human_messages[-1].get(\"content\", \"\")\n",
        "    \n",
        "    # Extract the actual expression for calculator\n",
        "    if next_step == \"calculator\":\n",
        "        # Extract everything after \"calculate\" or just numbers and operators\n",
        "        import re\n",
        "        if \"calculate\" in content.lower():\n",
        "            expression = content.lower().split(\"calculate\")[-1].strip()\n",
        "        else:\n",
        "            expression = re.sub(r'[^0-9+\\-*/\\s()]', '', content)\n",
        "        content = expression.strip()\n",
        "    \n",
        "    # Execute the tool\n",
        "    result = tools[next_step](content)\n",
        "    \n",
        "    # Add the tool result to messages\n",
        "    messages.append({\n",
        "        \"content\": json.dumps(result),\n",
        "        \"type\": \"tool\",\n",
        "        \"tool\": next_step\n",
        "    })\n",
        "    \n",
        "    # Add an AI message interpreting the result\n",
        "    if result.get(\"error\") is None:\n",
        "        ai_response = f\"The {next_step} returned: {result.get('result')}\"\n",
        "    else:\n",
        "        ai_response = f\"Sorry, there was an error: {result.get('error')}\"\n",
        "        \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\"\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": \"\"  # Clear the next step since we've handled it\n",
        "    }\n",
        "\n",
        "def should_continue(state: dict) -> bool:\n",
        "    \"\"\"Determine if the workflow should continue.\"\"\"\n",
        "    return bool(state.get(\"next_step\")) or any(\n",
        "        msg.get(\"type\") == \"human\" \n",
        "        for msg in state.get(\"messages\", [])[-1:]\n",
        "    )\n",
        "\n",
        "# Create the workflow\n",
        "def build_workflow():\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(State)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"observe\", observe)\n",
        "    workflow.add_node(\"think\", think)\n",
        "    workflow.add_node(\"act\", act)\n",
        "    \n",
        "    # Define edges\n",
        "    workflow.add_edge(\"observe\", \"think\")\n",
        "    workflow.add_edge(\"think\", \"act\")\n",
        "    \n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"observe\")\n",
        "    \n",
        "    # Add conditional edges for continuation or completion\n",
        "    workflow.add_conditional_edges(\n",
        "        \"act\",\n",
        "        should_continue,\n",
        "        {\n",
        "            True: \"observe\",  # Continue to observe if there's more to do\n",
        "            False: END   # End the workflow if we're done\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Create the agent\n",
        "agent = build_workflow()\n",
        "\n",
        "# Example usage\n",
        "initial_state = {\n",
        "    \"messages\": [{\n",
        "        \"content\": \"Please calculate 2 + 2\",\n",
        "        \"type\": \"human\"\n",
        "    }],\n",
        "    \"next_step\": \"\"\n",
        "}\n",
        "\n",
        "# Execute the agent\n",
        "result = agent.invoke(initial_state)\n",
        "print(\"\\nFinal state:\")\n",
        "for msg in result[\"messages\"]:\n",
        "    print(f\"\\n{msg['type'].upper()}: {msg['content']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/var/folders/0h/3c1x50w901n85pq5kbbn9mn020dh7m/T/ipykernel_656/483232748.py\", line 6, in <module>\n",
            "    import transformers\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-24 18:19:30.774 DEBUG [483232748.py:52] Logging system initialized at DEBUG level\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ab5abd8162a456eaf71f576bb272a69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "BFloat16 is not supported on MPS",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 491\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[1;32m    490\u001b[0m model_config \u001b[38;5;241m=\u001b[39m ModelConfig()\n\u001b[0;32m--> 491\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m initialize_model(model_config)\n\u001b[1;32m    492\u001b[0m agent \u001b[38;5;241m=\u001b[39m build_workflow()\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_message\u001b[39m(message: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n",
            "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_model\u001b[39m(config: ModelConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m transformers\u001b[38;5;241m.\u001b[39mPipeline:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         model\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     90\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: config\u001b[38;5;241m.\u001b[39mtorch_dtype}\n\u001b[1;32m     91\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_class(model\u001b[38;5;241m=\u001b[39mmodel, framework\u001b[38;5;241m=\u001b[39mframework, task\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:97\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     99\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/pipelines/base.py:982\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ${HUGGINGFACE_TOKEN} \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m ):\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# If the model can generate:\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massistant_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massistant_tokenizer \u001b[38;5;241m=\u001b[39m load_assistant_model(\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    990\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/transformers/modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3109\u001b[0m         )\n\u001b[0;32m-> 3110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
            "\u001b[0;31mTypeError\u001b[0m: BFloat16 is not supported on MPS"
          ]
        }
      ],
      "source": [
        "from typing import TypedDict, Annotated, Optional, Any, Callable, List, Dict, Union\n",
        "from functools import wraps, lru_cache\n",
        "import time\n",
        "from langgraph.graph import StateGraph, END\n",
        "import json\n",
        "import transformers\n",
        "import ast\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Set up logging with detailed formatting for both file and console\n",
        "import sys\n",
        "from logging.handlers import RotatingFileHandler\n",
        "\n",
        "# First, configure the root logger\n",
        "logging.getLogger().setLevel(logging.DEBUG)\n",
        "\n",
        "# Create formatters\n",
        "detailed_formatter = logging.Formatter(\n",
        "    '%(asctime)s.%(msecs)03d %(levelname)s [%(filename)s:%(lineno)d] %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Set up file handler\n",
        "file_handler = RotatingFileHandler('agent_debug.log', maxBytes=10000000, backupCount=5)\n",
        "file_handler.setLevel(logging.DEBUG)\n",
        "file_handler.setFormatter(detailed_formatter)\n",
        "\n",
        "# Set up console handler\n",
        "console_handler = logging.StreamHandler(sys.stdout)\n",
        "console_handler.setLevel(logging.DEBUG)  # Explicitly set handler level to DEBUG\n",
        "console_handler.setFormatter(detailed_formatter)\n",
        "\n",
        "# Configure logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Explicitly set logger level to DEBUG\n",
        "\n",
        "# Remove any existing handlers\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "# Add our handlers\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Prevent logging from propagating to the root logger\n",
        "logger.propagate = False\n",
        "\n",
        "# Verify logging level\n",
        "logger.debug(\"Logging system initialized at DEBUG level\")\n",
        "\n",
        "# Prompt Templates\n",
        "PROMPTS = {\n",
        "    \"TOOL_SELECTION\": \"\"\"\n",
        "Given the user message: '{message}'\n",
        "Select the most appropriate tool:\n",
        "- calculator: For mathematical calculations\n",
        "- wiki_search: For information queries\n",
        "- none: If no tool is needed\n",
        "\n",
        "Respond with the tool name only.\n",
        "\"\"\",\n",
        "    \"ACTION_PLANNING\": \"\"\"\n",
        "Given the user message: '{message}'\n",
        "And the selected tool: '{tool}'\n",
        "Generate a brief, natural response about how you'll help with this task.\n",
        "\"\"\",\n",
        "    \"RESULT_EXPLANATION\": \"\"\"\n",
        "Given the tool result: {result}\n",
        "Generate a natural, helpful response explaining the result.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# Model Configuration\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_name: str = \"microsoft/phi-4\"\n",
        "    max_new_tokens: int = 50\n",
        "    temperature: float = 0.7\n",
        "    device: str = \"cpu\"\n",
        "    torch_dtype: str = \"auto\"\n",
        "\n",
        "# Initialize model with configuration\n",
        "def initialize_model(config: ModelConfig) -> transformers.Pipeline:\n",
        "    return transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=config.model_name,\n",
        "        model_kwargs={\"torch_dtype\": config.torch_dtype}\n",
        "    )\n",
        "\n",
        "# Cache LLM responses to avoid redundant computations\n",
        "@lru_cache(maxsize=1000)\n",
        "def generate_llm_response(prompt: str, max_length: int = 50) -> str:\n",
        "    \"\"\"Generate response using language model with caching\"\"\"\n",
        "    try:\n",
        "        response = pipeline(prompt, max_new_tokens=max_length)\n",
        "        if isinstance(response, list):\n",
        "            return response[0][\"generated_text\"].strip()\n",
        "        elif isinstance(response, dict):\n",
        "            return response[\"generated_text\"].strip()\n",
        "        elif isinstance(response, str):\n",
        "            return response.strip()\n",
        "        else:\n",
        "            logger.error(f\"Unexpected response type from LLM: {type(response)}\")\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating LLM response: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Tool Definitions\n",
        "class ToolResult(TypedDict):\n",
        "    \"\"\"Type definition for tool results\"\"\"\n",
        "    error: Optional[str]\n",
        "    result: Optional[Any]\n",
        "\n",
        "def tool(func: Callable) -> Callable:\n",
        "    \"\"\"Enhanced decorator to mark and process tool functions\"\"\"\n",
        "    @wraps(func)\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "    def wrapper(*args, **kwargs) -> ToolResult:\n",
        "        try:\n",
        "            logger.info(f\"Executing tool: {func.__name__}\")\n",
        "            return func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Tool execution failed: {func.__name__}, error: {e}\")\n",
        "            return {'result': None, 'error': f\"{func.__name__} failed: {str(e)}\"}\n",
        "    wrapper.is_tool = True\n",
        "    wrapper.name = func.__name__\n",
        "    wrapper.description = func.__doc__\n",
        "    return wrapper\n",
        "\n",
        "def safe_eval(expression: str) -> Optional[float]:\n",
        "    \"\"\"Safely evaluate mathematical expressions\"\"\"\n",
        "    try:\n",
        "        # Remove any whitespace and validate characters\n",
        "        expression = ''.join(expression.split())\n",
        "        if not re.match(r'^[\\d+\\-*/.()]+$', expression):\n",
        "            return None\n",
        "            \n",
        "        # Parse and validate AST\n",
        "        tree = ast.parse(expression, mode='eval')\n",
        "        \n",
        "        # Only allow basic arithmetic operations\n",
        "        allowed_nodes = (ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num,\n",
        "                        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow)\n",
        "                        \n",
        "        for node in ast.walk(tree):\n",
        "            if not isinstance(node, allowed_nodes):\n",
        "                return None\n",
        "                \n",
        "        return eval(compile(tree, '<string>', 'eval'))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Safe eval failed: {e}\")\n",
        "        return None\n",
        "\n",
        "@tool\n",
        "def calculator(input_text: str) -> ToolResult:\n",
        "    \"\"\"Calculate the result of a mathematical expression.\"\"\"\n",
        "    try:\n",
        "        # Clean and validate the input\n",
        "        expression = input_text.strip()\n",
        "        if not expression:\n",
        "            return {'result': None, 'error': \"No expression provided\"}\n",
        "            \n",
        "        # Evaluate safely\n",
        "        result = safe_eval(expression)\n",
        "        if result is None:\n",
        "            return {'result': None, 'error': \"Invalid expression\"}\n",
        "            \n",
        "        return {'result': str(result), 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Calculation failed: {str(e)}\"}\n",
        "\n",
        "@tool\n",
        "def wiki_search(query: str) -> ToolResult:\n",
        "    \"\"\"Search for information about a topic.\"\"\"\n",
        "    try:\n",
        "        # For demonstration, using LLM. In production, integrate with actual Wikipedia API\n",
        "        prompt = f\"Provide a brief, factual summary about: {query}\"\n",
        "        result = generate_llm_response(prompt)\n",
        "        return {'result': result, 'error': None}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'error': f\"Search failed: {str(e)}\"}\n",
        "\n",
        "# Define available tools\n",
        "tools = {\n",
        "    \"calculator\": calculator,\n",
        "    \"wiki_search\": wiki_search\n",
        "}\n",
        "\n",
        "# Enhanced State and Message Types\n",
        "class MessageBase(TypedDict):\n",
        "    content: str\n",
        "    type: str\n",
        "    timestamp: float  # Add timestamp for message ordering\n",
        "\n",
        "class HumanMessage(MessageBase):\n",
        "    pass\n",
        "\n",
        "class AIMessage(MessageBase):\n",
        "    confidence: float  # Add confidence score\n",
        "\n",
        "class ToolMessage(MessageBase):\n",
        "    tool: str\n",
        "    attempts: int  # Track retry attempts\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: List[MessageBase]\n",
        "    next_step: str\n",
        "    context: Dict[str, Any]  # Add context for maintaining state\n",
        "\n",
        "def parse_tool_selection(llm_response: str) -> Optional[str]:\n",
        "    \"\"\"Parse and validate tool selection from LLM response\"\"\"\n",
        "    valid_tools = set(tools.keys())\n",
        "    tool_name = llm_response.strip().lower()\n",
        "    return tool_name if tool_name in valid_tools else None\n",
        "\n",
        "def observe(state: State) -> State:\n",
        "    \"\"\"Enhanced observation phase\"\"\"\n",
        "    print(\"\\n=== OBSERVE PHASE ===\")\n",
        "    print(f\"Messages count: {len(state.get('messages', []))}\")\n",
        "    print(f\"Current next_step: {state.get('next_step', 'None')}\")\n",
        "    logger.debug(\"=== OBSERVE PHASE ===\")\n",
        "    logger.debug(f\"Current state: {json.dumps(state, default=str)}\")\n",
        "    \n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        logger.debug(\"No messages in state, returning unchanged\")\n",
        "        return state\n",
        "\n",
        "    # Get the last message\n",
        "    last_message = messages[-1]\n",
        "    content = last_message.get(\"content\", \"\").lower()\n",
        "    \n",
        "    # Simple pattern matching for mathematical operations\n",
        "    math_patterns = [\n",
        "        \"calculate\", \"computation\", \"sum\", \"add\", \"subtract\", \n",
        "        \"multiply\", \"divide\", \"square root\", \"sqrt\", \"root\"\n",
        "    ]\n",
        "    \n",
        "    # Determine the tool based on message content\n",
        "    next_step = \"\"\n",
        "    if any(pattern in content.lower() for pattern in math_patterns):\n",
        "        next_step = \"calculator\"\n",
        "        logger.debug(f\"Detected mathematical operation, setting next_step to: {next_step}\")\n",
        "    elif \"what\" in content and \"is\" in content:\n",
        "        next_step = \"wiki_search\"\n",
        "        logger.debug(f\"Detected information query, setting next_step to: {next_step}\")\n",
        "        \n",
        "    print(f\"Selected next_step: {next_step}\")\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step,\n",
        "        \"context\": state.get(\"context\", {})\n",
        "    }\n",
        "\n",
        "    last_message = messages[-1]\n",
        "    if not isinstance(last_message, dict):\n",
        "        return state\n",
        "\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    prompt = PROMPTS[\"TOOL_SELECTION\"].format(message=content)\n",
        "    \n",
        "    next_step = parse_tool_selection(generate_llm_response(prompt))\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step or \"\",\n",
        "        \"context\": state.get(\"context\", {})\n",
        "    }\n",
        "\n",
        "def think(state: State) -> State:\n",
        "    \"\"\"Enhanced thinking phase\"\"\"\n",
        "    print(\"\\n=== THINK PHASE ===\")\n",
        "    print(f\"Next step to consider: {state.get('next_step', 'None')}\")\n",
        "    print(f\"Messages count: {len(state.get('messages', []))}\")\n",
        "    \n",
        "    logger.debug(\"=== THINK PHASE ===\")\n",
        "    logger.debug(f\"Incoming state: {json.dumps(state, default=str)}\")\n",
        "    \n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    messages = state.get(\"messages\", [])\n",
        "    context = state.get(\"context\", {})\n",
        "    \n",
        "    # If there's no next step, just return the state\n",
        "    if not next_step or not messages:\n",
        "        print(\"No next step or messages, skipping think phase\")\n",
        "        return state\n",
        "    \n",
        "    try:\n",
        "        # Get the last message content\n",
        "        last_message = messages[-1]\n",
        "        content = last_message.get(\"content\", \"\")\n",
        "        \n",
        "        # For calculator, we don't need LLM response\n",
        "        if next_step == \"calculator\":\n",
        "            ai_response = \"I'll help you calculate that.\"\n",
        "        else:\n",
        "            # Use LLM to generate a response about the planned action\n",
        "            prompt = f\"\"\"Given the user message: '{content}'\n",
        "            And the selected tool: '{next_step}'\n",
        "            Generate a brief, natural response about how you'll help with this task.\"\"\"\n",
        "            \n",
        "            ai_response = generate_llm_response(prompt)\n",
        "        \n",
        "        print(f\"Generated AI response: {ai_response}\")\n",
        "        \n",
        "        # Add the AI response to messages\n",
        "        messages.append({\n",
        "            \"content\": ai_response,\n",
        "            \"type\": \"ai\",\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "        \n",
        "        return {\n",
        "            \"messages\": messages,\n",
        "            \"next_step\": next_step,\n",
        "            \"context\": context\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in think phase: {e}\")\n",
        "        # Return original state on error\n",
        "        return state\n",
        "\n",
        "    if not next_step or not messages:\n",
        "        return state\n",
        "\n",
        "    last_message = messages[-1]\n",
        "    content = last_message.get(\"content\", \"\")\n",
        "    \n",
        "    prompt = PROMPTS[\"ACTION_PLANNING\"].format(\n",
        "        message=content,\n",
        "        tool=next_step\n",
        "    )\n",
        "    \n",
        "    ai_response = generate_llm_response(prompt)\n",
        "    \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\",\n",
        "        \"timestamp\": time.time(),\n",
        "        \"confidence\": 0.8  # Example confidence score\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": next_step,\n",
        "        \"context\": context\n",
        "    }\n",
        "\n",
        "def act(state: State) -> State:\n",
        "    \"\"\"Enhanced action phase\"\"\"\n",
        "    print(\"\\n=== ACT PHASE ===\")\n",
        "    print(f\"Executing step: {state.get('next_step', 'None')}\")\n",
        "    print(f\"Messages count: {len(state.get('messages', []))}\")\n",
        "    print(f\"Last message type: {state.get('messages', [{}])[-1].get('type', 'None') if state.get('messages') else 'None'}\")\n",
        "    logger.debug(\"=== ACT PHASE ===\")\n",
        "    logger.debug(f\"Incoming state: {json.dumps(state, default=str)}\")\n",
        "    \n",
        "    messages = state.get(\"messages\", [])\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    context = state.get(\"context\", {})\n",
        "    \n",
        "    logger.debug(f\"Executing step: {next_step}\")\n",
        "    logger.debug(f\"Message count: {len(messages)}\")\n",
        "    logger.debug(f\"Context: {context}\")\n",
        "\n",
        "    if not next_step or next_step not in tools:\n",
        "        return {\"messages\": messages, \"next_step\": \"\", \"context\": context}\n",
        "\n",
        "    human_messages = [msg for msg in messages if msg.get(\"type\") == \"human\"]\n",
        "    if not human_messages:\n",
        "        return {\"messages\": messages, \"next_step\": \"\", \"context\": context}\n",
        "    \n",
        "    content = human_messages[-1].get(\"content\", \"\")\n",
        "    \n",
        "    # Extract expression for calculator\n",
        "    if next_step == \"calculator\":\n",
        "        expression = re.findall(r'[\\d+\\-*/().]+', content)\n",
        "        content = expression[0] if expression else content\n",
        "    \n",
        "    # Execute tool with retry mechanism\n",
        "    result = execute_tool(next_step, content)\n",
        "    \n",
        "    messages.append({\n",
        "        \"content\": json.dumps(result),\n",
        "        \"type\": \"tool\",\n",
        "        \"tool\": next_step,\n",
        "        \"timestamp\": time.time(),\n",
        "        \"attempts\": context.get(\"retry_count\", 1)\n",
        "    })\n",
        "    \n",
        "    # Generate response about result\n",
        "    prompt = PROMPTS[\"RESULT_EXPLANATION\"].format(result=json.dumps(result))\n",
        "    ai_response = generate_llm_response(prompt)\n",
        "    \n",
        "    messages.append({\n",
        "        \"content\": ai_response,\n",
        "        \"type\": \"ai\",\n",
        "        \"timestamp\": time.time(),\n",
        "        \"confidence\": 0.9\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        \"messages\": messages,\n",
        "        \"next_step\": \"\",\n",
        "        \"context\": context\n",
        "    }\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def execute_tool(tool_name: str, content: str) -> ToolResult:\n",
        "    \"\"\"Execute tool with retry mechanism\"\"\"\n",
        "    tool = tools.get(tool_name)\n",
        "    if not tool:\n",
        "        return {\"error\": \"Tool not found\", \"result\": None}\n",
        "    return tool(content)\n",
        "\n",
        "def should_continue(state: State) -> bool:\n",
        "    \"\"\"Enhanced continuation check with recursion protection\"\"\"\n",
        "    print(\"\\n=== CONTINUE CHECK ===\")\n",
        "    print(f\"Recursion count: {state.get('context', {}).get('recursion_count', 0)}\")\n",
        "    print(f\"Has next_step: {bool(state.get('next_step'))}\")\n",
        "    print(f\"Messages count: {len(state.get('messages', []))}\")\n",
        "    logger.debug(\"=== CONTINUE CHECK ===\")\n",
        "    logger.debug(f\"Checking state: {json.dumps(state, default=str)}\")\n",
        "    messages = state.get(\"messages\", [])\n",
        "    next_step = state.get(\"next_step\", \"\")\n",
        "    context = state.get(\"context\", {})\n",
        "    \n",
        "    # Get the recursion count from context\n",
        "    recursion_count = context.get(\"recursion_count\", 0)\n",
        "    \n",
        "    # Update recursion count\n",
        "    context[\"recursion_count\"] = recursion_count + 1\n",
        "    \n",
        "    # Check if we've hit the recursion limit (20 is a safe number)\n",
        "    if recursion_count >= 20:\n",
        "        logger.warning(\"Recursion limit reached, forcing stop\")\n",
        "        return False\n",
        "    \n",
        "    # Check for unfinished business\n",
        "    has_pending_action = bool(next_step)\n",
        "    has_recent_human_message = any(\n",
        "        msg.get(\"type\") == \"human\" \n",
        "        for msg in messages[-1:]\n",
        "    )\n",
        "    \n",
        "    # Only consider recent errors to prevent infinite loops\n",
        "    has_errors = any(\n",
        "        msg.get(\"type\") == \"tool\" and msg.get(\"error\")\n",
        "        for msg in messages[-2:]  # Only look at the last 2 messages\n",
        "    )\n",
        "    \n",
        "    # Don't continue if we have errors but no pending action\n",
        "    if has_errors and not (has_pending_action or has_recent_human_message):\n",
        "        return False\n",
        "    \n",
        "    return has_pending_action or has_recent_human_message\n",
        "\n",
        "def build_workflow() -> Callable:\n",
        "    \"\"\"Build the enhanced workflow\"\"\"\n",
        "    workflow = StateGraph(State)  # Initialize without recursion_limit\n",
        "    \n",
        "    workflow.add_node(\"observe\", observe)\n",
        "    workflow.add_node(\"think\", think)\n",
        "    workflow.add_node(\"act\", act)\n",
        "    \n",
        "    # Add edges\n",
        "    workflow.add_edge(\"observe\", \"think\")\n",
        "    workflow.add_edge(\"think\", \"act\")\n",
        "    \n",
        "    workflow.set_entry_point(\"observe\")\n",
        "    \n",
        "    workflow.add_conditional_edges(\n",
        "        \"act\",\n",
        "        should_continue,\n",
        "        {\n",
        "            True: \"observe\",\n",
        "            False: END\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Initialize the system\n",
        "model_config = ModelConfig()\n",
        "pipeline = initialize_model(model_config)\n",
        "agent = build_workflow()\n",
        "\n",
        "def process_message(message: str) -> Dict[str, Any]:\n",
        "    \"\"\"Process a single message through the agent\"\"\"\n",
        "    print(\"\\n=== STARTING NEW MESSAGE PROCESSING ===\")\n",
        "    print(f\"Input message: {message}\")\n",
        "    initial_state = {\n",
        "        \"messages\": [{\n",
        "            \"content\": message,\n",
        "            \"type\": \"human\",\n",
        "            \"timestamp\": time.time()\n",
        "        }],\n",
        "        \"next_step\": \"\",\n",
        "        \"context\": {}\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        result = agent.invoke(initial_state)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing message: {e}\")\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"content\": \"I encountered an error processing your request. Please try again.\",\n",
        "                    \"type\": \"ai\",\n",
        "                    \"timestamp\": time.time(),\n",
        "                    \"confidence\": 0.0\n",
        "                }\n",
        "            ],\n",
        "            \"next_step\": \"\",\n",
        "            \"context\": {\"error\": str(e)}\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    test_message = \"What is the square root of 16?\"\n",
        "    result = process_message(test_message)\n",
        "    \n",
        "    print(\"\\nConversation flow:\")\n",
        "    for msg in result[\"messages\"]:\n",
        "        print(f\"\\n{msg['type'].upper()}: {msg['content']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}