{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "e2283361-fa09-4bcd-9dbe-a665bda2c873",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "a154926a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
            "env: LANGSMITH_TRACING=true\n",
            "env: LANGSMITH_API_KEY=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\n",
            "env: TAVILY_API_KEY=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\n",
            "env: OPENAI_API_KEY=${OPENAI_API_KEY}\n",
            "env: LANGCHAIN_API_KEY=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\n",
            "env: LANGCHAIN_TRACING_V2=true\n",
            "env: LANGCHAIN_PROJECT=\"Multi-agent Collaboration\"\n"
          ]
        }
      ],
      "source": [
        "%set_env ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
        "%set_env LANGSMITH_TRACING=true\n",
        "%set_env LANGSMITH_API_KEY=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\n",
        "%set_env TAVILY_API_KEY=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\n",
        "\n",
        "\n",
        "%set_env OPENAI_API_KEY=${OPENAI_API_KEY}\n",
        "%set_env LANGCHAIN_API_KEY=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\n",
        "\n",
        "%set_env LANGCHAIN_TRACING_V2=true\n",
        "%set_env LANGCHAIN_PROJECT=\"Multi-agent Collaboration\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "2ae034e9-1c05-47e1-aae2-6fb30701166a",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\"  #우리꺼\n",
        "# llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님\n",
        "# llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url\n",
        ")\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "\n",
        "def ChatAnthropicSKT(model=\"skt/claude-3-7-sonnet-20250219\", max_tokens=100):\n",
        "    llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\" #우리꺼 # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님  # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "    \n",
        "    llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "    \n",
        "    # llm_api_url = \"https://43.203.77.11:443/v1\"\n",
        "\n",
        "    # model = \"anthropic/claude-3-5-sonnet-20240620\"\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        openai_api_key=llm_api_key, \n",
        "        openai_api_base=llm_api_url, \n",
        "        model=model,\n",
        "        max_tokens=max_tokens\n",
        "        )\n",
        "    return model\n",
        "\n",
        "llm_cld37 = ChatAnthropicSKT()\n",
        "llm_gem3 = ChatAnthropicSKT(model='skt/gemma3-12b-it')\n",
        "\n",
        "\n",
        "# llm_cld37 = ChatAnthropic(\n",
        "#     api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "#     model=\"claude-3-7-sonnet-20250219\",\n",
        "#     max_tokens=3000\n",
        "# )\n",
        "\n",
        "llm_chat = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        model=\"gpt-4o\",\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        max_tokens=2000,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "1aecb3f7-22b9-4612-a6ec-a54c42482dd8",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \n",
        "    # json_str = broken_json.replace(\"'\",'\"')\n",
        "    \n",
        "    # Fix unquoted values (like NI00001863)\n",
        "    json_str = re.sub(r':\\s*([a-zA-Z0-9_]+)(\\s*[,}])', r': \"\\1\"\\2', broken_json)\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/a.x-3-lg\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n",
        "\n",
        "def convert_to_custom_format(json_items):\n",
        "    custom_format = []\n",
        "    \n",
        "    for item in json_items:\n",
        "        item_name = item.get(\"item_name_in_message\", \"\")\n",
        "        item_id = item.get(\"item_id\", \"\")\n",
        "        category = item.get(\"category\", \"\")\n",
        "        \n",
        "        # Create custom format for each item\n",
        "        custom_line = f\"[Item Name] {item_name} [Item ID] {item_id} [Item Category] {category}\"\n",
        "        custom_format.append(custom_line)\n",
        "    \n",
        "    return \"\\n\".join(custom_format)\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "    # Replace URLs with an empty string\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "def remove_custom_pattern(text, keyword=\"바로가기\"):\n",
        "    # Create a pattern that matches any text followed by the specified keyword\n",
        "    # We escape the keyword to handle any special regex characters it might contain\n",
        "    escaped_keyword = re.escape(keyword)\n",
        "    pattern = re.compile(r'.*? ' + escaped_keyword)\n",
        "    \n",
        "    # Replace the matched pattern with an empty string\n",
        "    return pattern.sub('', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "6d6a3fb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_colored_title(title):\n",
        "    # ANSI 색상 코드\n",
        "    BLUE = '\\033[94m'\n",
        "    BOLD = '\\033[1m'\n",
        "    END = '\\033[0m'\n",
        "    \n",
        "    width = 50\n",
        "    print(\"\\n\" + BLUE + \"=\"*width + END)\n",
        "    print(BLUE + BOLD + title.center(width) + END)\n",
        "    print(BLUE + \"=\"*width + END + \"\\n\")\n",
        "\n",
        "def make_styled_message(head_str):\n",
        "    return f'''\n",
        "        <div style=\"\n",
        "        background-color: #1a237e;  /* 진한 파란색 배경 */\n",
        "        color: #ffffff;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        margin: 20px 0;\n",
        "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
        "        border: 1px solid #3949ab;  /* 테두리 추가 */\n",
        "        \">\n",
        "            <h2 style=\"color: #42a5f5; margin-bottom: 20px; font-size: 25px;\">{head_str}</h2>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "def make_styled_prompt(mms_msg, schema, extraction_guide, rag_context):\n",
        "    return f'''\n",
        "    <div style=\"\n",
        "        background-color: #000000;\n",
        "        color: #ffffff;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        margin: 20px 0;\n",
        "    \">\n",
        "        <h2 style=\"color: #00ff00; margin-bottom: 20px; font-size: 20px;\">SKT 캠페인 메시지 분석</h2>\n",
        "        \n",
        "        <div style=\"margin-bottom: 20px;\">\n",
        "            <h3 style=\"color: #00ff00; font-size: 18px;\">분석 대상 광고 메세지</h3>\n",
        "            <pre style=\"\n",
        "                background-color: #1a1a1a;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                font-family: 'Courier New', monospace;\n",
        "                font-size: 16px;\n",
        "                line-height: 1.6;\n",
        "                margin: 10px 0;\n",
        "                white-space: pre-wrap;\n",
        "            \">{mms_msg}</pre>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-bottom: 20px;\">\n",
        "            <h3 style=\"color: #00ff00; font-size: 18px;\">결과 Schema</h3>\n",
        "            <pre style=\"\n",
        "                background-color: #1a1a1a;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                font-family: 'Courier New', monospace;\n",
        "                font-size: 16px;\n",
        "                line-height: 1.6;\n",
        "                margin: 10px 0;\n",
        "                white-space: pre-wrap;\n",
        "            \">{schema}</pre>\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "            <h3 style=\"color: #00ff00; font-size: 18px;\">추출 가이드</h3>\n",
        "            <pre style=\"\n",
        "                background-color: #1a1a1a;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                font-family: 'Courier New', monospace;\n",
        "                font-size: 16px;\n",
        "                line-height: 1.6;\n",
        "                margin: 10px 0;\n",
        "                white-space: pre-wrap;\n",
        "            \">{extraction_guide}</pre>\n",
        "        </div>\n",
        "\n",
        "            <div>\n",
        "            <h3 style=\"color: #00ff00; font-size: 18px;\">RAG Context</h3>\n",
        "            <pre style=\"\n",
        "                background-color: #1a1a1a;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                font-family: 'Courier New', monospace;\n",
        "                font-size: 16px;\n",
        "                line-height: 1.6;\n",
        "                margin: 10px 0;\n",
        "                white-space: pre-wrap;\n",
        "            \">{rag_context}</pre>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "859204ed-4376-4b9b-b64f-71bfa6174257",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "\n",
        "class KoreanEntityMatcher:\n",
        "    def __init__(self, min_similarity=70, ngram_size=2, min_entity_length=2, token_similarity=True):\n",
        "        self.min_similarity = min_similarity\n",
        "        self.ngram_size = ngram_size\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.token_similarity = token_similarity  # 토큰 단위 유사도 비교 옵션 추가\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "    def build_from_list(self, entities):\n",
        "        \"\"\"Build entity index from a list of entities\"\"\"\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "        for i, entity in enumerate(entities):\n",
        "            if isinstance(entity, tuple) and len(entity) == 2:\n",
        "                entity_name, data = entity\n",
        "                self.entities.append(entity_name)\n",
        "                self.entity_data[entity_name] = data\n",
        "            else:\n",
        "                self.entities.append(entity)\n",
        "                self.entity_data[entity] = {'id': i, 'entity': entity}\n",
        "                \n",
        "        # 각 엔티티의 정규화된 형태를 저장 (검색 최적화)\n",
        "        self.normalized_entities = {}\n",
        "        for entity in self.entities:\n",
        "            normalized = self._normalize_text(entity)\n",
        "            self.normalized_entities[normalized] = entity\n",
        "                \n",
        "        # Create n-gram index for faster candidate selection\n",
        "        self._build_ngram_index(n=self.ngram_size)\n",
        "    \n",
        "    def _normalize_text(self, text):\n",
        "        \"\"\"텍스트 정규화 - 소문자 변환, 공백 제거 등\"\"\"\n",
        "        # 소문자로 변환\n",
        "        text = text.lower()\n",
        "        # 연속된 공백을 하나로 통일\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"텍스트를 토큰으로 분리 (한글, 영문, 숫자 분리)\"\"\"\n",
        "        # 한글, 영문, 숫자 토큰 추출\n",
        "        tokens = re.findall(r'[가-힣]+|[a-z0-9]+', self._normalize_text(text))\n",
        "        return tokens\n",
        "    \n",
        "    def _build_ngram_index(self, n=2):\n",
        "        \"\"\"Build n-gram index optimized for Korean characters\"\"\"\n",
        "        self.ngram_index = {}\n",
        "        \n",
        "        for entity in self.entities:\n",
        "            # Skip entities shorter than min_entity_length\n",
        "            if len(entity) < self.min_entity_length:\n",
        "                continue\n",
        "                \n",
        "            # 정규화된 엔티티 사용\n",
        "            normalized_entity = self._normalize_text(entity)\n",
        "            \n",
        "            # Create n-grams for the entity\n",
        "            entity_chars = list(normalized_entity)  # Split into characters for proper Korean handling\n",
        "            ngrams = []\n",
        "            \n",
        "            # Create character-level n-grams (better for Korean)\n",
        "            for i in range(len(entity_chars) - n + 1):\n",
        "                ngram = ''.join(entity_chars[i:i+n])\n",
        "                ngrams.append(ngram)\n",
        "            \n",
        "            # Add entity to the index for each n-gram\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram_index:\n",
        "                    self.ngram_index[ngram] = set()\n",
        "                self.ngram_index[ngram].add(entity)\n",
        "                \n",
        "            # 토큰 기반 n-gram도 추가 (실험적)\n",
        "            tokens = self._tokenize(normalized_entity)\n",
        "            for token in tokens:\n",
        "                if len(token) >= n:\n",
        "                    token_key = f\"TOKEN:{token}\"\n",
        "                    if token_key not in self.ngram_index:\n",
        "                        self.ngram_index[token_key] = set()\n",
        "                    self.ngram_index[token_key].add(entity)\n",
        "    \n",
        "    def _get_candidates(self, text, n=None):\n",
        "        \"\"\"Get candidate entities based on n-gram overlap (optimized for Korean)\"\"\"\n",
        "        if n is None:\n",
        "            n = self.ngram_size\n",
        "            \n",
        "        # 텍스트 정규화\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        \n",
        "        # 정규화된 텍스트가 정확히 일치하는지 확인 (빠른 경로)\n",
        "        if normalized_text in self.normalized_entities:\n",
        "            entity = self.normalized_entities[normalized_text]\n",
        "            return [(entity, float('inf'))]  # 정확한 일치는 무한대 점수로 표시\n",
        "        \n",
        "        text_chars = list(normalized_text)  # Split into characters for proper Korean handling\n",
        "        text_ngrams = set()\n",
        "        \n",
        "        # Create character-level n-grams\n",
        "        for i in range(len(text_chars) - n + 1):\n",
        "            ngram = ''.join(text_chars[i:i+n])\n",
        "            text_ngrams.add(ngram)\n",
        "        \n",
        "        # 토큰 기반 n-gram 추가\n",
        "        tokens = self._tokenize(normalized_text)\n",
        "        for token in tokens:\n",
        "            if len(token) >= n:\n",
        "                text_ngrams.add(f\"TOKEN:{token}\")\n",
        "        \n",
        "        candidates = set()\n",
        "        for ngram in text_ngrams:\n",
        "            if ngram in self.ngram_index:\n",
        "                candidates.update(self.ngram_index[ngram])\n",
        "        \n",
        "        # Prioritize candidates with multiple n-gram matches\n",
        "        candidate_scores = {}\n",
        "        for candidate in candidates:\n",
        "            candidate_normalized = self._normalize_text(candidate)\n",
        "            candidate_chars = list(candidate_normalized)\n",
        "            candidate_ngrams = set()\n",
        "            \n",
        "            # 문자 n-gram\n",
        "            for i in range(len(candidate_chars) - n + 1):\n",
        "                ngram = ''.join(candidate_chars[i:i+n])\n",
        "                candidate_ngrams.add(ngram)\n",
        "            \n",
        "            # 토큰 기반 n-gram\n",
        "            candidate_tokens = self._tokenize(candidate_normalized)\n",
        "            for token in candidate_tokens:\n",
        "                if len(token) >= n:\n",
        "                    candidate_ngrams.add(f\"TOKEN:{token}\")\n",
        "            \n",
        "            # n-gram 교집합 크기로 초기 점수 계산\n",
        "            overlap = len(candidate_ngrams.intersection(text_ngrams))\n",
        "            \n",
        "            # 토큰 수준 유사도 보너스 점수 추가\n",
        "            token_bonus = 0\n",
        "            if self.token_similarity:\n",
        "                query_tokens = set(tokens)\n",
        "                cand_tokens = set(candidate_tokens)\n",
        "                \n",
        "                # 공통 토큰 비율 계산\n",
        "                if query_tokens and cand_tokens:\n",
        "                    common = query_tokens.intersection(cand_tokens)\n",
        "                    token_bonus = len(common) * 2  # 토큰 일치에 높은 가중치 부여\n",
        "            \n",
        "            candidate_scores[candidate] = overlap + token_bonus\n",
        "        \n",
        "        # Return candidates sorted by n-gram overlap score\n",
        "        return sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    def _calculate_similarity(self, text, entity):\n",
        "        \"\"\"다양한 유사도 측정 방법을 결합하여 더 정확한 유사도 계산\"\"\"\n",
        "        normalized_text = self._normalize_text(text)\n",
        "        normalized_entity = self._normalize_text(entity)\n",
        "        \n",
        "        # 정확히 일치하면 100점 반환\n",
        "        if normalized_text == normalized_entity:\n",
        "            return 100\n",
        "        \n",
        "        # 기본 문자열 유사도 (fuzz.ratio)\n",
        "        ratio_score = fuzz.ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 부분 문자열 체크 (한 문자열이 다른 문자열의 부분 문자열인 경우)\n",
        "        partial_score = 0\n",
        "        if normalized_text in normalized_entity:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (text_len / entity_len) * 100 if entity_len > 0 else 0\n",
        "        elif normalized_entity in normalized_text:\n",
        "            text_len = len(normalized_text)\n",
        "            entity_len = len(normalized_entity)\n",
        "            partial_score = (entity_len / text_len) * 100 if text_len > 0 else 0\n",
        "        \n",
        "        # 토큰 유사도 (토큰 단위로 비교)\n",
        "        token_score = 0\n",
        "        if self.token_similarity:\n",
        "            text_tokens = set(self._tokenize(normalized_text))\n",
        "            entity_tokens = set(self._tokenize(normalized_entity))\n",
        "            \n",
        "            if text_tokens and entity_tokens:\n",
        "                common_tokens = text_tokens.intersection(entity_tokens)\n",
        "                all_tokens = text_tokens.union(entity_tokens)\n",
        "                \n",
        "                if all_tokens:\n",
        "                    token_score = (len(common_tokens) / len(all_tokens)) * 100\n",
        "        \n",
        "        # 토큰 순서 무시 유사도 (fuzz.token_sort_ratio)\n",
        "        token_sort_score = fuzz.token_sort_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 토큰 집합 유사도 (fuzz.token_set_ratio)\n",
        "        token_set_score = fuzz.token_set_ratio(normalized_text, normalized_entity)\n",
        "        \n",
        "        # 최종 유사도는 여러 점수의 가중 평균\n",
        "        # 토큰 유사도에 높은 가중치를 부여하여 \"T우주\"와 \"T 우주패스\"의 매칭 향상\n",
        "        final_score = (\n",
        "            ratio_score * 0.3 +  # 기본 유사도\n",
        "            max(partial_score, 0) * 0.1 +  # 부분 문자열 유사도\n",
        "            token_score * 0.2 +  # 토큰 유사도\n",
        "            token_sort_score * 0.2 +  # 토큰 순서 무시 유사도\n",
        "            token_set_score * 0.2  # 토큰 집합 유사도\n",
        "        )\n",
        "        \n",
        "        return final_score\n",
        "    \n",
        "    def find_entities(self, text, max_candidates_per_span=10):\n",
        "        \"\"\"Find entity matches in Korean text using fuzzy matching\"\"\"\n",
        "        # Extract spans that might contain entities\n",
        "        potential_spans = self._extract_korean_spans(text)\n",
        "        matches = []\n",
        "        \n",
        "        for span_text, start, end in potential_spans:\n",
        "            if len(span_text.strip()) < self.min_entity_length:  # Skip spans shorter than min_entity_length\n",
        "                continue\n",
        "\n",
        "            # Get candidate entities based on n-gram overlap\n",
        "            candidates = self._get_candidates(span_text)\n",
        "\n",
        "            # If no candidates found through n-gram filtering, skip\n",
        "            if not candidates:\n",
        "                continue\n",
        "            \n",
        "            # Limit the number of candidates to check\n",
        "            top_candidates = [c[0] for c in candidates[:max_candidates_per_span]]\n",
        "            \n",
        "            # 각 후보 엔티티에 대해 개선된 유사도 계산\n",
        "            scored_matches = []\n",
        "            for entity in top_candidates:\n",
        "                score = self._calculate_similarity(span_text, entity)\n",
        "                \n",
        "                if score >= self.min_similarity:\n",
        "                    scored_matches.append((entity, score, 0))  # 호환성을 위해 3번째 매개변수 추가\n",
        "\n",
        "            # 기존 process.extract 대신 개선된 유사도 계산 사용\n",
        "            best_matches = scored_matches\n",
        "\n",
        "            for entity, score, _ in best_matches:\n",
        "                matches.append({\n",
        "                    'text': span_text,\n",
        "                    'matched_entity': entity,\n",
        "                    'score': score,\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'data': self.entity_data.get(entity, {})\n",
        "                })\n",
        "\n",
        "        # Sort by position in text\n",
        "        matches.sort(key=lambda x: (x['start'], -x['score']))\n",
        "        \n",
        "        # Handle overlapping matches by keeping the best match\n",
        "        final_matches = self._resolve_overlapping_matches(matches)\n",
        "\n",
        "        return final_matches\n",
        "    \n",
        "    def _extract_korean_spans(self, text):\n",
        "        \"\"\"한국어와 영어가 혼합된 텍스트에서 엔티티일 수 있는 잠재적 텍스트 범위 추출\"\"\"\n",
        "        spans = []\n",
        "        min_len = self.min_entity_length\n",
        "        \n",
        "        # 1. 영문+한글 혼합 패턴 (붙여쓰기) 예: \"T우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 2. 영문+한글 혼합 패턴 (띄어쓰기) 예: \"T 우주\"\n",
        "        for match in re.finditer(r'[a-zA-Z]+\\s+[가-힣]+(?:\\s+[가-힣가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 3. 특수한 혼합 패턴 - 영문+한글+숫자 (예: \"T우주365\", \"SK텔레콤\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+(?:[0-9]+)?', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 4. 연속된 두 단어까지 확장 (예: \"T우주 패스\")\n",
        "        # 영문+한글 후 공백 하나를 두고 다른 한글 단어가 나오는 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "                \n",
        "        # 5. 연속된 세 단어까지 확장 (예: \"T우주 멤버십 패스\")\n",
        "        for match in re.finditer(r'[a-zA-Z]+[가-힣]+\\s+[가-힣]+\\s+[가-힣]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 6. 브랜드명 + 제품명 패턴 (예: \"SK텔레콤 T우주\")\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣]+(?:\\s+[a-zA-Z가-힣]+){1,3}', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 7. 숫자와 영문 결합 패턴 (숫자 space 영문 패턴, e.g. \"0 day\")\n",
        "        for match in re.finditer(r'\\d+\\s+[a-zA-Z]+', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 8. 더 일반적인 영한 혼합 패턴\n",
        "        for match in re.finditer(r'[a-zA-Z가-힣0-9]+(?:\\s+[a-zA-Z가-힣0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 9. 일반적인 구분자로 분리된 텍스트 조각도 추출\n",
        "        for span in re.split(r'[,\\.!?;:\"\\'…\\(\\)\\[\\]\\{\\}\\s_/]+', text):\n",
        "            if span and len(span) >= min_len:\n",
        "                span_pos = text.find(span)\n",
        "                if span_pos != -1:\n",
        "                    spans.append((span, span_pos, span_pos + len(span)))\n",
        "                \n",
        "        return spans\n",
        "    \n",
        "    def _remove_duplicate_entities(self, matches):\n",
        "        \"\"\"Keep only one instance of each unique entity\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Dictionary to track highest-scoring match for each entity\n",
        "        best_matches = {}\n",
        "        \n",
        "        for match in matches:\n",
        "            entity_key = match['matched_entity']\n",
        "            \n",
        "            # If we haven't seen this entity before, or if this match has a higher score\n",
        "            # than the previously saved match for this entity, save this one\n",
        "            if (entity_key not in best_matches or \n",
        "                match['score'] > best_matches[entity_key]['score']):\n",
        "                best_matches[entity_key] = match\n",
        "        \n",
        "        # Return the best matches sorted by start position\n",
        "        return sorted(best_matches.values(), key=lambda x: x['start'])\n",
        "    \n",
        "    def _resolve_overlapping_matches(self, matches, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # 점수 내림차순, 길이 오름차순으로 정렬 (높은 점수, 짧은 매치 우선)\n",
        "        sorted_matches = sorted(matches, key=lambda x: (-x['score'], x['end'] - x['start']))\n",
        "        \n",
        "        final_matches = []\n",
        "        \n",
        "        for current_match in sorted_matches:\n",
        "            current_score = current_match['score']\n",
        "            current_start, current_end = current_match['start'], current_match['end']\n",
        "            current_range = set(range(current_start, current_end))\n",
        "            current_len = len(current_range)\n",
        "            \n",
        "            # 수정: overlap_ratio를 0으로 초기화\n",
        "            current_match['overlap_ratio'] = 0.0\n",
        "            \n",
        "            # 높은 점수의 매치는 항상 포함\n",
        "            if current_score >= high_score_threshold:\n",
        "                # 기존 매치들과 비교하여 너무 많은 중복이 있는지 확인\n",
        "                is_too_similar = False\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    if existing_match['score'] < high_score_threshold:\n",
        "                        continue  # 낮은 점수의 기존 매치와는 비교하지 않음\n",
        "                        \n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    \n",
        "                    # 현재 매치에 대한 중복 비율 \n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 수정: 중복 비율 저장 - 가장 높은 중복 비율 저장\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하고, 동일한 엔티티면 추가하지 않음\n",
        "                    if (current_overlap_ratio > overlap_tolerance\n",
        "                        and current_match['matched_entity'] == existing_match['matched_entity']\n",
        "                        ):\n",
        "                        is_too_similar = True\n",
        "                        break\n",
        "                \n",
        "                if not is_too_similar:\n",
        "                    final_matches.append(current_match)\n",
        "            else:\n",
        "                # 낮은 점수의 매치는 기존 로직 적용 (중복 확인)\n",
        "                should_add = True\n",
        "                \n",
        "                for existing_match in final_matches:\n",
        "                    existing_start, existing_end = existing_match['start'], existing_match['end']\n",
        "                    existing_range = set(range(existing_start, existing_end))\n",
        "                    \n",
        "                    # 교집합 비율 계산\n",
        "                    intersection = current_range.intersection(existing_range)\n",
        "                    current_overlap_ratio = len(intersection) / current_len if current_len > 0 else 0\n",
        "                    \n",
        "                    # 수정: 중복 비율 저장 - 가장 높은 중복 비율 저장\n",
        "                    current_match['overlap_ratio'] = max(current_match['overlap_ratio'], current_overlap_ratio)\n",
        "                    \n",
        "                    # 중복 비율이 허용 범위를 초과하면 추가하지 않음\n",
        "                    if current_overlap_ratio > (1 - overlap_tolerance):\n",
        "                        should_add = False\n",
        "                        break\n",
        "                \n",
        "                if should_add:\n",
        "                    final_matches.append(current_match)\n",
        "        \n",
        "        # 시작 위치별로 정렬\n",
        "        final_matches.sort(key=lambda x: x['start'])\n",
        "        \n",
        "        return final_matches\n",
        "\n",
        "def find_entities_in_text(text, entity_list, min_similarity=70, ngram_size=3, min_entity_length=2, \n",
        "                         token_similarity=True, high_score_threshold=50, overlap_tolerance=0.5):\n",
        "    \"\"\"\n",
        "    Find entity matches in text using fuzzy matching.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The text to search for entities\n",
        "    entity_list : list\n",
        "        List of entities to match against\n",
        "    min_similarity : int, default=70\n",
        "        Minimum similarity score (0-100) for fuzzy matching\n",
        "    ngram_size : int, default=2\n",
        "        Size of character n-grams to use for indexing (2 or 3 recommended for Korean)\n",
        "    min_entity_length : int, default=2\n",
        "        Minimum length of entities to consider (characters)\n",
        "    token_similarity : bool, default=True\n",
        "        Whether to use token-based similarity measures\n",
        "    high_score_threshold : int, default=50\n",
        "        Score threshold above which matches are always kept regardless of overlap\n",
        "    overlap_tolerance : float, default=0.5\n",
        "        Overlap tolerance ratio (0-1), higher values allow more overlapping matches\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        List of matched entities with position and metadata\n",
        "    \"\"\"\n",
        "    matcher = KoreanEntityMatcher(\n",
        "        min_similarity=min_similarity,\n",
        "        ngram_size=ngram_size,\n",
        "        min_entity_length=min_entity_length,\n",
        "        token_similarity=token_similarity\n",
        "    )\n",
        "    matcher.build_from_list(entity_list)\n",
        "    \n",
        "    matches = matcher.find_entities(text)\n",
        "    \n",
        "    # 기존 _resolve_overlapping_matches 메서드 대신 직접 호출\n",
        "    final_matches = matcher._resolve_overlapping_matches(\n",
        "        matches, \n",
        "        high_score_threshold=high_score_threshold,\n",
        "        overlap_tolerance=overlap_tolerance\n",
        "    )\n",
        "    \n",
        "    return final_matches\n",
        "# Function to highlight entities in text\n",
        "def highlight_entities(text, matches):\n",
        "    marked_text = text\n",
        "    offset = 0\n",
        "    for match in sorted(matches, key=lambda x: x['start'], reverse=True):\n",
        "        start = match['start'] + offset\n",
        "        end = match['end'] + offset\n",
        "        entity = match['matched_entity']\n",
        "        score = match['score']\n",
        "        marked_text = marked_text[:start] + f\"[{marked_text[start:end]}→{entity} ({score:.1f}%)]\" + marked_text[end:]\n",
        "        offset += len(f\"[→{entity} ({score:.1f}%)]\") + 2\n",
        "    \n",
        "    return marked_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ffb01b88",
      "metadata": {},
      "outputs": [],
      "source": [
        "import kiwipiepy\n",
        "\n",
        "def reconstruct_text_from_tokens(tokens):\n",
        "    \"\"\"\n",
        "    tokens: Kiwi.analyze()의 결과로 반환된 Token 객체 리스트\n",
        "    filter_rules: 필터링 규칙 (예: 특정 품사만 선택, 특정 단어 제외 등)\n",
        "    \"\"\"\n",
        "\n",
        "    if tokens:\n",
        "        max_pos = max(token.start + token.len for token in tokens)\n",
        "        text_array = [' '] * max_pos  # 공백으로 초기화\n",
        "        \n",
        "        for token in tokens:\n",
        "            for i in range(token.start, token.start + token.len):\n",
        "                if i < max_pos:\n",
        "                    text_array[i] = token.form[i - token.start]\n",
        "        \n",
        "        text = ''.join(text_array)\n",
        "    else:\n",
        "        text = ''\n",
        "    \n",
        "    return text\n",
        "\n",
        "def extract_by_tag_pattern(result, tag_patterns, original_text=None):\n",
        "    \"\"\"\n",
        "    kiwi.analyze() 결과에서 특정 tag 패턴에 일치하는 form들을 추출합니다.\n",
        "    \n",
        "    Parameters:\n",
        "    - result: kiwi.analyze()의 결과\n",
        "    - tag_patterns: 찾고자 하는 tag 패턴 리스트\n",
        "    - original_text: 원본 텍스트 (위치 기반 추출 시 필요)\n",
        "    \"\"\"\n",
        "    # kiwi.analyze()의 결과에서 토큰 리스트 추출\n",
        "    if isinstance(result, list) and len(result) > 0 and isinstance(result[0], tuple):\n",
        "        tokens = result[0][0]  # 첫 번째 분석 결과에서 토큰 리스트 추출\n",
        "    else:\n",
        "        tokens = result  # 이미 토큰 리스트인 경우\n",
        "    \n",
        "    matches = {i: [] for i in range(len(tag_patterns))}  # 패턴별 매치 결과\n",
        "    \n",
        "    # 각 패턴에 대해 검색\n",
        "    for pattern_idx, pattern in enumerate(tag_patterns):\n",
        "        pattern_len = len(pattern)\n",
        "        \n",
        "        # 슬라이딩 윈도우 방식으로 패턴 검색\n",
        "        for i in range(len(tokens) - pattern_len + 1):\n",
        "            window = tokens[i:i+pattern_len]\n",
        "            \n",
        "            # 현재 윈도우가 패턴과 일치하는지 확인\n",
        "            if all(token.tag == pattern[j] for j, token in enumerate(window)):\n",
        "                # 패턴과 일치하면 form들을 추출하여 저장\n",
        "                if original_text and window:\n",
        "                    start_pos = window[0].start\n",
        "                    end_pos = window[-1].start + window[-1].len\n",
        "                    matched_forms = original_text[start_pos:end_pos]\n",
        "                else:\n",
        "                    # 원본 텍스트가 없으면 토큰 form을 직접 합침 (공백 없이)\n",
        "                    matched_forms = ''.join(token.form for token in window)\n",
        "                \n",
        "                matches[pattern_idx].append(matched_forms)\n",
        "    \n",
        "    return matches\n",
        "\n",
        "def extract_by_flexible_tag_pattern(result, tag_patterns, max_gap=0, original_text=None):\n",
        "    \"\"\"\n",
        "    kiwi.analyze() 결과에서 특정 tag 패턴에 일치하는 form들을 유연하게 추출합니다.\n",
        "    중간에 다른 태그가 있어도 일정 개수 이내라면 매칭합니다.\n",
        "    \n",
        "    Parameters:\n",
        "    - result: kiwi.analyze()의 결과\n",
        "    - tag_patterns: 찾고자 하는 tag 패턴 리스트. 예: [['NNG', 'JKS', 'VV'], ['VA', 'NNG']]\n",
        "    - max_gap: 패턴 사이에 허용되는 최대 간격(다른 태그의 개수)\n",
        "    - original_text: 원본 텍스트 (위치 기반 추출 시 필요)\n",
        "    \n",
        "    Returns:\n",
        "    - 패턴별 일치하는 form의 리스트 딕셔너리와 매칭된 토큰 인덱스\n",
        "    \"\"\"\n",
        "    # kiwi.analyze()의 결과에서 토큰 리스트 추출\n",
        "    if isinstance(result, list) and len(result) > 0 and isinstance(result[0], tuple):\n",
        "        tokens = result[0][0]  # 첫 번째 분석 결과에서 토큰 리스트 추출\n",
        "    else:\n",
        "        tokens = result  # 이미 토큰 리스트인 경우\n",
        "    \n",
        "    matches = {i: [] for i in range(len(tag_patterns))}  # 패턴별 매치 결과\n",
        "    match_indices = {i: [] for i in range(len(tag_patterns))}  # 매치된 토큰 인덱스\n",
        "    \n",
        "    # 각 패턴에 대해 검색\n",
        "    for pattern_idx, pattern in enumerate(tag_patterns):\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            matched_indices = []\n",
        "            pattern_pos = 0\n",
        "            gaps = 0\n",
        "            j = i\n",
        "            \n",
        "            # 패턴 매칭 시도\n",
        "            match_found = False\n",
        "            while j < len(tokens) and pattern_pos < len(pattern):\n",
        "                if tokens[j].tag == pattern[pattern_pos]:\n",
        "                    matched_indices.append(j)\n",
        "                    pattern_pos += 1\n",
        "                    gaps = 0\n",
        "                else:\n",
        "                    gaps += 1\n",
        "                    if gaps > max_gap:\n",
        "                        # 허용된 간격을 초과하면 매칭 실패\n",
        "                        break\n",
        "                j += 1\n",
        "                \n",
        "                # 패턴을 모두 매칭했는지 확인\n",
        "                if pattern_pos == len(pattern):\n",
        "                    match_found = True\n",
        "                    break\n",
        "            \n",
        "            # 매칭에 성공한 경우 결과 추가\n",
        "            if match_found:\n",
        "                matched_tokens = [tokens[idx] for idx in matched_indices]\n",
        "                \n",
        "                # 원본 텍스트에서 추출하거나 토큰 form 직접 연결\n",
        "                if original_text and matched_tokens:\n",
        "                    start_pos = matched_tokens[0].start\n",
        "                    end_pos = matched_tokens[-1].start + matched_tokens[-1].len\n",
        "                    matched_forms = original_text[start_pos:end_pos]\n",
        "                else:\n",
        "                    # 원본 텍스트가 없으면 토큰 form을 직접 합침 (공백 없이)\n",
        "                    matched_forms = ''.join(token.form for token in matched_tokens)\n",
        "                    \n",
        "                matches[pattern_idx].append(matched_forms)\n",
        "                match_indices[pattern_idx].append(matched_indices)\n",
        "                \n",
        "                # 매칭 후 겹치지 않도록 다음 위치로 이동\n",
        "                i = matched_indices[-1] + 1\n",
        "            else:\n",
        "                i += 1  # 매칭 실패한 경우 다음 위치로 이동\n",
        "    \n",
        "    return matches, match_indices\n",
        "\n",
        "def reconstruct_with_tag_patterns(result, tag_patterns, include_unmatched=False, original_text=None):\n",
        "    \"\"\"\n",
        "    kiwi.analyze() 결과에서 특정 tag 패턴에 일치하는 부분을 강조하여 텍스트 재구성\n",
        "    \n",
        "    Parameters:\n",
        "    - result: kiwi.analyze()의 결과\n",
        "    - tag_patterns: 찾고자 하는 tag 패턴 리스트\n",
        "    - include_unmatched: 패턴에 일치하지 않는 토큰도 포함할지 여부\n",
        "    - original_text: 원본 텍스트 (위치 기반 추출 시 필요)\n",
        "    \n",
        "    Returns:\n",
        "    - 재구성된 텍스트 (패턴 일치 부분은 <match>로 강조)\n",
        "    \"\"\"\n",
        "    # kiwi.analyze()의 결과에서 토큰 리스트 추출\n",
        "    if isinstance(result, list) and len(result) > 0 and isinstance(result[0], tuple):\n",
        "        tokens = result[0][0]  # 첫 번째 분석 결과에서 토큰 리스트 추출\n",
        "    else:\n",
        "        tokens = result  # 이미 토큰 리스트인 경우\n",
        "    \n",
        "    _, match_indices = extract_by_flexible_tag_pattern(result, tag_patterns, original_text=original_text)\n",
        "    \n",
        "    # 모든 매치 인덱스를 하나의 집합으로 합침\n",
        "    all_matched_indices = set()\n",
        "    for indices_list in match_indices.values():\n",
        "        for indices in indices_list:\n",
        "            all_matched_indices.update(indices)\n",
        "    \n",
        "    # 텍스트 재구성\n",
        "    reconstructed_parts = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i in all_matched_indices:\n",
        "            # 매치된 패턴의 시작 찾기\n",
        "            for pattern_idx, indices_list in match_indices.items():\n",
        "                for indices in indices_list:\n",
        "                    if i == indices[0]:  # 패턴의 시작\n",
        "                        pattern_tokens = [tokens[idx] for idx in indices]\n",
        "                        \n",
        "                        # 원본 텍스트에서 추출하거나 토큰 form 직접 연결\n",
        "                        if original_text and pattern_tokens:\n",
        "                            start_pos = pattern_tokens[0].start\n",
        "                            end_pos = pattern_tokens[-1].start + pattern_tokens[-1].len\n",
        "                            pattern_text = original_text[start_pos:end_pos]\n",
        "                        else:\n",
        "                            pattern_text = ''.join(token.form for token in pattern_tokens)\n",
        "                            \n",
        "                        reconstructed_parts.append(f\"<match>{pattern_text}</match>\")\n",
        "                        i = indices[-1] + 1  # 패턴 이후로 인덱스 이동\n",
        "                        break\n",
        "                else:\n",
        "                    continue\n",
        "                break\n",
        "        else:\n",
        "            # 매치되지 않은 토큰\n",
        "            if include_unmatched:\n",
        "                reconstructed_parts.append(tokens[i].form)\n",
        "            i += 1\n",
        "    \n",
        "    return ''.join(reconstructed_parts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "0655b0b0-b7ec-4627-a3bc-ad8ece7cb65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def advanced_sequential_similarity(str1, str2, metrics=None, visualize=False):\n",
        "    \"\"\"\n",
        "    Calculate multiple character-level similarity metrics between two strings.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    str1 : str\n",
        "        First string\n",
        "    str2 : str\n",
        "        Second string\n",
        "    metrics : list\n",
        "        List of metrics to compute. Options: \n",
        "        ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "        If None, all metrics will be computed\n",
        "    visualize : bool\n",
        "        If True, visualize the differences between strings\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing similarity scores for each metric\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['ngram', 'lcs', 'subsequence', 'difflib']\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Handle empty strings\n",
        "    if not str1 or not str2:\n",
        "        return {metric: 0.0 for metric in metrics}\n",
        "    \n",
        "    # Prepare strings\n",
        "    s1, s2 = str1.lower(), str2.lower()\n",
        "    \n",
        "    # 1. N-gram similarity (with multiple window sizes)\n",
        "    if 'ngram' in metrics:\n",
        "        ngram_scores = {}\n",
        "        for window in range(min([len(s1),len(s2),2]), min([5,max([len(s1),len(s2)])+1])):\n",
        "            # Skip if strings are shorter than window\n",
        "            if len(s1) < window or len(s2) < window:\n",
        "                ngram_scores[f'window_{window}'] = 0.0\n",
        "                continue\n",
        "                \n",
        "            # Generate character n-grams\n",
        "            ngrams1 = [s1[i:i+window] for i in range(len(s1) - window + 1)]\n",
        "            ngrams2 = [s2[i:i+window] for i in range(len(s2) - window + 1)]\n",
        "            \n",
        "            # Count matches\n",
        "            matches = sum(1 for ng in ngrams1 if ng in ngrams2)\n",
        "            max_possible = max(len(ngrams1), len(ngrams2))\n",
        "            \n",
        "            # Normalize\n",
        "            score = matches / max_possible if max_possible > 0 else 0.0\n",
        "            ngram_scores[f'window_{window}'] = score\n",
        "            \n",
        "        # Average of all n-gram scores\n",
        "        results['ngram'] = max(ngram_scores.values())#sum(ngram_scores.values()) / len(ngram_scores)\n",
        "        results['ngram_details'] = ngram_scores\n",
        "    \n",
        "    # 2. Longest Common Substring (LCS)\n",
        "    if 'lcs' in metrics:\n",
        "        def longest_common_substring(s1, s2):\n",
        "            # Dynamic programming approach\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            max_length = 0\n",
        "            \n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                        max_length = max(max_length, dp[i][j])\n",
        "            \n",
        "            return max_length\n",
        "        \n",
        "        lcs_length = longest_common_substring(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['lcs'] = lcs_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    # 3. Longest Common Subsequence\n",
        "    if 'subsequence' in metrics:\n",
        "        def longest_common_subsequence(s1, s2):\n",
        "            # Dynamic programming approach for subsequence\n",
        "            m, n = len(s1), len(s2)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "            \n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if s1[i-1] == s2[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "            \n",
        "            return dp[m][n]\n",
        "        \n",
        "        subseq_length = longest_common_subsequence(s1, s2)\n",
        "        max_length = max(len(s1), len(s2))\n",
        "        results['subsequence'] = subseq_length / max_length if max_length > 0 else 0.0\n",
        "    \n",
        "    # 4. SequenceMatcher from difflib\n",
        "    if 'difflib' in metrics:\n",
        "        sm = SequenceMatcher(None, s1, s2)\n",
        "        results['difflib'] = sm.ratio()\n",
        "    \n",
        "    # Visualization of differences\n",
        "    if visualize:\n",
        "        try:\n",
        "            # Only works in notebooks or environments that support plotting\n",
        "            sm = SequenceMatcher(None, s1, s2)\n",
        "            matches = sm.get_matching_blocks()\n",
        "            \n",
        "            # Prepare for visualization\n",
        "            fig, ax = plt.subplots(figsize=(10, 3))\n",
        "            \n",
        "            # Draw strings as horizontal bars\n",
        "            ax.barh(0, len(s1), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            ax.barh(1, len(s2), height=0.4, left=0, color='lightgray', alpha=0.3)\n",
        "            \n",
        "            # Draw matching parts\n",
        "            for match in matches:\n",
        "                i, j, size = match\n",
        "                if size > 0:  # Ignore zero-length matches\n",
        "                    ax.barh(0, size, height=0.4, left=i, color='green', alpha=0.5)\n",
        "                    ax.barh(1, size, height=0.4, left=j, color='green', alpha=0.5)\n",
        "                    \n",
        "                    # Draw connection lines between matches\n",
        "                    ax.plot([i + size/2, j + size/2], [0.2, 0.8], 'k-', alpha=0.3)\n",
        "            \n",
        "            # Add string texts\n",
        "            for i, c in enumerate(s1):\n",
        "                ax.text(i + 0.5, 0, c, ha='center', va='center')\n",
        "            for i, c in enumerate(s2):\n",
        "                ax.text(i + 0.5, 1, c, ha='center', va='center')\n",
        "                \n",
        "            ax.set_yticks([0, 1])\n",
        "            ax.set_yticklabels(['String 1', 'String 2'])\n",
        "            ax.set_xlabel('Character Position')\n",
        "            ax.set_title('Character-Level String Comparison')\n",
        "            ax.grid(False)\n",
        "            plt.tight_layout()\n",
        "            # plt.show()  # Uncomment to display\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "    \n",
        "    # Calculate overall similarity score (average of all metrics)\n",
        "    metrics_to_average = [m for m in results.keys() if not m.endswith('_details')]\n",
        "    results['overall'] = sum(results[m] for m in metrics_to_average) / len(metrics_to_average)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# advanced_sequential_similarity('시크릿', '시크릿', metrics='ngram')\n",
        "# advanced_sequential_similarity('에이닷_자사', '에이닷')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "f8b275d1-1e26-40b4-93f4-898cc1345c22",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# mms_pdf = pd.read_excel(\"./data/mms_data_250408.xlsx\", engine=\"openpyxl\")\n",
        "mms_pdf = pd.read_csv(\"./data/mms_data_250408.csv\")\n",
        "\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "\n",
        "mms_pdf = mms_pdf.reset_index()\n",
        "\n",
        "mms_pdf = mms_pdf.astype('str')\n",
        "\n",
        "# mms_pdf.sample(100)[['msg']].to_csv(\"./data/mms_sample.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "8ba8f0a7-090e-44df-bf2d-7bfd5189bdca",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # item_pdf_raw = pd.read_excel(\"./data/item_info_250401.xlsx\", engine=\"openpyxl\")\n",
        "# # item_pdf_raw = pd.read_csv(\"./data/item_info_250401.csv\")\n",
        "# # item_pdf_raw = pd.read_pickle(\"./data/item_info_250513.pkl\")\n",
        "# item_pdf_raw = pd.read_csv(\"./data/item_info_250516.csv\")\n",
        "\n",
        "\n",
        "# item_pdf = item_pdf_raw.drop_duplicates(['item_nm']).copy()\n",
        "# item_pdf['item_item'] = item_pdf['item_nm']+\"\\n\"+item_pdf['item_desc']\n",
        "\n",
        "# item_pdf['item_nm_cl'] = item_pdf['item_nm'].apply(clean_text)\n",
        "# item_pdf['item_desc_cl'] = item_pdf['item_desc'].fillna('').astype(str).apply(clean_text)\n",
        "# item_pdf['item_item_cl'] = item_pdf['item_nm_cl']+\"\\n\"+item_pdf['item_desc_cl']\n",
        "\n",
        "# entity_list = []\n",
        "# for row in item_pdf.to_dict('records'):\n",
        "#     entity_list.append((row['item_nm'], {'item_id':row['item_id'],'category':row['item_cate_ax'], 'description':row['item_desc'], 'create_dt':row['create_dt'], 'rank':row['rank']}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "319db51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import konlpy.tag\n",
        "\n",
        "# For Korean text preprocessing, use KoNLPy\n",
        "class KoreanTokenizer:\n",
        "    def __init__(self, tagger_type='Okt'):\n",
        "        # Choose a Korean morphological analyzer\n",
        "        # Options: Okt, Mecab, Komoran, Hannanum, Kkma\n",
        "        if tagger_type == 'Okt':\n",
        "            from konlpy.tag import Okt\n",
        "            self.tagger = Okt()\n",
        "        elif tagger_type == 'Mecab':\n",
        "            from konlpy.tag import Mecab\n",
        "            self.tagger = Mecab()\n",
        "        # Add other taggers as needed\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        # Extract nouns, adjectives, and verbs (you can customize this)\n",
        "        tokens = self.tagger.morphs(text)\n",
        "        return tokens\n",
        "\n",
        "# Initialize the tokenizer (Okt is generally good for most purposes)\n",
        "korean_tokenizer = KoreanTokenizer(tagger_type='Okt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "340e8748",
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_ext = {\n",
        "    \"title\": {\n",
        "        \"type\": \"string\", \n",
        "        'description': '광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성'\n",
        "    },\n",
        "    'purpose': {\n",
        "        'type': 'array', \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    'product': {\n",
        "        'type': 'array',\n",
        "        'items': {\n",
        "            'type': 'object',\n",
        "            'properties': {\n",
        "            'name': {'type': 'string', 'description': '광고하는 제품이나 서비스 이름'},\n",
        "            'action': {'type': 'string', 'description': '고객에게 기대하는 행동: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타] 중에서 선택'}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'pgm':{\n",
        "         'type': 'array', \n",
        "        'description': '아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공'\n",
        "    },\n",
        "'required': ['purpose', 'product', 'channel', 'pgm'], \n",
        "'objectType': 'object'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989c8e8a",
      "metadata": {},
      "source": [
        "<h1 style=\"color: #0066cc;\">2025.05.29 시연</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2f0db9",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">인기 아이템</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "3fdf0cd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "item_pdf_raw = pd.read_csv(\"./data/item_info_pop_250520.csv\", encoding='cp949')\n",
        "\n",
        "item_pdf_pop = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','domain','start_dt','end_dt','rank']].copy()\n",
        "# item_pdf['item_item'] = item_pdf['item_nm']+\"\\n\"+item_pdf['item_desc']\n",
        "\n",
        "# item_pdf['item_nm_cl'] = item_pdf['item_nm'].apply(clean_text)\n",
        "# item_pdf['item_desc_cl'] = item_pdf['item_desc'].fillna('').astype(str).apply(clean_text)\n",
        "# item_pdf['item_item_cl'] = item_pdf['item_nm_cl']+\"\\n\"+item_pdf['item_desc_cl']\n",
        "\n",
        "# entity_list_pop = []\n",
        "# for row in item_pdf.to_dict('records'):\n",
        "#     entity_list_pop.append({'item_nm':row['item_nm'],'item_id':row['item_id'], 'description':row['item_desc'], 'start_dt':row['start_dt'], 'end_dt':row['end_dt'], 'rank':row['rank']})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449a487d",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">전체 아이템</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "e01464ad",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_desc</th>\n",
              "      <th>domain</th>\n",
              "      <th>start_dt</th>\n",
              "      <th>end_dt</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>NA00008721</td>\n",
              "      <td>1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...</td>\n",
              "      <td>product</td>\n",
              "      <td>20240627.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>403.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7218</th>\n",
              "      <td>5GX 플래티넘(넷플릭스)</td>\n",
              "      <td>NA00008719</td>\n",
              "      <td>1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...</td>\n",
              "      <td>product</td>\n",
              "      <td>20240919.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>467.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13351</th>\n",
              "      <td>5GX 프리미엄(넷플릭스)</td>\n",
              "      <td>NA00008720</td>\n",
              "      <td>1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...</td>\n",
              "      <td>product</td>\n",
              "      <td>20240627.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>163.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14141</th>\n",
              "      <td>0 청년 다이렉트 69(넷플릭스)</td>\n",
              "      <td>NA00008730</td>\n",
              "      <td>SK텔레콤 온라인 공식채널에서만 가입할 수 있는 만 34세 이하 청년 요금제</td>\n",
              "      <td>product</td>\n",
              "      <td>20240627.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>825.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20739</th>\n",
              "      <td>5GX 프라임(넷플릭스)</td>\n",
              "      <td>NA00008722</td>\n",
              "      <td>1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...</td>\n",
              "      <td>product</td>\n",
              "      <td>20240627.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>232.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  item_nm     item_id  \\\n",
              "1571     5GX 프라임플러스(넷플릭스)  NA00008721   \n",
              "7218       5GX 플래티넘(넷플릭스)  NA00008719   \n",
              "13351      5GX 프리미엄(넷플릭스)  NA00008720   \n",
              "14141  0 청년 다이렉트 69(넷플릭스)  NA00008730   \n",
              "20739       5GX 프라임(넷플릭스)  NA00008722   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 item_desc  \\\n",
              "1571   1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...   \n",
              "7218   1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...   \n",
              "13351  1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...   \n",
              "14141                                                                                                                                                                                                                                                                                                                                                                                                                                                                           SK텔레콤 온라인 공식채널에서만 가입할 수 있는 만 34세 이하 청년 요금제   \n",
              "20739  1. 기본 제공 혜택5GX 플랜 (넷플릭스) 기본제공 혜택 안내 - 요금제, 월정액 (VAT포함), 음성/문자, 기본 제공 데이터로 구성되어 있습니다.요금제월정액_*선택약정 반영 금액)음성/문자기본 제공_데이터_5GX_(넷플릭스)125,000원_(*93,705원)집전화/휴대폰 음성통화 무제한,_영상/부가통화 300분,_문자 기본제공완전 무제한_5GX_(넷플릭스)109,000원_(*81,720원)완전 무제한_5GX_(넷플릭스)99,000원_(*74,250원)완전 무제한_5GX_(넷플릭스)89,000원_(*66,725원)완전 무제한_부가통화 1초 이용 시 부가통화 제공량에서 1초 차감_영상통화 1초 이용 시 부가통화 제공량에서 1.66초 차감_초과 요금: 음성통화 1.98원/초, 영상통화 3.3원/초, SMS 22원/건, LMS 33원/건, MMS 110원/건_2. 공유 데이터공유 데이터 안내 - 요금제명, 테더링？공유 데이터 통합 한도, 모바일 인터넷전화(mVoIP) 한도, 리...   \n",
              "\n",
              "        domain    start_dt      end_dt   rank  \n",
              "1571   product  20240627.0  99991231.0  403.0  \n",
              "7218   product  20240919.0  99991231.0  467.0  \n",
              "13351  product  20240627.0  99991231.0  163.0  \n",
              "14141  product  20240627.0  99991231.0  825.0  \n",
              "20739  product  20240627.0  99991231.0  232.0  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_pdf_raw = pd.read_csv(\"./data/item_info_all_250527.csv\")\n",
        "\n",
        "item_pdf_all = item_pdf_raw.drop_duplicates(['item_nm','item_id'])[['item_nm','item_id','item_desc','domain','start_dt','end_dt','rank']].copy()\n",
        "\n",
        "\n",
        "# item_pdf = item_pdf_raw.drop_duplicates(['item_nm','item_desc']).copy()\n",
        "# item_pdf['item_item'] = item_pdf['item_nm']+\"\\n\"+item_pdf['item_desc']\n",
        "\n",
        "# item_pdf['item_nm_cl'] = item_pdf['item_nm'].apply(clean_text)\n",
        "# item_pdf['item_desc_cl'] = item_pdf['item_desc'].fillna('').astype(str).apply(clean_text)\n",
        "# item_pdf['item_item_cl'] = item_pdf['item_nm_cl']+\"\\n\"+item_pdf['item_desc_cl']\n",
        "\n",
        "# item_pdf_all = item_pdf_all.merge(item_pdf_pop, on=list(item_pdf_all.columns), how='left')\n",
        "# item_pdf_all['rank'] = item_pdf_all['rank'].fillna(1000000)\n",
        "\n",
        "item_pdf_all.query(\"rank<1000 and item_nm.str.contains('넷플릭스', case=False)\").head()\n",
        "\n",
        "# item_pdf_all.query(\"rank<1000\")[['item_nm']].drop_duplicates().to_csv(\"./data/item_nm_1000.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "43a9f100",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>item_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>NA00008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>5GX 프라임플러스(Netflix)</td>\n",
              "      <td>NA00008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>5GX 프라임Plus(넷플릭스)</td>\n",
              "      <td>NA00008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>5GX 프라임PLUS(넷플릭스)</td>\n",
              "      <td>NA00008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>5GX 프라임플러스(넷플릭스)</td>\n",
              "      <td>5GX 프라임plus(넷플릭스)</td>\n",
              "      <td>NA00008721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36982</th>\n",
              "      <td>5GX 플래티넘(티빙&amp;웨이브)</td>\n",
              "      <td>5GX 플래티넘(티빙&amp;wavve)</td>\n",
              "      <td>NA00009207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36982</th>\n",
              "      <td>5GX 플래티넘(티빙&amp;웨이브)</td>\n",
              "      <td>5GX 플래티넘(TVING&amp;웨이브)</td>\n",
              "      <td>NA00009207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36982</th>\n",
              "      <td>5GX 플래티넘(티빙&amp;웨이브)</td>\n",
              "      <td>5GX 플래티넘(tving&amp;웨이브)</td>\n",
              "      <td>NA00009207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40820</th>\n",
              "      <td>0 청년 109(넷플릭스)</td>\n",
              "      <td>0 청년 109(넷플릭스)</td>\n",
              "      <td>NA00008726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40820</th>\n",
              "      <td>0 청년 109(넷플릭스)</td>\n",
              "      <td>0 청년 109(Netflix)</td>\n",
              "      <td>NA00008726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                item_nm        item_nm_alias     item_id\n",
              "1571   5GX 프라임플러스(넷플릭스)     5GX 프라임플러스(넷플릭스)  NA00008721\n",
              "1571   5GX 프라임플러스(넷플릭스)  5GX 프라임플러스(Netflix)  NA00008721\n",
              "1571   5GX 프라임플러스(넷플릭스)    5GX 프라임Plus(넷플릭스)  NA00008721\n",
              "1571   5GX 프라임플러스(넷플릭스)    5GX 프라임PLUS(넷플릭스)  NA00008721\n",
              "1571   5GX 프라임플러스(넷플릭스)    5GX 프라임plus(넷플릭스)  NA00008721\n",
              "...                 ...                  ...         ...\n",
              "36982  5GX 플래티넘(티빙&웨이브)   5GX 플래티넘(티빙&wavve)  NA00009207\n",
              "36982  5GX 플래티넘(티빙&웨이브)  5GX 플래티넘(TVING&웨이브)  NA00009207\n",
              "36982  5GX 플래티넘(티빙&웨이브)  5GX 플래티넘(tving&웨이브)  NA00009207\n",
              "40820    0 청년 109(넷플릭스)       0 청년 109(넷플릭스)  NA00008726\n",
              "40820    0 청년 109(넷플릭스)    0 청년 109(Netflix)  NA00008726\n",
              "\n",
              "[76 rows x 3 columns]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alia_rule_set = list(zip(pd.read_csv(\"./data/alias_rules.csv\")['alias_1'], pd.read_csv(\"./data/alias_rules.csv\")['alias_2']))\n",
        "\n",
        "def apply_alias_rule(item_nm):\n",
        "    item_nm_list = [item_nm]\n",
        "\n",
        "    for r in alia_rule_set:\n",
        "        if r[0] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[0], r[1]))\n",
        "        if r[1] in item_nm:\n",
        "            item_nm_list.append(item_nm.replace(r[1], r[0]))\n",
        "    return item_nm_list\n",
        "\n",
        "item_pdf_all['item_nm_alias'] = item_pdf_all['item_nm'].apply(apply_alias_rule)\n",
        "\n",
        "item_pdf_all = item_pdf_all.explode('item_nm_alias')\n",
        "\n",
        "item_pdf_all.query(\"rank<1000 and item_nm.str.contains('넷플릭스', case=False) or item_nm.str.contains('웨이브', case=False)\")[['item_nm','item_nm_alias','item_id']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086ad1e0",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">사용자 정의 아이템</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "48391294",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "user_defined_entity = ['AIA Vitality' , '부스트 파크 건대입구' , 'Boost Park 건대입구']\n",
        "item_pdf_ext = pd.DataFrame([{'item_nm':e,'item_id':e,'item_desc':e, 'domain':'user_defined', 'start_dt':20250101, 'end_dt':99991231, 'rank':1, 'item_nm_alias':e} for e in user_defined_entity])\n",
        "item_pdf_all = pd.concat([item_pdf_all,item_pdf_ext])\n",
        "\n",
        "# entity_list_pop = [{\"entity\":e[0], \"data\":e[1]} for e in entity_list_pop] + [{\"entity\":e, \"data\":{'item_id':e,'description':e, 'start_dt':20250101, 'end_dt':99991231, 'rank':1}} for e in user_defined_entity]\n",
        "# entity_list_all = [{\"entity\":e[0], \"data\":e[1]} for e in entity_list_all] + [{\"entity\":e, \"data\":{'item_id':e,'description':e, 'start_dt':20250101, 'end_dt':99991231, 'rank':1}} for e in user_defined_entity]\n",
        "\n",
        "entity_list_for_fuzzy = []\n",
        "for row in item_pdf_all.to_dict('records'):\n",
        "    entity_list_for_fuzzy.append((row['item_nm'], {'item_id':row['item_id'], 'description':row['item_desc'], 'domain':row['domain'], 'start_dt':row['start_dt'], 'end_dt':row['end_dt'], 'rank':1, 'item_nm_alias':row['item_nm_alias']}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "862efc4e",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">제외 상품 이름</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "c1603276",
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_item_names = pd.read_csv(\"./data/stop_words.csv\")['stop_words'].to_list()\n",
        "\n",
        "# stop_item_names = stop_item_names_str.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57c8820",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">Bert Model Loading</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "c491fc5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "model = SentenceTransformer('jhgan/ko-sbert-nli')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "6f9e944d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mms_pdf.query(\"msg.str.contains('넷플릭스') and offer_dt>'20240101'\")['msg'].to_list()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "c77f4a2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "msg_text_list = [\"\"\"\n",
        "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
        "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:통화 부가서비스를 패키지로 저렴하게!\n",
        "광고 내용:(광고)[SKT] 콜링플러스 이용 안내  #04 고객님, 안녕하세요. <콜링플러스>에 가입하고 콜키퍼, 컬러링, 통화가능통보플러스까지 총 3가지의 부가서비스를 패키지로 저렴하게 이용해보세요.  ■ 콜링플러스 - 이용요금: 월 1,650원, 부가세 포함 - 콜키퍼(550원), 컬러링(990원), 통화가능통보플러스(770원)를 저렴하게 이용할 수 있는 상품  ■ 콜링플러스 가입 방법 - T월드 앱: 오른쪽 위에 있는 돋보기를 눌러 콜링플러스 검색 > 가입  ▶ 콜링플러스 가입하기: http://t-mms.kr/t.do?m=#61&u=https://skt.sh/17tNH  ■ 유의 사항 - 콜링플러스에 가입하면 기존에 이용 중인 콜키퍼, 컬러링, 통화가능통보플러스 서비스는 자동으로 해지됩니다. - 기존에 구매한 컬러링 음원은 콜링플러스 가입 후에도 계속 이용할 수 있습니다.(시간대, 발신자별 설정 정보는 다시 설정해야 합니다.)  * 최근 다운로드한 음원은 보관함에서 무료로 재설정 가능(다운로드한 날로부터 1년 이내)   ■ 문의: SKT 고객센터(114)  SKT와 함께해주셔서 감사합니다.  무료 수신거부 1504\\n    ', \n",
        "\"\"\",\n",
        "\"\"\"\n",
        "(광고)[SKT] 1월 0 day 혜택 안내_ _[1월 20일(토) 혜택]_만 13~34세 고객이라면 _CU에서 핫바 1,000원에 구매 하세요!_(선착순 1만 명 증정)_▶ 자세히 보기 : http://t-mms.kr/t.do?m=#61&s=24264&a=&u=https://bit.ly/3H2OHSs__■ 에이닷 X T 멤버십 구독캘린더 이벤트_0 day 일정을 에이닷 캘린더에 등록하고 혜택 날짜에 알림을 받아보세요! _알림 설정하면 추첨을 통해 [스타벅스 카페 라떼tall 모바일쿠폰]을 드립니다. _▶ 이벤트 참여하기 : https://bit.ly/3RVSojv_ _■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "  '[T 우주] 넷플릭스와 웨이브를 월 9,900원에! \\n(광고)[SKT] 넷플릭스+웨이브 월 9,900원, 이게 되네! __#04 고객님,_넷플릭스와 웨이브 둘 다 보고 싶었지만, 가격 때문에 망설이셨다면 지금이 바로 기회! __오직 T 우주에서만, _2개월 동안 월 9,900원에 넷플릭스와 웨이브를 모두 즐기실 수 있습니다.__8월 31일까지만 드리는 혜택이니, 지금 바로 가입해 보세요! __■ 우주패스 Netflix 런칭 프로모션 _- 기간 : 2024년 8월 31일(토)까지_- 혜택 : 우주패스 Netflix(광고형 스탠다드)를 2개월 동안 월 9,900원에 이용 가능한 쿠폰 제공_▶ 프로모션 자세히 보기: http://t-mms.kr/jAs/#74__■ 우주패스 Netflix(월 12,000원)  _- 기본 혜택 : Netflix 광고형 스탠다드 멤버십_- 추가 혜택 : Wavve 콘텐츠 팩 _* 추가 요금을 내시면 Netflix 스탠다드와 프리미엄 멤버십 상품으로 가입 가능합니다.  __■ 유의 사항_-  프로모션 쿠폰은 1인당 1회 다운로드 가능합니다. _-  쿠폰 할인 기간이 끝나면 정상 이용금액으로 자동 결제 됩니다. __■ 문의: T 우주 고객센터 (1505, 무료)__나만의 구독 유니버스, T 우주 __무료 수신거부 1504'\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "광고 제목:[SK텔레콤] T건강습관 X AIA Vitality, 우리 가족의 든든한 보험!\n",
        "광고 내용:(광고)[SKT] 가족의 든든한 보험 (무배당)AIA Vitality 베스트핏 보장보험 안내  고객님, 안녕하세요. 4인 가족 표준생계비, 준비하고 계시나요? (무배당)AIA Vitality 베스트핏 보장보험(디지털 전용)으로 최대 20% 보험료 할인과 가족의 든든한 보험 보장까지 누려 보세요.   ▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&u=https://bit.ly/36oWjgX  ■ AIA Vitality  혜택 - 매달 리워드 최대 12,000원 - 등급 업그레이드 시 특별 리워드 - T건강습관 제휴 할인 최대 40% ※ 제휴사별 할인 조건과 주간 미션 달성 혜택 등 자세한 내용은 AIA Vitality 사이트에서 확인하세요. ※ 이 광고는 AIA생명의 광고이며 SK텔레콤은 모집 행위를 하지 않습니다.  - 보험료 납입 기간 중 피보험자가 장해분류표 중 동일한 재해 또는 재해 이외의 동일한 원인으로 여러 신체 부위의 장해지급률을 더하여 50% 이상인 장해 상태가 된 경우 차회 이후의 보험료 납입 면제 - 사망보험금은 계약일(부활일/효력회복일)로부터 2년 안에 자살한 경우 보장하지 않음 - 일부 특약 갱신 시 보험료 인상 가능 - 기존 계약 해지 후 신계약 체결 시 보험인수 거절, 보험료 인상, 보장 내용 변경 가능 - 해약 환급금(또는 만기 시 보험금이나 사고보험금)에 기타 지급금을 합해 5천만 원까지(본 보험 회사 모든 상품 합산) 예금자 보호 - 계약 체결 전 상품 설명서 및 약관 참조 - 월 보험료 5,500원(부가세 포함)  * 생명보험협회 심의필 제2020-03026호(2020-09-22) COM-2020-09-32426  ■문의: 청약 관련(1600-0880)  무료 수신거부 1504    \n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "message_idx = 0\n",
        "\n",
        "mms_msg = msg_text_list[message_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bfff73",
      "metadata": {},
      "source": [
        "<h2 style=\"color:rgb(102, 34, 0); font-size: 1.5em;\">PGM 준비</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "5bb714f5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pgm_id</th>\n",
              "      <th>pgm_nm</th>\n",
              "      <th>clue_tag</th>\n",
              "      <th>sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2019CCEPM01</td>\n",
              "      <td>[마케팅_Care]사용법/혜택/이벤트안내_T멤버십</td>\n",
              "      <td>멤버십, Tday, T day, VIP, 등급, 할인, 쿠폰, 이벤트, 혜택, 적립, 무료</td>\n",
              "      <td>0.666757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2019CCEPM03</td>\n",
              "      <td>[마케팅_Care]사용법/혜택/이벤트안내_SKT</td>\n",
              "      <td>이벤트, 사용법, 상품, 부가서비스, 부가 서비스, 혜택, 안내, 할인, 무료, 팁, 추천</td>\n",
              "      <td>0.658921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2021CCEET20</td>\n",
              "      <td>[마케팅_Care]사용법/혜택/이벤트안내_구독</td>\n",
              "      <td>우주, 구독, 혜택, 이벤트, T우주, T 우주, 콘텐츠, 할인, 추천, 사용법, 서비스</td>\n",
              "      <td>0.633909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2024SCEPM02</td>\n",
              "      <td>[마케팅_Sales]타사회선(가망)_win-back</td>\n",
              "      <td>쓰던폰그대로, 15990111, 통신사이동, 통신사 이동, 번호이동, 번호 이동, 전환, 혜택, 할인, KT, LG, 알뜰폰</td>\n",
              "      <td>0.618190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>#2025ABCD02</td>\n",
              "      <td>[마케팅_Care]사용법/혜택/이벤트안내_AI</td>\n",
              "      <td>에이닷, 이벤트, 사용법, AI, 기능, 업데이트, 혜택, 추천, 서비스, 안내</td>\n",
              "      <td>0.616425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         pgm_id                        pgm_nm  \\\n",
              "21  2019CCEPM01   [마케팅_Care]사용법/혜택/이벤트안내_T멤버십   \n",
              "23  2019CCEPM03    [마케팅_Care]사용법/혜택/이벤트안내_SKT   \n",
              "20  2021CCEET20     [마케팅_Care]사용법/혜택/이벤트안내_구독   \n",
              "16  2024SCEPM02  [마케팅_Sales]타사회선(가망)_win-back   \n",
              "22  #2025ABCD02     [마케팅_Care]사용법/혜택/이벤트안내_AI   \n",
              "\n",
              "                                                                 clue_tag  \\\n",
              "21                     멤버십, Tday, T day, VIP, 등급, 할인, 쿠폰, 이벤트, 혜택, 적립, 무료   \n",
              "23                     이벤트, 사용법, 상품, 부가서비스, 부가 서비스, 혜택, 안내, 할인, 무료, 팁, 추천   \n",
              "20                      우주, 구독, 혜택, 이벤트, T우주, T 우주, 콘텐츠, 할인, 추천, 사용법, 서비스   \n",
              "16  쓰던폰그대로, 15990111, 통신사이동, 통신사 이동, 번호이동, 번호 이동, 전환, 혜택, 할인, KT, LG, 알뜰폰   \n",
              "22                           에이닷, 이벤트, 사용법, AI, 기능, 업데이트, 혜택, 추천, 서비스, 안내   \n",
              "\n",
              "         sim  \n",
              "21  0.666757  \n",
              "23  0.658921  \n",
              "20  0.633909  \n",
              "16  0.618190  \n",
              "22  0.616425  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "num_cand_pgms = 5\n",
        "\n",
        "pgm_pdf = pd.read_csv(\"./data/pgm_tag_ext_250516.csv\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 특수문자를 공백으로 변환\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # 여러 공백을 하나로 통일\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 앞뒤 공백 제거\n",
        "    return text.strip()\n",
        "\n",
        "clue_embeddings = model.encode(pgm_pdf[[\"pgm_nm\",\"clue_tag\"]].apply(lambda x: preprocess_text(x['pgm_nm'].lower())+\" \"+x['clue_tag'].lower(), axis=1).tolist(), convert_to_tensor=True)\n",
        "\n",
        "mms_embedding = model.encode([mms_msg.lower()], convert_to_tensor=True)\n",
        "\n",
        "similarities = torch.nn.functional.cosine_similarity(\n",
        "    mms_embedding,  \n",
        "    clue_embeddings,  \n",
        "    dim=1 \n",
        ").cpu().numpy()\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf.copy()\n",
        "pgm_pdf_tmp['sim'] = similarities\n",
        "\n",
        "pgm_pdf_tmp = pgm_pdf_tmp.sort_values('sim', ascending=False)\n",
        "\n",
        "pgm_pdf_tmp[['pgm_id','pgm_nm','clue_tag','sim']].head(num_cand_pgms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b4a433",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">개체명 추출 후 LLM</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "084bfa61",
      "metadata": {},
      "outputs": [],
      "source": [
        "from kiwipiepy import Kiwi\n",
        "\n",
        "kiwi = Kiwi()\n",
        "\n",
        "stop_item_names = list(set(stop_item_names + [x.lower() for x in stop_item_names]))\n",
        "\n",
        "entity_list_for_kiwi = list(item_pdf_all['item_nm_alias'].unique())\n",
        "\n",
        "for w in entity_list_for_kiwi:\n",
        "    kiwi.add_user_word(w, \"NNP\") \n",
        "\n",
        "for w in stop_item_names:\n",
        "    kiwi.add_user_word(w, \"NNG\") \n",
        "\n",
        "# result = kiwi.analyze(msg_text_list[0])\n",
        "# result = kiwi.tokenize(msg_text_list[4])\n",
        "# result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2daf43",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">불용어 사전 구축</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "f9e38616",
      "metadata": {},
      "outputs": [],
      "source": [
        "# kiwi = Kiwi()\n",
        "# result_iter = kiwi.analyze(mms_pdf['msg'].tolist(), normalize_coda=True)\n",
        "# analysis_list = sum(list(result_iter),[])\n",
        "\n",
        "# from collections import Counter\n",
        "# entities = []\n",
        "# for result_msg in analysis_list:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "#     for t in result_msg[0]:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "#         if t[1] == 'NNP' and not t[0].lower() in stop_item_names:  # 고유명사인 경우\n",
        "#         # if token.tag == 'NNG' and token.form in stop_item_names_ext:  # 고유명사인 경우\n",
        "#             entities.append(t[0])\n",
        "\n",
        "# counter = Counter(entities)\n",
        "# # result_nnp = [{\"item\": item, \"frequency\": freq} for item, freq in counter.items()]\n",
        "# result_nnp = [{\"item\": item, \"frequency\": freq} for item, freq in counter.most_common()]\n",
        "\n",
        "# nnp_in_vaca = [w for w in result_nnp if w['item'] in entity_list_ext]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7addaf61",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">개체명 사전 생성 (Kiwi)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "f6e782a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "kiwi_raw = Kiwi()\n",
        "kiwi_raw.space_tolerance = 2\n",
        "\n",
        "tags_to_exclude = ['W_SERIAL','W_URL','JKO','SSO','SSC','SW','SF','SP','SS','SE','SO','SB','SH']\n",
        "\n",
        "# edf = pd.DataFrame([{\"entity\":e[0], \"data\":e[1]} for e in entity_list_all] + [{\"entity\":e, \"data\":{'item_id':e,'description':e, 'start_dt':20250101, 'end_dt':99991231, 'rank':1}} for e in user_defined_entity])\n",
        "# edf['item_id'] = edf['data'].apply(lambda x: x['item_id'])\n",
        "\n",
        "edf = item_pdf_all.copy()\n",
        "\n",
        "\n",
        "# result_iter = kiwi_raw.analyze([e[0] for e in entity_list_all], normalize_coda=True)\n",
        "# analysis_list = sum(list(result_iter),[])\n",
        "# analysis_list = [l[0] for l in analysis_list]\n",
        "# edf['token_entity'] = analysis_list\n",
        "\n",
        "edf['token_entity'] = edf.apply(lambda x: kiwi_raw.tokenize(x['item_nm_alias'], normalize_coda=True, z_coda=False, split_complex=False), axis=1)\n",
        "# edf['token_entity'] = edf.apply(lambda x: [{'form_entity':d[0],'tag_entity':d[1],'start_entity':d[2],'end_entity':d[2]+d[3]} for d in x['token_entity']], axis=1)\n",
        "edf['token_entity'] = edf.apply(lambda x: [d[0] for d in x['token_entity'] if d[1] not in tags_to_exclude], axis=1)\n",
        "\n",
        "# edf = edf.explode('token_entity')\n",
        "# edf = pd.concat([edf.reset_index(drop=True), pd.json_normalize(edf['token_entity']).reset_index(drop=True)], axis=1)\n",
        "\n",
        "edf['char_entity'] = edf.apply(lambda x: list(x['item_nm_alias'].lower().replace(' ', '')), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d307f24c",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">메세지 토큰화 (Kiwi)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "9e313a6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
            "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
            "\n",
            "추출된 개체명: ['시크릿', '베어유', '에이닷이빵쏜닷', '0 day', '데일리', '뚜레쥬르']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[['광고'], ['NNG']],\n",
              " [['제목'], ['NNG']],\n",
              " [['SK텔레콤'], ['NNP']],\n",
              " [['2'], ['SN']],\n",
              " [['월'], ['NNB']],\n",
              " [['0'], ['SN']],\n",
              " [['day'], ['SL']],\n",
              " [['혜택'], ['NNG']],\n",
              " [['안내'], ['NNG']],\n",
              " [['광고'], ['NNG']]]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def filter_specific_terms(strings: List[str]) -> List[str]:\n",
        "    unique_strings = list(set(strings))  # 중복 제거\n",
        "    unique_strings.sort(key=len, reverse=True)  # 길이 기준 내림차순 정렬\n",
        "\n",
        "    filtered = []\n",
        "    for s in unique_strings:\n",
        "        if not any(s in other for other in filtered):\n",
        "            filtered.append(s)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def sliding_window_with_step(data, window_size, step=1):\n",
        "    \"\"\"Sliding window with configurable step size.\"\"\"\n",
        "    return [data[i:i + window_size] for i in range(0, len(data) - window_size + 1, step)]\n",
        "\n",
        "print(mms_msg)\n",
        "\n",
        "# tdf = pd.DataFrame([{'form_text':d[0],'tag_text':d[1],'start_text':d[2],'end_text':d[2]+d[3]} for d in kiwi_raw.analyze(mms_msg)[0][0]])\n",
        "result_msg_raw = kiwi_raw.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "token_list_msg = [d for d in result_msg_raw \n",
        "                  if d[1] not in tags_to_exclude\n",
        "                  ]\n",
        "\n",
        "result_msg = kiwi.tokenize(mms_msg, normalize_coda=True, z_coda=False, split_complex=False)\n",
        "entities_from_kiwi = []\n",
        "for token in result_msg:  # 첫 번째 분석 결과의 토큰 리스트\n",
        "    if token.tag == 'NNP' and token.form not in stop_item_names+['-'] and len(token.form)>=2 and not token.form.lower() in stop_item_names:  # 고유명사인 경우\n",
        "    # if token.tag == 'NNG' and token.form in stop_item_names_ext:  # 고유명사인 경우\n",
        "        entities_from_kiwi.append(token.form)\n",
        "\n",
        "from typing import List\n",
        "# 결과\n",
        "entities_from_kiwi = filter_specific_terms(entities_from_kiwi)\n",
        "\n",
        "print(\"추출된 개체명:\", list(set(entities_from_kiwi)))\n",
        "\n",
        "exc_tag_patterns = [\n",
        "    ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],\n",
        "    ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'], ['VV', 'ETN']\n",
        "]+[[t] for t in tags_to_exclude]\n",
        "\n",
        "ngram_list_msg = []\n",
        "for w_size in range(1,5):\n",
        "    windows = sliding_window_with_step(token_list_msg, w_size, step=1)\n",
        "    windows_new = []\n",
        "    for w in windows:\n",
        "        tag_str = ','.join([t.tag for t in w])\n",
        "        flag = True\n",
        "        for et in exc_tag_patterns:\n",
        "            if ','.join(et) in tag_str:\n",
        "                flag = False\n",
        "                # print(w)\n",
        "                break\n",
        "    \n",
        "        if flag:\n",
        "            windows_new.append([[d.form for d in w], [d.tag for d in w]])\n",
        "\n",
        "    ngram_list_msg.extend(windows_new)\n",
        "\n",
        "ngram_list_msg[:10]\n",
        "\n",
        "# edf['ngram_list_msg'] = edf.apply(lambda row: ngram_list_msg.copy(), axis=1)\n",
        "# edf = edf.explode('ngram_list_msg')\n",
        "\n",
        "\n",
        "\n",
        "# edf['ngram_list_form'] = edf['ngram_list_msg'].apply(lambda x: [d.form for d in x])\n",
        "# edf['ngram_list_tag'] = edf['ngram_list_msg'].apply(lambda x: [d.tag for d in x])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b384e2",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">메세지 정제</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "1208922d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Token:\n",
        "    def __init__(self, form, tag, start, len):\n",
        "        self.form = form\n",
        "        self.tag = tag\n",
        "        self.start = start\n",
        "        self.len = len\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Token(form='{self.form}', tag='{self.tag}', start={self.start}, len={self.len})\"\n",
        "\n",
        "# 제외할 품사 패턴\n",
        "exc_tag_patterns = [\n",
        "    ['SN', 'NNB'], ['W_SERIAL'], ['JKO'], ['W_URL'], ['W_EMAIL'],\n",
        "    ['XSV', 'EC'], ['VV', 'EC'], ['VCP', 'ETM'], ['XSA', 'ETM'], ['VV', 'ETN']\n",
        "]+[[t] for t in tags_to_exclude]\n",
        "\n",
        "# 패턴에 해당하는 토큰 인덱스 찾기\n",
        "def find_pattern_indices(tokens, patterns):\n",
        "    indices_to_exclude = set()\n",
        "    \n",
        "    # 단일 태그 패턴 먼저 체크\n",
        "    for i in range(len(tokens)):\n",
        "        for pattern in patterns:\n",
        "            if len(pattern) == 1 and tokens[i].tag == pattern[0]:\n",
        "                indices_to_exclude.add(i)\n",
        "    \n",
        "    # 연속된 패턴 검사\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i in indices_to_exclude:\n",
        "            i += 1\n",
        "            continue\n",
        "            \n",
        "        for pattern in patterns:\n",
        "            if len(pattern) > 1:  # 두 개 이상의 태그로 구성된 패턴\n",
        "                if i + len(pattern) <= len(tokens):  # 패턴 길이만큼 토큰이 남아있는지 확인\n",
        "                    match = True\n",
        "                    for j in range(len(pattern)):\n",
        "                        if tokens[i+j].tag != pattern[j]:\n",
        "                            match = False\n",
        "                            break\n",
        "                    \n",
        "                    if match:  # 패턴이 일치하면 해당 토큰들의 인덱스를 모두 추가\n",
        "                        for j in range(len(pattern)):\n",
        "                            indices_to_exclude.add(i+j)\n",
        "        i += 1\n",
        "                    \n",
        "    return indices_to_exclude\n",
        "\n",
        "# 패턴에 해당하지 않는 토큰만 필터링\n",
        "def filter_tokens_by_patterns(tokens, patterns):\n",
        "    indices_to_exclude = find_pattern_indices(tokens, patterns)\n",
        "    return [tokens[i] for i in range(len(tokens)) if i not in indices_to_exclude]\n",
        "\n",
        "# 제외된 토큰 없이 텍스트 재구성 - 단순 연결 방식\n",
        "def reconstruct_text_without_spaces(tokens):\n",
        "    # 토큰들을 원래 시작 위치에 따라 정렬\n",
        "    sorted_tokens = sorted(tokens, key=lambda token: token.start)\n",
        "    \n",
        "    result = []\n",
        "    for token in sorted_tokens:\n",
        "        result.append(token.form)\n",
        "    \n",
        "    # 토큰들을 공백 하나로 구분하여 결합\n",
        "    return ' '.join(result)\n",
        "\n",
        "# 더 자연스러운 텍스트 재구성 - 원본 위치 기반 보존, 제외된 토큰은 건너뜀\n",
        "def reconstruct_text_preserved_positions(original_tokens, filtered_tokens):\n",
        "    # 원본 토큰의 위치와 형태를 기록할 사전 생성\n",
        "    token_map = {}\n",
        "    for i, token in enumerate(original_tokens):\n",
        "        token_map[(token.start, token.len)] = (i, token.form)\n",
        "    \n",
        "    # 필터링된 토큰의 인덱스 찾기\n",
        "    filtered_indices = set()\n",
        "    for token in filtered_tokens:\n",
        "        key = (token.start, token.len)\n",
        "        if key in token_map:\n",
        "            filtered_indices.add(token_map[key][0])\n",
        "    \n",
        "    # 원본 순서대로 필터링된 토큰만 선택\n",
        "    result = []\n",
        "    for i, token in enumerate(original_tokens):\n",
        "        if i in filtered_indices:\n",
        "            result.append(token.form)\n",
        "    \n",
        "    return ' '.join(result)\n",
        "\n",
        "# 결과 출력\n",
        "filtered_tokens = filter_tokens_by_patterns(result_msg_raw, exc_tag_patterns)\n",
        "msg_text_filtered = reconstruct_text_preserved_positions(result_msg_raw, filtered_tokens)\n",
        "\n",
        "msg_text_filtered\n",
        "\n",
        "ngram_list_msg_filtered = []\n",
        "for w_size in range(2,4):\n",
        "    windows = sliding_window_with_step(list(msg_text_filtered.lower().replace(' ', '')), w_size, step=1)\n",
        "    ngram_list_msg_filtered.extend(windows)\n",
        "\n",
        "# ngram_list_msg_filtered[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c8af59",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">Joint PDF 생성</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "8e60a188",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_desc</th>\n",
              "      <th>domain</th>\n",
              "      <th>start_dt</th>\n",
              "      <th>end_dt</th>\n",
              "      <th>rank</th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>token_entity</th>\n",
              "      <th>char_entity</th>\n",
              "      <th>token_txt</th>\n",
              "      <th>token_tag</th>\n",
              "      <th>char_msg</th>\n",
              "      <th>token_txt_str</th>\n",
              "      <th>token_tag_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>A1J7</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20191021.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>[IPAD, 10.2]</td>\n",
              "      <td>[i, p, a, d, 1, 0, ., 2]</td>\n",
              "      <td>[10]</td>\n",
              "      <td>[SN]</td>\n",
              "      <td>[1, 0]</td>\n",
              "      <td>10</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>A1J7</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20191021.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>[IPAD, 10.2]</td>\n",
              "      <td>[i, p, a, d, 1, 0, ., 2]</td>\n",
              "      <td>[월, 10]</td>\n",
              "      <td>[NNB, SN]</td>\n",
              "      <td>[월, 1, 0]</td>\n",
              "      <td>월,10</td>\n",
              "      <td>NNB,SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>A1J7</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20191021.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>아이패드 10.2</td>\n",
              "      <td>[아이패드, 10.2]</td>\n",
              "      <td>[아, 이, 패, 드, 1, 0, ., 2]</td>\n",
              "      <td>[10]</td>\n",
              "      <td>[SN]</td>\n",
              "      <td>[1, 0]</td>\n",
              "      <td>10</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>IPAD 10.2</td>\n",
              "      <td>A1J7</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20191021.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>아이패드 10.2</td>\n",
              "      <td>[아이패드, 10.2]</td>\n",
              "      <td>[아, 이, 패, 드, 1, 0, ., 2]</td>\n",
              "      <td>[월, 10]</td>\n",
              "      <td>[NNB, SN]</td>\n",
              "      <td>[월, 1, 0]</td>\n",
              "      <td>월,10</td>\n",
              "      <td>NNB,SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>KTF_IPAD PRO 12.9 5세대 2T(리퍼단말)</td>\n",
              "      <td>A36X</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20210524.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>KTF_IPAD PRO 12.9 5세대 2T(리퍼단말)</td>\n",
              "      <td>[KTF, IPAD, PRO, 12.9, 5, 세대, 2, T, 리퍼단말]</td>\n",
              "      <td>[k, t, f, _, i, p, a, d, p, r, o, 1, 2, ., 9, 5, 세, 대, 2, t, (, 리, 퍼, 단, 말, )]</td>\n",
              "      <td>[광고, SKT]</td>\n",
              "      <td>[NNG, SL]</td>\n",
              "      <td>[광, 고, s, k, t]</td>\n",
              "      <td>광고,SKT</td>\n",
              "      <td>NNG,SL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369539</th>\n",
              "      <td>스마트차량관제50</td>\n",
              "      <td>PD00001634</td>\n",
              "      <td>#</td>\n",
              "      <td>product</td>\n",
              "      <td>20131201.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>SMART차량관제50</td>\n",
              "      <td>[SMART, 차량, 관제, 50]</td>\n",
              "      <td>[s, m, a, r, t, 차, 량, 관, 제, 5, 0]</td>\n",
              "      <td>[무료, 수신, 거부, 1504]</td>\n",
              "      <td>[NNG, NNG, NNG, SN]</td>\n",
              "      <td>[무, 료, 수, 신, 거, 부, 1, 5, 0, 4]</td>\n",
              "      <td>무료,수신,거부,1504</td>\n",
              "      <td>NNG,NNG,NNG,SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369540</th>\n",
              "      <td>스마트차량관제50</td>\n",
              "      <td>PD00001634</td>\n",
              "      <td>#</td>\n",
              "      <td>product</td>\n",
              "      <td>20131201.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>smart차량관제50</td>\n",
              "      <td>[smart, 차량, 관제, 50]</td>\n",
              "      <td>[s, m, a, r, t, 차, 량, 관, 제, 5, 0]</td>\n",
              "      <td>[1504]</td>\n",
              "      <td>[SN]</td>\n",
              "      <td>[1, 5, 0, 4]</td>\n",
              "      <td>1504</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369541</th>\n",
              "      <td>스마트차량관제50</td>\n",
              "      <td>PD00001634</td>\n",
              "      <td>#</td>\n",
              "      <td>product</td>\n",
              "      <td>20131201.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>smart차량관제50</td>\n",
              "      <td>[smart, 차량, 관제, 50]</td>\n",
              "      <td>[s, m, a, r, t, 차, 량, 관, 제, 5, 0]</td>\n",
              "      <td>[거부, 1504]</td>\n",
              "      <td>[NNG, SN]</td>\n",
              "      <td>[거, 부, 1, 5, 0, 4]</td>\n",
              "      <td>거부,1504</td>\n",
              "      <td>NNG,SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369542</th>\n",
              "      <td>스마트차량관제50</td>\n",
              "      <td>PD00001634</td>\n",
              "      <td>#</td>\n",
              "      <td>product</td>\n",
              "      <td>20131201.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>smart차량관제50</td>\n",
              "      <td>[smart, 차량, 관제, 50]</td>\n",
              "      <td>[s, m, a, r, t, 차, 량, 관, 제, 5, 0]</td>\n",
              "      <td>[수신, 거부, 1504]</td>\n",
              "      <td>[NNG, NNG, SN]</td>\n",
              "      <td>[수, 신, 거, 부, 1, 5, 0, 4]</td>\n",
              "      <td>수신,거부,1504</td>\n",
              "      <td>NNG,NNG,SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369543</th>\n",
              "      <td>스마트차량관제50</td>\n",
              "      <td>PD00001634</td>\n",
              "      <td>#</td>\n",
              "      <td>product</td>\n",
              "      <td>20131201.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>smart차량관제50</td>\n",
              "      <td>[smart, 차량, 관제, 50]</td>\n",
              "      <td>[s, m, a, r, t, 차, 량, 관, 제, 5, 0]</td>\n",
              "      <td>[무료, 수신, 거부, 1504]</td>\n",
              "      <td>[NNG, NNG, NNG, SN]</td>\n",
              "      <td>[무, 료, 수, 신, 거, 부, 1, 5, 0, 4]</td>\n",
              "      <td>무료,수신,거부,1504</td>\n",
              "      <td>NNG,NNG,NNG,SN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>233086 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               item_nm     item_id   item_desc         domain  \\\n",
              "46                           IPAD 10.2        A1J7  모바일 단말기 이름  mobile_device   \n",
              "47                           IPAD 10.2        A1J7  모바일 단말기 이름  mobile_device   \n",
              "48                           IPAD 10.2        A1J7  모바일 단말기 이름  mobile_device   \n",
              "49                           IPAD 10.2        A1J7  모바일 단말기 이름  mobile_device   \n",
              "51      KTF_IPAD PRO 12.9 5세대 2T(리퍼단말)        A36X  모바일 단말기 이름  mobile_device   \n",
              "...                                ...         ...         ...            ...   \n",
              "369539                       스마트차량관제50  PD00001634           #        product   \n",
              "369540                       스마트차량관제50  PD00001634           #        product   \n",
              "369541                       스마트차량관제50  PD00001634           #        product   \n",
              "369542                       스마트차량관제50  PD00001634           #        product   \n",
              "369543                       스마트차량관제50  PD00001634           #        product   \n",
              "\n",
              "          start_dt      end_dt       rank                   item_nm_alias  \\\n",
              "46      20191021.0  99991231.0  1000000.0                       IPAD 10.2   \n",
              "47      20191021.0  99991231.0  1000000.0                       IPAD 10.2   \n",
              "48      20191021.0  99991231.0  1000000.0                       아이패드 10.2   \n",
              "49      20191021.0  99991231.0  1000000.0                       아이패드 10.2   \n",
              "51      20210524.0  99991231.0  1000000.0  KTF_IPAD PRO 12.9 5세대 2T(리퍼단말)   \n",
              "...            ...         ...        ...                             ...   \n",
              "369539  20131201.0  99991231.0  1000000.0                     SMART차량관제50   \n",
              "369540  20131201.0  99991231.0  1000000.0                     smart차량관제50   \n",
              "369541  20131201.0  99991231.0  1000000.0                     smart차량관제50   \n",
              "369542  20131201.0  99991231.0  1000000.0                     smart차량관제50   \n",
              "369543  20131201.0  99991231.0  1000000.0                     smart차량관제50   \n",
              "\n",
              "                                     token_entity  \\\n",
              "46                                   [IPAD, 10.2]   \n",
              "47                                   [IPAD, 10.2]   \n",
              "48                                   [아이패드, 10.2]   \n",
              "49                                   [아이패드, 10.2]   \n",
              "51      [KTF, IPAD, PRO, 12.9, 5, 세대, 2, T, 리퍼단말]   \n",
              "...                                           ...   \n",
              "369539                        [SMART, 차량, 관제, 50]   \n",
              "369540                        [smart, 차량, 관제, 50]   \n",
              "369541                        [smart, 차량, 관제, 50]   \n",
              "369542                        [smart, 차량, 관제, 50]   \n",
              "369543                        [smart, 차량, 관제, 50]   \n",
              "\n",
              "                                                                           char_entity  \\\n",
              "46                                                            [i, p, a, d, 1, 0, ., 2]   \n",
              "47                                                            [i, p, a, d, 1, 0, ., 2]   \n",
              "48                                                            [아, 이, 패, 드, 1, 0, ., 2]   \n",
              "49                                                            [아, 이, 패, 드, 1, 0, ., 2]   \n",
              "51      [k, t, f, _, i, p, a, d, p, r, o, 1, 2, ., 9, 5, 세, 대, 2, t, (, 리, 퍼, 단, 말, )]   \n",
              "...                                                                                ...   \n",
              "369539                                               [s, m, a, r, t, 차, 량, 관, 제, 5, 0]   \n",
              "369540                                               [s, m, a, r, t, 차, 량, 관, 제, 5, 0]   \n",
              "369541                                               [s, m, a, r, t, 차, 량, 관, 제, 5, 0]   \n",
              "369542                                               [s, m, a, r, t, 차, 량, 관, 제, 5, 0]   \n",
              "369543                                               [s, m, a, r, t, 차, 량, 관, 제, 5, 0]   \n",
              "\n",
              "                 token_txt            token_tag  \\\n",
              "46                    [10]                 [SN]   \n",
              "47                 [월, 10]            [NNB, SN]   \n",
              "48                    [10]                 [SN]   \n",
              "49                 [월, 10]            [NNB, SN]   \n",
              "51               [광고, SKT]            [NNG, SL]   \n",
              "...                    ...                  ...   \n",
              "369539  [무료, 수신, 거부, 1504]  [NNG, NNG, NNG, SN]   \n",
              "369540              [1504]                 [SN]   \n",
              "369541          [거부, 1504]            [NNG, SN]   \n",
              "369542      [수신, 거부, 1504]       [NNG, NNG, SN]   \n",
              "369543  [무료, 수신, 거부, 1504]  [NNG, NNG, NNG, SN]   \n",
              "\n",
              "                              char_msg  token_txt_str   token_tag_str  \n",
              "46                              [1, 0]             10              SN  \n",
              "47                           [월, 1, 0]           월,10          NNB,SN  \n",
              "48                              [1, 0]             10              SN  \n",
              "49                           [월, 1, 0]           월,10          NNB,SN  \n",
              "51                     [광, 고, s, k, t]         광고,SKT          NNG,SL  \n",
              "...                                ...            ...             ...  \n",
              "369539  [무, 료, 수, 신, 거, 부, 1, 5, 0, 4]  무료,수신,거부,1504  NNG,NNG,NNG,SN  \n",
              "369540                    [1, 5, 0, 4]           1504              SN  \n",
              "369541              [거, 부, 1, 5, 0, 4]        거부,1504          NNG,SN  \n",
              "369542        [수, 신, 거, 부, 1, 5, 0, 4]     수신,거부,1504      NNG,NNG,SN  \n",
              "369543  [무, 료, 수, 신, 거, 부, 1, 5, 0, 4]  무료,수신,거부,1504  NNG,NNG,NNG,SN  \n",
              "\n",
              "[233086 rows x 15 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col_for_form_tmp_ent = 'char_entity'\n",
        "col_for_form_tmp_msg = 'char_msg'\n",
        "\n",
        "\n",
        "edf['form_tmp'] = edf[col_for_form_tmp_ent].apply(lambda x: [' '.join(s) for s in sliding_window_with_step(x, 2, step=1)])\n",
        "\n",
        "tdf = pd.DataFrame(ngram_list_msg).rename(columns={0:'token_txt', 1:'token_tag'})\n",
        "tdf['token_key'] = tdf.apply(lambda x: ''.join(x['token_txt'])+''.join(x['token_tag']), axis=1)\n",
        "tdf = tdf.drop_duplicates(['token_key']).drop(['token_key'], axis=1)\n",
        "tdf['char_msg'] = tdf.apply(lambda x: list((\" \".join(x['token_txt'])).lower().replace(' ', '')), axis=1)\n",
        "\n",
        "tdf['form_tmp'] = tdf[col_for_form_tmp_msg].apply(lambda x: [' '.join(s) for s in sliding_window_with_step(x, 2, step=1)])\n",
        "tdf['token_txt_str'] = tdf['token_txt'].str.join(',')\n",
        "tdf['token_tag_str'] = tdf['token_tag'].str.join(',')\n",
        "\n",
        "# tdf['txt'] = tdf.apply(lambda x: ' '.join(x['token_txt']), axis=1) \n",
        "\n",
        "fdf = edf.explode('form_tmp').merge(tdf.explode('form_tmp'), on='form_tmp').drop(['form_tmp'], axis=1)\n",
        "\n",
        "\n",
        "fdf = fdf.query(\"item_nm_alias.str.lower() not in @stop_item_names and token_txt_str.replace(',','').str.lower() not in @stop_item_names\").drop_duplicates(['item_nm','item_nm_alias','item_id','token_txt_str','token_tag_str'])\n",
        "\n",
        "fdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "f4ad4cc6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_desc</th>\n",
              "      <th>domain</th>\n",
              "      <th>start_dt</th>\n",
              "      <th>end_dt</th>\n",
              "      <th>rank</th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>token_entity</th>\n",
              "      <th>char_entity</th>\n",
              "      <th>token_txt</th>\n",
              "      <th>token_tag</th>\n",
              "      <th>char_msg</th>\n",
              "      <th>token_txt_str</th>\n",
              "      <th>token_tag_str</th>\n",
              "      <th>sim_score_token</th>\n",
              "      <th>sim_score_char</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25319</th>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>V498</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>affiliated_service</td>\n",
              "      <td>20130724.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>[뚜레쥬르]</td>\n",
              "      <td>[뚜, 레, 쥬, 르]</td>\n",
              "      <td>[뚜레쥬르]</td>\n",
              "      <td>[NNP]</td>\n",
              "      <td>[뚜, 레, 쥬, 르]</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>NNP</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51170</th>\n",
              "      <td>시크릿</td>\n",
              "      <td>LGAV</td>\n",
              "      <td>모바일 단말기 이름</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>20101022.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>[시크릿]</td>\n",
              "      <td>[시, 크, 릿]</td>\n",
              "      <td>[시크릿]</td>\n",
              "      <td>[NNP]</td>\n",
              "      <td>[시, 크, 릿]</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>NNP</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161440</th>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>0 day</td>\n",
              "      <td>membership</td>\n",
              "      <td>20231101.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>0 day</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[0, d, a, y]</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[SN, SL]</td>\n",
              "      <td>[0, d, a, y]</td>\n",
              "      <td>0,day</td>\n",
              "      <td>SN,SL</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161441</th>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>0 day</td>\n",
              "      <td>membership</td>\n",
              "      <td>20231101.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>0 day</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[0, d, a, y]</td>\n",
              "      <td>[월, 0, day]</td>\n",
              "      <td>[NNB, SN, SL]</td>\n",
              "      <td>[월, 0, d, a, y]</td>\n",
              "      <td>월,0,day</td>\n",
              "      <td>NNB,SN,SL</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161445</th>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>0 day</td>\n",
              "      <td>membership</td>\n",
              "      <td>20231101.0</td>\n",
              "      <td>99991231.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>0 day</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[0, d, a, y]</td>\n",
              "      <td>[day]</td>\n",
              "      <td>[SL]</td>\n",
              "      <td>[d, a, y]</td>\n",
              "      <td>day</td>\n",
              "      <td>SL</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       item_nm item_id   item_desc              domain    start_dt  \\\n",
              "25319     뚜레쥬르    V498        뚜레쥬르  affiliated_service  20130724.0   \n",
              "51170      시크릿    LGAV  모바일 단말기 이름       mobile_device  20101022.0   \n",
              "161440   0 day    9301       0 day          membership  20231101.0   \n",
              "161441   0 day    9301       0 day          membership  20231101.0   \n",
              "161445   0 day    9301       0 day          membership  20231101.0   \n",
              "\n",
              "            end_dt       rank item_nm_alias token_entity   char_entity  \\\n",
              "25319   99991231.0       11.0          뚜레쥬르       [뚜레쥬르]  [뚜, 레, 쥬, 르]   \n",
              "51170   99991231.0  1000000.0           시크릿        [시크릿]     [시, 크, 릿]   \n",
              "161440  99991231.0  1000000.0         0 day     [0, day]  [0, d, a, y]   \n",
              "161441  99991231.0  1000000.0         0 day     [0, day]  [0, d, a, y]   \n",
              "161445  99991231.0  1000000.0         0 day     [0, day]  [0, d, a, y]   \n",
              "\n",
              "          token_txt      token_tag         char_msg token_txt_str  \\\n",
              "25319        [뚜레쥬르]          [NNP]     [뚜, 레, 쥬, 르]          뚜레쥬르   \n",
              "51170         [시크릿]          [NNP]        [시, 크, 릿]           시크릿   \n",
              "161440     [0, day]       [SN, SL]     [0, d, a, y]         0,day   \n",
              "161441  [월, 0, day]  [NNB, SN, SL]  [월, 0, d, a, y]       월,0,day   \n",
              "161445        [day]           [SL]        [d, a, y]           day   \n",
              "\n",
              "       token_tag_str  sim_score_token  sim_score_char  \n",
              "25319            NNP         1.000000        1.000000  \n",
              "51170            NNP         1.000000        1.000000  \n",
              "161440         SN,SL         1.000000        1.000000  \n",
              "161441     NNB,SN,SL         0.800000        0.888889  \n",
              "161445            SL         0.666667        0.857143  "
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ngram_jaccard_similarity(list1, list2, n=2):\n",
        "    \"\"\"Calculate similarity using Jaccard similarity of n-grams.\"\"\"\n",
        "    # Generate n-grams for both lists\n",
        "    def get_ngrams(lst, n):\n",
        "        return [tuple(lst[i:i+n]) for i in range(len(lst)-n+1)]\n",
        "    \n",
        "    # Handle edge cases\n",
        "    if len(list1) < n or len(list2) < n:\n",
        "        if list1 == list2:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "    \n",
        "    # Generate n-grams and calculate Jaccard similarity\n",
        "    ngrams1 = set(get_ngrams(list1, n))\n",
        "    ngrams2 = set(get_ngrams(list2, n))\n",
        "    \n",
        "    intersection = ngrams1.intersection(ngrams2)\n",
        "    union = ngrams1.union(ngrams2)\n",
        "    \n",
        "    return len(intersection) / len(union) if union else 0\n",
        "\n",
        "def needleman_wunsch_similarity(list1, list2, match_score=1, mismatch_penalty=1, gap_penalty=1):\n",
        "    \"\"\"Global sequence alignment with Needleman-Wunsch algorithm.\"\"\"\n",
        "    m, n = len(list1), len(list2)\n",
        "    \n",
        "    # Initialize score matrix\n",
        "    score = np.zeros((m+1, n+1))\n",
        "    \n",
        "    # Initialize first row and column with gap penalties\n",
        "    for i in range(m+1):\n",
        "        score[i][0] = -i * gap_penalty\n",
        "    for j in range(n+1):\n",
        "        score[0][j] = -j * gap_penalty\n",
        "    \n",
        "    # Fill the score matrix\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            match = score[i-1][j-1] + (match_score if list1[i-1] == list2[j-1] else -mismatch_penalty)\n",
        "            delete = score[i-1][j] - gap_penalty\n",
        "            insert = score[i][j-1] - gap_penalty\n",
        "            score[i][j] = max(match, delete, insert)\n",
        "    \n",
        "    # Calculate similarity score\n",
        "    max_possible_score = min(m, n) * match_score\n",
        "    alignment_score = score[m][n]\n",
        "    \n",
        "    # Normalize to 0-1 range\n",
        "    min_possible_score = -max(m, n) * max(gap_penalty, mismatch_penalty)\n",
        "    normalized_score = (alignment_score - min_possible_score) / (max_possible_score - min_possible_score)\n",
        "    \n",
        "    return normalized_score\n",
        "\n",
        "fdf['sim_score_token'] = fdf.apply(lambda row: needleman_wunsch_similarity(row['token_txt'], row['token_entity']), axis=1)\n",
        "fdf['sim_score_char'] = fdf.apply(lambda row: advanced_sequential_similarity((''.join(row['char_msg'])), (''.join(row['char_entity'])),metrics='difflib')['difflib'], axis=1)\n",
        "\n",
        "\n",
        "fdf.query(\"sim_score_char>0.8\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "40c78aeb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>item_id</th>\n",
              "      <th>token_entity</th>\n",
              "      <th>token_txt</th>\n",
              "      <th>sim_score_token</th>\n",
              "      <th>sim_score_char</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>161440</th>\n",
              "      <td>0 day</td>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51170</th>\n",
              "      <td>시크릿</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>LGAV</td>\n",
              "      <td>[시크릿]</td>\n",
              "      <td>[시크릿]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25319</th>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>V498</td>\n",
              "      <td>[뚜레쥬르]</td>\n",
              "      <td>[뚜레쥬르]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183450</th>\n",
              "      <td>베어유</td>\n",
              "      <td>베어유</td>\n",
              "      <td>V899</td>\n",
              "      <td>[베, 어유]</td>\n",
              "      <td>[베어유]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161441</th>\n",
              "      <td>0 day</td>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>[0, day]</td>\n",
              "      <td>[월, 0, day]</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       item_nm item_nm_alias item_id token_entity    token_txt  \\\n",
              "161440   0 day         0 day    9301     [0, day]     [0, day]   \n",
              "51170      시크릿           시크릿    LGAV        [시크릿]        [시크릿]   \n",
              "25319     뚜레쥬르          뚜레쥬르    V498       [뚜레쥬르]       [뚜레쥬르]   \n",
              "183450     베어유           베어유    V899      [베, 어유]        [베어유]   \n",
              "161441   0 day         0 day    9301     [0, day]  [월, 0, day]   \n",
              "\n",
              "        sim_score_token  sim_score_char  \n",
              "161440              1.0        1.000000  \n",
              "51170               1.0        1.000000  \n",
              "25319               1.0        1.000000  \n",
              "183450              0.0        1.000000  \n",
              "161441              0.8        0.888889  "
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fdf.query(\"sim_score_char>0.7\")[['item_nm','item_nm_alias','item_id','token_entity','token_txt','sim_score_token','sim_score_char']].sort_values('sim_score_char', ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "1c4a5892",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['에이닷이빵쏜닷', '0 day', '뚜레쥬르', '시크릿', '베어유', '데일리']"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entities_from_kiwi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "90661a18",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm</th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>item_id</th>\n",
              "      <th>token_in_message</th>\n",
              "      <th>domain</th>\n",
              "      <th>sim_score_token</th>\n",
              "      <th>sim_score_char</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25319</th>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>V498</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>affiliated_service</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51170</th>\n",
              "      <td>시크릿</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>LGAV</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>mobile_device</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161440</th>\n",
              "      <td>0 day</td>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>0,day</td>\n",
              "      <td>membership</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183450</th>\n",
              "      <td>베어유</td>\n",
              "      <td>베어유</td>\n",
              "      <td>V899</td>\n",
              "      <td>베어유</td>\n",
              "      <td>affiliated_service</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       item_nm item_nm_alias item_id token_in_message              domain  \\\n",
              "25319     뚜레쥬르          뚜레쥬르    V498             뚜레쥬르  affiliated_service   \n",
              "51170      시크릿           시크릿    LGAV              시크릿       mobile_device   \n",
              "161440   0 day         0 day    9301            0,day          membership   \n",
              "183450     베어유           베어유    V899              베어유  affiliated_service   \n",
              "\n",
              "        sim_score_token  sim_score_char  \n",
              "25319               1.0             1.0  \n",
              "51170               1.0             1.0  \n",
              "161440              1.0             1.0  \n",
              "183450              0.0             1.0  "
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entity_list = [e.replace(' ', '').lower() for e in list(edf['item_nm_alias'].unique())]\n",
        "entities_from_kiwi_rev = [e.replace(' ', '').lower() for e in entities_from_kiwi]\n",
        "\n",
        "kdf = fdf.query(\"item_nm_alias in @entities_from_kiwi_rev or token_txt_str.str.replace(',',' ').str.lower() in @entities_from_kiwi_rev or token_txt_str.str.replace(',','').str.lower() in @entities_from_kiwi_rev\").copy()\n",
        "kdf = kdf.query(\"(sim_score_token>=0.75 and sim_score_char>=0.75) or sim_score_char>=1\").query(\"item_nm_alias.str.replace(',','').str.lower() in @entity_list or item_nm_alias.str.replace(' ','').str.lower() in @entity_list\")\n",
        "kdf['rank'] = kdf.groupby(['token_txt_str'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "kdf = kdf.query(\"rank<=1\")[['item_nm','item_nm_alias','item_id','token_txt_str','domain','sim_score_token','sim_score_char']].drop_duplicates()\n",
        "# kdf = kdf.query(\"rank<=1\")\n",
        "\n",
        "# kdf = kdf.groupby('item_nm_alias', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 2), random_state=42))\n",
        "\n",
        "kdf.rename(columns={'token_txt_str':'token_in_message'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "9b15a789",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_nm_alias</th>\n",
              "      <th>item_id</th>\n",
              "      <th>token_txt_str</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25319</th>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>V498</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>affiliated_service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51170</th>\n",
              "      <td>시크릿</td>\n",
              "      <td>LGAV</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>mobile_device</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161440</th>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>0,day</td>\n",
              "      <td>membership</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       item_nm_alias item_id token_txt_str              domain\n",
              "25319           뚜레쥬르    V498          뚜레쥬르  affiliated_service\n",
              "51170            시크릿    LGAV           시크릿       mobile_device\n",
              "161440         0 day    9301         0,day          membership"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tags_to_exclude_final = ['SN']\n",
        "filtering_condition = [\n",
        "\"\"\"not token_tag_str in @tags_to_exclude_final\"\"\"\n",
        ",\"\"\"and token_txt_str.str.len()>=2\"\"\"\n",
        ",\"\"\"and not token_txt_str in @stop_item_names\"\"\"\n",
        ",\"\"\"and not token_txt_str.str.replace(',','').str.lower() in @stop_item_names\"\"\"\n",
        ",\"\"\"and not item_nm_alias in @stop_item_names\"\"\"\n",
        "]\n",
        "\n",
        "sdf = (\n",
        "    fdf\n",
        "    .query(\"item_nm_alias.str.lower() not in @stop_item_names\")\n",
        "    .query(\"(sim_score_token>=0.7 and sim_score_char>=0.8) or (sim_score_token>=0.1 and sim_score_char>=0.9)\")\n",
        "    # .query(\"item_nm_alias.str.contains('에이닷', case=False)\")\n",
        "    .query(' '.join(filtering_condition))\n",
        "    .sort_values('sim_score_char', ascending=False)\n",
        "    [['item_nm_alias','item_id','token_txt','token_txt_str','sim_score_token','sim_score_char','domain']]\n",
        ").copy()\n",
        "\n",
        "sdf['rank_e'] = sdf.groupby(['item_nm_alias'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "sdf['rank_t'] = sdf.groupby(['token_txt_str'])['sim_score_char'].rank(ascending=False, method='dense')#.reset_index(name='rank')\n",
        "sdf = sdf.query(\"rank_t<=1 and rank_e<=1\")[['item_nm_alias','item_id','token_txt_str','domain']].drop_duplicates()\n",
        "\n",
        "# sdf = sdf.groupby('item_nm_alias', group_keys=False).apply(lambda x: x.sample(n=min(len(x), 2), random_state=42), include_groups=False)\n",
        "\n",
        "\n",
        "sdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "64ced0ed",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_name_in_message</th>\n",
              "      <th>item_name_in_voca</th>\n",
              "      <th>item_id</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0 day</td>\n",
              "      <td>0 day</td>\n",
              "      <td>9301</td>\n",
              "      <td>membership</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>뚜레쥬르</td>\n",
              "      <td>V498</td>\n",
              "      <td>affiliated_service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>베어유</td>\n",
              "      <td>베어유</td>\n",
              "      <td>V899</td>\n",
              "      <td>affiliated_service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>시크릿</td>\n",
              "      <td>시크릿</td>\n",
              "      <td>LGAV</td>\n",
              "      <td>mobile_device</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  item_name_in_message item_name_in_voca item_id              domain\n",
              "0                0 day             0 day    9301          membership\n",
              "1                 뚜레쥬르              뚜레쥬르    V498  affiliated_service\n",
              "2                  베어유               베어유    V899  affiliated_service\n",
              "3                  시크릿               시크릿    LGAV       mobile_device"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "product_df = pd.concat([kdf,sdf]).drop_duplicates(['item_id','item_nm','item_nm_alias','domain']).groupby([\"item_nm\",\"item_nm_alias\",\"item_id\",\"domain\"])['token_txt_str'].apply(list).reset_index(name='item_name_in_message').rename(columns={'item_nm':'item_name_in_voca'}).sort_values('item_name_in_voca')\n",
        "\n",
        "product_df['item_name_in_message'] = product_df['item_name_in_message'].apply(lambda x: \",\".join(list(set([w.replace(',',' ') for w in x]))))\n",
        "\n",
        "product_df[['item_name_in_message','item_name_in_voca','item_id','domain']]#.query(\"item_name_in_voca.str.contains('netflix', case=False)\").drop_duplicates(['item_name_in_voca']).sort_values('item_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "cf6b2c3f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div style=\"\n",
              "        background-color: #1a237e;  /* 진한 파란색 배경 */\n",
              "        color: #ffffff;\n",
              "        padding: 20px;\n",
              "        border-radius: 10px;\n",
              "        font-family: 'Arial', sans-serif;\n",
              "        margin: 20px 0;\n",
              "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
              "        border: 1px solid #3949ab;  /* 테두리 추가 */\n",
              "        \">\n",
              "            <h2 style=\"color: #42a5f5; margin-bottom: 20px; font-size: 25px;\">Prompt</h2>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div style=\"\n",
              "        background-color: #000000;\n",
              "        color: #ffffff;\n",
              "        padding: 20px;\n",
              "        border-radius: 10px;\n",
              "        font-family: 'Arial', sans-serif;\n",
              "        margin: 20px 0;\n",
              "    \">\n",
              "        <h2 style=\"color: #00ff00; margin-bottom: 20px; font-size: 20px;\">SKT 캠페인 메시지 분석</h2>\n",
              "        \n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <h3 style=\"color: #00ff00; font-size: 18px;\">분석 대상 광고 메세지</h3>\n",
              "            <pre style=\"\n",
              "                background-color: #1a1a1a;\n",
              "                padding: 15px;\n",
              "                border-radius: 5px;\n",
              "                font-family: 'Courier New', monospace;\n",
              "                font-size: 16px;\n",
              "                line-height: 1.6;\n",
              "                margin: 10px 0;\n",
              "                white-space: pre-wrap;\n",
              "            \">\n",
              "광고 제목:[SK텔레콤] 2월 0 day 혜택 안내\n",
              "광고 내용:(광고)[SKT] 2월 0 day 혜택 안내__[2월 10일(토) 혜택]_만 13~34세 고객이라면_베어유 모든 강의 14일 무료 수강 쿠폰 드립니다!_(선착순 3만 명 증정)_▶ 자세히 보기: http://t-mms.kr/t.do?m=#61&s=24589&a=&u=https://bit.ly/3SfBjjc__■ 에이닷 X T 멤버십 시크릿코드 이벤트_에이닷 T 멤버십 쿠폰함에 ‘에이닷이빵쏜닷’을 입력해보세요!_뚜레쥬르 데일리우유식빵 무료 쿠폰을 드립니다._▶ 시크릿코드 입력하러 가기: https://bit.ly/3HCUhLM__■ 문의: SKT 고객센터(1558, 무료)_무료 수신거부 1504\n",
              "</pre>\n",
              "        </div>\n",
              "\n",
              "        <div style=\"margin-bottom: 20px;\">\n",
              "            <h3 style=\"color: #00ff00; font-size: 18px;\">결과 Schema</h3>\n",
              "            <pre style=\"\n",
              "                background-color: #1a1a1a;\n",
              "                padding: 15px;\n",
              "                border-radius: 5px;\n",
              "                font-family: 'Courier New', monospace;\n",
              "                font-size: 16px;\n",
              "                line-height: 1.6;\n",
              "                margin: 10px 0;\n",
              "                white-space: pre-wrap;\n",
              "            \">{\n",
              "  \"title\": {\n",
              "    \"type\": \"string\",\n",
              "    \"description\": \"광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성\"\n",
              "  },\n",
              "  \"purpose\": {\n",
              "    \"type\": \"array\",\n",
              "    \"description\": \"광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]\"\n",
              "  },\n",
              "  \"product\": [\n",
              "    {\n",
              "      \"item_name_in_voca\": \"0 day\",\n",
              "      \"item_id\": \"9301\",\n",
              "      \"domain\": \"membership\",\n",
              "      \"action\": \"고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택\"\n",
              "    },\n",
              "    {\n",
              "      \"item_name_in_voca\": \"뚜레쥬르\",\n",
              "      \"item_id\": \"V498\",\n",
              "      \"domain\": \"affiliated_service\",\n",
              "      \"action\": \"고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택\"\n",
              "    },\n",
              "    {\n",
              "      \"item_name_in_voca\": \"베어유\",\n",
              "      \"item_id\": \"V899\",\n",
              "      \"domain\": \"affiliated_service\",\n",
              "      \"action\": \"고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택\"\n",
              "    },\n",
              "    {\n",
              "      \"item_name_in_voca\": \"시크릿\",\n",
              "      \"item_id\": \"LGAV\",\n",
              "      \"domain\": \"mobile_device\",\n",
              "      \"action\": \"고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택\"\n",
              "    }\n",
              "  ],\n",
              "  \"channel\": {\n",
              "    \"type\": \"array\",\n",
              "    \"items\": {\n",
              "      \"type\": \"object\",\n",
              "      \"properties\": {\n",
              "        \"type\": {\n",
              "          \"type\": \"string\",\n",
              "          \"description\": \"채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택\"\n",
              "        },\n",
              "        \"value\": {\n",
              "          \"type\": \"string\",\n",
              "          \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보\"\n",
              "        },\n",
              "        \"action\": {\n",
              "          \"type\": \"string\",\n",
              "          \"description\": \"채널 목적: [방문, 접속, 가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"\n",
              "        },\n",
              "        \"store_code\": {\n",
              "          \"type\": \"string\",\n",
              "          \"description\": \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"\n",
              "        }\n",
              "      }\n",
              "    }\n",
              "  },\n",
              "  \"pgm\": {\n",
              "    \"type\": \"array\",\n",
              "    \"description\": \"아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공\"\n",
              "  }\n",
              "}</pre>\n",
              "        </div>\n",
              "\n",
              "        <div>\n",
              "            <h3 style=\"color: #00ff00; font-size: 18px;\">추출 가이드</h3>\n",
              "            <pre style=\"\n",
              "                background-color: #1a1a1a;\n",
              "                padding: 15px;\n",
              "                border-radius: 5px;\n",
              "                font-family: 'Courier New', monospace;\n",
              "                font-size: 16px;\n",
              "                line-height: 1.6;\n",
              "                margin: 10px 0;\n",
              "                white-space: pre-wrap;\n",
              "            \">\n",
              "### 분석 목표 ###\n",
              "* Schema의 Product 태그 내에 action을 추출하세요.\n",
              "* Schema내 action 항목 외 태그 정보는 원본 그대로 두세요.\n",
              "\n",
              "### 고려사항 ###\n",
              "* 상품 정보에 있는 항목을 임의로 변형하거나 누락시키지 마세요.\n",
              "* 광고 분류 기준 정보는 pgm_nm : clue_tag 로 구성\n",
              "\n",
              "### JSON 응답 형식 ###\n",
              "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
              "</pre>\n",
              "        </div>\n",
              "\n",
              "            <div>\n",
              "            <h3 style=\"color: #00ff00; font-size: 18px;\">RAG Context</h3>\n",
              "            <pre style=\"\n",
              "                background-color: #1a1a1a;\n",
              "                padding: 15px;\n",
              "                border-radius: 5px;\n",
              "                font-family: 'Courier New', monospace;\n",
              "                font-size: 16px;\n",
              "                line-height: 1.6;\n",
              "                margin: 10px 0;\n",
              "                white-space: pre-wrap;\n",
              "            \">\n",
              "### 광고 분류 기준 정보 ###\n",
              "\t사용법/혜택/이벤트안내_T멤버십 : 멤버십, Tday, T day, VIP, 등급, 할인, 쿠폰, 이벤트, 혜택, 적립, 무료\n",
              "\t사용법/혜택/이벤트안내_SKT : 이벤트, 사용법, 상품, 부가서비스, 부가 서비스, 혜택, 안내, 할인, 무료, 팁, 추천\n",
              "\t사용법/혜택/이벤트안내_구독 : 우주, 구독, 혜택, 이벤트, T우주, T 우주, 콘텐츠, 할인, 추천, 사용법, 서비스\n",
              "\t타사회선(가망)_win-back : 쓰던폰그대로, 15990111, 통신사이동, 통신사 이동, 번호이동, 번호 이동, 전환, 혜택, 할인, KT, LG, 알뜰폰\n",
              "\t사용법/혜택/이벤트안내_AI : 에이닷, 이벤트, 사용법, AI, 기능, 업데이트, 혜택, 추천, 서비스, 안내</pre>\n",
              "        </div>\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <div style=\"\n",
              "        background-color: #1a237e;  /* 진한 파란색 배경 */\n",
              "        color: #ffffff;\n",
              "        padding: 20px;\n",
              "        border-radius: 10px;\n",
              "        font-family: 'Arial', sans-serif;\n",
              "        margin: 20px 0;\n",
              "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
              "        border: 1px solid #3949ab;  /* 테두리 추가 */\n",
              "        \">\n",
              "            <h2 style=\"color: #42a5f5; margin-bottom: 20px; font-size: 25px;\">LLM Result</h2>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>pre { line-height: 125%; }\n",
              "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              ".hll { background-color: #49483e }\n",
              ".c { color: #75715e } /* Comment */\n",
              ".err { color: #960050; background-color: #1e0010 } /* Error */\n",
              ".esc { color: #f8f8f2 } /* Escape */\n",
              ".g { color: #f8f8f2 } /* Generic */\n",
              ".k { color: #66d9ef } /* Keyword */\n",
              ".l { color: #ae81ff } /* Literal */\n",
              ".n { color: #f8f8f2 } /* Name */\n",
              ".o { color: #f92672 } /* Operator */\n",
              ".x { color: #f8f8f2 } /* Other */\n",
              ".p { color: #f8f8f2 } /* Punctuation */\n",
              ".ch { color: #75715e } /* Comment.Hashbang */\n",
              ".cm { color: #75715e } /* Comment.Multiline */\n",
              ".cp { color: #75715e } /* Comment.Preproc */\n",
              ".cpf { color: #75715e } /* Comment.PreprocFile */\n",
              ".c1 { color: #75715e } /* Comment.Single */\n",
              ".cs { color: #75715e } /* Comment.Special */\n",
              ".gd { color: #f92672 } /* Generic.Deleted */\n",
              ".ge { color: #f8f8f2; font-style: italic } /* Generic.Emph */\n",
              ".gr { color: #f8f8f2 } /* Generic.Error */\n",
              ".gh { color: #f8f8f2 } /* Generic.Heading */\n",
              ".gi { color: #a6e22e } /* Generic.Inserted */\n",
              ".go { color: #66d9ef } /* Generic.Output */\n",
              ".gp { color: #f92672; font-weight: bold } /* Generic.Prompt */\n",
              ".gs { color: #f8f8f2; font-weight: bold } /* Generic.Strong */\n",
              ".gu { color: #75715e } /* Generic.Subheading */\n",
              ".gt { color: #f8f8f2 } /* Generic.Traceback */\n",
              ".kc { color: #66d9ef } /* Keyword.Constant */\n",
              ".kd { color: #66d9ef } /* Keyword.Declaration */\n",
              ".kn { color: #f92672 } /* Keyword.Namespace */\n",
              ".kp { color: #66d9ef } /* Keyword.Pseudo */\n",
              ".kr { color: #66d9ef } /* Keyword.Reserved */\n",
              ".kt { color: #66d9ef } /* Keyword.Type */\n",
              ".ld { color: #e6db74 } /* Literal.Date */\n",
              ".m { color: #ae81ff } /* Literal.Number */\n",
              ".s { color: #e6db74 } /* Literal.String */\n",
              ".na { color: #a6e22e } /* Name.Attribute */\n",
              ".nb { color: #f8f8f2 } /* Name.Builtin */\n",
              ".nc { color: #a6e22e } /* Name.Class */\n",
              ".no { color: #66d9ef } /* Name.Constant */\n",
              ".nd { color: #a6e22e } /* Name.Decorator */\n",
              ".ni { color: #f8f8f2 } /* Name.Entity */\n",
              ".ne { color: #a6e22e } /* Name.Exception */\n",
              ".nf { color: #a6e22e } /* Name.Function */\n",
              ".nl { color: #f8f8f2 } /* Name.Label */\n",
              ".nn { color: #f8f8f2 } /* Name.Namespace */\n",
              ".nx { color: #a6e22e } /* Name.Other */\n",
              ".py { color: #f8f8f2 } /* Name.Property */\n",
              ".nt { color: #f92672 } /* Name.Tag */\n",
              ".nv { color: #f8f8f2 } /* Name.Variable */\n",
              ".ow { color: #f92672 } /* Operator.Word */\n",
              ".pm { color: #f8f8f2 } /* Punctuation.Marker */\n",
              ".w { color: #f8f8f2 } /* Text.Whitespace */\n",
              ".mb { color: #ae81ff } /* Literal.Number.Bin */\n",
              ".mf { color: #ae81ff } /* Literal.Number.Float */\n",
              ".mh { color: #ae81ff } /* Literal.Number.Hex */\n",
              ".mi { color: #ae81ff } /* Literal.Number.Integer */\n",
              ".mo { color: #ae81ff } /* Literal.Number.Oct */\n",
              ".sa { color: #e6db74 } /* Literal.String.Affix */\n",
              ".sb { color: #e6db74 } /* Literal.String.Backtick */\n",
              ".sc { color: #e6db74 } /* Literal.String.Char */\n",
              ".dl { color: #e6db74 } /* Literal.String.Delimiter */\n",
              ".sd { color: #e6db74 } /* Literal.String.Doc */\n",
              ".s2 { color: #e6db74 } /* Literal.String.Double */\n",
              ".se { color: #ae81ff } /* Literal.String.Escape */\n",
              ".sh { color: #e6db74 } /* Literal.String.Heredoc */\n",
              ".si { color: #e6db74 } /* Literal.String.Interpol */\n",
              ".sx { color: #e6db74 } /* Literal.String.Other */\n",
              ".sr { color: #e6db74 } /* Literal.String.Regex */\n",
              ".s1 { color: #e6db74 } /* Literal.String.Single */\n",
              ".ss { color: #e6db74 } /* Literal.String.Symbol */\n",
              ".bp { color: #f8f8f2 } /* Name.Builtin.Pseudo */\n",
              ".fm { color: #a6e22e } /* Name.Function.Magic */\n",
              ".vc { color: #f8f8f2 } /* Name.Variable.Class */\n",
              ".vg { color: #f8f8f2 } /* Name.Variable.Global */\n",
              ".vi { color: #f8f8f2 } /* Name.Variable.Instance */\n",
              ".vm { color: #f8f8f2 } /* Name.Variable.Magic */\n",
              ".il { color: #ae81ff } /* Literal.Number.Integer.Long */</style><style>\n",
              ".highlight {\n",
              "    font-size: 16px !important;  /* Increase base font size */\n",
              "    line-height: 1.6 !important;  /* Increase line height for better readability */\n",
              "}\n",
              ".highlight pre {\n",
              "    font-size: 16px !important;\n",
              "    padding: 15px !important;\n",
              "}\n",
              "</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;title&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;[SK텔레콤] 2월 0 day 혜택 안내&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;purpose&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;혜택 안내&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;쿠폰 제공 안내&quot;</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;product&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;0 day&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;9301&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;membership&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;참여&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;뚜레쥬르&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;V498&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;affiliated_service&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;쿠폰다운로드&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;베어유&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;V899&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;affiliated_service&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;사용&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;시크릿&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;LGAV&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;mobile_device&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;코드입력&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;channel&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;URL&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;https://bit.ly/3SfBjjc&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;접속&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;URL&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;https://bit.ly/3HCUhLM&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;접속&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;전화번호&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;1558&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;문의&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;전화번호&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;1504&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;수신 거부&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;pgm&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;사용법/혜택/이벤트안내_T멤버십&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;사용법/혜택/이벤트안내_AI&quot;</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">]</span>\n",
              "<span class=\"p\">}</span>\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <div style=\"\n",
              "        background-color: #1a237e;  /* 진한 파란색 배경 */\n",
              "        color: #ffffff;\n",
              "        padding: 20px;\n",
              "        border-radius: 10px;\n",
              "        font-family: 'Arial', sans-serif;\n",
              "        margin: 20px 0;\n",
              "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
              "        border: 1px solid #3949ab;  /* 테두리 추가 */\n",
              "        \">\n",
              "            <h2 style=\"color: #42a5f5; margin-bottom: 20px; font-size: 25px;\">Final Result</h2>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>pre { line-height: 125%; }\n",
              "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              ".hll { background-color: #49483e }\n",
              ".c { color: #75715e } /* Comment */\n",
              ".err { color: #960050; background-color: #1e0010 } /* Error */\n",
              ".esc { color: #f8f8f2 } /* Escape */\n",
              ".g { color: #f8f8f2 } /* Generic */\n",
              ".k { color: #66d9ef } /* Keyword */\n",
              ".l { color: #ae81ff } /* Literal */\n",
              ".n { color: #f8f8f2 } /* Name */\n",
              ".o { color: #f92672 } /* Operator */\n",
              ".x { color: #f8f8f2 } /* Other */\n",
              ".p { color: #f8f8f2 } /* Punctuation */\n",
              ".ch { color: #75715e } /* Comment.Hashbang */\n",
              ".cm { color: #75715e } /* Comment.Multiline */\n",
              ".cp { color: #75715e } /* Comment.Preproc */\n",
              ".cpf { color: #75715e } /* Comment.PreprocFile */\n",
              ".c1 { color: #75715e } /* Comment.Single */\n",
              ".cs { color: #75715e } /* Comment.Special */\n",
              ".gd { color: #f92672 } /* Generic.Deleted */\n",
              ".ge { color: #f8f8f2; font-style: italic } /* Generic.Emph */\n",
              ".gr { color: #f8f8f2 } /* Generic.Error */\n",
              ".gh { color: #f8f8f2 } /* Generic.Heading */\n",
              ".gi { color: #a6e22e } /* Generic.Inserted */\n",
              ".go { color: #66d9ef } /* Generic.Output */\n",
              ".gp { color: #f92672; font-weight: bold } /* Generic.Prompt */\n",
              ".gs { color: #f8f8f2; font-weight: bold } /* Generic.Strong */\n",
              ".gu { color: #75715e } /* Generic.Subheading */\n",
              ".gt { color: #f8f8f2 } /* Generic.Traceback */\n",
              ".kc { color: #66d9ef } /* Keyword.Constant */\n",
              ".kd { color: #66d9ef } /* Keyword.Declaration */\n",
              ".kn { color: #f92672 } /* Keyword.Namespace */\n",
              ".kp { color: #66d9ef } /* Keyword.Pseudo */\n",
              ".kr { color: #66d9ef } /* Keyword.Reserved */\n",
              ".kt { color: #66d9ef } /* Keyword.Type */\n",
              ".ld { color: #e6db74 } /* Literal.Date */\n",
              ".m { color: #ae81ff } /* Literal.Number */\n",
              ".s { color: #e6db74 } /* Literal.String */\n",
              ".na { color: #a6e22e } /* Name.Attribute */\n",
              ".nb { color: #f8f8f2 } /* Name.Builtin */\n",
              ".nc { color: #a6e22e } /* Name.Class */\n",
              ".no { color: #66d9ef } /* Name.Constant */\n",
              ".nd { color: #a6e22e } /* Name.Decorator */\n",
              ".ni { color: #f8f8f2 } /* Name.Entity */\n",
              ".ne { color: #a6e22e } /* Name.Exception */\n",
              ".nf { color: #a6e22e } /* Name.Function */\n",
              ".nl { color: #f8f8f2 } /* Name.Label */\n",
              ".nn { color: #f8f8f2 } /* Name.Namespace */\n",
              ".nx { color: #a6e22e } /* Name.Other */\n",
              ".py { color: #f8f8f2 } /* Name.Property */\n",
              ".nt { color: #f92672 } /* Name.Tag */\n",
              ".nv { color: #f8f8f2 } /* Name.Variable */\n",
              ".ow { color: #f92672 } /* Operator.Word */\n",
              ".pm { color: #f8f8f2 } /* Punctuation.Marker */\n",
              ".w { color: #f8f8f2 } /* Text.Whitespace */\n",
              ".mb { color: #ae81ff } /* Literal.Number.Bin */\n",
              ".mf { color: #ae81ff } /* Literal.Number.Float */\n",
              ".mh { color: #ae81ff } /* Literal.Number.Hex */\n",
              ".mi { color: #ae81ff } /* Literal.Number.Integer */\n",
              ".mo { color: #ae81ff } /* Literal.Number.Oct */\n",
              ".sa { color: #e6db74 } /* Literal.String.Affix */\n",
              ".sb { color: #e6db74 } /* Literal.String.Backtick */\n",
              ".sc { color: #e6db74 } /* Literal.String.Char */\n",
              ".dl { color: #e6db74 } /* Literal.String.Delimiter */\n",
              ".sd { color: #e6db74 } /* Literal.String.Doc */\n",
              ".s2 { color: #e6db74 } /* Literal.String.Double */\n",
              ".se { color: #ae81ff } /* Literal.String.Escape */\n",
              ".sh { color: #e6db74 } /* Literal.String.Heredoc */\n",
              ".si { color: #e6db74 } /* Literal.String.Interpol */\n",
              ".sx { color: #e6db74 } /* Literal.String.Other */\n",
              ".sr { color: #e6db74 } /* Literal.String.Regex */\n",
              ".s1 { color: #e6db74 } /* Literal.String.Single */\n",
              ".ss { color: #e6db74 } /* Literal.String.Symbol */\n",
              ".bp { color: #f8f8f2 } /* Name.Builtin.Pseudo */\n",
              ".fm { color: #a6e22e } /* Name.Function.Magic */\n",
              ".vc { color: #f8f8f2 } /* Name.Variable.Class */\n",
              ".vg { color: #f8f8f2 } /* Name.Variable.Global */\n",
              ".vi { color: #f8f8f2 } /* Name.Variable.Instance */\n",
              ".vm { color: #f8f8f2 } /* Name.Variable.Magic */\n",
              ".il { color: #ae81ff } /* Literal.Number.Integer.Long */</style><style>\n",
              ".highlight {\n",
              "    font-size: 16px !important;  /* Increase base font size */\n",
              "    line-height: 1.6 !important;  /* Increase line height for better readability */\n",
              "}\n",
              ".highlight pre {\n",
              "    font-size: 16px !important;\n",
              "    padding: 15px !important;\n",
              "}\n",
              "</style><div class=\"highlight\"><pre><span></span><span class=\"p\">{</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;title&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;[SK텔레콤] 2월 0 day 혜택 안내&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;purpose&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;혜택 안내&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">        </span><span class=\"s2\">&quot;쿠폰 제공 안내&quot;</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;product&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;0 day&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;9301&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;membership&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;참여&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;뚜레쥬르&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;V498&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;affiliated_service&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;쿠폰다운로드&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;베어유&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;V899&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;affiliated_service&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;사용&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_name_in_voca&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;시크릿&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;item_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;LGAV&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;domain&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;mobile_device&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;코드입력&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;channel&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;URL&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;https://bit.ly/3SfBjjc&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;접속&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;URL&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;https://bit.ly/3HCUhLM&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;접속&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;전화번호&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;1558&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;문의&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;type&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;전화번호&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;1504&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;수신 거부&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">],</span>\n",
              "<span class=\"w\">    </span><span class=\"nt\">&quot;pgm&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;pgm_nm&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;[마케팅_Care]사용법/혜택/이벤트안내_T멤버십&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;pgm_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;2019CCEPM01&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">},</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">{</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;pgm_nm&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;[마케팅_Care]사용법/혜택/이벤트안내_AI&quot;</span><span class=\"p\">,</span>\n",
              "<span class=\"w\">            </span><span class=\"nt\">&quot;pgm_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;#2025ABCD02&quot;</span>\n",
              "<span class=\"w\">        </span><span class=\"p\">}</span>\n",
              "<span class=\"w\">    </span><span class=\"p\">]</span>\n",
              "<span class=\"p\">}</span>\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "import re\n",
        "from pygments import highlight\n",
        "from pygments.lexers import JsonLexer\n",
        "from pygments.formatters import HtmlFormatter\n",
        "from IPython.display import HTML\n",
        "\n",
        "formatter = HtmlFormatter(style='monokai')  # You can choose different styles like 'monokai', 'vs', 'friendly'\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".highlight {\n",
        "    font-size: 16px !important;  /* Increase base font size */\n",
        "    line-height: 1.6 !important;  /* Increase line height for better readability */\n",
        "}\n",
        ".highlight pre {\n",
        "    font-size: 16px !important;\n",
        "    padding: 15px !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# product_info = (\",\\n\".join(product_df.apply(lambda x: f'\"item_name_in_msg\":\"{x['item_name_in_msg']}\", \"item_name_in_voca\":\"{x['item_name_in_voca']}\", \"item_id\":\"{x['item_id']}:, \"action\":고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택', axis=1).tolist()))\n",
        "# product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "# product_df = product_df.drop_duplicates(['item_name_in_message','item_name_in_voca'])\n",
        "# product_df = product_df.merge(product_df.groupby('item_name_in_message')['item_id'].size().reset_index(name='count').sort_values('count', ascending=False), on='item_name_in_message', how='left').query('count<=3')\n",
        "product_df = product_df[['item_name_in_voca','item_id','domain']].drop_duplicates()\n",
        "product_df['action'] = '고객에게 기대하는 행동. [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 없음, 기타] 중에서 선택'\n",
        "\n",
        "product_element = product_df.to_dict(orient='records') if product_df.shape[0]>0 else schema_ext['product']\n",
        "\n",
        "pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "rag_context = f\"\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "schema_prd_1 = {\n",
        "    \"title\": {\n",
        "        \"type\": \"string\", \n",
        "        'description': '광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명할 수 있도록 생성'\n",
        "    },\n",
        "    \"purpose\": {\n",
        "        \"type\": \"array\", \n",
        "        'description': '광고의 주요 목적을 다음 중에서 선택(복수 가능): [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]'\n",
        "    },\n",
        "    \"product\": \n",
        "        product_element\n",
        "    ,\n",
        "    'channel': {\n",
        "        'type': 'array', \n",
        "        'items': {\n",
        "            'type': 'object', \n",
        "            'properties': {\n",
        "                'type': {'type': 'string', 'description': '채널 종류: [URL, 전화번호, 앱, 대리점] 중에서 선택'},\n",
        "                'value': {'type': 'string', 'description': '실제 URL, 전화번호, 앱 이름, 대리점 이름 등 구체적 정보'},\n",
        "                'action': {'type': 'string', 'description': '채널 목적: [방문, 접속, 가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택'},\n",
        "                # 'benefit': {'type': 'string', 'description': '해당 채널 이용 시 특별 혜택'},\n",
        "                'store_code': {'type': 'string', 'description': \"매장 코드 - tworldfriends.co.kr URL에서 D+숫자 9자리(D[0-9]{9}) 패턴의 코드 추출하여 대리점 채널에 설정\"}\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    'pgm':{\n",
        "         'type': 'array', \n",
        "        'description': '아래 광고 분류 기준 정보에서 선택. 메세지 내용과 광고 분류 기준을 참고하여, 광고 메세지에 가장 부합하는 2개의 pgm_nm을 적합도 순서대로 제공'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Improved extraction guidance\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 목표 ###\n",
        "* Schema의 Product 태그 내에 action을 추출하세요.\n",
        "* Schema내 action 항목 외 태그 정보는 원본 그대로 두세요.\n",
        "\n",
        "### 고려사항 ###\n",
        "* 상품 정보에 있는 항목을 임의로 변형하거나 누락시키지 마세요.\n",
        "* 광고 분류 기준 정보는 pgm_nm : clue_tag 로 구성\n",
        "\n",
        "### JSON 응답 형식 ###\n",
        "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# Create the system message with clear JSON output requirements\n",
        "user_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "### 분석 대상 광고 메세지 ###\n",
        "{mms_msg}\n",
        "\n",
        "### 결과 Schema ###\n",
        "{json.dumps(schema_prd_1, indent=2, ensure_ascii=False)}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(make_styled_message('Prompt')))\n",
        "\n",
        "display(HTML(make_styled_prompt(mms_msg, json.dumps(schema_prd_1, indent=2, ensure_ascii=False), extraction_guide, rag_context)))\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        # model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "      model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    # print(\"\\n\\n\\n\"+'%'*15+\" LLM Result \"+'%'*15+\"\\n\\n\")\n",
        "    display(HTML(make_styled_message('LLM Result')))\n",
        "    # print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "\n",
        "    # Your existing JSON objects\n",
        "    json_str = json.dumps(json_objects, indent=4, ensure_ascii=False)\n",
        "    # Create HTML with syntax highlighting\n",
        "    highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "    # Display in notebook\n",
        "    display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))\n",
        "\n",
        "\n",
        "    display(HTML(make_styled_message('Final Result')))\n",
        "\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "\n",
        "    final_json = json_objects.copy()\n",
        "    final_json['pgm'] = pgm_json\n",
        "\n",
        "    json_str = json.dumps(final_json, indent=4, ensure_ascii=False)\n",
        "    # Create HTML with syntax highlighting\n",
        "    highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "    # Display in notebook\n",
        "    display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))\n",
        "\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d66de7",
      "metadata": {},
      "source": [
        "<h2 style=\"color: #006600; font-size: 1.5em;\">LLM 추출 후 개체명 연결</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866a36ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_prd_2 = schema_ext\n",
        "\n",
        "extraction_guide = \"\"\"\n",
        "### 분석 시 고려사항 ###\n",
        "* 하나의 광고에 여러 상품이 포함될 수 있으며, 각 상품별로 별도 객체 생성\n",
        "* 재현율이 높도록 모든 상품을 선택\n",
        "* 상품 후보 정보는 상품 이름 (도메인) 형식으로 제공\n",
        "* 광고 분류 기준 정보는 pgm_nm : clue_tag 로 구성\n",
        "\n",
        "### 분석 목표 ###\n",
        "* 텍스트 매칭 기법으로 만들어진 상품 후보 정보가 제공되면 이를 확인하여 참고하라.\n",
        "* 제공된 상품 이름이 적합하지 않으면 무시하고, 목록에 없어도 적합한 상품이 있으면 추출하세요.\n",
        "\n",
        "### JSON 응답 형식 ###\n",
        "응답은 설명 없이 순수한 JSON 형식으로만 제공하세요. 응답의 시작과 끝은 '{'와 '}'여야 합니다. 어떠한 추가 텍스트나 설명도 포함하지 마세요.\n",
        "\"\"\"\n",
        "\n",
        "# product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "rag_context = f\"### 상품 후보 정보 ###\\n\\t{product_info}\" if product_df.shape[0]>0 else \"\"\n",
        "\n",
        "pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "rag_context += f\"\\n\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "# Create the system message with clear JSON output requirements\n",
        "user_message = f\"\"\"당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. 아래 schema에 따라 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요:\n",
        "\n",
        "### 분석 대상 광고 메세지 ###\n",
        "{mms_msg}\n",
        "\n",
        "### 결과 Schema ###\n",
        "{json.dumps(schema_prd_2, indent=2, ensure_ascii=False)}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(make_styled_message('Prompt')))\n",
        "\n",
        "\n",
        "display(HTML(make_styled_prompt(mms_msg, json.dumps(schema_prd_2, indent=2, ensure_ascii=False), extraction_guide, rag_context)))\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "    #   model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    display(HTML(make_styled_message('LLM Result')))\n",
        "\n",
        "    # print(chat_history)\n",
        "    # print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "\n",
        "    json_str = json.dumps(json_objects, indent=4, ensure_ascii=False)\n",
        "    # Create HTML with syntax highlighting\n",
        "    highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "    # Display in notebook\n",
        "\n",
        "    # Display in notebook\n",
        "    display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")\n",
        "\n",
        "\n",
        "display(HTML(make_styled_message('Final Result')))\n",
        "\n",
        "matches = []\n",
        "for item_name_message in json_objects['product']:\n",
        "    matches.extend(find_entities_in_text(\n",
        "        item_name_message['name'], \n",
        "        entity_list_for_fuzzy, \n",
        "        min_similarity=50,\n",
        "        high_score_threshold=50,\n",
        "        overlap_tolerance=0.5\n",
        "    ))\n",
        "\n",
        "mdf = pd.DataFrame(matches)\n",
        "if len(matches)>0:\n",
        "    mdf = mdf.query(\"text.str.lower() not in @stop_item_names and matched_entity.str.lower() not in @stop_item_names\")\n",
        "\n",
        "if mdf.shape[0]>0:\n",
        "    mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    mdf['domain'] = mdf['data'].apply(lambda x: x['domain'])\n",
        "    mdf = mdf.query(\"not matched_entity.str.contains('test', case=False)\").drop_duplicates(['item_id','domain'])\n",
        "\n",
        "    mdf = mdf.merge(mdf.groupby(['text','start'])['end'].max().reset_index(name='end'), on=['text', 'start', 'end'])\n",
        "\n",
        "    mdf['rank'] = mdf['data'].apply(lambda x: x['rank'])\n",
        "    mdf['re_rank'] = mdf.groupby('text')['score'].rank(ascending=False)\n",
        "    mdf = mdf.query(\"re_rank<=2\")\n",
        "\n",
        "    mdf = mdf.merge(pd.DataFrame(json_objects['product']).rename(columns={'name':'text'}), on='text', how='left')\n",
        "\n",
        "    product_tag = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','domain']].drop_duplicates().to_dict(orient='records')\n",
        "\n",
        "    final_result = {\n",
        "        \"title\":json_objects['title'],\n",
        "        \"purpose\":json_objects['purpose'],\n",
        "        \"product\":product_tag,\n",
        "        \"channel\":json_objects['channel'],\n",
        "        \"pgm\":json_objects['pgm']\n",
        "    }\n",
        "\n",
        "else:\n",
        "    final_result = json_objects\n",
        "    final_result['product'] = [{'item_name_in_message':d['name'], 'item_name_in_voca':d['name'], 'item_id': '#', 'domain': '#'} for d in final_result['product']]\n",
        "\n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "# print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "json_str = json.dumps(final_result, indent=4, ensure_ascii=False)\n",
        "highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "\n",
        "display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1478861e",
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_prd_3 = {\n",
        "  \"reasoning\": {\n",
        "    \"type\": \"object\",\n",
        "    \"description\": \"단계별 분석 과정 (최종 JSON에는 포함하지 않음)\",\n",
        "    \"properties\": {\n",
        "      \"step1_purpose_analysis\": \"광고 목적 분석 과정\",\n",
        "      \"step2_product_identification\": \"상품 식별 및 도메인 매칭 과정\", \n",
        "      \"step3_channel_extraction\": \"채널 정보 추출 과정\",\n",
        "      \"step4_pgm_classification\": \"프로그램 분류 과정\"\n",
        "    }\n",
        "  },\n",
        "  \"title\": {\n",
        "    \"type\": \"string\",\n",
        "    \"description\": \"광고 제목. 광고의 핵심 주제와 가치 제안을 명확하게 설명\"\n",
        "  },\n",
        "  \"purpose\": {\n",
        "    \"type\": \"array\",\n",
        "    \"description\": \"STEP 1에서 분석한 광고의 주요 목적 (복수 가능)\"\n",
        "  },\n",
        "  \"product\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"name\": {\n",
        "          \"type\": \"string\", \n",
        "          \"description\": \"STEP 2에서 식별한 제품/서비스 이름\"\n",
        "        },\n",
        "        \"action\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"STEP 2-3에서 결정한 고객 기대 행동\"\n",
        "        },\n",
        "        \"domain_match\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"매칭된 상품 후보의 도메인 정보 (참고용, 최종 JSON에는 제외)\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"channel\": {\n",
        "    \"type\": \"array\",\n",
        "    \"items\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"type\": {\"type\": \"string\"},\n",
        "        \"value\": {\"type\": \"string\"},\n",
        "        \"action\": {\"type\": \"string\"},\n",
        "        \"benefit\": {\"type\": \"string\"},\n",
        "        \"store_code\": {\"type\": \"string\"}\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"pgm\": {\n",
        "    \"type\": \"array\",\n",
        "    \"description\": \"STEP 4에서 선택한 프로그램 분류 (적합도 순 2개)\"\n",
        "  }\n",
        "}\n",
        "\n",
        "extraction_guide = \"\"\"\n",
        "## 분석 지침\n",
        "1. **재현율 우선**: 광고에서 언급된 모든 상품을 누락 없이 추출\n",
        "2. **도메인 활용**: 상품 후보의 도메인 정보를 적극 활용하여 정확한 매칭 수행\n",
        "3. **목적 기반 추론**: 광고 목적을 명확히 파악한 후 다른 요소들을 일관성 있게 분석\n",
        "4. **컨텍스트 고려**: 제공된 상품 후보가 부적합하면 무시하고, 누락된 중요 상품이 있으면 추가\n",
        "\n",
        "## JSON 응답 형식\n",
        "- reasoning 섹션은 분석 과정 설명용이며 최종 JSON에는 포함하지 않음\n",
        "- 순수한 JSON 형식으로만 응답\n",
        "- 시작과 끝은 '{'와 '}'\n",
        "- 추가 텍스트나 설명 없이 JSON만 제공\n",
        "\"\"\"\n",
        "\n",
        "# product_info = \", \".join(product_df['item_name_in_voca'].unique().tolist())\n",
        "product_info = \", \".join(product_df[['item_name_in_voca','domain']].apply(lambda x: x['item_name_in_voca']+\"(\"+x['domain']+\")\", axis=1))\n",
        "\n",
        "rag_context = f\"### 상품 후보 정보 ###\\n\\t{product_info}\" if product_df.shape[0]>0 else \"\"\n",
        "\n",
        "pgm_cand_info = \"\\n\\t\".join(pgm_pdf_tmp.iloc[:num_cand_pgms][['pgm_nm','clue_tag']].apply(lambda x: re.sub(r'\\[.*?\\]', '', x['pgm_nm'])+\" : \"+x['clue_tag'], axis=1).to_list())\n",
        "rag_context += f\"\\n\\n### 광고 분류 기준 정보 ###\\n\\t{pgm_cand_info}\" if num_cand_pgms>0 else \"\"\n",
        "\n",
        "# Create the system message with clear JSON output requirements\n",
        "user_message = f\"\"\"당당신은 SKT 캠페인 메시지에서 정확한 정보를 추출하는 전문가입니다. **단계별 사고 과정(Chain of Thought)**을 통해 광고 메시지를 분석하여 완전하고 정확한 JSON 객체를 생성해 주세요.\n",
        "\n",
        "## 분석 단계 (Chain of Thought)\n",
        "\n",
        "### STEP 1: 광고 목적(Purpose) 분석\n",
        "먼저 광고 메시지 전체를 읽고 다음 질문들에 답하여 광고의 주요 목적을 파악하세요:\n",
        "- 이 광고가 고객에게 무엇을 하라고 요구하는가?\n",
        "- 어떤 행동을 유도하려고 하는가? (가입, 방문, 다운로드, 참여 등)\n",
        "- 어떤 혜택이나 정보를 제공하고 있는가?\n",
        "\n",
        "**목적 후보**: [상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공]\n",
        "\n",
        "### STEP 2: 상품(Product) 식별 및 도메인 매칭\n",
        "파악된 목적을 바탕으로 다음 과정을 거쳐 상품을 식별하세요:\n",
        "\n",
        "**2-1. 광고 메시지에서 언급된 모든 상품/서비스 추출**\n",
        "- 직접적으로 언급된 상품명을 모두 나열\n",
        "- 묵시적으로 언급된 서비스나 혜택도 포함\n",
        "\n",
        "**2-2. RAG Context의 상품 후보 정보와 도메인 매칭**\n",
        "- 각 추출된 상품을 상품 후보 정보와 비교\n",
        "- 도메인 정보(product, subscription_service 등)를 고려하여 가장 적합한 매칭 수행\n",
        "- 상품 후보에 없어도 광고에서 중요하게 다뤄지는 상품이 있다면 추가\n",
        "\n",
        "**2-3. 각 상품별 고객 행동(Action) 결정**\n",
        "- STEP 1에서 파악한 목적과 연결하여 각 상품에 대한 기대 행동 결정\n",
        "- 행동 후보: [구매, 가입, 사용, 방문, 참여, 코드입력, 쿠폰다운로드, 기타]\n",
        "\n",
        "### STEP 3: 채널(Channel) 및 기타 정보 추출\n",
        "- URL, 전화번호, 앱, 대리점 정보 추출\n",
        "- 각 채널의 목적과 혜택 파악\n",
        "- 대리점 URL에서 매장 코드(D[0-9]{9}) 패턴 확인\n",
        "\n",
        "### STEP 4: 프로그램 분류(PGM) 결정\n",
        "- 광고 분류 기준 정보의 키워드와 메시지 내용 매칭\n",
        "- 적합도 순서대로 2개 선택\n",
        "\n",
        "### 분석 대상 광고 메세지 ###\n",
        "{mms_msg}\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "### 결과 Schema ###\n",
        "{json.dumps(schema_prd_3, indent=2, ensure_ascii=False)}\n",
        "\n",
        "{extraction_guide}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(make_styled_message('Prompt')))\n",
        "\n",
        "\n",
        "display(HTML(make_styled_prompt(mms_msg, json.dumps(schema_prd_3, indent=2, ensure_ascii=False), extraction_guide, rag_context)))\n",
        "\n",
        "try:\n",
        "    # Use OpenAI's ChatCompletion with the current API format\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # Or your preferred OpenAI model\n",
        "    #   model=\"skt/claude-3-5-sonnet-20241022\",\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=4000,\n",
        "        top_p=0.95,  # Reduces randomness\n",
        "        frequency_penalty=0.0,  # Avoid repetition in JSON\n",
        "        presence_penalty=0.0,\n",
        "        response_format={\"type\": \"json_object\"}  # Explicitly request JSON format\n",
        "    )\n",
        "    \n",
        "    # Extract the JSON from the response\n",
        "    result_json_text = response.choices[0].message.content\n",
        "    json_objects = extract_json_objects(result_json_text)[0]\n",
        "\n",
        "    display(HTML(make_styled_message('LLM Result')))\n",
        "\n",
        "    # print(chat_history)\n",
        "    # print(json.dumps(json_objects, indent=4, ensure_ascii=False))\n",
        "\n",
        "    json_str = json.dumps(json_objects, indent=4, ensure_ascii=False)\n",
        "    # Create HTML with syntax highlighting\n",
        "    highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "    # Display in notebook\n",
        "\n",
        "    # Display in notebook\n",
        "    display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"Error with API call: {e}\")\n",
        "\n",
        "\n",
        "display(HTML(make_styled_message('Final Result')))\n",
        "\n",
        "matches = []\n",
        "for item_name_message in json_objects['product']:\n",
        "    matches.extend(find_entities_in_text(\n",
        "        item_name_message['name'], \n",
        "        entity_list_for_fuzzy, \n",
        "        min_similarity=50,\n",
        "        high_score_threshold=50,\n",
        "        overlap_tolerance=0.5\n",
        "    ))\n",
        "\n",
        "mdf = pd.DataFrame(matches)\n",
        "if len(matches)>0:\n",
        "    mdf = mdf.query(\"text.str.lower() not in @stop_item_names and matched_entity.str.lower() not in @stop_item_names\")\n",
        "\n",
        "if mdf.shape[0]>0:\n",
        "    mdf['item_id'] = mdf['data'].apply(lambda x: x['item_id'])\n",
        "    mdf['domain'] = mdf['data'].apply(lambda x: x['domain'])\n",
        "    mdf = mdf.query(\"not matched_entity.str.contains('test', case=False)\").drop_duplicates(['item_id','domain'])\n",
        "\n",
        "    mdf = mdf.merge(mdf.groupby(['text','start'])['end'].max().reset_index(name='end'), on=['text', 'start', 'end'])\n",
        "\n",
        "    mdf['rank'] = mdf['data'].apply(lambda x: x['rank'])\n",
        "    mdf['re_rank'] = mdf.groupby('text')['score'].rank(ascending=False)\n",
        "    mdf = mdf.query(\"re_rank<=2\")\n",
        "\n",
        "    mdf = mdf.merge(pd.DataFrame(json_objects['product']).rename(columns={'name':'text'}), on='text', how='left')\n",
        "\n",
        "    product_tag = mdf.rename(columns={'text':'item_name_in_message','matched_entity':'item_name_in_voca'})[['item_name_in_message','item_name_in_voca','item_id','domain']].drop_duplicates().to_dict(orient='records')\n",
        "\n",
        "    final_result = {\n",
        "        \"title\":json_objects['title'],\n",
        "        \"purpose\":json_objects['purpose'],\n",
        "        \"product\":product_tag,\n",
        "        \"channel\":json_objects['channel'],\n",
        "        \"pgm\":json_objects['pgm']\n",
        "    }\n",
        "\n",
        "else:\n",
        "    final_result = json_objects\n",
        "    final_result['product'] = [{'item_name_in_message':d['name'], 'item_name_in_voca':d['name'], 'item_id': '#', 'domain': '#'} for d in final_result['product']]\n",
        "\n",
        "if num_cand_pgms>0:\n",
        "    pgm_json = pgm_pdf[pgm_pdf['pgm_nm'].apply(lambda x: re.sub(r'\\[.*?\\]', '', x) in ' '.join(json_objects['pgm']))][['pgm_nm','pgm_id']].to_dict('records')\n",
        "    final_result['pgm'] = pgm_json\n",
        "\n",
        "# print(json.dumps(final_result, indent=4, ensure_ascii=False))\n",
        "\n",
        "json_str = json.dumps(final_result, indent=4, ensure_ascii=False)\n",
        "highlighted_json = highlight(json_str, JsonLexer(), formatter)\n",
        "\n",
        "display(HTML(f'<style>{formatter.get_style_defs()}</style><style>{custom_css}</style>{highlighted_json}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe5a667",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}