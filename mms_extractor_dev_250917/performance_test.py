#!/usr/bin/env python3
"""
MMS ì¶”ì¶œê¸° ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” MMS ì¶”ì¶œê¸°ì˜ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ (ìˆœì°¨ vs ë³‘ë ¬)
2. ë°°ì¹˜ ë©”ì‹œì§€ ì²˜ë¦¬ (ìˆœì°¨ vs ë³‘ë ¬)
3. DAG ì¶”ì¶œ í¬í•¨/ë¯¸í¬í•¨ ì„±ëŠ¥ ë¹„êµ
4. ì›Œì»¤ ìˆ˜ë³„ ì„±ëŠ¥ ë³€í™”
"""

import time
import json
import statistics
from pathlib import Path
import sys
import logging
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import matplotlib.pyplot as plt
import pandas as pd

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

from mms_extractor import MMSExtractor, process_message_with_dag, process_messages_batch, make_entity_dag

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë©”ì‹œì§€ë“¤
SAMPLE_MESSAGES = [
    """[SKT] ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3 ì•ˆë‚´
    (ê´‘ê³ )[SKT] ìš°ë¦¬ ì•„ì´ ì²« ë²ˆì§¸ ìŠ¤ë§ˆíŠ¸í°, ZEM í‚¤ì¦ˆí°__#04 ê³ ê°ë‹˜, ì•ˆë…•í•˜ì„¸ìš”!
    ìš°ë¦¬ ì•„ì´ ìŠ¤ë§ˆíŠ¸í° ê³ ë¯¼ ì¤‘ì´ì…¨ë‹¤ë©´, ìë…€ ìŠ¤ë§ˆíŠ¸í° ê´€ë¦¬ ì•± ZEMì´ ì„¤ì¹˜ëœ SKTë§Œì˜ ì•ˆì „í•œ í‚¤ì¦ˆí°,
    ZEMí° í¬ì¼“ëª¬ì—ë””ì…˜3ìœ¼ë¡œ ìš°ë¦¬ ì•„ì´ ì·¨í–¥ì„ ì €ê²©í•´ ë³´ì„¸ìš”!""",
    
    """[KT] 5G í”„ë¦¬ë¯¸ì—„ ìš”ê¸ˆì œ í˜œíƒ ì•ˆë‚´
    ì•ˆë…•í•˜ì„¸ìš”! KTì…ë‹ˆë‹¤. ìƒˆë¡œìš´ 5G í”„ë¦¬ë¯¸ì—„ ìš”ê¸ˆì œë¡œ ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í†µì‹  ì„œë¹„ìŠ¤ë¥¼ ê²½í—˜í•´ë³´ì„¸ìš”.
    ì›” 59,000ì›ìœ¼ë¡œ ë°ì´í„° ë¬´ì œí•œ, ì˜ìƒí†µí™” ë¬´ì œí•œ í˜œíƒì„ ë“œë¦½ë‹ˆë‹¤.""",
    
    """[LGU+] U+ì•Œëœ°í° ê°€ì… í˜œíƒ
    LGìœ í”ŒëŸ¬ìŠ¤ U+ì•Œëœ°í° ê°€ì…í•˜ê³  ì›” í†µì‹ ë¹„ ì ˆì•½í•˜ì„¸ìš”!
    ë°ì´í„° 5GB + í†µí™” 300ë¶„ ì›” 25,000ì› íŠ¹ê°€ í˜œíƒì„ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”.""",
    
    """[í˜„ëŒ€ì¹´ë“œ] Mí¬ì¸íŠ¸ ì ë¦½ ì•ˆë‚´
    í˜„ëŒ€ì¹´ë“œ ì‚¬ìš© ì‹œ Mí¬ì¸íŠ¸ê°€ ìë™ ì ë¦½ë©ë‹ˆë‹¤.
    ì ë¦½ëœ í¬ì¸íŠ¸ë¡œ ë‹¤ì–‘í•œ í˜œíƒì„ ë°›ì•„ë³´ì„¸ìš”. í¬ì¸íŠ¸ ì¡°íšŒëŠ” ì•±ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.""",
    
    """[ì‚¼ì„±ì „ì] ê°¤ëŸ­ì‹œ S24 ì¶œì‹œ ê¸°ë…
    ìƒˆë¡œìš´ ê°¤ëŸ­ì‹œ S24ê°€ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!
    AI ì¹´ë©”ë¼ì™€ í–¥ìƒëœ ë°°í„°ë¦¬ë¡œ ë” ìŠ¤ë§ˆíŠ¸í•œ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤."""
]

class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í…ŒìŠ¤í„° ì´ˆê¸°í™”"""
        logger.info("ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™” ì¤‘...")
        self.extractor = MMSExtractor(
            offer_info_data_src='local',
            product_info_extraction_mode='nlp',
            entity_extraction_mode='logic',
            llm_model='ax',
            extract_entity_dag=False
        )
        logger.info("ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.results = {
            'single_message': {},
            'batch_processing': {},
            'dag_comparison': {},
            'worker_scaling': {}
        }
    
    def test_single_message_performance(self, iterations: int = 3) -> Dict[str, Any]:
        """ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ìˆœì°¨ vs ë³‘ë ¬)"""
        logger.info(f"ë‹¨ì¼ ë©”ì‹œì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë°˜ë³µ: {iterations}íšŒ)")
        
        message = SAMPLE_MESSAGES[0]
        
        # ìˆœì°¨ ì²˜ë¦¬ (DAG ì—†ìŒ)
        sequential_times = []
        for i in range(iterations):
            start_time = time.time()
            result = self.extractor.process_message(message)
            elapsed = time.time() - start_time
            sequential_times.append(elapsed)
            logger.info(f"ìˆœì°¨ ì²˜ë¦¬ {i+1}/{iterations}: {elapsed:.3f}ì´ˆ")
        
        # ìˆœì°¨ ì²˜ë¦¬ (DAG í¬í•¨)
        sequential_dag_times = []
        for i in range(iterations):
            start_time = time.time()
            result = self.extractor.process_message(message)
            dag_result = make_entity_dag(message, self.extractor.llm_model)
            elapsed = time.time() - start_time
            sequential_dag_times.append(elapsed)
            logger.info(f"ìˆœì°¨ ì²˜ë¦¬ (DAG) {i+1}/{iterations}: {elapsed:.3f}ì´ˆ")
        
        # ë³‘ë ¬ ì²˜ë¦¬ (DAG í¬í•¨)
        parallel_dag_times = []
        for i in range(iterations):
            start_time = time.time()
            result = process_message_with_dag(self.extractor, message, extract_dag=True)
            elapsed = time.time() - start_time
            parallel_dag_times.append(elapsed)
            logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ (DAG) {i+1}/{iterations}: {elapsed:.3f}ì´ˆ")
        
        results = {
            'sequential_avg': statistics.mean(sequential_times),
            'sequential_std': statistics.stdev(sequential_times) if len(sequential_times) > 1 else 0,
            'sequential_dag_avg': statistics.mean(sequential_dag_times),
            'sequential_dag_std': statistics.stdev(sequential_dag_times) if len(sequential_dag_times) > 1 else 0,
            'parallel_dag_avg': statistics.mean(parallel_dag_times),
            'parallel_dag_std': statistics.stdev(parallel_dag_times) if len(parallel_dag_times) > 1 else 0,
            'speedup_ratio': statistics.mean(sequential_dag_times) / statistics.mean(parallel_dag_times)
        }
        
        self.results['single_message'] = results
        return results
    
    def test_batch_processing_performance(self, worker_counts: List[int] = [1, 2, 4, 8]) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì›Œì»¤ ìˆ˜ë³„)"""
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {worker_counts})")
        
        messages = SAMPLE_MESSAGES * 2  # 10ê°œ ë©”ì‹œì§€
        
        results = {}
        
        for worker_count in worker_counts:
            logger.info(f"ì›Œì»¤ {worker_count}ê°œë¡œ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            
            # DAG ì—†ìŒ
            start_time = time.time()
            batch_results = process_messages_batch(
                self.extractor, 
                messages, 
                extract_dag=False,
                max_workers=worker_count
            )
            elapsed_no_dag = time.time() - start_time
            
            # DAG í¬í•¨
            start_time = time.time()
            batch_results_dag = process_messages_batch(
                self.extractor, 
                messages, 
                extract_dag=True,
                max_workers=worker_count
            )
            elapsed_with_dag = time.time() - start_time
            
            results[f'workers_{worker_count}'] = {
                'no_dag_time': elapsed_no_dag,
                'with_dag_time': elapsed_with_dag,
                'no_dag_per_message': elapsed_no_dag / len(messages),
                'with_dag_per_message': elapsed_with_dag / len(messages),
                'successful_count': len([r for r in batch_results if not r.get('error')])
            }
            
            logger.info(f"ì›Œì»¤ {worker_count}ê°œ - DAG ì—†ìŒ: {elapsed_no_dag:.3f}ì´ˆ, DAG í¬í•¨: {elapsed_with_dag:.3f}ì´ˆ")
        
        self.results['batch_processing'] = results
        return results
    
    def test_dag_extraction_impact(self, iterations: int = 3) -> Dict[str, Any]:
        """DAG ì¶”ì¶œì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í…ŒìŠ¤íŠ¸"""
        logger.info(f"DAG ì¶”ì¶œ ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ë°˜ë³µ: {iterations}íšŒ)")
        
        message = SAMPLE_MESSAGES[0]
        
        # DAG ì—†ì´ ì²˜ë¦¬
        no_dag_times = []
        for i in range(iterations):
            start_time = time.time()
            result = self.extractor.process_message(message)
            elapsed = time.time() - start_time
            no_dag_times.append(elapsed)
        
        # DAG í¬í•¨ ë³‘ë ¬ ì²˜ë¦¬
        with_dag_times = []
        for i in range(iterations):
            start_time = time.time()
            result = process_message_with_dag(self.extractor, message, extract_dag=True)
            elapsed = time.time() - start_time
            with_dag_times.append(elapsed)
        
        results = {
            'no_dag_avg': statistics.mean(no_dag_times),
            'with_dag_avg': statistics.mean(with_dag_times),
            'overhead_ratio': statistics.mean(with_dag_times) / statistics.mean(no_dag_times),
            'overhead_seconds': statistics.mean(with_dag_times) - statistics.mean(no_dag_times)
        }
        
        self.results['dag_comparison'] = results
        return results
    
    def generate_performance_report(self) -> str:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 80)
        report.append("ğŸš€ MMS ì¶”ì¶œê¸° ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        report.append("=" * 80)
        
        # ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ê²°ê³¼
        if 'single_message' in self.results:
            single = self.results['single_message']
            report.append("\nğŸ“‹ 1. ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì„±ëŠ¥")
            report.append("-" * 50)
            report.append(f"ìˆœì°¨ ì²˜ë¦¬ (DAG ì—†ìŒ):     {single['sequential_avg']:.3f}ì´ˆ (Â±{single['sequential_std']:.3f})")
            report.append(f"ìˆœì°¨ ì²˜ë¦¬ (DAG í¬í•¨):     {single['sequential_dag_avg']:.3f}ì´ˆ (Â±{single['sequential_dag_std']:.3f})")
            report.append(f"ë³‘ë ¬ ì²˜ë¦¬ (DAG í¬í•¨):     {single['parallel_dag_avg']:.3f}ì´ˆ (Â±{single['parallel_dag_std']:.3f})")
            report.append(f"ğŸ¯ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ:    {single['speedup_ratio']:.2f}x ë¹ ë¦„")
            report.append(f"â° ì‹œê°„ ë‹¨ì¶•:             {(single['sequential_dag_avg'] - single['parallel_dag_avg']):.3f}ì´ˆ")
        
        # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        if 'batch_processing' in self.results:
            batch = self.results['batch_processing']
            report.append("\nğŸ“¦ 2. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ (10ê°œ ë©”ì‹œì§€)")
            report.append("-" * 50)
            
            for key, data in batch.items():
                worker_count = key.split('_')[1]
                report.append(f"ì›Œì»¤ {worker_count}ê°œ:")
                report.append(f"  - DAG ì—†ìŒ:  {data['no_dag_time']:.3f}ì´ˆ ({data['no_dag_per_message']:.3f}ì´ˆ/ë©”ì‹œì§€)")
                report.append(f"  - DAG í¬í•¨:  {data['with_dag_time']:.3f}ì´ˆ ({data['with_dag_per_message']:.3f}ì´ˆ/ë©”ì‹œì§€)")
                report.append(f"  - ì„±ê³µë¥ :    {data['successful_count']}/10")
        
        # DAG ì¶”ì¶œ ì˜í–¥
        if 'dag_comparison' in self.results:
            dag = self.results['dag_comparison']
            report.append("\nğŸ¯ 3. DAG ì¶”ì¶œ ì„±ëŠ¥ ì˜í–¥")
            report.append("-" * 50)
            report.append(f"DAG ì—†ìŒ:        {dag['no_dag_avg']:.3f}ì´ˆ")
            report.append(f"DAG í¬í•¨:        {dag['with_dag_avg']:.3f}ì´ˆ")
            report.append(f"ì˜¤ë²„í—¤ë“œ ë¹„ìœ¨:   {dag['overhead_ratio']:.2f}x")
            report.append(f"ì¶”ê°€ ì‹œê°„:       {dag['overhead_seconds']:.3f}ì´ˆ")
        
        # ê¶Œì¥ì‚¬í•­
        report.append("\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­")
        report.append("-" * 50)
        report.append("â€¢ DAG ì¶”ì¶œì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ í™œì„±í™”")
        report.append("â€¢ ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì ì ˆí•œ ì›Œì»¤ ìˆ˜ ì„¤ì • (CPU ì½”ì–´ ìˆ˜ì˜ 50-100%)")
        report.append("â€¢ ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ 50-100ê°œë¡œ ì œí•œ")
        report.append("â€¢ ë¡œì»¬ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš© ê¶Œì¥ (DBë³´ë‹¤ ë¹ ë¦„)")
        
        return "\n".join(report)
    
    def save_results_to_json(self, filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            filename = f"performance_test_results_{int(time.time())}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=4, ensure_ascii=False)
        
        logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")
        return filename

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ MMS ì¶”ì¶œê¸° ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    tester = PerformanceTester()
    
    try:
        # 1. ë‹¨ì¼ ë©”ì‹œì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        single_results = tester.test_single_message_performance(iterations=3)
        
        # 2. DAG ì¶”ì¶œ ì˜í–¥ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ DAG ì¶”ì¶œ ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸...")
        dag_results = tester.test_dag_extraction_impact(iterations=3)
        
        # 3. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        batch_results = tester.test_batch_processing_performance(worker_counts=[1, 2, 4])
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
        report = tester.generate_performance_report()
        print("\n" + report)
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        json_file = tester.save_results_to_json()
        
        print(f"\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“„ ìƒì„¸ ê²°ê³¼: {json_file}")
        
        # í•µì‹¬ ì„±ê³¼ ìš”ì•½
        if 'single_message' in tester.results:
            speedup = tester.results['single_message']['speedup_ratio']
            time_saved = tester.results['single_message']['sequential_dag_avg'] - tester.results['single_message']['parallel_dag_avg']
            print(f"\nğŸ¯ í•µì‹¬ ì„±ê³¼:")
            print(f"   â€¢ ë³‘ë ¬ ì²˜ë¦¬ë¡œ {speedup:.1f}x ì„±ëŠ¥ í–¥ìƒ")
            print(f"   â€¢ ë©”ì‹œì§€ë‹¹ {time_saved:.3f}ì´ˆ ì‹œê°„ ë‹¨ì¶•")
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main()
