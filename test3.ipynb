{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
            "env: LANGSMITH_TRACING=true\n",
            "env: LANGSMITH_API_KEY=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\n",
            "env: TAVILY_API_KEY=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\n",
            "env: OPENAI_API_KEY=${OPENAI_API_KEY}\n",
            "env: LANGCHAIN_API_KEY=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\n",
            "env: LANGCHAIN_TRACING_V2=true\n",
            "env: LANGCHAIN_PROJECT=\"Multi-agent Collaboration\"\n"
          ]
        }
      ],
      "source": [
        "%set_env ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
        "%set_env LANGSMITH_TRACING=true\n",
        "%set_env LANGSMITH_API_KEY=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\n",
        "%set_env TAVILY_API_KEY=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\n",
        "\n",
        "\n",
        "%set_env OPENAI_API_KEY=${OPENAI_API_KEY}\n",
        "%set_env LANGCHAIN_API_KEY=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\n",
        "\n",
        "%set_env LANGCHAIN_TRACING_V2=true\n",
        "%set_env LANGCHAIN_PROJECT=\"Multi-agent Collaboration\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "import pandasql as psql\n",
        "\n",
        "\n",
        "def ChatAnthropicSKT(model=\"anthropic/claude-3-5-sonnet-20240620\"):\n",
        "    llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "\n",
        "    # model = \"anthropic/claude-3-5-sonnet-20240620\"\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        openai_api_key=llm_api_key, \n",
        "        openai_api_base=llm_api_url, \n",
        "        model=model\n",
        "        )\n",
        "    return model\n",
        "\n",
        "llm = ChatAnthropicSKT()\n",
        "\n",
        "# llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# llm = ChatAnthropicSKT(model=\"azure/openai/gpt-4-1106\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Annotated, List\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from pathlib import Path\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "@tool\n",
        "def check_schema(\n",
        "    # dataframe_json: Annotated[str, \"dataframe in JSON format.\"], \n",
        "    dataframe_path: Annotated[str, \"path to the dataframe.\"]\n",
        "):    \n",
        "    \"\"\"\n",
        "    Check the schema of a dataframe.\n",
        "    \n",
        "    Args:\n",
        "        # dataframe_json (str): The dataframe in JSON format.\n",
        "        dataframe_path (str): The path to the dataframe.\n",
        "    \n",
        "    Returns:\n",
        "        str: The schema of the dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"=====check_schema>dataframe_json: \",dataframe_json)\n",
        "    print(\"=====check_schema>dataframe_path: \",dataframe_path)\n",
        "\n",
        "    try:\n",
        "        # If it's a string, try to parse it as JSON\n",
        "        # try:\n",
        "        #     if isinstance(dataframe_json, str):\n",
        "        #         json_data = json.loads(dataframe_json)\n",
        "        #     else:\n",
        "        #         json_data = dataframe_json\n",
        "            \n",
        "        #     # Create a DataFrame\n",
        "        #     df = pd.DataFrame(json_data)\n",
        "        # except:\n",
        "        #     df = pd.read_csv(dataframe_path)\n",
        "\n",
        "        df = pd.read_csv(dataframe_path)\n",
        "        \n",
        "        # Get the schema\n",
        "        schema = df.dtypes.to_string()\n",
        "        return f\"DataFrame schema:\\n{schema}\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        error_msg = f\"Error decoding JSON string: {str(e)}\"\n",
        "        return error_msg\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error checking schema: {str(e)}\"\n",
        "        return error_msg\n",
        "\n",
        "@tool\n",
        "def generate_sql_query(\n",
        "    user_request: Annotated[str,\"user request to generate a sql query\"],\n",
        "    dataframe_schema: Annotated[str,\"dataframe schema to generate a sql query\"],\n",
        "    dataframe_name: Annotated[str,\"dataframe name to search\"]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    A tool to generate a SQL query based on the user request and schema in the message.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"=====generate_sql_query>dataframe_schema: \",dataframe_schema)\n",
        "    # print(\"=====generate_sql_query>dataframe_name: \",dataframe_name)\n",
        "\n",
        "    try:\n",
        "        prompt = f\"Given the following user request:'{user_request}' and schema:\\n{dataframe_schema} and table name:'{dataframe_name}'\\n\\nGenerate a SQL query for the following message.\"\n",
        "        sql_query = llm.invoke([HumanMessage(content=prompt)]).content\n",
        "        return sql_query\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "# @tool\n",
        "# def search_dataframe(\n",
        "#     dataframe_json: Annotated[str, \"dataframe in JSON format.\"], \n",
        "#     sql_query: Annotated[str, \"SQL query to execute.\"]\n",
        "# ) -> str:\n",
        "#     \"\"\"\n",
        "#     A tool to search a dataframe using a SQL query from SQL_Expert.\n",
        "#     \"\"\"\n",
        "#     print(\"=====search_dataframe>dataframe_json: \",dataframe_json)\n",
        "#     print(\"=====search_dataframe>sql_query: \",sql_query)\n",
        "#     try:\n",
        "#         df = pd.read_json(dataframe_json)\n",
        "#         result = psql.sqldf(sql_query, locals())\n",
        "#         return result.to_json(orient='records')\n",
        "#     except Exception as e:\n",
        "#         return \"[]\"\n",
        "\n",
        "@tool\n",
        "def search_dataframe(\n",
        "    dataframe_path: Annotated[str, \"path to the dataframe.\"],\n",
        "    sql_query: Annotated[str, \"SQL query to execute.\"]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    A tool to search a dataframe using a SQL query from SQL_Expert.\n",
        "    \"\"\"\n",
        "    # print(\"=====search_dataframe>dataframe_path: \",dataframe_path)\n",
        "    print(\"=====search_dataframe>sql_query: \",sql_query)\n",
        "    try:\n",
        "        df = pd.read_csv(dataframe_path)\n",
        "        # print(\"=====search_dataframe>result: \",stock_data_krx)\n",
        "        result = psql.sqldf(sql_query, locals())\n",
        "        return result.to_json(orient='records')\n",
        "    except Exception as e:\n",
        "        return \"[]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Optional, Dict, Any, Annotated\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "def create_agent(\n",
        "    llm,\n",
        "    tools: list,\n",
        "    system_message: str,\n",
        ") -> str:\n",
        "\n",
        "    tool_names = [tool.name for tool in tools]\n",
        "    tool_descriptions = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
        "\n",
        "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
        "    \n",
        "    system_prompt = \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "    \" Use the provided tools to progress towards answering the question.\"\n",
        "    \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "    \" will help where you left off. Execute what you can to make progress.\"\n",
        "    \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "    \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "    \" You have access to the following tools: \\n{tool_descriptions}.\\n{system_message}\"\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    # prompt = prompt.partial(tool_names=tool_names)\n",
        "    prompt = prompt.partial(tool_descriptions=tool_descriptions)\n",
        "\n",
        "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor\n",
        "\n",
        "# def create_agent(llm, tools: list, system_message: str):\n",
        "#     \"\"\"Create an agent.\"\"\"\n",
        "\n",
        "#     tool_names = [tool.name for tool in tools]\n",
        "#     tool_descriptions = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
        "\n",
        "#     prompt = ChatPromptTemplate.from_messages(\n",
        "#         [\n",
        "#             (\n",
        "#                 \"system\",\n",
        "#                 \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "#                 \" Use the provided tools to progress towards answering the question.\"\n",
        "#                 \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "#                 \" will help where you left off. Execute what you can to make progress.\"\n",
        "#                 \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "#                 \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "#                 \" You have access to the following tools: {tool_names}\\n{tool_descriptions}.\\n{system_message}\",\n",
        "#             ),\n",
        "#             MessagesPlaceholder(variable_name=\"messages\"),\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     prompt = prompt.partial(system_message=system_message)\n",
        "#     prompt = prompt.partial(tool_names=tool_names)\n",
        "#     prompt = prompt.partial(tool_descriptions=tool_descriptions)\n",
        "#     return prompt | llm.bind_tools(tools)\n",
        "\n",
        "def agent_node(state, agent, name):\n",
        "\n",
        "    # print(f\"=====1. agent_node>new_state({name}): \",state['messages'][-1].content)\n",
        "\n",
        "    state_tmp = state.copy()\n",
        "\n",
        "    state_tmp[\"messages\"] = [HumanMessage(content=state_tmp['messages'][-1].content+\"\\n\\n[Current Available Arguments and Values for Tools]\\n\"+\"\\n\".join([\"\\t \"+k+\": \"+str(v) for k, v in state_tmp.items() if k not in ['messages','team_members','next','agent_result']]\n",
        ")\n",
        "# +\"\\n\\n\"+state_tmp[\"agent_result\"]\n",
        ", name=name)]\n",
        "    \n",
        "    # print(f\"=====2. agent_node>new_state({name}): \",state_tmp['messages'][-1].content)\n",
        "\n",
        "    result = agent.invoke(state_tmp)\n",
        "    output = result[\"output\"]\n",
        "\n",
        "    new_state = state.copy()\n",
        "\n",
        "    new_state[\"agent_result\"] = new_state[\"agent_result\"]  + f\"\"\"\\n\\n{len(state_tmp[\"messages\"])}:\\n\"\"\" + output\n",
        "    new_state[\"messages\"] = new_state[\"messages\"] + [HumanMessage(content=output, name=name)]\n",
        "\n",
        "    # print(f\"=====3. agent_node>new_state({name}): \",output)\n",
        "\n",
        "    # return {\"messages\": [HumanMessage(content=\"END\", name=name)]}    \n",
        "    return new_state\n",
        "\n",
        "schema_agent = create_agent(llm, [check_schema], \"You are a data analyst specializing in understanding data schemas. Use the provided tools to analyze the schema of the given data. Just identify the schema and Do not generate SQL queries.\")\n",
        "schema_node = functools.partial(agent_node, agent=schema_agent, name=\"Schema_Analyst\")\n",
        "\n",
        "sql_agent = create_agent(llm, [generate_sql_query], \"You are a SQL expert who generates queries based on natural language requests and data schemas. Use the provided tools your expertise to generate SQL queries. Remark that all table names for SQL queries shoudld be 'df'. If you have the request with SQL queries already, you can skip your turn.\")\n",
        "sql_node = functools.partial(agent_node, agent=sql_agent, name=\"SQL_Expert\")\n",
        "\n",
        "search_agent = create_agent(llm, [search_dataframe], \"You are a data scientist who executes or runs SQL queries on pandas DataFrames and interprets the results. Use the provided tools to search the DataFrame. If you have given SQL queries, run them on the DataFrame.\")\n",
        "search_node = functools.partial(agent_node, agent=search_agent, name=\"Search_Agent\")\n",
        "\n",
        "data_scientist = create_agent(llm, [check_schema, generate_sql_query, search_dataframe], \"You are a data analyst specializing in understanding data schemas. You are a SQL expert who generates queries based on natural language requests and data schemas. You are a data scientist who executes SQL queries on pandas DataFrames and interprets the results\")\n",
        "data_scientist_node = functools.partial(agent_node, agent=data_scientist, name=\"Data_Scientist\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: List[str]\n",
        "    next: Optional[str]\n",
        "    # dataframe_json: str\n",
        "    dataframe_path: str\n",
        "    table_name: Optional[str]\n",
        "    # sql_query: Optional[str]\n",
        "    # search_result: Optional[str]\n",
        "    agent_result: Optional[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
        "    \"\"\"An LLM-based router.\"\"\"\n",
        "    options = [\"FINISH\"] + members\n",
        "    function_def = {\n",
        "        \"name\": \"route\",\n",
        "        \"description\": \"Select the next role.\",\n",
        "        \"parameters\": {\n",
        "            \"title\": \"routeSchema\",\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"next\": {\n",
        "                    \"title\": \"Next\",\n",
        "                    \"anyOf\": [\n",
        "                        {\"enum\": options},\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"next\"],\n",
        "        },\n",
        "    }\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Given the conversation above, who should act next?\"\n",
        "                \" Or should we FINISH? Select one of: {options}\",\n",
        "            ),\n",
        "        ]\n",
        "    ).partial(options=str(options), team_members=\", \".join(members))\n",
        "    \n",
        "    return (\n",
        "        prompt\n",
        "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "        | JsonOutputFunctionsParser()\n",
        "    )\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from typing import List, Dict, Callable, Union\n",
        "\n",
        "def create_team_supervisor2(llm, system_prompt: str, members: List[str]) -> Callable:\n",
        "    \"\"\"An LLM-based router compatible with Claude.\"\"\"\n",
        "    options = [\"FINISH\"] + members\n",
        "    \n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given the conversation above, who should act next?\"\n",
        "                \" Or should we FINISH? Select one of: {options}\"\n",
        "                \" Respond with only the name of the next actor or FINISH, nothing else.\"\n",
        "            ),\n",
        "        ]\n",
        "    ).partial(options=str(options), team_members=\", \".join(members))\n",
        "\n",
        "    def route(state: Dict[str, Union[List[Dict], str, None]]) -> Dict[str, str]:\n",
        "        # Extract the messages from the state\n",
        "        messages = state.get('messages', [])\n",
        "        \n",
        "        # Convert dictionary messages to BaseMessage objects if necessary\n",
        "        chat_history = []\n",
        "        for msg in messages:\n",
        "            # print(\"************msg: \",msg)\n",
        "            if isinstance(msg, dict):\n",
        "                # print(\"************msg.get('type'): \",msg.get('type'))\n",
        "                if msg.get('type') == 'human':\n",
        "                    chat_history.append(HumanMessage(content=msg['content']))\n",
        "                elif msg.get('type') == 'ai':\n",
        "                    chat_history.append(SystemMessage(content=msg['content']))\n",
        "                    # print(\"************msg['content']: \",msg['content'])\n",
        "            else:\n",
        "                chat_history.append(msg)\n",
        "\n",
        "        # print(\"************prompt: \",prompt)\n",
        "\n",
        "        # Format the prompt with the chat history\n",
        "        formatted_prompt = prompt.format_messages(chat_history=chat_history[-1:])\n",
        "        \n",
        "        # Invoke the LLM\n",
        "        # print(f\"=====create_team_supervisor2>formatted_prompt: \",formatted_prompt)\n",
        "        response = llm.invoke(formatted_prompt)\n",
        "        next_actor = response.content.strip()\n",
        "        \n",
        "        if next_actor not in options:\n",
        "            print(f\"Invalid response from AI: {next_actor}. Defaulting to FINISH.\")\n",
        "            next_actor = \"FINISH\"\n",
        "        \n",
        "        return {\"next\": next_actor}\n",
        "\n",
        "    return route\n",
        "\n",
        "\n",
        "team_members_dict = {\n",
        "        # \"Schema_Analyst\": schema_node,\n",
        "        # \"SQL_Expert\": sql_node, \n",
        "        # \"Search_Agent\": search_node,\n",
        "        \"Data_Scientist\": data_scientist_node,\n",
        "}\n",
        "\n",
        "team_members = list(team_members_dict.keys())\n",
        "\n",
        "supervisor_agent = create_team_supervisor2(\n",
        "    llm,\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    f\" following workers: {', '.join(team_members)}.\" \n",
        "    \"Usually, in order to search a dataframe, you will need to analyze the schema, generate a SQL query, and then search the dataframe.\"\n",
        "    \"Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\",\n",
        "    team_members,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# workflow = StateGraph(AgentState)\n",
        "# workflow.add_node(\"Schema_Analyst\", schema_node)\n",
        "# workflow.add_node(\"SQL_Expert\", sql_node)\n",
        "# workflow.add_node(\"Data_Scientist\", search_node)\n",
        "# workflow.add_node(\"supervisor\", supervisor_agent)\n",
        "\n",
        "# workflow.add_edge(\"Schema_Analyst\", \"supervisor\")\n",
        "# workflow.add_edge(\"SQL_Expert\", \"supervisor\")\n",
        "# workflow.add_edge(\"Data_Scientist\", \"supervisor\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"supervisor\", supervisor_agent)\n",
        "for agt, nd in team_members_dict.items():\n",
        "    workflow.add_node(agt, nd)\n",
        "    workflow.add_edge(agt, \"supervisor\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"supervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    dict(\n",
        "        **{d:d for d in team_members},\n",
        "        **{\"FINISH\": END}\n",
        "        ),\n",
        ")\n",
        "\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "initial_state = AgentState(\n",
        "    messages=[HumanMessage(content=\"Search the latest close price of symbol '000100.KS' from the dataframe in the path I give you.\", name=\"Human\")],\n",
        "    # messages=[HumanMessage(content=\"Search the latest close price of symbol '000100.KS'. You need to proceed as the process 1) find the schema of the dataframe, 2) generate a SQL query, and 3) search the dataframe with the SQL query from 2).\", name=\"Human\")],\n",
        "\n",
        "    team_members=team_members,\n",
        "    next=None,\n",
        "    # dataframe_json=None, #df.to_json(orient='records'),\n",
        "    dataframe_path=\"data/stock_data_krx.csv\",\n",
        "    table_name='df',\n",
        "    # sql_query=None,\n",
        "    # search_result=None,\n",
        "    agent_result=\"\"\n",
        ")\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# recursion_limit = 5\n",
        "# for output in graph.stream(initial_state, config={\"recursion_limit\": recursion_limit}):\n",
        "#     print(\"Workflow output:\", output)\n",
        "#     if \"messages\" in list(output.values())[-1]:\n",
        "#         print(\"Assistant:\", list(output.values())[-1]['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = workflow.compile()\n",
        "\n",
        "recursion_limit = 10\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    for event in graph.stream(AgentState(\n",
        "        messages=[HumanMessage(content=user_input, name=\"Human\")],\n",
        "        # messages=[HumanMessage(content=\"Search the latest close price of symbol '000100.KS'. You need to proceed as the process 1) find the schema of the dataframe, 2) generate a SQL query, and 3) search the dataframe with the SQL query from 2).\", name=\"Human\")],\n",
        "        team_members=team_members,\n",
        "        next=None,\n",
        "        # dataframe_json=None, #df.to_json(orient='records'),\n",
        "        dataframe_path=\"data/stock_data_krx.csv\",\n",
        "        table_name='df',\n",
        "        # sql_query=None,\n",
        "        # search_result=None,\n",
        "        agent_result=\"\"\n",
        "    ), config={\"recursion_limit\": recursion_limit}):\n",
        "        # print(\"Workflow output:\", event.values())\n",
        "        for value in event.values():\n",
        "            if \"messages\" in value:\n",
        "                # print(\"Assistant:\", value['messages'][-1].content)\n",
        "                value['messages'][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}