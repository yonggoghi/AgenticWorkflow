# ì¼ë°˜í™” ê°œì„  ê³„íš

## ğŸ¯ ëª©í‘œ
í˜„ì¬ ë°©ì‹ì„ ë‹¤ì–‘í•œ ì›¹ì‚¬ì´íŠ¸ì— ì ìš© ê°€ëŠ¥í•˜ë„ë¡ ê°œì„ 

## ğŸ“Š í˜„ì¬ ìƒíƒœ ë¶„ì„

### âœ… ì˜ ì‘ë™í•˜ëŠ” ë¶€ë¶„ (ìœ ì§€)
1. Playwright ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
2. LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
3. ë‹¤ì¤‘ selector fallback
4. í´ë¦­ â†’ URL ìº¡ì²˜ ë©”ì»¤ë‹ˆì¦˜

### âš ï¸ ê°œì„  í•„ìš”í•œ ë¶€ë¶„

#### 1. ë¬´í•œ ìŠ¤í¬ë¡¤ ì¬ì‹¤í–‰ (ê°€ì¥ í° ë¬¸ì œ)
**í˜„ì¬**: ë§¤ë²ˆ 5íšŒ ìŠ¤í¬ë¡¤ (ìƒí’ˆë‹¹ 2.5ì´ˆ ì¶”ê°€)
**ë¬¸ì œ**: 
- ì¼ë°˜ í˜ì´ì§€ì—ì„œ ë¶ˆí•„ìš”
- 180ê°œ ìƒí’ˆ: 7.5ë¶„ ë‚­ë¹„

**í•´ê²° ë°©ë²• 3ê°€ì§€:**

##### A. í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€ (ê¶Œì¥)
```python
def detect_infinite_scroll(page):
    """í˜ì´ì§€ê°€ ë¬´í•œ ìŠ¤í¬ë¡¤ì¸ì§€ ê°ì§€"""
    # ë°©ë²• 1: í˜ì´ì§€ë„¤ì´ì…˜ ë²„íŠ¼ í™•ì¸
    if page.locator('button.pagination, a.next-page').count() > 0:
        return False  # ì¼ë°˜ í˜ì´ì§€
    
    # ë°©ë²• 2: ìŠ¤í¬ë¡¤ í…ŒìŠ¤íŠ¸
    initial_height = page.evaluate('document.body.scrollHeight')
    page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
    page.wait_for_timeout(1000)
    new_height = page.evaluate('document.body.scrollHeight')
    
    return new_height > initial_height  # ë¬´í•œ ìŠ¤í¬ë¡¤

# ì‚¬ìš©
is_infinite = detect_infinite_scroll(list_page)
if is_infinite:
    # ë¬´í•œ ìŠ¤í¬ë¡¤ ì¬ì‹¤í–‰
else:
    # ë’¤ë¡œ ê°€ê¸°ë§Œ
```

##### B. ì„ íƒì  ì¬ìŠ¤í¬ë¡¤
```python
def smart_scroll_if_needed(page, target_id):
    """í•„ìš”í•  ë•Œë§Œ ìŠ¤í¬ë¡¤"""
    # selectorê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if page.locator(f'[prdid="{target_id}"]').count() == 0:
        # ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤
        for i in range(scroll_count):
            page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            page.wait_for_timeout(500)
            if page.locator(f'[prdid="{target_id}"]').count() > 0:
                break  # ì°¾ìœ¼ë©´ ì¤‘ë‹¨
```

##### C. URL íŒ¨í„´ í•™ìŠµ (ê°€ì¥ ë¹ ë¦„)
```python
def learn_url_pattern(captured_urls):
    """ì„±ê³µí•œ URLì—ì„œ íŒ¨í„´ ì¶”ì¶œ"""
    # ì˜ˆ: https://site.com/product/detail?prdId=PR00000538
    #  â†’ https://site.com/product/detail?prdId={ID}
    
    if len(captured_urls) >= 3:
        # íŒ¨í„´ ë¶„ì„
        common_pattern = extract_pattern(captured_urls)
        return common_pattern
    return None

# ì‚¬ìš©
url_pattern = learn_url_pattern(url_mapping.values())
if url_pattern:
    # í´ë¦­ ì—†ì´ URL ìƒì„±
    detail_url = url_pattern.format(ID=prd_id)
else:
    # í´ë¦­ ë°©ì‹
```

#### 2. Selector ì¼ë°˜í™”

**í˜„ì¬**: íŠ¹ì • ì†ì„±ëª… í•˜ë“œì½”ë”©
```python
f'a[prdid="{prd_id}"]'  # prdidë§Œ
```

**ê°œì„ **: ë‹¤ì–‘í•œ ì†ì„±ëª… ì‹œë„
```python
def generate_selectors(prd_id):
    """ì—¬ëŸ¬ ì†ì„±ëª… íŒ¨í„´ ìƒì„±"""
    attr_names = ['prdid', 'data-id', 'product-id', 'id', 'data-product-id']
    tag_names = ['a', 'div', 'button']
    
    selectors = []
    for attr in attr_names:
        for tag in tag_names:
            selectors.append(f'{tag}[{attr}="{prd_id}"]')
    
    return selectors
```

#### 3. ì„±ëŠ¥ ìµœì í™”

**í˜„ì¬**: ìˆœì°¨ ì²˜ë¦¬ (9ê°œ Ã— 5ì´ˆ = 45ì´ˆ)

**ê°œì„  1**: ë³‘ë ¬ ì²˜ë¦¬
```python
from concurrent.futures import ThreadPoolExecutor

def capture_urls_parallel(product_ids, max_workers=3):
    """ë³‘ë ¬ë¡œ URL ìº¡ì²˜"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ì›Œì»¤ê°€ ë³„ë„ ë¸Œë¼ìš°ì € ì‚¬ìš©
        futures = []
        for prd_id in product_ids:
            future = executor.submit(capture_single_url, prd_id)
            futures.append(future)
        
        results = [f.result() for f in futures]
    return results
```

**ê°œì„  2**: íƒ­ ì¬ì‚¬ìš©
```python
# ë’¤ë¡œ ê°€ê¸° ëŒ€ì‹  íƒ­ ë‹«ê¸°
detail_page = browser.new_page()
detail_page.goto(detail_url)
url = detail_page.url
detail_page.close()  # ëª©ë¡ í˜ì´ì§€ëŠ” ê·¸ëŒ€ë¡œ
```

## ğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (1-2ì‹œê°„)
- [ ] **URL íŒ¨í„´ í•™ìŠµ** (ê°€ì¥ íš¨ê³¼ì )
  - 3ê°œ ì„±ê³µ í›„ íŒ¨í„´ ì¶”ì¶œ
  - ë‚˜ë¨¸ì§€ëŠ” íŒ¨í„´ìœ¼ë¡œ ìƒì„±
  - ì˜ˆìƒ ì†ë„: 45ì´ˆ â†’ **10ì´ˆ**

### Phase 2: ë‹¨ê¸° ê°œì„  (1ì¼)
- [ ] **ì„ íƒì  ì¬ìŠ¤í¬ë¡¤**
  - ìš”ì†Œ ì—†ì„ ë•Œë§Œ ìŠ¤í¬ë¡¤
  - ì˜ˆìƒ ì†ë„: 45ì´ˆ â†’ 20ì´ˆ

### Phase 3: ì¤‘ê¸° ê°œì„  (3ì¼)
- [ ] **í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€**
  - ë¬´í•œ ìŠ¤í¬ë¡¤ vs ì¼ë°˜ í˜ì´ì§€
  - ìë™ ì „ëµ ì„ íƒ

### Phase 4: ì¥ê¸° ê°œì„  (1ì£¼)
- [ ] **ë³‘ë ¬ ì²˜ë¦¬**
  - 3ê°œ íƒ­ ë™ì‹œ ì²˜ë¦¬
  - ì˜ˆìƒ ì†ë„: 45ì´ˆ â†’ 15ì´ˆ

## ğŸ¯ ìµœì¢… ëª©í‘œ

### í˜„ì¬ (íŠ¹ì • ì‚¬ì´íŠ¸ ìµœì í™”)
```
180ê°œ ìƒí’ˆ ì²˜ë¦¬ ì‹œê°„: ~15ë¶„
- LLM ì¶”ì¶œ: 2ë¶„
- URL ìº¡ì²˜: 13ë¶„ (ìƒí’ˆë‹¹ 4-5ì´ˆ)
```

### Phase 1 ì ìš© í›„ (URL íŒ¨í„´ í•™ìŠµ)
```
180ê°œ ìƒí’ˆ ì²˜ë¦¬ ì‹œê°„: ~5ë¶„
- LLM ì¶”ì¶œ: 2ë¶„
- URL ìº¡ì²˜: 3ë¶„ (ì²˜ìŒ 3ê°œë§Œ í´ë¦­, ë‚˜ë¨¸ì§€ëŠ” íŒ¨í„´)
```

### Phase 4 ì™„ë£Œ í›„ (ëª¨ë“  ìµœì í™”)
```
180ê°œ ìƒí’ˆ ì²˜ë¦¬ ì‹œê°„: ~3ë¶„
- LLM ì¶”ì¶œ: 2ë¶„ (ë³‘ë ¬ ì²˜ë¦¬)
- URL ìº¡ì²˜: 1ë¶„ (ë³‘ë ¬ + íŒ¨í„´)
```

## ğŸ’¡ ê¶Œì¥ ì‚¬í•­

### 1. ì¦‰ì‹œ ì ìš©: URL íŒ¨í„´ í•™ìŠµ
**ê°€ì¥ í° íš¨ê³¼, ê°€ì¥ ì‰¬ìš´ êµ¬í˜„**

```python
# 3ê°œ ì„±ê³µ í›„
if len(url_mapping) >= 3:
    pattern = learn_pattern(url_mapping)
    # ë‚˜ë¨¸ì§€ëŠ” íŒ¨í„´ìœ¼ë¡œ ìƒì„±
    for remaining_id in product_ids[3:]:
        url_mapping[remaining_id] = pattern.format(ID=remaining_id)
```

### 2. ì„¤ì • ì˜µì…˜ ì¶”ê°€
```python
crawler = ProductCrawler(
    url=url,
    mode='auto',  # 'auto', 'infinite_scroll', 'normal', 'pattern_learning'
)
```

### 3. ìºì‹±
```python
# í•œ ë²ˆ í•™ìŠµí•œ íŒ¨í„´ ì €ì¥
cache = {
    'sktuniverse.co.kr': {
        'pattern': 'https://m.sktuniverse.co.kr/product/detail?prdId={ID}',
        'selector': 'a.inner-link[prdid="{ID}"]'
    }
}
```

## ğŸ” í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ ìœ í˜•
1. **ë¬´í•œ ìŠ¤í¬ë¡¤** (í˜„ì¬)
   - âœ… sktuniverse.co.kr

2. **ì¼ë°˜ í˜ì´ì§€ë„¤ì´ì…˜**
   - [ ] í…ŒìŠ¤íŠ¸ í•„ìš”

3. **ì •ì  ë§í¬**
   - [ ] í…ŒìŠ¤íŠ¸ í•„ìš”

4. **AJAX ë¡œë”©**
   - [ ] í…ŒìŠ¤íŠ¸ í•„ìš”

## ğŸ“š ì°¸ê³  ìë£Œ

### ìœ ì‚¬ í”„ë¡œì íŠ¸
- Scrapy: ë²”ìš© í¬ë¡¤ë§ í”„ë ˆì„ì›Œí¬
- Selenium Grid: ë³‘ë ¬ ë¸Œë¼ìš°ì €
- Puppeteer Cluster: ë¸Œë¼ìš°ì € í’€ë§

