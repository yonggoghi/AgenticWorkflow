#!/usr/bin/env python3
"""
ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ í¬ë¡¤ëŸ¬

ì›¹ í˜ì´ì§€ì—ì„œ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìƒì„¸ í˜ì´ì§€ê¹Œì§€ ìë™ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
LLMì„ ì‚¬ìš©í•˜ì—¬ ë¹„êµ¬ì¡°í™”ëœ ë°ì´í„°ë„ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import argparse
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

import pandas as pd
from tqdm import tqdm

try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("Warning: Playwright is not installed.")

try:
    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage, SystemMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("Warning: langchain-openai is not installed.")

try:
    from config import settings
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("Warning: config.py not found. Using default settings.")


class ProductCrawler:
    """ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str, use_llm: bool = True, model_name: str = "ax"):
        """
        Args:
            base_url: í¬ë¡¤ë§í•  ê¸°ë³¸ URL
            use_llm: LLMì„ ì‚¬ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œ ì—¬ë¶€
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ ("gemma", "ax", "claude", "gemini", "gpt")
        """
        self.base_url = base_url
        self.use_llm = use_llm
        self.model_name = model_name
        self.products = []
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if use_llm and LANGCHAIN_AVAILABLE:
            self._init_llm_client()
        else:
            if use_llm:
                print("Warning: langchain not available. LLM features disabled.")
            self.use_llm = False
            self.llm_client = None
    
    def _init_llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if not CONFIG_AVAILABLE:
            print("Warning: config not available. Using environment variables.")
            api_key = os.getenv("LLM_API_KEY", os.getenv("OPENAI_API_KEY"))
            base_url = os.getenv("LLM_API_URL", "https://api.openai.com/v1")
            model = "gpt-4o-mini"
            max_tokens = 8000
            temperature = 0.0
            seed = 42
        else:
            api_key = settings.API_CONFIG.llm_api_key
            base_url = settings.API_CONFIG.llm_api_url
            max_tokens = settings.ModelConfig.llm_max_tokens
            temperature = settings.ModelConfig.temperature
            seed = settings.ModelConfig.seed
            
            # ëª¨ë¸ ì„ íƒ
            model_map = {
                "gemma": settings.ModelConfig.gemma_model,
                "ax": settings.ModelConfig.ax_model,
                "claude": settings.ModelConfig.claude_model,
                "gemini": settings.ModelConfig.gemini_model,
                "gpt": settings.ModelConfig.gpt_model,
            }
            model = model_map.get(self.model_name, settings.ModelConfig.default_model)
        
        if not api_key:
            print("Warning: LLM API key not found. LLM features disabled.")
            self.use_llm = False
            self.llm_client = None
            return
        
        try:
            self.llm_client = ChatOpenAI(
                temperature=temperature,
                openai_api_key=api_key,
                openai_api_base=base_url,
                model=model,
                max_tokens=max_tokens,
                seed=seed
            )
            print(f"LLM ì´ˆê¸°í™”: {model}")
        except Exception as e:
            print(f"LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.use_llm = False
            self.llm_client = None
    
    def crawl_page(self, url: str, infinite_scroll: bool = False, 
                   scroll_count: int = 10, wait_time: int = 2000) -> Dict:
        """
        í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            infinite_scroll: ë¬´í•œ ìŠ¤í¬ë¡¤ ì—¬ë¶€
            scroll_count: ìŠ¤í¬ë¡¤ íšŸìˆ˜
            wait_time: ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwrightê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        result = {
            'url': url,
            'success': False,
            'html_content': '',
            'text_content': '',
            'links': [],
            'error': None
        }
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                print(f"  í˜ì´ì§€ ë¡œë”©: {url}")
                page.goto(url, wait_until='networkidle', timeout=30000)
                page.wait_for_timeout(wait_time)
                
                # ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
                if infinite_scroll:
                    for i in range(scroll_count):
                        previous_height = page.evaluate('document.body.scrollHeight')
                        page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        page.wait_for_timeout(2000)  # 2ì´ˆ ëŒ€ê¸° (ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ ë³´ì¥)
                        new_height = page.evaluate('document.body.scrollHeight')
                        
                        if new_height == previous_height:
                            break
                    
                    # ìµœì¢… ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ëª¨ë“  ì½˜í…ì¸  ì™„ì „ ë¡œë“œ)
                    print(f"  ìŠ¤í¬ë¡¤ ì™„ë£Œ, ìµœì¢… ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°...")
                    page.wait_for_timeout(3000)
                
                # ì½˜í…ì¸  ìˆ˜ì§‘
                result['html_content'] = page.content()
                result['text_content'] = page.inner_text('body')
                
                # ë§í¬ ìˆ˜ì§‘
                links = page.evaluate('''() => {
                    return Array.from(document.querySelectorAll('a[href]')).map(a => ({
                        text: a.innerText.trim(),
                        href: a.href
                    })).filter(link => link.href && link.text);
                }''')
                result['links'] = links
                result['success'] = True
                
                browser.close()
                
        except Exception as e:
            result['error'] = str(e)
            print(f"  í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return result
    
    def _chunk_text(self, text: str, chunk_size: int = 10000, overlap: int = 500) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
        
        Args:
            text: ë‚˜ëˆŒ í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
            overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(text):
                # ë‹¤ìŒ ì¤„ë°”ê¿ˆì´ë‚˜ ë§ˆì¹¨í‘œë¥¼ ì°¾ì•„ì„œ ìë¥´ê¸°
                for sep in ['\n\n', '\n', '. ', ' ']:
                    last_sep = text[start:end].rfind(sep)
                    if last_sep != -1:
                        end = start + last_sep + len(sep)
                        break
            
            chunks.append(text[start:end])
            
            # ë‹¤ìŒ ì²­í¬ëŠ” ì˜¤ë²„ë©ì„ ê³ ë ¤í•´ì„œ ì‹œì‘
            start = end - overlap if end < len(text) else end
        
        return chunks
    
    def _chunk_html(self, html_content: str, chunk_size: int = 15000) -> List[str]:
        """
        HTMLì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° - ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•œ ë°©ì‹
        
        Args:
            html_content: HTML ë‚´ìš©
            chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸° (ë¬¸ì ìˆ˜)
            
        Returns:
            HTML ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(html_content) <= chunk_size:
            return [html_content]
        
        from bs4 import BeautifulSoup
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # prdid ì†ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸° (ìƒí’ˆ ì»¨í…Œì´ë„ˆ)
            product_elements = soup.find_all(attrs={'prdid': True})
            
            if not product_elements or len(product_elements) < 5:
                # prdidê°€ ì—†ìœ¼ë©´ classì— 'product', 'item' í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
                product_elements = soup.find_all(class_=lambda x: x and 
                    any(word in str(x).lower() for word in ['product', 'item', 'pass', 'card']))
            
            if not product_elements or len(product_elements) < 5:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë‹¨ìˆœ ë¶„í• 
                print(f"  âš ï¸ ìƒí’ˆ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í•´ ë‹¨ìˆœ ë¶„í• í•©ë‹ˆë‹¤")
                chunks = []
                for i in range(0, len(html_content), chunk_size):
                    chunks.append(html_content[i:i+chunk_size])
                return chunks
            
            # ê° ìƒí’ˆ ìš”ì†Œë¥¼ HTML ë¬¸ìì—´ë¡œ ë³€í™˜
            product_htmls = [str(elem) for elem in product_elements]
            
            # ì²­í¬ í¬ê¸°ì— ë§ê²Œ ê·¸ë£¹í™” (ê° ì²­í¬ê°€ chunk_sizeë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡)
            chunks = []
            current_chunk = ""
            
            for prod_html in product_htmls:
                # ë‹¨ì¼ ìƒí’ˆ HTMLì´ chunk_sizeë³´ë‹¤ í¬ë©´ ê°•ì œë¡œ ì˜ë¼ì„œ ë³„ë„ ì²­í¬ë¡œ
                if len(prod_html) > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk)
                        current_chunk = ""
                    # í° HTMLì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
                    for i in range(0, len(prod_html), chunk_size):
                        chunks.append(prod_html[i:i+chunk_size])
                    continue
                
                # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•˜ë©´ chunk_size ì´ˆê³¼í•˜ëŠ”ì§€ ì²´í¬
                if len(current_chunk) + len(prod_html) > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = prod_html
                else:
                    current_chunk += "\n" + prod_html
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk)
            
            # ë””ë²„ê¹…: ì²­í¬ í¬ê¸° í™•ì¸
            max_chunk_size = max(len(c) for c in chunks) if chunks else 0
            if max_chunk_size > chunk_size * 1.5:
                print(f"  âš ï¸ ì²­í¬ í¬ê¸° ì´ˆê³¼ ê°ì§€: {max_chunk_size:,}ì (ëª©í‘œ: {chunk_size:,}ì)")
            
            return chunks if chunks else [html_content]
            
        except Exception as e:
            print(f"  âš ï¸ HTML íŒŒì‹± ì˜¤ë¥˜, ë‹¨ìˆœ ë¶„í• : {str(e)[:50]}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ë¶„í• 
            chunks = []
            for i in range(0, len(html_content), chunk_size):
                chunks.append(html_content[i:i+chunk_size])
            return chunks
    
    def extract_products_with_llm(self, html_content: str, text_content: str) -> List[Dict]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        HTMLì„ ì§ì ‘ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ID, ì´ë¦„, ê°€ê²© ë“±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            html_content: HTML ë‚´ìš©
            text_content: í…ìŠ¤íŠ¸ ë‚´ìš© (ì‚¬ìš© ì•ˆ í•¨)
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_llm or not self.llm_client:
            return []
        
        # HTMLì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (ìƒí’ˆ ì»¨í…Œì´ë„ˆ ë‹¨ìœ„ë¡œ)
        html_chunks = self._chunk_html(html_content, chunk_size=15000)
        
        # ğŸ”§ ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì²˜ë¦¬
        DEBUG_MODE = True
        if DEBUG_MODE:
            print(f"  ğŸ”§ [ë””ë²„ê¹… ëª¨ë“œ] ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            html_chunks = html_chunks[:1]
        
        print(f"  HTMLì„ {len(html_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ë””ë²„ê¹…: ì²­í¬ í¬ê¸° ë¶„í¬ ì¶œë ¥
        chunk_sizes = [len(c) for c in html_chunks]
        if chunk_sizes:
            print(f"  ì²­í¬ í¬ê¸°: ìµœì†Œ {min(chunk_sizes):,}ì, ìµœëŒ€ {max(chunk_sizes):,}ì, í‰ê·  {sum(chunk_sizes)//len(chunk_sizes):,}ì")
        
        all_products = []
        seen_products = set()  # ì¤‘ë³µ ì œê±°ìš© (ìƒí’ˆëª…+ID ê¸°ì¤€)
        
        # ê° HTML ì²­í¬ë§ˆë‹¤ LLM í˜¸ì¶œ
        for idx, html_chunk in enumerate(html_chunks):
            print(f"  ì²­í¬ {idx+1}/{len(html_chunks)} ì²˜ë¦¬ ì¤‘... ({len(html_chunk)} ë¬¸ì)")
            
            prompt = f"""ë‹¤ìŒì€ ì›¹ í˜ì´ì§€ì˜ HTMLì…ë‹ˆë‹¤. ì´ HTMLì—ì„œ ìƒí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

HTML êµ¬ì¡°ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì°¾ì•„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
- id: HTML ì†ì„±ì´ë‚˜ URLì— ìˆëŠ” ìƒí’ˆ ê³ ìœ  ì‹ë³„ì (prdid, data-id, product-id ë“±ì˜ ì†ì„±ì—ì„œ ì°¾ê¸°)
- name: ìƒí’ˆ/ì„œë¹„ìŠ¤ ì´ë¦„
- description: ìƒì„¸í•œ ì„¤ëª… (ìµœëŒ€ 50ì)
- detail_url: ìƒì„¸ í˜ì´ì§€ ë§í¬
  * <a> íƒœê·¸ì˜ href ì†ì„±ì—ì„œ ì‹¤ì œ URL ì°¾ê¸°
  * ìƒëŒ€ ê²½ë¡œë„ ê·¸ëŒ€ë¡œ ë°˜í™˜ (/detail, ./product ë“±)
  * javascript:void(0)ì´ê±°ë‚˜ ë§í¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ""

HTML:
---
{html_chunk}
---

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
[
  {{"id": "PR00000123", "name": "ìƒí’ˆëª…", "description": "í• ì¸ì¿ í°", "detail_url": "/detail?id=PR00000123"}},
  {{"id": "PR00000456", "name": "ìƒí’ˆëª…2", "description": "ë¬´ë£Œë°°ì†¡", "detail_url": ""}}
]

ì¤‘ìš”:
1. ì™„ì „í•œ JSON ë°°ì—´ ë°˜í™˜ (ë§ˆì§€ë§‰ ] í•„ìˆ˜)
2. idëŠ” HTML ì†ì„±ì—ì„œ ì°¾ì€ ì‹¤ì œ ê°’ ì‚¬ìš©
3. detail_urlì€ hrefì— ì‹¤ì œ URLì´ ìˆì„ ë•Œë§Œ ì‚¬ìš©
4. ëª¨ë“  ìƒí’ˆ ì¶”ì¶œ
5. JSONë§Œ ë°˜í™˜, ì„¤ëª… ê¸ˆì§€
"""
            
            try:
                # langchain ChatOpenAI ì‚¬ìš©
                messages = [HumanMessage(content=prompt)]
                response = self.llm_client.invoke(messages)
                response_text = response.content
                
                # ì‘ë‹µì´ ì˜ë ¸ëŠ”ì§€ í™•ì¸
                if hasattr(response, 'response_metadata'):
                    finish_reason = response.response_metadata.get('finish_reason', '')
                    if finish_reason == 'length':
                        print(f"    âš ï¸ ì£¼ì˜: LLM ì‘ë‹µì´ max_tokens ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤")
                
                # JSON íŒŒì‹± ì „ì²˜ë¦¬
                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                response_text = re.sub(r'^```json\s*', '', response_text, flags=re.MULTILINE)
                response_text = re.sub(r'^```\s*$', '', response_text, flags=re.MULTILINE)
                response_text = response_text.strip()
                
                # ë¹ˆ ì‘ë‹µ ì²´í¬
                if not response_text or response_text == "[]":
                    continue
                
                # JSON ë°°ì—´ ì°¾ê¸° (ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì œê±°)
                json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(0)
                
                products = json.loads(response_text)
                
                # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€ (ID + ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬)
                for product in products:
                    product_key = (product.get('id', '') + '|||' + 
                                  product.get('name', ''))
                    if product_key and product_key not in seen_products:
                        seen_products.add(product_key)
                        all_products.append(product)
                
                print(f"    â†’ {len(products)}ê°œ ìƒí’ˆ ë°œê²¬ (ì´ {len(all_products)}ê°œ)")
                
            except json.JSONDecodeError as e:
                print(f"    â†’ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                
                # ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„
                try:
                    # ë§ˆì§€ë§‰ í•­ëª©ì´ ì˜ë ¸ì„ ê°€ëŠ¥ì„± - ë§ˆì§€ë§‰ ì™„ì „í•œ ê°ì²´ê¹Œì§€ë§Œ íŒŒì‹±
                    # ë§ˆì§€ë§‰ ì™„ì „í•œ }ë¥¼ ì°¾ì•„ì„œ ê±°ê¸°ê¹Œì§€ë§Œ ì‚¬ìš©
                    last_complete = response_text.rfind('},')
                    if last_complete != -1:
                        # ë§ˆì§€ë§‰ ì™„ì „í•œ ê°ì²´ ë‹¤ìŒì— ]ë¥¼ ì¶”ê°€
                        recovered_text = response_text[:last_complete+1] + ']'
                        products = json.loads(recovered_text)
                        
                        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
                        for product in products:
                            product_key = (product.get('id', '') + '|||' + 
                                          product.get('name', ''))
                            if product_key and product_key not in seen_products:
                                seen_products.add(product_key)
                                all_products.append(product)
                        
                        print(f"    â†’ ë³µêµ¬ ì„±ê³µ: {len(products)}ê°œ ìƒí’ˆ ë°œê²¬ (ì´ {len(all_products)}ê°œ)")
                        continue
                except:
                    pass
                
                # ë””ë²„ê¹…ì„ ìœ„í•´ ì‘ë‹µ ì¼ë¶€ ì¶œë ¥
                print(f"    â†’ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
                print(f"    â†’ ë³µêµ¬ ì‹¤íŒ¨, ì´ ì²­í¬ ê±´ë„ˆëœ€")
                continue
            except Exception as e:
                print(f"    â†’ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"  ì´ {len(all_products)}ê°œì˜ ê³ ìœ  ìƒí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        
        # IDê°€ ìˆëŠ” ìƒí’ˆ ê°œìˆ˜ í™•ì¸
        products_with_id = sum(1 for p in all_products if p.get('id'))
        if products_with_id > 0:
            print(f"  {products_with_id}ê°œ ìƒí’ˆì— IDê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # detail_urlì´ ìˆëŠ” ìƒí’ˆ ê°œìˆ˜ í™•ì¸
        products_with_detail_url = sum(1 for p in all_products if p.get('detail_url'))
        if products_with_detail_url > 0:
            print(f"  {products_with_detail_url}ê°œ ìƒí’ˆì— ìƒì„¸ í˜ì´ì§€ URLì´ ìˆìŠµë‹ˆë‹¤.")
        
        return all_products
    
    def extract_product_details_with_llm(self, text_content: str) -> Dict:
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            text_content: í…ìŠ¤íŠ¸ ë‚´ìš©
            
        Returns:
            ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.use_llm or not self.llm_client:
            return {}
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ìƒì„¸ í˜ì´ì§€ëŠ” ë³´í†µ ì²˜ìŒì— ì¤‘ìš” ì •ë³´ê°€ ìˆìŒ)
        max_length = 8000
        content_for_llm = text_content[:max_length] if len(text_content) > max_length else text_content
        
        if len(text_content) > max_length:
            print(f"    í…ìŠ¤íŠ¸ ê¸¸ì´ {len(text_content)} â†’ {max_length}ë¡œ ì¶•ì†Œ")
        
        prompt = f"""ë‹¤ìŒì€ ìƒí’ˆ/ì„œë¹„ìŠ¤ ìƒì„¸ í˜ì´ì§€ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì›¹ í˜ì´ì§€ ë‚´ìš©:
---
{content_for_llm}
---

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):
{{
  "name": "ìƒí’ˆ/ì„œë¹„ìŠ¤ ì´ë¦„",
  "description": "ìƒì„¸ ì„¤ëª… (200ì ì´ë‚´ë¡œ ìš”ì•½)",
  "category": "ì¹´í…Œê³ ë¦¬",
  "features": ["ì£¼ìš” íŠ¹ì§•1", "ì£¼ìš” íŠ¹ì§•2", ...],
  "specifications": {{"ìŠ¤í™í‚¤": "ìŠ¤í™ê°’", ...}}
}}

ì •ë³´ê°€ ì—†ëŠ” í•„ë“œëŠ” ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë¹ˆ ë°°ì—´/ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        
        try:
            # langchain ChatOpenAI ì‚¬ìš©
            messages = [HumanMessage(content=prompt)]
            response = self.llm_client.invoke(messages)
            response_text = response.content
            
            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            response_text = re.sub(r'^```json\s*', '', response_text, flags=re.MULTILINE)
            response_text = re.sub(r'^```\s*$', '', response_text, flags=re.MULTILINE)
            response_text = response_text.strip()
            
            # JSON ê°ì²´ ì°¾ê¸° (ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì œê±°)
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)
            
            details = json.loads(response_text)
            return details
            
        except json.JSONDecodeError as e:
            print(f"  ìƒì„¸ ì •ë³´ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"  ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            return {}
        except Exception as e:
            print(f"  ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {}
    
    def extract_products_simple(self, html_content: str, links: List[Dict]) -> List[Dict]:
        """
        ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            html_content: HTML ë‚´ìš©
            links: ë§í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        products = []
        
        # ìƒí’ˆ ê´€ë ¨ í‚¤ì›Œë“œ
        product_keywords = ['product', 'item', 'detail', 'goods', 'ìƒí’ˆ', 'ì œí’ˆ', 'ì„œë¹„ìŠ¤']
        
        for idx, link in enumerate(links):
            text = link['text'].lower()
            href = link['href'].lower()
            
            # ìƒí’ˆ ê´€ë ¨ ë§í¬ì¸ì§€ í™•ì¸
            if any(keyword in text or keyword in href for keyword in product_keywords):
                products.append({
                    'id': str(idx + 1),
                    'name': link['text'][:100],  # ì´ë¦„ì€ 100ìë¡œ ì œí•œ
                    'description': '',
                    'price': '',
                    'detail_url': link['href']
                })
        
        print(f"  ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ {len(products)}ê°œì˜ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return products
    
    def extract_detail_urls_from_browser(self, url: str, product_ids: List[str], 
                                         infinite_scroll: bool = True, 
                                         scroll_count: int = 10) -> Dict[str, str]:
        """
        Playwrightë¡œ ì‹¤ì œ ë¸Œë¼ìš°ì €ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ í´ë¦­í•´ì„œ detail_urlì„ ìº¡ì²˜í•©ë‹ˆë‹¤.
        
        Args:
            url: ëª©ë¡ í˜ì´ì§€ URL
            product_ids: ìƒí’ˆ ID ë¦¬ìŠ¤íŠ¸
            infinite_scroll: ë¬´í•œ ìŠ¤í¬ë¡¤ ì—¬ë¶€
            scroll_count: ìŠ¤í¬ë¡¤ íšŸìˆ˜
            
        Returns:
            {product_id: detail_url} ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n[ì¶”ê°€] ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ detail_url ìº¡ì²˜ ì¤‘...")
        
        url_mapping = {}
        
        if not PLAYWRIGHT_AVAILABLE:
            print("  âš ï¸ Playwright ì—†ìŒ, URL ìº¡ì²˜ ìƒëµ")
            return url_mapping
        
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ëª©ë¡ í˜ì´ì§€ (ê³„ì† ìœ ì§€)
                list_page = browser.new_page()
                
                # ëª©ë¡ í˜ì´ì§€ ì ‘ì†
                print(f"  í˜ì´ì§€ ë¡œë”©: {url}")
                list_page.goto(url, wait_until='networkidle', timeout=30000)
                list_page.wait_for_timeout(2000)
                
                # ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
                if infinite_scroll:
                    for i in range(scroll_count):
                        previous_height = list_page.evaluate('document.body.scrollHeight')
                        list_page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        list_page.wait_for_timeout(2000)
                        new_height = list_page.evaluate('document.body.scrollHeight')
                        if new_height == previous_height:
                            break
                    list_page.wait_for_timeout(3000)
                
                # ê° ìƒí’ˆ IDì— ëŒ€í•´ ë§í¬ í´ë¦­ ì‹œë„
                captured_count = 0
                failed_count = 0
                failed_reasons = {}
                print(f"  {len(product_ids)}ê°œ ìƒí’ˆì˜ detail_url ìº¡ì²˜ ì‹œì‘...")
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ìƒí’ˆ IDë¡œ selector í…ŒìŠ¤íŠ¸
                if product_ids:
                    first_id = product_ids[0]
                    print(f"  [ë””ë²„ê¹…] ì²« ë²ˆì§¸ ìƒí’ˆ ID: {first_id}")
                    test_selectors = [
                        f'a.inner-link[prdid="{first_id}"][godetailyn="Y"]',
                        f'a.inner-link[prdid="{first_id}"]',
                        f'a[prdid="{first_id}"]',
                        f'[prdid="{first_id}"]',  # ëª¨ë“  ìš”ì†Œ
                    ]
                    for sel in test_selectors:
                        count = list_page.locator(sel).count()
                        print(f"  [ë””ë²„ê¹…] {sel}: {count}ê°œ ë°œê²¬")
                
                for idx, prd_id in enumerate(product_ids):
                    try:
                        # ğŸ”§ ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ìƒí’ˆì€ ìƒì„¸ ë¡œê·¸
                        VERBOSE = idx < 3
                        
                        if VERBOSE:
                            print(f"\n  [ìƒí’ˆ {idx+1}] ID: {prd_id}")
                        
                        # ì—¬ëŸ¬ selector íŒ¨í„´ ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
                        selectors = [
                            f'a.inner-link[prdid="{prd_id}"][godetailyn="Y"]',  # ì›ë˜ ì¡°ê±´
                            f'a.inner-link[prdid="{prd_id}"]',  # godetailyn ì—†ì–´ë„ OK
                            f'a[prdid="{prd_id}"]',  # inner-link í´ë˜ìŠ¤ ì—†ì–´ë„ OK
                        ]
                        
                        link = None
                        selected_selector = None
                        for sel_idx, selector in enumerate(selectors):
                            try:
                                count = list_page.locator(selector).count()
                                if VERBOSE:
                                    print(f"    selector {sel_idx+1}: {count}ê°œ ë°œê²¬")
                                
                                if count > 0:
                                    link = list_page.locator(selector).first
                                    selected_selector = selector
                                    if VERBOSE:
                                        print(f"    âœ… selector {sel_idx+1} ì„ íƒë¨")
                                    break
                            except Exception as e:
                                if VERBOSE:
                                    print(f"    âŒ selector {sel_idx+1} ì˜¤ë¥˜: {str(e)[:50]}")
                                continue
                        
                        if not link or not selected_selector:
                            failed_count += 1
                            failed_reasons['not_found'] = failed_reasons.get('not_found', 0) + 1
                            if VERBOSE:
                                print(f"    âŒ ëª¨ë“  selector ì‹¤íŒ¨ - not_found")
                            continue
                        
                        # ìš”ì†Œë¡œ ìŠ¤í¬ë¡¤í•´ì„œ viewportì— í‘œì‹œ
                        try:
                            if VERBOSE:
                                print(f"    ìŠ¤í¬ë¡¤ ì‹œë„...")
                            link.scroll_into_view_if_needed(timeout=2000)
                            list_page.wait_for_timeout(300)
                            if VERBOSE:
                                print(f"    âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ")
                        except Exception as e:
                            if VERBOSE:
                                print(f"    âš ï¸ ìŠ¤í¬ë¡¤ ì‹¤íŒ¨: {str(e)[:50]}")
                        
                        # ğŸ”§ ë‹¨ìˆœí™”ëœ ë°©ì‹: ëª©ë¡ í˜ì´ì§€ ìœ ì§€, í´ë¦­ í›„ ë’¤ë¡œ ê°€ê¸°
                        try:
                            if VERBOSE:
                                print(f"    í´ë¦­ ì‹œë„...")
                            
                            # í´ë¦­ ì „ URL ì €ì¥
                            original_list_url = list_page.url
                            
                            # ë§í¬ í´ë¦­
                            link.click()
                            list_page.wait_for_timeout(2000)  # URL ë³€ê²½ ëŒ€ê¸°
                            
                            # URLì´ ë³€ê²½ë˜ì—ˆë‹¤ë©´ ì„±ê³µ
                            if list_page.url != original_list_url:
                                detail_url = list_page.url
                                if VERBOSE:
                                    print(f"    âœ… ìƒì„¸ í˜ì´ì§€ URL: {detail_url[:80]}...")
                                
                                # URL ì €ì¥
                                url_mapping[prd_id] = detail_url
                                captured_count += 1
                                if VERBOSE:
                                    print(f"    âœ… URL ìº¡ì²˜ ì„±ê³µ!")
                                
                                # ëª©ë¡ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°
                                if VERBOSE:
                                    print(f"    ë’¤ë¡œ ê°€ê¸°...")
                                list_page.go_back()
                                list_page.wait_for_timeout(1000)
                                
                                # ğŸ”§ ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€: ë’¤ë¡œ ê°€ê¸° í›„ ë‹¤ì‹œ ìŠ¤í¬ë¡¤ í•„ìš”
                                if infinite_scroll and idx < len(product_ids) - 1:
                                    if VERBOSE:
                                        print(f"    ë¬´í•œ ìŠ¤í¬ë¡¤ ì¬ì‹¤í–‰...")
                                    for i in range(scroll_count):
                                        list_page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                                        list_page.wait_for_timeout(500)
                                    list_page.wait_for_timeout(1000)
                                    if VERBOSE:
                                        print(f"    âœ… ìŠ¤í¬ë¡¤ ì¬ì‹¤í–‰ ì™„ë£Œ")
                            else:
                                failed_count += 1
                                failed_reasons['url_not_changed'] = failed_reasons.get('url_not_changed', 0) + 1
                                if VERBOSE:
                                    print(f"    âŒ URL ë³€ê²½ ì—†ìŒ - url_not_changed")
                            
                            # ì§„í–‰ë¥  ì¶œë ¥ (ë§¤ 20ê°œë§ˆë‹¤)
                            if (idx + 1) % 20 == 0:
                                print(f"    ì§„í–‰: {idx + 1}/{len(product_ids)} ({captured_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨)")
                                
                        except Exception as e:
                            failed_count += 1
                            error_type = type(e).__name__
                            failed_reasons[error_type] = failed_reasons.get(error_type, 0) + 1
                            if VERBOSE:
                                print(f"    âŒ URL ìº¡ì²˜ ì‹¤íŒ¨: {str(e)[:50]}")
                            
                    except Exception as e:
                        # ê°œë³„ ìƒí’ˆ ì˜¤ë¥˜ ì¹´ìš´íŠ¸
                        failed_count += 1
                        error_type = type(e).__name__
                        failed_reasons[error_type] = failed_reasons.get(error_type, 0) + 1
                
                print(f"  âœ… {captured_count}/{len(product_ids)}ê°œ ìƒí’ˆì˜ detail_url ìº¡ì²˜ ì™„ë£Œ")
                
                # ì‹¤íŒ¨ ì›ì¸ í†µê³„ ì¶œë ¥
                if failed_count > 0 and failed_reasons:
                    print(f"  âŒ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")
                    for reason, count in sorted(failed_reasons.items(), key=lambda x: -x[1]):
                        print(f"     - {reason}: {count}ê°œ")
                
                browser.close()
                
        except Exception as e:
            print(f"  âš ï¸ URL ìº¡ì²˜ ì˜¤ë¥˜: {e}")
        
        return url_mapping
    
    def crawl_list_page(self, url: str, infinite_scroll: bool = True, 
                       scroll_count: int = 10) -> List[Dict]:
        """
        ëª©ë¡ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            url: ëª©ë¡ í˜ì´ì§€ URL
            infinite_scroll: ë¬´í•œ ìŠ¤í¬ë¡¤ ì—¬ë¶€
            scroll_count: ìŠ¤í¬ë¡¤ íšŸìˆ˜
            
        Returns:
            ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n[1ë‹¨ê³„] ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§")
        result = self.crawl_page(url, infinite_scroll=infinite_scroll, scroll_count=scroll_count)
        
        if not result['success']:
            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result['error']}")
            return []
        
        print(f"  í…ìŠ¤íŠ¸: {len(result['text_content'])} ë¬¸ì")
        print(f"  ë§í¬: {len(result['links'])} ê°œ")
        
        # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        print(f"\n[2ë‹¨ê³„] ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ ì¶”ì¶œ")
        if self.use_llm:
            products = self.extract_products_with_llm(
                result['html_content'], 
                result['text_content']
            )
        else:
            products = self.extract_products_simple(
                result['html_content'], 
                result['links']
            )
        
        # [ì¶”ê°€] ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤ì œ detail_url ìº¡ì²˜
        if products:
            product_ids = [p.get('id') for p in products if p.get('id')]
            if product_ids:
                url_mapping = self.extract_detail_urls_from_browser(
                    url, product_ids, infinite_scroll, scroll_count
                )
                
                # productsì— ì‹¤ì œ URL ë§¤í•‘
                for product in products:
                    prd_id = product.get('id')
                    if prd_id and prd_id in url_mapping:
                        product['detail_url'] = url_mapping[prd_id]
        
        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
        for product in products:
            if product.get('detail_url'):
                product['detail_url'] = urljoin(url, product['detail_url'])
        
        return products
    
    def crawl_detail_pages(self, products: List[Dict], max_pages: Optional[int] = None) -> List[Dict]:
        """
        ìƒì„¸ í˜ì´ì§€ë“¤ì„ í¬ë¡¤ë§í•˜ê³  ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            products: ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            max_pages: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n[3ë‹¨ê³„] ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§")
        
        # ìƒì„¸ í˜ì´ì§€ê°€ ìˆëŠ” ìƒí’ˆë§Œ í•„í„°ë§
        products_with_detail = [p for p in products if p.get('detail_url')]
        
        if not products_with_detail:
            print("  ìƒì„¸ í˜ì´ì§€ê°€ ìˆëŠ” ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return products
        
        # í¬ë¡¤ë§í•  ê°œìˆ˜ ì œí•œ
        if max_pages:
            products_with_detail = products_with_detail[:max_pages]
        
        print(f"  ì´ {len(products_with_detail)}ê°œì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        
        # ê° ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        for product in tqdm(products_with_detail, desc="  ìƒì„¸ í˜ì´ì§€"):
            detail_url = product['detail_url']
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if detail_url and not detail_url.startswith('http'):
                from urllib.parse import urljoin
                detail_url = urljoin(self.base_url, detail_url)
            
            # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
            result = self.crawl_page(detail_url, infinite_scroll=False)
            
            if not result['success']:
                continue
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            if self.use_llm:
                details = self.extract_product_details_with_llm(result['text_content'])
                
                # ê¸°ì¡´ ì •ë³´ ì—…ë°ì´íŠ¸
                if details:
                    product['name'] = details.get('name', product['name'])
                    product['description'] = details.get('description', product.get('description', ''))
                    product['category'] = details.get('category', '')
                    product['features'] = details.get('features', [])
                    product['specifications'] = details.get('specifications', {})
            else:
                # LLM ì—†ì´ëŠ” í…ìŠ¤íŠ¸ ìš”ì•½ë§Œ
                product['description'] = result['text_content'][:500]
        
        return products
    
    def save_to_dataframe(self, products: List[Dict], output_path: str = None) -> pd.DataFrame:
        """
        ìƒí’ˆ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            products: ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            output_path: ì €ì¥í•  ê²½ë¡œ (Noneì´ë©´ ì €ì¥ ì•ˆ í•¨)
            
        Returns:
            DataFrame
        """
        print(f"\n[4ë‹¨ê³„] ë°ì´í„° ì •ë¦¬ ë° ì €ì¥")
        
        if not products:
            print("  ì €ì¥í•  ìƒí’ˆ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(products)
        
        # price ì»¬ëŸ¼ ì œê±° (í•„ìš” ì—†ìŒ)
        if 'price' in df.columns:
            df = df.drop(columns=['price'])
        
        # ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸
        base_columns = ['id', 'name', 'description']
        for col in base_columns:
            if col not in df.columns:
                df[col] = ''
        
        print(f"  {len(df)}ê°œì˜ ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        print(f"\n  ì»¬ëŸ¼: {list(df.columns)}")
        
        # ì €ì¥
        if output_path:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # CSVë¡œ ì €ì¥
            csv_file = output_file.with_suffix('.csv')
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"  CSV ì €ì¥: {csv_file}")
            
            # JSONìœ¼ë¡œë„ ì €ì¥
            json_file = output_file.with_suffix('.json')
            df.to_json(json_file, orient='records', force_ascii=False, indent=2)
            print(f"  JSON ì €ì¥: {json_file}")
            
            # Excelë¡œë„ ì €ì¥ (openpyxlì´ ìˆëŠ” ê²½ìš°)
            try:
                excel_file = output_file.with_suffix('.xlsx')
                df.to_excel(excel_file, index=False, engine='openpyxl')
                print(f"  Excel ì €ì¥: {excel_file}")
            except ImportError:
                pass
        
        return df
    
    def run(self, url: str, infinite_scroll: bool = True, scroll_count: int = 10,
            crawl_details: bool = True, max_detail_pages: Optional[int] = None,
            output_path: str = None) -> pd.DataFrame:
        """
        ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            url: ì‹œì‘ URL
            infinite_scroll: ë¬´í•œ ìŠ¤í¬ë¡¤ ì—¬ë¶€
            scroll_count: ìŠ¤í¬ë¡¤ íšŸìˆ˜
            crawl_details: ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì—¬ë¶€
            max_detail_pages: ìµœëŒ€ ìƒì„¸ í˜ì´ì§€ ìˆ˜
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê²°ê³¼ DataFrame
        """
        print("="*80)
        print("ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ í¬ë¡¤ëŸ¬")
        print("="*80)
        print(f"URL: {url}")
        print(f"LLM: {'í™œì„±í™” (' + self.model_name + ')' if self.use_llm else 'ë¹„í™œì„±í™”'}")
        print(f"ë¬´í•œ ìŠ¤í¬ë¡¤: {infinite_scroll}")
        print(f"ìƒì„¸ í˜ì´ì§€: {crawl_details}")
        print("="*80)
        
        # 1. ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§
        products = self.crawl_list_page(url, infinite_scroll=infinite_scroll, 
                                       scroll_count=scroll_count)
        
        if not products:
            print("\nìƒí’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print(f"\nì¶”ì¶œëœ ìƒí’ˆ/ì„œë¹„ìŠ¤: {len(products)}ê°œ")
        
        # 2. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ì˜µì…˜)
        if crawl_details:
            products = self.crawl_detail_pages(products, max_pages=max_detail_pages)
        
        # 3. DataFrame ì €ì¥
        df = self.save_to_dataframe(products, output_path=output_path)
        
        print("\n" + "="*80)
        print("í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*80)
        
        return df


def main():
    parser = argparse.ArgumentParser(
        description='ìƒí’ˆ/ì„œë¹„ìŠ¤ ì •ë³´ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‚¬ìš© (LLM í™œì„±í™” - AX)
  python product_crawler.py "https://m.shop.com/products"
  
  # ë¬´í•œ ìŠ¤í¬ë¡¤ + ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
  python product_crawler.py "https://m.shop.com/products" --scroll --details
  
  # ìƒì„¸ í˜ì´ì§€ëŠ” ìµœëŒ€ 10ê°œë§Œ
  python product_crawler.py "https://m.shop.com/products" --details --max-details 10
  
  # LLM ì—†ì´ ì‚¬ìš© (ê·œì¹™ ê¸°ë°˜)
  python product_crawler.py "https://m.shop.com/products" --no-llm
  
  # ë‹¤ë¥¸ LLM ëª¨ë¸ ì‚¬ìš©
  python product_crawler.py "https://m.shop.com/products" --model gemini
  python product_crawler.py "https://m.shop.com/products" --model gpt
        '''
    )
    
    parser.add_argument('url', help='í¬ë¡¤ë§í•  í˜ì´ì§€ URL')
    parser.add_argument('--scroll', action='store_true',
                       help='ë¬´í•œ ìŠ¤í¬ë¡¤ í™œì„±í™”')
    parser.add_argument('--scroll-count', type=int, default=10,
                       help='ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--details', action='store_true',
                       help='ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§')
    parser.add_argument('--max-details', type=int, default=None,
                       help='ìµœëŒ€ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ê°œìˆ˜')
    parser.add_argument('--no-llm', action='store_true',
                       help='LLM ì‚¬ìš© ì•ˆ í•¨ (ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œ)')
    parser.add_argument('--model', choices=['gemma', 'ax', 'claude', 'gemini', 'gpt'], 
                       default='ax',
                       help='LLM ëª¨ë¸ (ê¸°ë³¸ê°’: ax)')
    parser.add_argument('--output', '-o', default='product_data',
                       help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (í™•ì¥ì ì œì™¸, ê¸°ë³¸ê°’: product_data)')
    
    args = parser.parse_args()
    
    # URL ìœ íš¨ì„± ê²€ì‚¬
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ProductCrawler(
        base_url=args.url,
        use_llm=not args.no_llm,
        model_name=args.model
    )
    
    # ì‹¤í–‰
    df = crawler.run(
        url=args.url,
        infinite_scroll=args.scroll,
        scroll_count=args.scroll_count,
        crawl_details=args.details,
        max_detail_pages=args.max_details,
        output_path=args.output
    )
    
    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    if not df.empty:
        print("\nê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head(10).to_string())
    
    return 0


if __name__ == '__main__':
    exit(main())

