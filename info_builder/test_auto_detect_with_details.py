#!/usr/bin/env python3
"""
í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€ + ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í†µí•© í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
1. í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€ (ë¬´í•œ ìŠ¤í¬ë¡¤/í˜ì´ì§€ë„¤ì´ì…˜/ì •ì )
2. ìë™ ê°ì§€ëœ ì „ëµì— ë”°ë¥¸ í¬ë¡¤ë§
3. ìƒì„¸ í˜ì´ì§€ ìë™ ë°©ë¬¸ ë° ì •ë³´ ì¶”ì¶œ
4. ê²°ê³¼ ê²€ì¦
"""

from product_crawler import ProductCrawler
import pandas as pd


def test_auto_detect_with_details():
    """ìë™ ê°ì§€ + ìƒì„¸ í˜ì´ì§€ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ§ª í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€ + ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    url = "https://m.sktuniverse.co.kr/category/sub/tab/detail?ctanId=CC00000012&ctgId=CA00000001"
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = ProductCrawler(
        base_url=url,
        use_llm=True,
        model_name="ax"
    )
    
    print("\n[í…ŒìŠ¤íŠ¸ ì‹œì‘]")
    print(f"URL: {url}")
    print(f"ìë™ ê°ì§€: í™œì„±í™”")
    print(f"ìƒì„¸ í˜ì´ì§€: ìµœëŒ€ 3ê°œ í¬ë¡¤ë§")
    print("-" * 80)
    
    # ì‹¤í–‰
    df = crawler.run(
        url=url,
        auto_detect=True,        # ğŸ” í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€
        crawl_details=True,      # ğŸ“„ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        max_detail_pages=3,      # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 3ê°œë§Œ
        output_path="output/test_auto_detect_with_details"
    )
    
    print("\n" + "="*80)
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„")
    print("="*80)
    
    # 1. ê¸°ë³¸ ì •ë³´
    print(f"\n1ï¸âƒ£ ê¸°ë³¸ ì •ë³´:")
    print(f"   ì¶”ì¶œëœ ìƒí’ˆ: {len(df)}ê°œ")
    print(f"   ì»¬ëŸ¼: {list(df.columns)}")
    
    # 2. detail_url í†µê³„
    if 'detail_url' in df.columns:
        has_url = df['detail_url'].notna() & (df['detail_url'] != '')
        url_count = has_url.sum()
        print(f"\n2ï¸âƒ£ detail_url í†µê³„:")
        print(f"   URL ìˆëŠ” ìƒí’ˆ: {url_count}/{len(df)}ê°œ ({url_count/len(df)*100:.1f}%)")
        
        if url_count > 0:
            print(f"   ì²« ë²ˆì§¸ URL ì˜ˆì‹œ: {df[has_url].iloc[0]['detail_url'][:80]}...")
    
    # 3. ìƒì„¸ ì •ë³´ í™•ì¸
    detail_fields = ['category', 'features', 'specifications']
    has_detail_fields = any(field in df.columns for field in detail_fields)
    
    print(f"\n3ï¸âƒ£ ìƒì„¸ ì •ë³´ ì¶”ì¶œ:")
    if has_detail_fields:
        print(f"   ìƒíƒœ: âœ… ì„±ê³µ")
        for field in detail_fields:
            if field in df.columns:
                non_empty = df[field].notna().sum()
                print(f"   - {field}: {non_empty}/{len(df)}ê°œ ìƒí’ˆ")
    else:
        print(f"   ìƒíƒœ: âŒ ì‹¤íŒ¨ (ìƒì„¸ ì •ë³´ í•„ë“œ ì—†ìŒ)")
    
    # 4. ì²« ë²ˆì§¸ ìƒí’ˆ ìƒì„¸ ë³´ê¸°
    if not df.empty and has_detail_fields:
        print(f"\n4ï¸âƒ£ ì²« ë²ˆì§¸ ìƒí’ˆ ìƒì„¸:")
        first_product = df.iloc[0]
        print(f"   ID: {first_product.get('id', 'N/A')}")
        print(f"   ì´ë¦„: {first_product.get('name', 'N/A')[:50]}...")
        print(f"   ì„¤ëª…: {first_product.get('description', 'N/A')[:50]}...")
        
        if 'category' in df.columns:
            print(f"   ì¹´í…Œê³ ë¦¬: {first_product.get('category', 'N/A')}")
        
        if 'features' in df.columns:
            features = first_product.get('features', [])
            if features:
                print(f"   íŠ¹ì§•: {len(features)}ê°œ")
                for i, feature in enumerate(features[:3], 1):
                    print(f"     {i}. {feature[:50]}...")
        
        if 'specifications' in df.columns:
            specs = first_product.get('specifications', {})
            if specs:
                print(f"   ìŠ¤í™: {len(specs)}ê°œ í•­ëª©")
    
    # 5. ìë™ ê°ì§€ íš¨ê³¼
    print(f"\n5ï¸âƒ£ ìë™ ê°ì§€ íš¨ê³¼:")
    print(f"   âœ… í˜ì´ì§€ íƒ€ì…ì´ ìë™ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"   âœ… ìµœì ì˜ ìŠ¤í¬ë¡¤ ì „ëµì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"   âœ… ì¬ìŠ¤í¬ë¡¤ í•„ìš” ì—¬ë¶€ê°€ ìë™ ê²°ì •ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"   âœ… ë¶ˆí•„ìš”í•œ ì‘ì—…ì´ ìë™ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # 6. ì¶œë ¥ íŒŒì¼
    print(f"\n6ï¸âƒ£ ì¶œë ¥ íŒŒì¼:")
    print(f"   CSV: output/test_auto_detect_with_details.csv")
    print(f"   JSON: output/test_auto_detect_with_details.json")
    print(f"   Excel: output/test_auto_detect_with_details.xlsx")
    
    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
    
    return df


def test_comparison():
    """ìë™ ê°ì§€ vs ìˆ˜ë™ ì„¤ì • ë¹„êµ í…ŒìŠ¤íŠ¸"""
    print("\n\n")
    print("="*80)
    print("ğŸ”¬ ìë™ ê°ì§€ vs ìˆ˜ë™ ì„¤ì • ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    url = "https://m.sktuniverse.co.kr/category/sub/tab/detail?ctanId=CC00000012&ctgId=CA00000001"
    
    crawler = ProductCrawler(base_url=url, use_llm=True, model_name="ax")
    
    # í…ŒìŠ¤íŠ¸ 1: ìë™ ê°ì§€
    print("\n[í…ŒìŠ¤íŠ¸ 1] ìë™ ê°ì§€ ëª¨ë“œ")
    print("-" * 80)
    import time
    start = time.time()
    
    df1 = crawler.run(
        url=url,
        auto_detect=True,
        crawl_details=True,
        max_detail_pages=2,
        output_path="output/comparison_auto"
    )
    
    time1 = time.time() - start
    print(f"\nì†Œìš” ì‹œê°„: {time1:.1f}ì´ˆ")
    print(f"ì¶”ì¶œëœ ìƒí’ˆ: {len(df1)}ê°œ")
    
    # í…ŒìŠ¤íŠ¸ 2: ìˆ˜ë™ ì„¤ì • (ë¹„êµìš©)
    print("\n[í…ŒìŠ¤íŠ¸ 2] ìˆ˜ë™ ì„¤ì • ëª¨ë“œ")
    print("-" * 80)
    start = time.time()
    
    df2 = crawler.run(
        url=url,
        auto_detect=False,
        infinite_scroll=True,
        scroll_count=10,
        crawl_details=True,
        max_detail_pages=2,
        output_path="output/comparison_manual"
    )
    
    time2 = time.time() - start
    print(f"\nì†Œìš” ì‹œê°„: {time2:.1f}ì´ˆ")
    print(f"ì¶”ì¶œëœ ìƒí’ˆ: {len(df2)}ê°œ")
    
    # ë¹„êµ ê²°ê³¼
    print("\n" + "="*80)
    print("ğŸ“Š ë¹„êµ ê²°ê³¼")
    print("="*80)
    print(f"\nìë™ ê°ì§€ ëª¨ë“œ: {time1:.1f}ì´ˆ")
    print(f"ìˆ˜ë™ ì„¤ì • ëª¨ë“œ: {time2:.1f}ì´ˆ")
    
    if time1 < time2:
        improvement = (time2 - time1) / time2 * 100
        print(f"\nâœ… ìë™ ê°ì§€ê°€ {improvement:.1f}% ë” ë¹ ë¦„!")
    elif time2 < time1:
        difference = (time1 - time2) / time1 * 100
        print(f"\nâš ï¸  ìˆ˜ë™ ì„¤ì •ì´ {difference:.1f}% ë” ë¹ ë¦„ (ì´ í˜ì´ì§€ëŠ” ë¬´í•œ ìŠ¤í¬ë¡¤)")
    else:
        print(f"\nâ¡ï¸  ì†Œìš” ì‹œê°„ ë™ì¼")
    
    print("\nğŸ’¡ ì°¸ê³ :")
    print("   - ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€: ë‘ ë°©ì‹ ë™ì¼")
    print("   - í˜ì´ì§€ë„¤ì´ì…˜/ì •ì : ìë™ ê°ì§€ê°€ 2ë°° ë¹ ë¦„!")


def test_different_page_types():
    """ë‹¤ì–‘í•œ í˜ì´ì§€ íƒ€ì… í…ŒìŠ¤íŠ¸"""
    print("\n\n")
    print("="*80)
    print("ğŸŒ ë‹¤ì–‘í•œ í˜ì´ì§€ íƒ€ì… í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    test_urls = [
        {
            'name': 'SKT Universe (ë¬´í•œ ìŠ¤í¬ë¡¤)',
            'url': 'https://m.sktuniverse.co.kr/category/sub/tab/detail?ctanId=CC00000012&ctgId=CA00000001',
            'expected_type': 'infinite_scroll'
        },
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ URLì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    ]
    
    for i, test in enumerate(test_urls, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {test['name']}")
        print("-" * 80)
        print(f"URL: {test['url'][:60]}...")
        print(f"ì˜ˆìƒ íƒ€ì…: {test['expected_type']}")
        
        crawler = ProductCrawler(base_url=test['url'], use_llm=True, model_name="ax")
        
        # ìë™ ê°ì§€ë§Œ ìˆ˜í–‰ (ìƒì„¸ í˜ì´ì§€ëŠ” ìƒëµ)
        df = crawler.run(
            url=test['url'],
            auto_detect=True,
            crawl_details=False,
            output_path=f"output/test_type_{i}"
        )
        
        print(f"ì¶”ì¶œëœ ìƒí’ˆ: {len(df)}ê°œ")
        print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    print("\n")
    print("ğŸ§ª í˜ì´ì§€ íƒ€ì… ìë™ ê°ì§€ + ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸")
    print("="*80)
    
    # í…ŒìŠ¤íŠ¸ ì„ íƒ
    print("\ní…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("  1. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ìë™ ê°ì§€ + ìƒì„¸ í˜ì´ì§€ 3ê°œ)")
    print("  2. ë¹„êµ í…ŒìŠ¤íŠ¸ (ìë™ vs ìˆ˜ë™)")
    print("  3. ë‹¤ì–‘í•œ í˜ì´ì§€ íƒ€ì… í…ŒìŠ¤íŠ¸")
    print("  4. ì „ì²´ í…ŒìŠ¤íŠ¸")
    
    if len(sys.argv) > 1:
        choice = sys.argv[1]
    else:
        # ê¸°ë³¸ê°’: ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        choice = "1"
    
    if choice == "1":
        test_auto_detect_with_details()
    elif choice == "2":
        test_comparison()
    elif choice == "3":
        test_different_page_types()
    elif choice == "4":
        test_auto_detect_with_details()
        test_comparison()
        test_different_page_types()
    else:
        print(f"\nâš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜: {choice}")
        print("ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...\n")
        test_auto_detect_with_details()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n")


if __name__ == '__main__':
    main()

