{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e2283361-fa09-4bcd-9dbe-a665bda2c873",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a154926a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%set_env ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n",
        "%set_env LANGSMITH_TRACING=true\n",
        "%set_env LANGSMITH_API_KEY=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\n",
        "%set_env TAVILY_API_KEY=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\n",
        "\n",
        "\n",
        "%set_env OPENAI_API_KEY=${OPENAI_API_KEY}\n",
        "%set_env LANGCHAIN_API_KEY=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\n",
        "\n",
        "%set_env LANGCHAIN_TRACING_V2=true\n",
        "%set_env LANGCHAIN_PROJECT=\"Multi-agent Collaboration\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2ae034e9-1c05-47e1-aae2-6fb30701166a",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\"  #우리꺼\n",
        "# llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님\n",
        "llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = llm_api_key,\n",
        "    base_url = llm_api_url,\n",
        "    # max_tokens=1000\n",
        ")\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import pandas as pd\n",
        "\n",
        "def ChatAnthropicSKT(model=\"skt/claude-3-5-sonnet-20241022\", max_tokens=100):\n",
        "    # llm_api_key = \"sk-gapk-GHjvOA9hXL8MQ7yNNlR7kLmfA-f8fSl6\" #우리꺼 # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    # llm_api_key = \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\" #도현님  # \"sk-gapk-1tQCB4O2KnH5GG68DeUKfjAKQ-vJ9kc9\"\n",
        "    llm_api_key = \"sk-gapk-Y70vdkPbXPRMWHK0dtaYU30hw-bi7B5C\" # 빌린거\n",
        "    \n",
        "    llm_api_url = \"https://api.platform.a15t.com/v1\"\n",
        "    \n",
        "    # llm_api_url = \"https://43.203.77.11:443/v1\"\n",
        "\n",
        "    # model = \"anthropic/claude-3-5-sonnet-20240620\"\n",
        "\n",
        "    model = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        openai_api_key=llm_api_key, \n",
        "        openai_api_base=llm_api_url, \n",
        "        model=model,\n",
        "        max_tokens=max_tokens\n",
        "        )\n",
        "    return model\n",
        "\n",
        "llm_cld35 = ChatAnthropicSKT()\n",
        "\n",
        "llm_cld37 = ChatAnthropic(\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "    model=\"claude-3-7-sonnet-20250219\",\n",
        "    max_tokens=3000\n",
        ")\n",
        "\n",
        "llm_chat = ChatOpenAI(\n",
        "        temperature=0,  \n",
        "        model=\"gpt-4o\",\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        max_tokens=2000,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1aecb3f7-22b9-4612-a6ec-a54c42482dd8",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict, Any\n",
        "import ast\n",
        "\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "\n",
        "def dataframe_to_markdown_prompt(df, max_rows=None):\n",
        "    # Limit rows if needed\n",
        "    if max_rows is not None and len(df) > max_rows:\n",
        "        display_df = df.head(max_rows)\n",
        "        truncation_note = f\"\\n[Note: Only showing first {max_rows} of {len(df)} rows]\"\n",
        "    else:\n",
        "        display_df = df\n",
        "        truncation_note = \"\"\n",
        "    \n",
        "    # Convert to markdown\n",
        "    df_markdown = display_df.to_markdown()\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    {df_markdown}\n",
        "    {truncation_note}\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def replace_strings(text, replacements):\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "        \n",
        "    return text\n",
        "\n",
        "def clean_segment(segment):\n",
        "    \"\"\"\n",
        "    Given a segment that is expected to be quoted (i.e. begins and ends with\n",
        "    the same single or double quote), remove any occurrences of that quote\n",
        "    from the inner content.\n",
        "    For example, if segment is:\n",
        "         \"에이닷 T 멤버십 쿠폰함에 \"에이닷은통화요약된닷\" 입력\"\n",
        "    then the outer quotes are preserved but the inner double quotes are removed.\n",
        "    \"\"\"\n",
        "    segment = segment.strip()\n",
        "    if len(segment) >= 2 and segment[0] in ['\"', \"'\"] and segment[-1] == segment[0]:\n",
        "        q = segment[0]\n",
        "        # Remove inner occurrences of the quote character.\n",
        "        inner = segment[1:-1].replace(q, '')\n",
        "        return q + inner + q\n",
        "    return segment\n",
        "\n",
        "def split_key_value(text):\n",
        "    \"\"\"\n",
        "    Splits text into key and value based on the first colon that appears\n",
        "    outside any quoted region.\n",
        "    If no colon is found outside quotes, the value will be returned empty.\n",
        "    \"\"\"\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for i, char in enumerate(text):\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # Toggle quote state (assumes well-formed starting/ending quotes for each token)\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "        elif char == ':' and not in_quote:\n",
        "            return text[:i], text[i+1:]\n",
        "    return text, ''\n",
        "\n",
        "def split_outside_quotes(text, delimiter=','):\n",
        "    \"\"\"\n",
        "    Splits the input text on the given delimiter (default comma) but only\n",
        "    if the delimiter occurs outside of quoted segments.\n",
        "    Returns a list of parts.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    current = []\n",
        "    in_quote = False\n",
        "    quote_char = ''\n",
        "    for char in text:\n",
        "        if char in ['\"', \"'\"]:\n",
        "            # When encountering a quote, toggle our state\n",
        "            if in_quote:\n",
        "                if char == quote_char:\n",
        "                    in_quote = False\n",
        "                    quote_char = ''\n",
        "            else:\n",
        "                in_quote = True\n",
        "                quote_char = char\n",
        "            current.append(char)\n",
        "        elif char == delimiter and not in_quote:\n",
        "            parts.append(''.join(current).strip())\n",
        "            current = []\n",
        "        else:\n",
        "            current.append(char)\n",
        "    if current:\n",
        "        parts.append(''.join(current).strip())\n",
        "    return parts\n",
        "\n",
        "def clean_ill_structured_json(text):\n",
        "    \"\"\"\n",
        "    Given a string that is intended to represent a JSON-like structure\n",
        "    but may be ill-formed (for example, it might contain nested quotes that\n",
        "    break standard JSON rules), attempt to “clean” it by processing each\n",
        "    key–value pair.\n",
        "    \n",
        "    The function uses the following heuristics:\n",
        "      1. Split the input text into comma-separated parts (only splitting\n",
        "         when the comma is not inside a quoted string).\n",
        "      2. For each part, split on the first colon (that is outside quotes) to separate key and value.\n",
        "      3. For any segment that begins and ends with a quote, remove any inner occurrences\n",
        "         of that same quote.\n",
        "      4. Rejoin the cleaned key and value.\n",
        "    \n",
        "    Note: This approach does not build a fully robust JSON parser. For very complex\n",
        "          or deeply nested ill-structured inputs further refinement would be needed.\n",
        "    \"\"\"\n",
        "    # First, split the text by commas outside of quotes.\n",
        "    parts = split_outside_quotes(text, delimiter=',')\n",
        "    \n",
        "    cleaned_parts = []\n",
        "    for part in parts:\n",
        "        # Try to split into key and value on the first colon not inside quotes.\n",
        "        key, value = split_key_value(part)\n",
        "        key_clean = clean_segment(key)\n",
        "        value_clean = clean_segment(value) if value.strip() != \"\" else \"\"\n",
        "        if value_clean:\n",
        "            cleaned_parts.append(f\"{key_clean}: {value_clean}\")\n",
        "        else:\n",
        "            cleaned_parts.append(key_clean)\n",
        "    \n",
        "    # Rejoin the cleaned parts with commas (or you can use another format if desired)\n",
        "    return ', '.join(cleaned_parts)\n",
        "\n",
        "def repair_json(broken_json):\n",
        "    \n",
        "    # json_str = broken_json.replace(\"'\",'\"')\n",
        "    \n",
        "    # Fix unquoted values (like NI00001863)\n",
        "    json_str = re.sub(r':\\s*([a-zA-Z0-9_]+)(\\s*[,}])', r': \"\\1\"\\2', broken_json)\n",
        "    \n",
        "    # Fix unquoted keys\n",
        "    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1 \"\\2\":', json_str)\n",
        "    \n",
        "    # Fix trailing commas\n",
        "    json_str = re.sub(r',\\s*}', '}', json_str)\n",
        "    \n",
        "    return json_str\n",
        "\n",
        "def extract_json_objects(text):\n",
        "    # More sophisticated pattern that tries to match proper JSON syntax\n",
        "    pattern = r'(\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\})'\n",
        "    \n",
        "    result = []\n",
        "    for match in re.finditer(pattern, text):\n",
        "        potential_json = match.group(0)\n",
        "        try:\n",
        "            # Try to parse and validate\n",
        "            # json_obj = json.loads(repair_json(potential_json))\n",
        "            json_obj = ast.literal_eval(clean_ill_structured_json(repair_json(potential_json)))\n",
        "            result.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            # Not valid JSON, skip\n",
        "            pass\n",
        "    \n",
        "    return result\n",
        "\n",
        "def extract_between(text, start_marker, end_marker):\n",
        "    start_index = text.find(start_marker)\n",
        "    if start_index == -1:\n",
        "        return None\n",
        "    \n",
        "    start_index += len(start_marker)\n",
        "    end_index = text.find(end_marker, start_index)\n",
        "    if end_index == -1:\n",
        "        return None\n",
        "    \n",
        "    return text[start_index:end_index]\n",
        "\n",
        "def extract_content(text: str, tag_name: str) -> List[str]:\n",
        "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    return matches\n",
        "\n",
        "\n",
        "def clean_bad_text(text):\n",
        "    import re\n",
        "    \n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    \n",
        "    # Keep Korean, alphanumeric, spaces, and specific punctuation\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3\\u1100-\\u11FF\\w\\s\\.\\?!,]', ' ', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing special characters that don't affect fine-tuning.\n",
        "    Preserves important structural elements like quotes, brackets, and JSON syntax.\n",
        "    Specifically handles Korean text (Hangul) properly.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input text to clean\n",
        "        \n",
        "    Returns:\n",
        "        str: Cleaned text ready for fine-tuning\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Preserve the basic structure by temporarily replacing important characters\n",
        "    # with placeholder tokens that won't be affected by cleanup\n",
        "    \n",
        "    # Step 1: Temporarily replace JSON structural elements\n",
        "    placeholders = {\n",
        "        '\"': \"DQUOTE_TOKEN\",\n",
        "        \"'\": \"SQUOTE_TOKEN\",\n",
        "        \"{\": \"OCURLY_TOKEN\",\n",
        "        \"}\": \"CCURLY_TOKEN\",\n",
        "        \"[\": \"OSQUARE_TOKEN\",\n",
        "        \"]\": \"CSQUARE_TOKEN\",\n",
        "        \":\": \"COLON_TOKEN\",\n",
        "        \",\": \"COMMA_TOKEN\"\n",
        "    }\n",
        "    \n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(char, placeholder)\n",
        "    \n",
        "    # Step 2: Remove problematic characters\n",
        "    \n",
        "    # Remove control characters (except newlines, carriage returns, and tabs which can be meaningful)\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    \n",
        "    # Normalize all types of newlines to \\n\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
        "    \n",
        "    # Remove zero-width characters and other invisible unicode\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u00A0]', '', text)\n",
        "    \n",
        "    # MODIFIED: Keep Korean characters (Hangul) along with other useful character sets\n",
        "    # This regex keeps:\n",
        "    # - ASCII (Basic Latin): \\x00-\\x7F\n",
        "    # - Latin-1 Supplement: \\u0080-\\u00FF\n",
        "    # - Latin Extended A & B: \\u0100-\\u017F\\u0180-\\u024F\n",
        "    # - Greek and Coptic: \\u0370-\\u03FF\n",
        "    # - Cyrillic: \\u0400-\\u04FF\n",
        "    # - Korean Hangul Syllables: \\uAC00-\\uD7A3\n",
        "    # - Hangul Jamo (Korean alphabet): \\u1100-\\u11FF\n",
        "    # - Hangul Jamo Extended-A: \\u3130-\\u318F\n",
        "    # - Hangul Jamo Extended-B: \\uA960-\\uA97F\n",
        "    # - Hangul Compatibility Jamo: \\u3130-\\u318F\n",
        "    # - CJK symbols and punctuation: \\u3000-\\u303F\n",
        "    # - Full-width forms (often used with CJK): \\uFF00-\\uFFEF\n",
        "    # - CJK Unified Ideographs (Basic common Chinese/Japanese characters): \\u4E00-\\u9FFF\n",
        "    \n",
        "    # Instead of removing characters, we'll define which ones to keep\n",
        "    allowed_chars_pattern = r'[^\\x00-\\x7F\\u0080-\\u00FF\\u0100-\\u024F\\u0370-\\u03FF\\u0400-\\u04FF' + \\\n",
        "                           r'\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\u3000-\\u303F' + \\\n",
        "                           r'\\uAC00-\\uD7A3\\uFF00-\\uFFEF\\u4E00-\\u9FFF\\n\\r\\t ]'\n",
        "    text = re.sub(allowed_chars_pattern, '', text)\n",
        "    \n",
        "    # Step 3: Normalize whitespace (but preserve deliberate line breaks)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Convert multiple spaces/tabs to single space\n",
        "    \n",
        "    # First ensure all newlines are standardized\n",
        "    text = re.sub(r'\\r\\n|\\r', '\\n', text)  # Convert all newline variants to \\n\n",
        "    \n",
        "    # Then normalize multiple blank lines to at most two\n",
        "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Convert multiple newlines to at most two\n",
        "    \n",
        "    # Step 4: Restore original JSON structural elements\n",
        "    for char, placeholder in placeholders.items():\n",
        "        text = text.replace(placeholder, char)\n",
        "    \n",
        "    # Step 5: Fix common JSON syntax issues that might remain\n",
        "    # Fix spaces between quotes and colons in JSON\n",
        "    text = re.sub(r'\"\\s+:', r'\":', text)\n",
        "    \n",
        "    # Fix trailing commas in arrays\n",
        "    text = re.sub(r',\\s*]', r']', text)\n",
        "    \n",
        "    # Fix trailing commas in objects\n",
        "    text = re.sub(r',\\s*}', r'}', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_control_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove control characters except commonly used whitespace\n",
        "        return re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
        "    return text\n",
        "\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.openai import OpenAIChat  # For compatibility with newer setup\n",
        "\n",
        "# Create a custom LLM class that uses the OpenAI client directly\n",
        "class CustomOpenAI:\n",
        "    def __init__(self, model=\"skt/a.x-3-lg\"):\n",
        "        self.model = model\n",
        "        \n",
        "    def __call__(self, prompt):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create a simple retrieval function\n",
        "def get_relevant_context(query, vectorstore, topk=5):\n",
        "    docs = vectorstore.similarity_search(query, k=topk)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    titles = \", \".join(set([doc.metadata['title'] for doc in docs if 'title' in doc.metadata.keys()]))\n",
        "    return {'title':titles, 'context':context}\n",
        "\n",
        "# Create a function to combine everything\n",
        "def answer_question(query, vectorstore):\n",
        "    # Get relevant context\n",
        "    context = get_relevant_context(query, vectorstore)\n",
        "    \n",
        "    # Create combined prompt\n",
        "    prompt = f\"Answer the following question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    # Use OpenAI directly\n",
        "    custom_llm = CustomOpenAI()  # Or your preferred model\n",
        "    response = custom_llm(prompt)\n",
        "    \n",
        "    return response\n",
        "\n",
        "def is_list_of_dicts(var):\n",
        "    # Check if the variable is a list\n",
        "    if not isinstance(var, list):\n",
        "        return False\n",
        "    \n",
        "    # Check if the list is not empty and all elements are dictionaries\n",
        "    if not var:  # Empty list\n",
        "        return False\n",
        "        \n",
        "    # Check that all elements are dictionaries\n",
        "    return all(isinstance(item, dict) for item in var)\n",
        "\n",
        "def remove_duplicate_dicts(dict_list):\n",
        "    result = []\n",
        "    seen = set()\n",
        "    for d in dict_list:\n",
        "        # Convert dictionary to a hashable tuple of items\n",
        "        t = tuple(sorted(d.items()))\n",
        "        if t not in seen:\n",
        "            seen.add(t)\n",
        "            result.append(d)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "859204ed-4376-4b9b-b64f-71bfa6174257",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rapidfuzz import fuzz, process\n",
        "import re\n",
        "\n",
        "class KoreanEntityMatcher:\n",
        "    def __init__(self, min_similarity=75, ngram_size=2, min_entity_length=2):\n",
        "        self.min_similarity = min_similarity\n",
        "        self.ngram_size = ngram_size\n",
        "        self.min_entity_length = min_entity_length\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "    def build_from_list(self, entities):\n",
        "        \"\"\"Build entity index from a list of entities\"\"\"\n",
        "        self.entities = []\n",
        "        self.entity_data = {}\n",
        "        \n",
        "        for i, entity in enumerate(entities):\n",
        "            if isinstance(entity, tuple) and len(entity) == 2:\n",
        "                entity_name, data = entity\n",
        "                self.entities.append(entity_name)\n",
        "                self.entity_data[entity_name] = data\n",
        "            else:\n",
        "                self.entities.append(entity)\n",
        "                self.entity_data[entity] = {'id': i, 'entity': entity}\n",
        "                \n",
        "        # Create n-gram index for faster candidate selection\n",
        "        self._build_ngram_index(n=self.ngram_size)\n",
        "    \n",
        "    def _build_ngram_index(self, n=2):\n",
        "        \"\"\"Build n-gram index optimized for Korean characters\"\"\"\n",
        "        self.ngram_index = {}\n",
        "        \n",
        "        for entity in self.entities:\n",
        "            # Skip entities shorter than min_entity_length\n",
        "            if len(entity) < self.min_entity_length:\n",
        "                continue\n",
        "                \n",
        "            # Create n-grams for the entity\n",
        "            entity_chars = list(entity)  # Split into characters for proper Korean handling\n",
        "            ngrams = []\n",
        "            \n",
        "            # Create character-level n-grams (better for Korean)\n",
        "            for i in range(len(entity_chars) - n + 1):\n",
        "                ngram = ''.join(entity_chars[i:i+n])\n",
        "                ngrams.append(ngram)\n",
        "            \n",
        "            # Add entity to the index for each n-gram\n",
        "            for ngram in ngrams:\n",
        "                if ngram not in self.ngram_index:\n",
        "                    self.ngram_index[ngram] = set()\n",
        "                self.ngram_index[ngram].add(entity)\n",
        "    \n",
        "    def _get_candidates(self, text, n=None):\n",
        "        \"\"\"Get candidate entities based on n-gram overlap (optimized for Korean)\"\"\"\n",
        "        if n is None:\n",
        "            n = self.ngram_size\n",
        "            \n",
        "        text_chars = list(text)  # Split into characters for proper Korean handling\n",
        "        text_ngrams = set()\n",
        "        \n",
        "        # Create character-level n-grams\n",
        "        for i in range(len(text_chars) - n + 1):\n",
        "            ngram = ''.join(text_chars[i:i+n])\n",
        "            text_ngrams.add(ngram)\n",
        "        \n",
        "        candidates = set()\n",
        "        for ngram in text_ngrams:\n",
        "            if ngram in self.ngram_index:\n",
        "                candidates.update(self.ngram_index[ngram])\n",
        "        \n",
        "        # Prioritize candidates with multiple n-gram matches\n",
        "        candidate_scores = {}\n",
        "        for candidate in candidates:\n",
        "            candidate_chars = list(candidate)\n",
        "            candidate_ngrams = set()\n",
        "            for i in range(len(candidate_chars) - n + 1):\n",
        "                ngram = ''.join(candidate_chars[i:i+n])\n",
        "                candidate_ngrams.add(ngram)\n",
        "            \n",
        "            overlap = len(candidate_ngrams.intersection(text_ngrams))\n",
        "            candidate_scores[candidate] = overlap\n",
        "        \n",
        "        # Return candidates sorted by n-gram overlap score\n",
        "        return sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    def find_entities(self, text, max_candidates_per_span=10):\n",
        "        \"\"\"Find entity matches in Korean text using fuzzy matching\"\"\"\n",
        "        # Extract spans that might contain entities\n",
        "        potential_spans = self._extract_korean_spans(text)\n",
        "        matches = []\n",
        "        \n",
        "        for span_text, start, end in potential_spans:\n",
        "            if len(span_text.strip()) < self.min_entity_length:  # Skip spans shorter than min_entity_length\n",
        "                continue\n",
        "                \n",
        "            # Get candidate entities based on n-gram overlap\n",
        "            candidates = self._get_candidates(span_text)\n",
        "            \n",
        "            # If no candidates found through n-gram filtering, skip\n",
        "            if not candidates:\n",
        "                continue\n",
        "            \n",
        "            # Limit the number of candidates to check\n",
        "            top_candidates = [c[0] for c in candidates[:max_candidates_per_span]]\n",
        "            \n",
        "            # Find best fuzzy matches\n",
        "            best_matches = process.extract(\n",
        "                span_text, \n",
        "                top_candidates, \n",
        "                scorer=fuzz.ratio,  # Better for Korean than token_sort_ratio\n",
        "                score_cutoff=self.min_similarity,\n",
        "                limit=3\n",
        "            )\n",
        "            \n",
        "            for entity, score, _ in best_matches:\n",
        "                matches.append({\n",
        "                    'text': span_text,\n",
        "                    'matched_entity': entity,\n",
        "                    'score': score,\n",
        "                    'start': start,\n",
        "                    'end': end,\n",
        "                    'data': self.entity_data.get(entity, {})\n",
        "                })\n",
        "        \n",
        "        # Sort by position in text\n",
        "        matches.sort(key=lambda x: (x['start'], -x['score']))\n",
        "        \n",
        "        # Handle overlapping matches by keeping the best match\n",
        "        final_matches = self._resolve_overlapping_matches(matches)\n",
        "        \n",
        "        return final_matches\n",
        "    \n",
        "    def _extract_korean_spans(self, text):\n",
        "        \"\"\"Extract potential text spans that might be entities in Korean text\"\"\"\n",
        "        spans = []\n",
        "        min_len = self.min_entity_length\n",
        "        \n",
        "        # Korean text segmentation patterns\n",
        "        \n",
        "        # 1. Extract spans separated by common Korean delimiters\n",
        "        for span in re.split(r'[,\\.!?;:\"\\'\\(\\)\\[\\]\\{\\}\\s]+', text):\n",
        "            if span and len(span) >= min_len:\n",
        "                span_pos = text.find(span)\n",
        "                if span_pos != -1:\n",
        "                    spans.append((span, span_pos, span_pos + len(span)))\n",
        "        \n",
        "        # 2. Extract noun phrases (sequences without particles/endings)\n",
        "        # Korean particles often follow nouns, so look for sequences without typical particles\n",
        "        for match in re.finditer(r'[가-힣a-zA-Z0-9]+(?:[^\\s.,!?;:은는이가을를에서의로와과]\\s*[가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 3. Extract sequences of Korean with numbers and English (common for product names, etc.)\n",
        "        for match in re.finditer(r'[가-힣a-zA-Z]+[\\s\\-]?[0-9]+(?:[\\s\\-][가-힣a-zA-Z0-9]+)*', text):\n",
        "            if len(match.group(0)) >= min_len:\n",
        "                spans.append((match.group(0), match.start(), match.end()))\n",
        "        \n",
        "        # 4. Extract fixed-length windows (useful for Korean where spaces are sometimes omitted)\n",
        "        words = list(text)  # Split into characters\n",
        "        \n",
        "        # Use window sizes starting from min_entity_length up to 8\n",
        "        window_sizes = list(range(min_len, 9))\n",
        "        \n",
        "        for size in window_sizes:\n",
        "            for i in range(len(words) - size + 1):\n",
        "                window = ''.join(words[i:i+size])\n",
        "                spans.append((window, i, i + size))\n",
        "        \n",
        "        return spans\n",
        "    \n",
        "    def _resolve_overlapping_matches(self, matches):\n",
        "        \"\"\"Resolve overlapping matches by keeping the highest scoring match\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "        \n",
        "        # Sort by start position and then by score (descending)\n",
        "        sorted_matches = sorted(matches, key=lambda x: (x['start'], -x['score']))\n",
        "        \n",
        "        final_matches = [sorted_matches[0]]\n",
        "        current_end = sorted_matches[0]['end']\n",
        "        \n",
        "        for match in sorted_matches[1:]:\n",
        "            # If this match starts after the previous match ends, add it\n",
        "            if match['start'] >= current_end:\n",
        "                final_matches.append(match)\n",
        "                current_end = match['end']\n",
        "            # If this match overlaps but has a higher score, replace the previous match\n",
        "            elif match['score'] > final_matches[-1]['score'] and match['start'] < current_end:\n",
        "                final_matches[-1] = match\n",
        "                current_end = match['end']\n",
        "        \n",
        "        return final_matches\n",
        "\n",
        "def find_entities_in_text(text, entity_list, min_similarity=75, ngram_size=2, min_entity_length=2):\n",
        "    \"\"\"\n",
        "    Find entity matches in text using fuzzy matching.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The text to search for entities\n",
        "    entity_list : list\n",
        "        List of entities to match against\n",
        "    min_similarity : int, default=75\n",
        "        Minimum similarity score (0-100) for fuzzy matching\n",
        "    ngram_size : int, default=2\n",
        "        Size of character n-grams to use for indexing (2 or 3 recommended for Korean)\n",
        "    min_entity_length : int, default=2\n",
        "        Minimum length of entities to consider (characters)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    list\n",
        "        List of matched entities with position and metadata\n",
        "    \"\"\"\n",
        "    matcher = KoreanEntityMatcher(\n",
        "        min_similarity=min_similarity,\n",
        "        ngram_size=ngram_size,\n",
        "        min_entity_length=min_entity_length\n",
        "    )\n",
        "    matcher.build_from_list(entity_list)\n",
        "    \n",
        "    matches = matcher.find_entities(text)\n",
        "    return matches\n",
        "\n",
        "# Function to highlight entities in text\n",
        "def highlight_entities(text, matches):\n",
        "    marked_text = text\n",
        "    offset = 0\n",
        "    for match in sorted(matches, key=lambda x: x['start'], reverse=True):\n",
        "        start = match['start'] + offset\n",
        "        end = match['end'] + offset\n",
        "        entity = match['matched_entity']\n",
        "        score = match['score']\n",
        "        marked_text = marked_text[:start] + f\"[{marked_text[start:end]}→{entity} ({score}%)]\" + marked_text[end:]\n",
        "        offset += len(f\"[→{entity} ({score}%)]\") + 2\n",
        "    \n",
        "    return marked_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0655b0b0-b7ec-4627-a3bc-ad8ece7cb65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def calculate_list_similarity(list1, list2):\n",
        "    # Handle cases where input might be a dictionary or other types\n",
        "    if isinstance(list1, dict):\n",
        "        list1 = [str(item) for item in list1.values()]\n",
        "    if isinstance(list2, dict):\n",
        "        list2 = [str(item) for item in list2.values()]\n",
        "    \n",
        "    # Ensure lists contain strings\n",
        "    list1 = [str(item) for item in list1]\n",
        "    list2 = [str(item) for item in list2]\n",
        "    \n",
        "    # Convert lists to sets for comparison\n",
        "    set1 = set(sorted(set(list1)))\n",
        "    set2 = set(sorted(set(list2)))\n",
        "    \n",
        "    # Calculate Jaccard similarity\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    \n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def calculate_text_similarity(text1, text2):\n",
        "    return SequenceMatcher(None, str(text1), str(text2)).ratio()\n",
        "\n",
        "def calculate_message_info_similarity(minfo1, minfo2):\n",
        "    if not isinstance(minfo1, dict) or not isinstance(minfo2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    sender_sim = calculate_text_similarity(minfo1.get('sender',''), minfo2.get('sender',''))\n",
        "    title_sim = calculate_text_similarity(minfo1.get('title',''), minfo2.get('title',''))\n",
        "    main_theme_sim = calculate_text_similarity(minfo1.get('main_theme',''), minfo2.get('main_theme',''))\n",
        "    period_sim = calculate_text_similarity(minfo1.get('period',''), minfo2.get('period',''))\n",
        "    \n",
        "    return (sender_sim + title_sim + main_theme_sim + period_sim) / 4\n",
        "\n",
        "def calculate_metadata_similarity(minfo1, minfo2):\n",
        "    if not isinstance(minfo1, dict) or not isinstance(minfo2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    campaign_id_sim = calculate_text_similarity(minfo1.get('campaign_id',''), minfo2.get('campaign_id',''))\n",
        "    message_type_sim = calculate_text_similarity(minfo1.get('message_type',''), minfo2.get('message_type',''))\n",
        "    target_response_sim = calculate_text_similarity(minfo1.get('target_response',''), minfo2.get('target_response',''))\n",
        "    \n",
        "    success_metrics_similarity = calculate_list_similarity(\n",
        "        minfo1.get('success_metrics',[]), \n",
        "        minfo2.get('success_metrics',[])\n",
        "    )\n",
        "    \n",
        "    return (campaign_id_sim + message_type_sim + target_response_sim + success_metrics_similarity) / 4\n",
        "\n",
        "\n",
        "def calculate_product_similarity(prod1, prod2):\n",
        "    if not isinstance(prod1, dict) or not isinstance(prod2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    name_sim = calculate_text_similarity(prod1.get('name',''), prod2.get('name',''))\n",
        "    category_sim = calculate_text_similarity(prod1.get('category',''), prod2.get('category',''))\n",
        "    benefit_sim = calculate_text_similarity(prod1.get('benefit',''), prod2.get('benefit',''))\n",
        "    action_sim = calculate_text_similarity(prod1.get('action',''), prod2.get('action',''))\n",
        "    conditions_sim = calculate_text_similarity(prod1.get('conditions',''), prod2.get('conditions',''))\n",
        "    \n",
        "    return (name_sim + category_sim + benefit_sim + action_sim + conditions_sim) / 5\n",
        "\n",
        "def calculate_channel_similarity(chan1, chan2):\n",
        "    if not isinstance(chan1, dict) or not isinstance(chan2, dict):\n",
        "        return 0.0\n",
        "    \n",
        "    type_sim = calculate_text_similarity(chan1.get('type',''), chan2.get('type',''))\n",
        "    value_sim = calculate_text_similarity(chan1.get('value',''), chan2.get('value',''))\n",
        "    action_sim = calculate_text_similarity(chan1.get('action',''), chan2.get('action',''))\n",
        "    availability_sim = calculate_text_similarity(chan1.get('availability',''), chan2.get('availability',''))\n",
        "    \n",
        "    return (type_sim + value_sim + action_sim + availability_sim) / 4\n",
        "\n",
        "def calculate_target_similarity(target1, target2):\n",
        "    if not isinstance(target1, list) or not isinstance(target2, list):\n",
        "        return 0.0\n",
        "    \n",
        "    similarities = []\n",
        "    for t1 in target1:\n",
        "        for t2 in target2:\n",
        "            segment_sim = calculate_text_similarity(t1.get('segment',''), t2.get('segment',''))\n",
        "            characteristics_sim = calculate_text_similarity(t1.get('characteristics',''), t2.get('characteristics',''))\n",
        "            similarities.append((segment_sim + characteristics_sim) / 2)\n",
        "    \n",
        "    return max(similarities) if similarities else 0.0\n",
        "\n",
        "def calculate_dictionary_similarity(dict1, dict2):\n",
        "    # Calculate similarities for each component\n",
        "    message_info_similarity = calculate_message_info_similarity(\n",
        "        dict1.get('message_info',{}), \n",
        "        dict2.get('message_info',{})\n",
        "    )\n",
        "    \n",
        "    meta_similarity = calculate_metadata_similarity(\n",
        "        dict1.get('metadata',{}), \n",
        "        dict2.get('metadata',{})\n",
        "    )\n",
        "    \n",
        "    purpose_similarity = calculate_list_similarity(\n",
        "        dict1.get('purpose',[]), \n",
        "        dict2.get('purpose',[])\n",
        "    )\n",
        "    \n",
        "    target_similarity = calculate_target_similarity(\n",
        "        dict1.get('target',[]), \n",
        "        dict2.get('target',[])\n",
        "    )\n",
        "    \n",
        "    # Calculate product similarity\n",
        "    product_similarities = []\n",
        "    products1 = dict1.get('product', [])\n",
        "    products2 = dict2.get('product', [])\n",
        "    for p1, p2 in zip(products1, products2):\n",
        "        product_similarities.append(calculate_product_similarity(p1, p2))\n",
        "    product_similarity = sum(product_similarities) / len(product_similarities) if product_similarities else 0.0\n",
        "    \n",
        "    # Calculate channel similarity\n",
        "    channel_similarities = []\n",
        "    channels1 = dict1.get('channel', [])\n",
        "    channels2 = dict2.get('channel', [])\n",
        "    for c1, c2 in zip(channels1, channels2):\n",
        "        channel_similarities.append(calculate_channel_similarity(c1, c2))\n",
        "    channel_similarity = sum(channel_similarities) / len(channel_similarities) if channel_similarities else 0.0\n",
        "\n",
        "    # Calculate overall similarity\n",
        "    overall_similarity = (\n",
        "        message_info_similarity + \n",
        "        purpose_similarity + \n",
        "        target_similarity + \n",
        "        product_similarity + \n",
        "        channel_similarity +\n",
        "        meta_similarity\n",
        "    ) / 5\n",
        "    \n",
        "    return {\n",
        "        'overall_similarity': overall_similarity,\n",
        "        'message_info_similarity': message_info_similarity,\n",
        "        'purpose_similarity': purpose_similarity,\n",
        "        'target_similarity': target_similarity,\n",
        "        'product_similarity': product_similarity,\n",
        "        'channel_similarity': channel_similarity,\n",
        "        'meta_similarity': meta_similarity\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b275d1-1e26-40b4-93f4-898cc1345c22",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# mms_pdf = pd.read_excel(\"./data/mms_data_250408.xlsx\", engine=\"openpyxl\")\n",
        "mms_pdf = pd.read_pickle(\"./data/mms_data_250408.pkl\")\n",
        "\n",
        "mms_pdf['msg'] = mms_pdf['msg_nm']+\"\\n\"+mms_pdf['mms_phrs']\n",
        "mms_pdf = mms_pdf.groupby([\"msg_nm\",\"mms_phrs\",\"msg\"])['offer_dt'].min().reset_index(name=\"offer_dt\")\n",
        "\n",
        "mms_pdf = mms_pdf.reset_index()\n",
        "\n",
        "mms_pdf = mms_pdf.astype('str')\n",
        "\n",
        "mms_pdf.head(1)\n",
        "# item_pdf = pd.read_pickle(\"/home/skinet/myfiles/tos_ace/data/item_info_250318.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ba8f0a7-090e-44df-bf2d-7bfd5189bdca",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# item_pdf_raw = pd.read_excel(\"./data/item_info_250401.xlsx\", engine=\"openpyxl\")\n",
        "item_pdf_raw = pd.read_pickle(\"./data/item_info_250401.pkl\")\n",
        "\n",
        "item_pdf = item_pdf_raw.drop_duplicates(['item_nm','item_desc']).copy()\n",
        "item_pdf['item_item'] = item_pdf['item_nm']+\"\\n\"+item_pdf['item_desc']\n",
        "\n",
        "item_pdf['item_nm_cl'] = item_pdf['item_nm'].apply(clean_text)\n",
        "item_pdf['item_desc_cl'] = item_pdf['item_desc'].fillna('').astype(str).apply(clean_text)\n",
        "item_pdf['item_item_cl'] = item_pdf['item_nm_cl']+\"\\n\"+item_pdf['item_desc_cl']\n",
        "\n",
        "entity_list = []\n",
        "for row in item_pdf.to_dict('records'):\n",
        "    entity_list.append((row['item_nm'], {'item_id':row['item_id'],'category':row['item_cate_ax'], 'description':row['item_desc'], 'create_dt':row['create_dt']}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47fc4e7-bcfa-4992-b4be-1739a48d9a76",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pysqlite3\n",
        "sys.modules[\"sqlite3\"] = pysqlite3\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from chromadb import Client\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Get the collection and delete it if it exists\n",
        "collection_name=\"item_embeddings_collection\"\n",
        "\n",
        "# client_chromadb = Client(Settings(\n",
        "#     persist_directory=\"./chroma_db\",\n",
        "#     is_persistent=True\n",
        "# ))\n",
        "\n",
        "# try:\n",
        "#     client_chromadb.delete_collection(collection_name)\n",
        "#     print(f\"Deleted collection: {collection_name}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Collection doesn't exist or couldn't be deleted: {e}\")\n",
        "\n",
        "\n",
        "item_pdf_rag = item_pdf_raw.copy()\n",
        "for i in range(10):\n",
        "    item_pdf_rag[f'item_nm_{i+1}'] = item_pdf_rag['item_nm']\n",
        "    \n",
        "documents = []\n",
        "for i, row in item_pdf_rag.iterrows():\n",
        "    # You can format the content however makes sense for your data\n",
        "    content = f\"Row {i}: \" + \", \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
        "    # Or more specifically formatted based on your needs\n",
        "    # content = f\"Customer: {row['name']}, Purchase: {row['item']}, Amount: ${row['amount']}\"\n",
        "    names =  f\"{row['item_nm']}\"\n",
        "    \n",
        "    metadata = {\"title\":names, \"source\": \"dataframe\", \"row_id\": i}\n",
        "    doc = Document(page_content=content, metadata=metadata)\n",
        "    documents.append(doc)\n",
        "    \n",
        "# Create embeddings and vectorstore\n",
        "# embeddings = OpenAIEmbeddings(openai_api_key=llm_api_key, openai_api_base=llm_api_url)\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents, \n",
        "    embeddings,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_cld35,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e3cf3f-68fe-4abd-b6f1-7457493f88d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "아래 광고 메세지의 구조 분석해라.\n",
        "분석의 목적은 메세지에서 구조화된 정보를 추출하기 위함이다.\n",
        "다양한 구조의 다른 메세지들도 분석할 예정인데, 이를 위한 prompt에 추가할 JSON이 아래와 같은 데, 개선점을 파악해서, 새로운 JSON을 추천해라.\n",
        "===광고 메세지 예시===\n",
        "{mms_pdf['msg'][:100]}\n",
        "===PROMPT에 넣을 JSON===\n",
        "{schema_cld}\n",
        "\"\"\"\n",
        "\n",
        "cot_list = []\n",
        "for row in few_shot_pdf.to_dict('records'):\n",
        "    prompt = f\"\"\"\n",
        "    당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 매칭되는 JSON 개체를 만들기 위한 Chain-of-Thought를 만들어주세요.\n",
        "    당신이 제공한 Chain-of-Thought는 다른 LLM이 유사한 광고 메세지에서 구조화된 정보를 추출하는 데 사용됩니다.\n",
        "    다른 LLM이 이 Chain-of-Thought를 사용하여 광고 메세지에서 구조화된 정보를 추출할 수 있도록, 예시 광고 메세지의 특징을 파악해야 하지만, 그렇다고 너무 의존적이어서는 안됩니다. \n",
        "    왜나허면, 동일한 메세지 분석에 사용되는 것이 아니기 때문입니다.\n",
        "    Chain-of-Thought는 한국어로 작성하고, 다른 LLM에 prompt로 바로 사용될 수 있도록 hain-of-Thought 결과만을 제공해 주세요.\n",
        "    \n",
        "    ===광고 메세지 예시===\n",
        "    메세지 제목: {row['msg_head']}\n",
        "    메세지 내용: {row['msg_body']}\n",
        "\n",
        "    ===결과 Schema===\n",
        "    {schema_cld}\n",
        "    \"\"\"\n",
        "    row['cot_chat'] = (llm_chat.invoke(prompt).content)\n",
        "\n",
        "    cot_list.append(row)\n",
        "\n",
        "    if len(cot_list) % 10 == 0:\n",
        "        print(f\"Processed {len(cot_list)} rows\")\n",
        "\n",
        "    # print((llm_chat.invoke(prompt).content))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8be43b81-d653-482e-87c3-8bbe4e5a8951",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"message_info\": {\n",
            "        \"title\": \"[SK텔레콤] ACT대리점 SKT 신현본점에서 꼭꼭 숨겨뒀던 할인 꿀팁 방출\",\n",
            "        \"main_theme\": \"통신비 할인 혜택 안내\",\n",
            "        \"period\": \"5월 한정 프로모션(기가인터넷+TV 요금무료 관련)\"\n",
            "    },\n",
            "    \"purpose\": [\n",
            "        \"대리점 방문 유도\",\n",
            "        \"할인 혜택 안내\",\n",
            "        \"상품 가입 유도\"\n",
            "    ],\n",
            "    \"target\": [\n",
            "        {\n",
            "            \"segment\": \"루원이편한세상 하늘채아파트 거주자\",\n",
            "            \"characteristics\": \"통신비 절감에 관심 있는 고객\",\n",
            "            \"priority\": 1\n",
            "        },\n",
            "        {\n",
            "            \"segment\": \"스마트폰 구매 예정 고객\",\n",
            "            \"characteristics\": \"갤럭시 S23 또는 퀀텀3에 관심 있는 고객\",\n",
            "            \"priority\": 2\n",
            "        },\n",
            "        {\n",
            "            \"segment\": \"인터넷/TV 서비스 가입 고객\",\n",
            "            \"characteristics\": \"하나 원더카드 보유 또는 가입 가능 고객\",\n",
            "            \"priority\": 3\n",
            "        }\n",
            "    ],\n",
            "    \"product\": [\n",
            "        {\n",
            "            \"name\": \"기가인터넷+TV\",\n",
            "            \"category\": \"통신 서비스\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"요금무료\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"하나 원더카드가 SK인터넷요금 2년간 지원(전월 카드 40만원이상 사용기준)\",\n",
            "            \"action\": \"가입\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"S23\",\n",
            "            \"category\": \"스마트폰\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"약정할인과 제휴카드 할인 중복\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"약정 가입 및 제휴카드 사용\",\n",
            "            \"action\": \"구매\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"갤럭시 퀀텀3\",\n",
            "            \"category\": \"스마트폰\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"약정할인과 제휴카드 할인 중복\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"약정 가입 및 제휴카드 사용\",\n",
            "            \"action\": \"구매\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"SK매직/ADT캡스\",\n",
            "            \"category\": \"가전/보안\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"저렴한 이용\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"SK공식인증매장 방문\",\n",
            "            \"action\": \"가입\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"지인찬스 쿠폰\",\n",
            "            \"category\": \"쿠폰\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"4만원권\",\n",
            "                \"type\": \"쿠폰\"\n",
            "            },\n",
            "            \"conditions\": \"단골등록 및 스마트폰 구매 시\",\n",
            "            \"action\": \"참여\"\n",
            "        }\n",
            "    ],\n",
            "    \"channel\": [\n",
            "        {\n",
            "            \"type\": \"대리점\",\n",
            "            \"value\": \"ACT대리점 신현본점\",\n",
            "            \"action\": \"가입\",\n",
            "            \"primary\": true,\n",
            "            \"availability\": \"매장 영업시간\",\n",
            "            \"benefit\": \"더 많은 할인 혜택\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"https://m.place.naver.com/place/1108219331/home\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"24시간\",\n",
            "            \"benefit\": \"네이버로 매장 정보 확인\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"https://m.tworld.co.kr/customer/agentsearch/detail?code=D13546-0297\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"24시간\",\n",
            "            \"benefit\": \"매장 위치 확인\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"URL\",\n",
            "            \"value\": \"https://tworldfriends.co.kr/D135460297\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"24시간\",\n",
            "            \"benefit\": \"매장 홈페이지 접속\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"0507-1404-2560\",\n",
            "            \"action\": \"문의\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"매장 영업시간\",\n",
            "            \"benefit\": \"직접 문의 가능\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"1558\",\n",
            "            \"action\": \"문의\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"SK텔레콤 고객센터 운영시간\",\n",
            "            \"benefit\": \"무료 문의\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"1504\",\n",
            "            \"action\": \"수신 거부\",\n",
            "            \"primary\": false,\n",
            "            \"availability\": \"24시간\",\n",
            "            \"benefit\": \"무료 수신거부\"\n",
            "        }\n",
            "    ],\n",
            "    \"metadata\": {\n",
            "        \"message_type\": \"프로모션\",\n",
            "        \"target_response\": \"대리점 방문 및 상품 가입\",\n",
            "        \"success_metrics\": [\n",
            "            \"방문율\",\n",
            "            \"가입율\",\n",
            "            \"쿠폰 이용률\"\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Define your schema\n",
        "schema_cld = {\n",
        "    \"properties\": {\n",
        "        \"message_info\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\", \"description\": \"광고 제목\"},\n",
        "                \"main_theme\": {\"type\": \"string\", \"description\": \"광고의 주요 테마/주제\"},\n",
        "                \"period\": {\"type\": \"string\", \"description\": \"이벤트/프로모션 기간\"}\n",
        "            }\n",
        "        },\n",
        "        \"purpose\": {\"type\": \"array\", \"description\": \"[상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공] 중에서 선택. 애매모호한 상황인 경우, 복수 선택 가능.\"},\n",
        "        \"target\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"segment\": {\"type\": \"string\", \"description\": \"타겟 고객층\"},\n",
        "                    \"characteristics\": {\"type\": \"string\", \"description\": \"해당 타겟의 특성\"},\n",
        "                    \"priority\": {\"type\": \"integer\", \"description\": \"타겟팅 우선순위\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"product\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\"type\": \"string\", \"description\": \"광고하는 제품이나 서비스. 광고 메세지에 표현된 원본 그대로 추출해 주세요.\"},\n",
        "                    \"category\": {\"type\": \"string\", \"description\": \"광고 상품의 카테고리\"},\n",
        "                    \"benefit\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\", \"description\": \"제공되는 혜택 이름\"},\n",
        "                            \"type\": {\"type\": \"string\", \"description\": \"혜택 타입. [할인, 쿠폰, 경품, 기타] 중에서 선택\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"conditions\": {\"type\": \"string\", \"description\": \"혜택/구매 조건\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"해당 상품에 대해서 고객이 하기 바라는 행동. [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택\"}\n",
        "                },\n",
        "                \"required\": [\"name\", \"info\"]\n",
        "            }\n",
        "        },\n",
        "        \"channel\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"type\": {\"type\": \"string\", \"description\": \"채널의 종류 (URL, 전화번호, 앱, 대리점)\"},\n",
        "                    \"value\": {\"type\": \"string\", \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등의 상세 정보\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"채널 제공의 목적. [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"},\n",
        "                    \"primary\": {\"type\": \"boolean\", \"description\": \"주요 채널 여부\"},\n",
        "                    \"availability\": {\"type\": \"string\", \"description\": \"채널 이용 가능 시간/조건\"},\n",
        "                    \"benefit\": {\"type\": \"string\", \"description\": \"해당 채널 이용 시 혜택\"}\n",
        "                },\n",
        "                \"required\": [\"type\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message_type\": {\"type\": \"string\"},\n",
        "            \"target_response\": {\"type\": \"string\"},\n",
        "            \"success_metrics\": {\"type\": \"array\"}\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"purpose\", \"target\", \"product\", \"channel\",\"metadata\"],\n",
        "    \"objectType\": \"object\"\n",
        "}\n",
        "\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 매칭되는 JSON 개체를 만들어 주세요:\\n {schema}\\n\\n제품 참조 정보:\\n{rag_context}\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Create the parser\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# Create the chain\n",
        "extraction_chain = prompt | llm_cld37 | parser\n",
        "\n",
        "# Run the extraction\n",
        "msg_text = \"\"\"\n",
        "광고 제목: [SK텔레콤] ACT대리점 SKT 신현본점에서 꼭꼭 숨겨뒀던 할인 꿀팁 방출\n",
        "    \n",
        "광고 내용: (광고)[SKT] 고객님 안녕하세요._루원이편한세상 하늘채아파트 정문 신현본점에서_통신비 할인 비책 공개합니다!__① 기가인터넷+TV 요금무료_ 5월 한정 프로모션, 하나 원더카드가_ SK인터넷요금 2년간 지원_ (전월 카드 40만원이상 사용기준)__② S23 할인이 중복_ S23은 약정할인과 제휴카드 할인이 중복된다는 사실!_ 갓성비 갤럭시 퀀텀3도 마찬가지__③ SK공식인증매장에서 더 싸다?_ SK매직/ADT캡스와 제휴하여_ 같은 공기청정기/정수기/홈보안이라도 훨씬 저렴하게 이용!__□ 매장으로 문의주세요 (네이버,매장 홈페이지로 방문예약 가능)__◆ 단골등록 이벤트_ 단골고객 모두에게_ 스마트폰 구매 시 나와 지인 같이 할인받는_ \"지인찬스 쿠폰\" 4만원권을 발송__■ ACT대리점 신현본점_- 주소: 인천 서구 가정로 387, 가동 201호 SK텔레콤_- 연락처: 0507-1404-2560_- 네이버로 보기: http://t-mms.kr/t.do?m=#61&s=19725&a=&u=https://m.place.naver.com/place/1108219331/home_- 매장 위치보기: http://t-mms.kr/t.do?m=#61&s=19726&a=&u=https://m.tworld.co.kr/customer/agentsearch/detail?code=D13546-0297_- 매장 홈페이지: http://t-mms.kr/t.do?m=#61&s=19727&a=&u=https://tworldfriends.co.kr/D135460297__■ 문의 : SKT고객센터(1558,무료)_무료 수신거부 1504_\n",
        "\"\"\".replace('_','\\n')\n",
        "# print(msg_text)\n",
        "\n",
        "# rag_context = get_relevant_context(msg_text, vectorstore, topk=20)['title']\n",
        "\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list, \n",
        "    min_similarity=70,  # Lower threshold for more matches\n",
        "    ngram_size=3,       # Bigrams work well for Korean\n",
        "    min_entity_length=3 # Only consider entities with 3+ characters\n",
        ")\n",
        "\n",
        "rag_context = \", \".join(set([match['text'] for match in matches])) # matched_entity, text\n",
        "\n",
        "result_cld = extraction_chain.invoke({\"text\": msg_text, \"schema\": str(schema_cld), \"rag_context\":rag_context})\n",
        "\n",
        "print(json.dumps(result_cld, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bbb040-d956-42ed-9786-e8c9c3013e2b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for index in range(len(result_cld['product'])):\n",
        "    item_name = result_cld['product'][index]['name'] #+ \" \" + result['광고 상품'][0]['상품 정보']\n",
        "    # get_relevant_context(item_name, vectorstore, topk=20)['title']\n",
        "\n",
        "    matches = find_entities_in_text(\n",
        "        result_cld['product'][index]['name'], \n",
        "        entity_list, \n",
        "        min_similarity=70,  # Lower threshold for more matches\n",
        "        ngram_size=3,       # Bigrams work well for Korean\n",
        "        min_entity_length=3 # Only consider entities with 3+ characters\n",
        "    )\n",
        "\n",
        "    schema_res = {\n",
        "    \"ext_item_nm\":{\"type\":\"string\",\"description\":\"기준 item의 name\"},\n",
        "    \"item_id\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 id\"},\n",
        "    \"item_nm\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 name\"},\n",
        "    \"domain\":{\"type\":\"string\",\"description\":\"해당 item의 도메인\"},    \n",
        "    \"reason\":{\"type\":\"string\",\"description\":\"선택한 이유\"}\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    아래 광고 메세지에서 추출한 상품 정보와 후보 상품 리스트 중에서 광고 메세지의 맥락에 가장 부합하는 상품을 선택해라.  다음과 같은 schema에 참고하여 JSON 개체로 만들어 주세요:\\n {schema_res}\n",
        "    ---상품 정보---\n",
        "    상품명: {result_cld['product'][index]['name']}\n",
        "    카테고리: {result_cld['product'][index]['category']}\n",
        "    혜택: {result_cld['product'][index]['benefit']}\n",
        "    조건: {result_cld['product'][index]['conditions']}\n",
        "    반응: {result_cld['product'][index]['action']}\n",
        "    \n",
        "    ---원본 광고 메세지---\n",
        "    {msg_text}\n",
        "\n",
        "    ---후보 상품 리스트---\n",
        "    {pd.DataFrame(matches)}\n",
        "    \"\"\"\n",
        "\n",
        "    res_cld = extract_json_objects(llm_cld37.invoke(prompt).content)[0]\n",
        "    print(json.dumps(res_cld, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72808d81-f3d8-40b7-b70f-afbb1d131f93",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "아래 광고 메세지의 구조 분석해라.\n",
        "분석의 목적은 메세지에서 구조화된 정보를 추출하기 위함이다.\n",
        "다양한 구조의 다른 메세지들도 분석할 예정인데, 이를 위한 prompt에 추가할 JSON이 아래와 같은 데, 개선점을 파악해서, 새로운 JSON을 추천해라.\n",
        "===광고 메세지 예시===\n",
        "{mdf[:15]}\n",
        "===PROMPT에 넣을 JSON===\n",
        "{schema_ax}\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\":prompt}\n",
        "    ],\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1a65dcff-458d-4b3a-b359-0ede6bb11aca",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"message_info\": {\n",
            "        \"title\": \"[SK텔레콤] ACT대리점 SKT 신현본점에서 꼭꼭 숨겨뒀던 할인 꿀팁 방출\",\n",
            "        \"main_theme\": \"통신비 할인 및 제휴 혜택\",\n",
            "        \"period\": \"5월 한정 프로모션\"\n",
            "    },\n",
            "    \"purpose\": [\n",
            "        \"상품 가입 유도\",\n",
            "        \"대리점 방문 유도\",\n",
            "        \"웹/앱 접속 유도\",\n",
            "        \"이벤트 응모 유도\",\n",
            "        \"할인 혜택 안내\",\n",
            "        \"쿠폰 제공 안내\"\n",
            "    ],\n",
            "    \"target\": [\n",
            "        {\n",
            "            \"segment\": \"SK텔레콤 고객\",\n",
            "            \"characteristics\": \"통신비 절약에 관심 있는 고객\",\n",
            "            \"priority\": \"1\"\n",
            "        },\n",
            "        {\n",
            "            \"segment\": \"루원이편한세상 하늘채아파트 주민\",\n",
            "            \"characteristics\": \"가까운 대리점을 이용할 가능성이 높은 고객\",\n",
            "            \"priority\": \"2\"\n",
            "        }\n",
            "    ],\n",
            "    \"product\": [\n",
            "        {\n",
            "            \"name\": \"기가인터넷+TV 요금무료\",\n",
            "            \"category\": \"통신 서비스\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"통신비 할인\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"하나 원더카드 전월 40만원 이상 사용 기준\",\n",
            "            \"action\": \"가입\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"S23 할인\",\n",
            "            \"category\": \"스마트폰\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"중복 할인\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"약정할인 및 제휴카드 할인 적용\",\n",
            "            \"action\": \"구매\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"SK매직/ADT캡스 제휴 할인\",\n",
            "            \"category\": \"생활 가전 및 보안 서비스\",\n",
            "            \"benefit\": {\n",
            "                \"name\": \"저렴한 이용\",\n",
            "                \"type\": \"할인\"\n",
            "            },\n",
            "            \"conditions\": \"제휴 상품 이용\",\n",
            "            \"action\": \"가입\"\n",
            "        }\n",
            "    ],\n",
            "    \"channel\": [\n",
            "        {\n",
            "            \"type\": \"대리점\",\n",
            "            \"value\": \"ACT대리점 신현본점\",\n",
            "            \"action\": \"방문\",\n",
            "            \"primary\": \"true\",\n",
            "            \"availability\": \"상시 이용 가능\",\n",
            "            \"benefit\": \"단골등록 이벤트 참여 시 지인찬스 쿠폰 제공\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"네이버\",\n",
            "            \"value\": \"http://t-mms.kr/t.do?m=#61&s=19725&a=&u=https://m.place.naver.com/place/1108219331/home\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"primary\": \"false\",\n",
            "            \"availability\": \"상시 이용 가능\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"매장 홈페이지\",\n",
            "            \"value\": \"http://t-mms.kr/t.do?m=#61&s=19727&a=&u=https://tworldfriends.co.kr/D135460297\",\n",
            "            \"action\": \"추가 정보\",\n",
            "            \"primary\": \"false\",\n",
            "            \"availability\": \"상시 이용 가능\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"전화번호\",\n",
            "            \"value\": \"0507-1404-2560\",\n",
            "            \"action\": \"문의\",\n",
            "            \"primary\": \"false\",\n",
            "            \"availability\": \"상시 이용 가능\"\n",
            "        }\n",
            "    ],\n",
            "    \"metadata\": {\n",
            "        \"message_type\": \"광고\",\n",
            "        \"target_response\": \"고객 방문 및 가입 유도\",\n",
            "        \"success_metrics\": [\n",
            "            \"매장 방문 수\",\n",
            "            \"가입 건수\"\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "schema_ax = {\n",
        "    \"message_info\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"title\": {\"type\": \"string\", \"description\": \"광고 제목\"},\n",
        "            \"main_theme\": {\"type\": \"string\", \"description\": \"광고의 주요 테마/주제\"},\n",
        "            \"period\": {\"type\": \"string\", \"description\": \"이벤트/프로모션 기간\"}\n",
        "        }\n",
        "    },\n",
        "    \"purpose\": {\"type\": \"array\", \"description\": \"[상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공] 중에서 선택. 애매모호한 상황인 경우, 복수 선택 가능.\"},\n",
        "    \"target\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"segment\": {\"type\": \"string\", \"description\": \"타겟 고객층\"},\n",
        "                \"characteristics\": {\"type\": \"string\", \"description\": \"해당 타겟의 특성\"},\n",
        "                \"priority\": {\"type\": \"integer\", \"description\": \"타겟팅 우선순위\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"product\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\"type\": \"string\", \"description\": \"광고하는 제품이나 서비스. 광고 메세지에 표현된 원본 그대로 추출해 주세요.\"},\n",
        "                    \"category\": {\"type\": \"string\", \"description\": \"광고 상품의 카테고리\"},\n",
        "                    \"benefit\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\", \"description\": \"제공되는 혜택 이름\"},\n",
        "                            \"type\": {\"type\": \"string\", \"description\": \"혜택 타입. [할인, 쿠폰, 경품, 기타] 중에서 선택\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"conditions\": {\"type\": \"string\", \"description\": \"혜택/구매 조건\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"해당 상품에 대해서 고객이 하기 바라는 행동. [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택\"}\n",
        "                },\n",
        "                \"required\": [\"name\", \"info\"]\n",
        "            }\n",
        "    },\n",
        "    \"channel\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"type\": {\"type\": \"string\", \"description\": \"채널의 종류 (URL, 전화번호, 앱, 대리점)\"},\n",
        "                \"value\": {\"type\": \"string\", \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등의 상세 정보\"},\n",
        "                \"action\": {\"type\": \"string\", \"description\": \"채널 제공의 목적. [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"},\n",
        "                \"primary\": {\"type\": \"boolean\", \"description\": \"주요 채널 여부\"},\n",
        "                \"availability\": {\"type\": \"string\", \"description\": \"채널 이용 가능 시간/조건\"},\n",
        "                \"benefit\": {\"type\": \"string\", \"description\": \"해당 채널 이용 시 혜택\"}\n",
        "            },\n",
        "            \"required\": [\"type\", \"value\"]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"metadata\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message_type\": {\"type\": \"string\"},\n",
        "            \"target_response\": {\"type\": \"string\"},\n",
        "            \"success_metrics\": {\"type\": \"array\"}\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"required\": [\"purpose\", \"target\", \"product\", \"channel\",\"metadata\"],\n",
        "    \"objectType\": \"object\"\n",
        "}\n",
        "\n",
        "\n",
        "msg_text = \"\"\"\n",
        "광고 제목: [SK텔레콤] ACT대리점 SKT 신현본점에서 꼭꼭 숨겨뒀던 할인 꿀팁 방출\n",
        "    \n",
        "광고 내용: (광고)[SKT] 고객님 안녕하세요._루원이편한세상 하늘채아파트 정문 신현본점에서_통신비 할인 비책 공개합니다!__① 기가인터넷+TV 요금무료_ 5월 한정 프로모션, 하나 원더카드가_ SK인터넷요금 2년간 지원_ (전월 카드 40만원이상 사용기준)__② S23 할인이 중복_ S23은 약정할인과 제휴카드 할인이 중복된다는 사실!_ 갓성비 갤럭시 퀀텀3도 마찬가지__③ SK공식인증매장에서 더 싸다?_ SK매직/ADT캡스와 제휴하여_ 같은 공기청정기/정수기/홈보안이라도 훨씬 저렴하게 이용!__□ 매장으로 문의주세요 (네이버,매장 홈페이지로 방문예약 가능)__◆ 단골등록 이벤트_ 단골고객 모두에게_ 스마트폰 구매 시 나와 지인 같이 할인받는_ \"지인찬스 쿠폰\" 4만원권을 발송__■ ACT대리점 신현본점_- 주소: 인천 서구 가정로 387, 가동 201호 SK텔레콤_- 연락처: 0507-1404-2560_- 네이버로 보기: http://t-mms.kr/t.do?m=#61&s=19725&a=&u=https://m.place.naver.com/place/1108219331/home_- 매장 위치보기: http://t-mms.kr/t.do?m=#61&s=19726&a=&u=https://m.tworld.co.kr/customer/agentsearch/detail?code=D13546-0297_- 매장 홈페이지: http://t-mms.kr/t.do?m=#61&s=19727&a=&u=https://tworldfriends.co.kr/D135460297__■ 문의 : SKT고객센터(1558,무료)_무료 수신거부 1504_\n",
        "\"\"\".replace('_','\\n')\n",
        "\n",
        "# msg_text = remove_control_characters(msg_text)#[-700:]\n",
        "# print(msg_text)\n",
        "\n",
        "similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "# Assign similarities to dataframe\n",
        "few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "few_shot_temp['sim'] = similarities\n",
        "few_shot_temp['rank'] = few_shot_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "\n",
        "# Get top 5 examples\n",
        "few_shot_exm = []\n",
        "top_examples = few_shot_temp[\n",
        "    (few_shot_temp['rank'] >= 2) & \n",
        "    (few_shot_temp['rank'] <= 2)\n",
        "].sort_values(\"rank\")\n",
        "\n",
        "for _, r in top_examples.iterrows():\n",
        "    few_shot_exm.append(f\"\"\"[학습용 광고 메세지]\n",
        "    광고 제목:{r['msg_head']}\n",
        "    광고 내용:{r['msg_body']}\n",
        "\n",
        "    [학습용 정답 결과]\n",
        "    {r['res_chat']}\"\"\")\n",
        "\n",
        "few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "\n",
        "cot_str = \"\"\"\n",
        "이 광고는 SK텔레콤 대리점의 복합 프로모션을 안내하는 메시지입니다. 주요 목적은 할인 혜택 안내와 대리점 방문 유도이며, 특정 아파트 단지를 타겟팅하고 있습니다. 통신 서비스, 스마트폰, 가전제품 등 다양한 제품군에 대한 할인 혜택을 제시하며, 여러 채널을 통한 접근 방법을 안내하고 있습니다.\n",
        "\"\"\"\n",
        "\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list, \n",
        "    min_similarity=70,  # Lower threshold for more matches\n",
        "    ngram_size=3,       # Bigrams work well for Korean\n",
        "    min_entity_length=3 # Only consider entities with 3+ characters\n",
        ")\n",
        "\n",
        "rag_context = \", \".join(set([match['matched_entity'] for match in matches])) # matched_entity, text\n",
        "# print(rag_context)\n",
        "\n",
        "# rag_context = rag_context+\", \"+ \", \".join([r['상품명'] for r in result_cld['광고 상품']])\n",
        "\n",
        "# Create the system and user messages\n",
        "system_message_ax = f\"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 매칭되는 JSON 개체를 만들어 주세요:\\n {schema_ax}\"\n",
        "\n",
        "# Create the chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message_ax},\n",
        "        {\"role\": \"user\", \"content\": \n",
        "         # \"\\n\\n\"+system_message_ax+\n",
        "         \"\\n\\n=====분석 대상 광고 메세지=====\\n\"+msg_text+\n",
        "         \"\\n\\n=====관련 상품 이름=====\\n\"+rag_context+\n",
        "        # \"\\n\\n=====추론 가이드=====\\n\"+cot_str+\n",
        "        #  \"\\n\\n아래에 몇 가지 예시들이 있다.\"\n",
        "        # \"\\n=====학습용 예시=====\\n\"+few_shot_exm_str\n",
        "         \"\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# Parse the response\n",
        "result_ax = extract_json_objects(response.choices[0].message.content)[0]\n",
        "\n",
        "print(json.dumps(result_ax, indent=4, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f068bc-d4fc-451d-b569-f5e82a58e920",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# schema_ax = {\n",
        "#         \"purpose\": {\"type\": \"array\", \"description\": \"[상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공] 중에서 선택.\"},\n",
        "#         \"target\": {\"type\": \"array\", \"description\": \"광고가 타겟팅하는 고객층\"},\n",
        "#         \"product\": {\n",
        "#             \"type\": \"array\",\n",
        "#             \"items\": {\n",
        "#                 \"type\": \"object\",\n",
        "#                 \"properties\": {\n",
        "#                     \"name\": {\"type\": \"string\", \"description\": \"광고하는 제품이나 서비스. 광고 메세지에 표현된 원본 그대로 추출해 주세요.\"},\n",
        "#                     \"category\": {\"type\": \"string\", \"description\": \"광고 상품의 카테고리\"},\n",
        "#                     \"info\": {\"type\": \"string\", \"description\": \"광고 상품에 대한 정보\"},\n",
        "#                     \"action\": {\"type\": \"string\", \"description\": \"해당 상품에 대해서 고객이 하기 바라는 행동. [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택\"}\n",
        "#                 },\n",
        "#                 \"required\": [\"name\", \"info\"]\n",
        "#             }\n",
        "#         },\n",
        "#         \"channel\": {\n",
        "#             \"type\": \"array\",\n",
        "#             \"items\": {\n",
        "#                 \"type\": \"object\",\n",
        "#                 \"properties\": {\n",
        "#                     \"type\": {\"type\": \"string\", \"description\": \"채널의 종류 (URL, 전화번호, 앱, 대리점)\"},\n",
        "#                     \"value\": {\"type\": \"string\", \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등의 상세 정보\"},\n",
        "#                     \"action\": {\"type\": \"string\", \"description\": \"채널 제공의 목적. [가입, 추가 정보, 문의, 수신] 중에서 선택\"}\n",
        "#                 },\n",
        "#                 \"required\": [\"type\", \"value\"]\n",
        "#             }\n",
        "#         }\n",
        "          \n",
        "# }\n",
        "\n",
        "msg_text = \"\"\"\n",
        "광고 제목: [SK텔레콤] ACT대리점 SKT 신현본점에서 꼭꼭 숨겨뒀던 할인 꿀팁 방출\n",
        "    \n",
        "광고 내용: (광고)[SKT] 고객님 안녕하세요._루원이편한세상 하늘채아파트 정문 신현본점에서_통신비 할인 비책 공개합니다!__① 기가인터넷+TV 요금무료_ 5월 한정 프로모션, 하나 원더카드가_ SK인터넷요금 2년간 지원_ (전월 카드 40만원이상 사용기준)__② S23 할인이 중복_ S23은 약정할인과 제휴카드 할인이 중복된다는 사실!_ 갓성비 갤럭시 퀀텀3도 마찬가지__③ SK공식인증매장에서 더 싸다?_ SK매직/ADT캡스와 제휴하여_ 같은 공기청정기/정수기/홈보안이라도 훨씬 저렴하게 이용!__□ 매장으로 문의주세요 (네이버,매장 홈페이지로 방문예약 가능)__◆ 단골등록 이벤트_ 단골고객 모두에게_ 스마트폰 구매 시 나와 지인 같이 할인받는_ \"지인찬스 쿠폰\" 4만원권을 발송__■ ACT대리점 신현본점_- 주소: 인천 서구 가정로 387, 가동 201호 SK텔레콤_- 연락처: 0507-1404-2560_- 네이버로 보기: http://t-mms.kr/t.do?m=#61&s=19725&a=&u=https://m.place.naver.com/place/1108219331/home_- 매장 위치보기: http://t-mms.kr/t.do?m=#61&s=19726&a=&u=https://m.tworld.co.kr/customer/agentsearch/detail?code=D13546-0297_- 매장 홈페이지: http://t-mms.kr/t.do?m=#61&s=19727&a=&u=https://tworldfriends.co.kr/D135460297__■ 문의 : SKT고객센터(1558,무료)_무료 수신거부 1504_\n",
        "\"\"\".replace('_','\\n')\n",
        "\n",
        "matches = find_entities_in_text(\n",
        "    msg_text, \n",
        "    entity_list, \n",
        "    min_similarity=70,  # Lower threshold for more matches\n",
        "    ngram_size=3,       # Bigrams work well for Korean\n",
        "    min_entity_length=3 # Only consider entities with 3+ characters\n",
        ")\n",
        "\n",
        "rag_context = \", \".join(set([match['text'] for match in matches])) # matched_entity, text\n",
        "\n",
        "prompt_list = [{\"schema\":schema_ax,\"msg_text\": \"=====분석 대상 광고 메세지=====\\n\"+msg_text+\"\\n=====후보 상품 이름=====\\n\"+rag_context}, {\"schema\":{\"channel\":schema_ax['channel']},\"msg_text\":msg_text}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d12449f-7069-438e-b255-eb8f356af539",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def process_prompt(row):\n",
        "    \n",
        "    schema_ax = row['schema']\n",
        "    msg_text = row['msg_text']\n",
        "    \n",
        "    system_message_ax = f\"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 부합하도록 JSON 개체를 만들어 주세요:\\n {schema_ax}\"\n",
        "        \n",
        "    # Create the chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message_ax},\n",
        "            {\"role\": \"user\", \"content\": msg_text}\n",
        "        ],\n",
        "        temperature=0.0\n",
        "    )\n",
        "    \n",
        "    # print(response.choices[0].message.content)\n",
        "\n",
        "    # Parse the response\n",
        "    result_ax = extract_json_objects(response.choices[0].message.content)[0]\n",
        "\n",
        "    return result_ax\n",
        "\n",
        "results_ax_mul = []\n",
        "with ThreadPoolExecutor(max_workers=len(prompt_list)) as executor:  # More threads for I/O-bound tasks\n",
        "    results_ax_mul += list(executor.map(process_prompt, prompt_list))\n",
        "\n",
        "result_ax = {}\n",
        "for key in list(set(sum([list(d.keys()) for d in results_ax_mul], []))):\n",
        "    lst = []\n",
        "    for dic in results_ax_mul:\n",
        "        if key in dic:\n",
        "            lst.append(dic[key])\n",
        "    \n",
        "    dict_list = sum(lst,[])\n",
        "\n",
        "    if is_list_of_dicts(dict_list):\n",
        "        df = pd.DataFrame(dict_list)\n",
        "        df = df.drop_duplicates()\n",
        "        dict_list = df.to_dict('records')\n",
        "                \n",
        "    result_ax[key] = dict_list\n",
        "\n",
        "print(json.dumps(result_ax, indent=4, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0c7686-2177-4097-9d37-7459a624f31c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for index in range(len(result_ax['product'])):\n",
        "    item_name = result_ax['product'][index]['name'] #+ \" \" + result['광고 상품'][0]['상품 정보']\n",
        "    # get_relevant_context(item_name, vectorstore, topk=20)['title']\n",
        "\n",
        "    # item_pdf['sim'] = cosine_similarity(\n",
        "    #     tfidf_name.transform([result_ax['광고 상품'][index]['상품명'] ]),\n",
        "    #     tfidf_matrix_name\n",
        "    # )[0]\n",
        "    \n",
        "    matches = find_entities_in_text(\n",
        "        result_ax['product'][index]['name'], \n",
        "        entity_list, \n",
        "        min_similarity=70,  # Lower threshold for more matches\n",
        "        ngram_size=3,       # Bigrams work well for Korean\n",
        "        min_entity_length=3 # Only consider entities with 3+ characters\n",
        "    )\n",
        "\n",
        "    schema_res = {\n",
        "    \"ext_item_nm\":{\"type\":\"string\",\"description\":\"기준 item의 name\"},\n",
        "    \"item_id\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 id\"},\n",
        "    \"item_nm\":{\"type\":\"string\",\"description\":\"가장 유사한 item의 name\"},\n",
        "    \"domain\":{\"type\":\"string\",\"description\":\"해당 item의 도메인\"},    \n",
        "    \"reason\":{\"type\":\"string\",\"description\":\"선택한 이유\"}\n",
        "    }\n",
        "\n",
        "    text = f\"\"\"\n",
        "    ---상품 정보---\n",
        "    상품명: {result_ax['product'][index]['name']}\n",
        "    상품 카테고리: {result_ax['product'][index]['category']}\n",
        "    상품 정보: {result_ax['product'][index]['info']}\n",
        "    \n",
        "    ---원본 광고 메세지---\n",
        "    {msg_text}\n",
        "\n",
        "    ---후보 상품 리스트---\n",
        "    {pd.DataFrame(matches)}\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    system_message_ax = f\"\"\"\n",
        "    아래 광고 메세지에서 추출한 상품 정보와 후보 상품 리스트 중에서 광고 메세지의 맥락에 가장 부합하는 상품을 선택해라. 다음과 같은 schema에 참고하여 JSON 개체로 만들어 주세요:\\n {schema_res}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the chat completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message_ax},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "        \n",
        "    res_ax = extract_json_objects((response.choices[0].message.content).replace(\"'\",'\"'))[0]\n",
        "    print(json.dumps(res_ax, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "25e7be66-f2ca-4ba7-b948-b619e83d9e1e",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import konlpy.tag\n",
        "\n",
        "# For Korean text preprocessing, use KoNLPy\n",
        "class KoreanTokenizer:\n",
        "    def __init__(self, tagger_type='Okt'):\n",
        "        # Choose a Korean morphological analyzer\n",
        "        # Options: Okt, Mecab, Komoran, Hannanum, Kkma\n",
        "        if tagger_type == 'Okt':\n",
        "            from konlpy.tag import Okt\n",
        "            self.tagger = Okt()\n",
        "        elif tagger_type == 'Mecab':\n",
        "            from konlpy.tag import Mecab\n",
        "            self.tagger = Mecab()\n",
        "        # Add other taggers as needed\n",
        "    \n",
        "    def __call__(self, text):\n",
        "        # Extract nouns, adjectives, and verbs (you can customize this)\n",
        "        tokens = self.tagger.morphs(text)\n",
        "        return tokens\n",
        "\n",
        "# Initialize the tokenizer (Okt is generally good for most purposes)\n",
        "korean_tokenizer = KoreanTokenizer(tagger_type='Okt')\n",
        "\n",
        "tfidf_name = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(3,10),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "tfidf_cate = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2,4),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "tfidf_desc = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(3,5),\n",
        "            min_df=1,\n",
        "            # stop_words=None,  # Don't use stop words initially\n",
        "            # token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "            # strip_accents='unicode',  # Handle accented characters\n",
        "        )\n",
        "\n",
        "# tfidf_desc = TfidfVectorizer(\n",
        "#             analyzer='word',\n",
        "#             ngram_range=(1,5),\n",
        "#             min_df=1,\n",
        "#             stop_words=None,  # Don't use stop words initially\n",
        "#             token_pattern=r'(?u)\\b\\w+\\b',  # Match any word character (more permissive)\n",
        "#             strip_accents='unicode',  # Handle accented characters\n",
        "#         )\n",
        "\n",
        "# tfidf_name = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "# tfidf_desc = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "# tfidf_item = TfidfVectorizer(\n",
        "#     tokenizer=korean_tokenizer,\n",
        "#     ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "#     max_features=10000,  # Limit vocabulary size\n",
        "#     min_df=2,            # Minimum document frequency\n",
        "#     use_idf=True,\n",
        "#     sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
        "# )\n",
        "\n",
        "few_shot_pdf = pd.read_pickle(\"./data/few_shot_data_chat_250410_rd_500.pkl\")\n",
        "few_shot_pdf = few_shot_pdf.astype('str')\n",
        "\n",
        "tfidf_matrix_name = tfidf_name.fit_transform(item_pdf['item_nm_cl'])\n",
        "tfidf_matrix_cate = tfidf_cate.fit_transform(item_pdf['item_cate_ax'])\n",
        "tfidf_matrix_desc = tfidf_desc.fit_transform(few_shot_pdf[\"msg_body\"])\n",
        "# tfidf_matrix_item = tfidf_item.fit_transform(item_pdf['item_item_cl'])\n",
        "\n",
        "# import torch\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# emb_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')  # Or other appropriate model\n",
        "# emb_model = emb_model.to(device)\n",
        "# item_all = item_pdf['item_item_cl'].tolist()\n",
        "# item_embeddings = emb_model.encode(item_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c62d55f5-1980-4217-adb4-45c3f4f379f6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Define your schema\n",
        "schema_cld = {\n",
        "    \"properties\": {\n",
        "        \"message_info\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"string\", \"description\": \"광고 제목\"},\n",
        "                \"main_theme\": {\"type\": \"string\", \"description\": \"광고의 주요 테마/주제\"},\n",
        "                \"period\": {\"type\": \"string\", \"description\": \"이벤트/프로모션 기간\"}\n",
        "            }\n",
        "        },\n",
        "        \"purpose\": {\"type\": \"array\", \"description\": \"[상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공] 중에서 선택. 애매모호한 상황인 경우, 복수 선택 가능.\"},\n",
        "        \"target\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"segment\": {\"type\": \"string\", \"description\": \"타겟 고객층\"},\n",
        "                    \"characteristics\": {\"type\": \"string\", \"description\": \"해당 타겟의 특성\"},\n",
        "                    \"priority\": {\"type\": \"integer\", \"description\": \"타겟팅 우선순위\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"product\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\"type\": \"string\", \"description\": \"광고하는 제품이나 서비스. 광고 메세지에 표현된 원본 그대로 추출해 주세요.\"},\n",
        "                    \"category\": {\"type\": \"string\", \"description\": \"광고 상품의 카테고리\"},\n",
        "                    \"benefit\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\", \"description\": \"제공되는 혜택 이름\"},\n",
        "                            \"type\": {\"type\": \"string\", \"description\": \"혜택 타입. [할인, 쿠폰, 경품, 기타] 중에서 선택\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"conditions\": {\"type\": \"string\", \"description\": \"혜택/구매 조건\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"해당 상품에 대해서 고객이 하기 바라는 행동. [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택\"}\n",
        "                },\n",
        "                \"required\": [\"name\", \"info\"]\n",
        "            }\n",
        "        },\n",
        "        \"channel\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"type\": {\"type\": \"string\", \"description\": \"채널의 종류 (URL, 전화번호, 앱, 대리점)\"},\n",
        "                    \"value\": {\"type\": \"string\", \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등의 상세 정보\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"채널 제공의 목적. [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"},\n",
        "                    \"primary\": {\"type\": \"boolean\", \"description\": \"주요 채널 여부\"},\n",
        "                    \"availability\": {\"type\": \"string\", \"description\": \"채널 이용 가능 시간/조건\"},\n",
        "                    \"benefit\": {\"type\": \"string\", \"description\": \"해당 채널 이용 시 혜택\"}\n",
        "                },\n",
        "                \"required\": [\"type\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message_type\": {\"type\": \"string\"},\n",
        "            \"target_response\": {\"type\": \"string\"},\n",
        "            \"success_metrics\": {\"type\": \"array\"}\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"purpose\", \"target\", \"product\", \"channel\",\"metadata\"],\n",
        "    \"objectType\": \"object\"\n",
        "}\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_cld = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema에 매칭되는 JSON 개체를 만들어 주세요:\\n {schema}\"\n",
        "     # \"\\n\\n제품 참조 정보:\\n{rag_context}\"\n",
        "    ),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "schema_ax = {\n",
        "    \"message_info\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"title\": {\"type\": \"string\", \"description\": \"광고 제목\"},\n",
        "            \"main_theme\": {\"type\": \"string\", \"description\": \"광고의 주요 테마/주제\"},\n",
        "            \"period\": {\"type\": \"string\", \"description\": \"이벤트/프로모션 기간\"}\n",
        "        }\n",
        "    },\n",
        "    \"purpose\": {\"type\": \"array\", \"description\": \"[상품 가입 유도, 대리점 방문 유도, 웹/앱 접속 유도, 이벤트 응모 유도, 할인 혜택 안내, 쿠폰 제공 안내, 경품 제공 안내, 기타 정보 제공] 중에서 선택. 애매모호한 상황인 경우, 복수 선택 가능.\"},\n",
        "    \"target\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"segment\": {\"type\": \"string\", \"description\": \"타겟 고객층\"},\n",
        "                \"characteristics\": {\"type\": \"string\", \"description\": \"해당 타겟의 특성\"},\n",
        "                \"priority\": {\"type\": \"integer\", \"description\": \"타겟팅 우선순위\"}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"product\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\"type\": \"string\", \"description\": \"광고하는 제품이나 서비스. 광고 메세지에 표현된 원본 그대로 추출해 주세요.\"},\n",
        "                    \"category\": {\"type\": \"string\", \"description\": \"광고 상품의 카테고리\"},\n",
        "                    \"benefit\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\", \"description\": \"제공되는 혜택 이름\"},\n",
        "                            \"type\": {\"type\": \"string\", \"description\": \"혜택 타입. [할인, 쿠폰, 경품, 기타] 중에서 선택\"}\n",
        "                        }\n",
        "                    },\n",
        "                    \"conditions\": {\"type\": \"string\", \"description\": \"혜택/구매 조건\"},\n",
        "                    \"action\": {\"type\": \"string\", \"description\": \"해당 상품에 대해서 고객이 하기 바라는 행동. [구매, 가입, 사용, 방문, 참여, 작성, 기타] 중에서 선택\"}\n",
        "                },\n",
        "                \"required\": [\"name\", \"info\"]\n",
        "            }\n",
        "    },\n",
        "    \"channel\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"type\": {\"type\": \"string\", \"description\": \"채널의 종류 (URL, 전화번호, 앱, 대리점)\"},\n",
        "                \"value\": {\"type\": \"string\", \"description\": \"실제 URL, 전화번호, 앱 이름, 대리점 이름 등의 상세 정보\"},\n",
        "                \"action\": {\"type\": \"string\", \"description\": \"채널 제공의 목적. [가입, 추가 정보, 문의, 수신, 수신 거부] 중에서 선택\"},\n",
        "                \"primary\": {\"type\": \"boolean\", \"description\": \"주요 채널 여부\"},\n",
        "                \"availability\": {\"type\": \"string\", \"description\": \"채널 이용 가능 시간/조건\"},\n",
        "                \"benefit\": {\"type\": \"string\", \"description\": \"해당 채널 이용 시 혜택\"}\n",
        "            },\n",
        "            \"required\": [\"type\", \"value\"]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"metadata\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message_type\": {\"type\": \"string\"},\n",
        "            \"target_response\": {\"type\": \"string\"},\n",
        "            \"success_metrics\": {\"type\": \"array\"}\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"required\": [\"purpose\", \"target\", \"product\", \"channel\",\"metadata\"],\n",
        "    \"objectType\": \"object\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc181efc-5cea-4129-9261-ae350ac58bf6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_llm_result_by_fsl(msg_text, rag_context, few_shot_temp_temp, res_dic_empt, row, fs_model, num_fs=1):\n",
        "    # Get top 5 examples\n",
        "    few_shot_exm = []\n",
        "    top_examples = few_shot_temp_temp[\n",
        "        (few_shot_temp_temp['rank'] >= 1) & \n",
        "        (few_shot_temp_temp['rank'] <= num_fs)\n",
        "    ].sort_values(\"rank\")\n",
        "\n",
        "    for _, r in top_examples.iterrows():\n",
        "        few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지]\n",
        "광고 제목:{r['msg_head']}\n",
        "광고 내용:{r['msg_body']}\n",
        "\n",
        "[학습용 정답 결과]\n",
        "{r[f'res_{fs_model}']}\n",
        "\n",
        "        \"\"\")\n",
        "        row[f\"\"\"fsl_head_{int(r['rank'])}\"\"\"] = r['msg_head']\n",
        "        row[f\"\"\"fsl_body_{int(r['rank'])}\"\"\"] = r['msg_body']\n",
        "\n",
        "    few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "\n",
        "    # print(few_shot_exm_str)\n",
        "\n",
        "    try:\n",
        "        system_message_ax = f\"위 예시들을 참고하여, 다음과 같은 schema에 부합하도록 JSON 개체만을 만들어 주세요.:\\n {schema_ax}\"\n",
        "        prompt = (\n",
        "\"광고 메세지에서 정보들을 추출하고자 한다. 아래에 몇 가지 예시들이 있다.\"\n",
        "\"\\n\\n=====학습용 예시=====\\n\"+few_shot_exm_str+\n",
        "\"\\n\\n\"+system_message_ax+\n",
        "\"\\n\\n=====분석 대상 광고 메세지=====\\n\"+msg_text\n",
        "+\"\\n=====후보 상품 이름=====\\n\"+rag_context\n",
        "                )\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "            messages=[\n",
        "                # {\"role\": \"system\", \"content\": system_message_ax},\n",
        "                {\"role\": \"user\", \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        result_raw = response.choices[0].message.content\n",
        "        result = extract_json_objects(result_raw)[0]\n",
        "        row[f'res_ax_f{num_fs}'] = result\n",
        "    except Exception as e:\n",
        "        print(f\"Error message when calling ax_f{num_fs}: \\n{result_raw}\")\n",
        "        row[f'res_ax_f{num_fs}'] = res_dic_empt#.replace('\\n','')\n",
        "        \n",
        "    return row\n",
        "\n",
        "def get_llm_result_by_fsl_exp(msg_text, rag_context, few_shot_temp_temp, res_dic_empt, row, fs_model, num_fs=1):\n",
        "    # Get top 5 examples\n",
        "    few_shot_exm = []\n",
        "    top_examples = few_shot_temp_temp[\n",
        "        (few_shot_temp_temp['rank'] >= 1) & \n",
        "        (few_shot_temp_temp['rank'] <= num_fs)\n",
        "    ].sort_values(\"rank\")\n",
        "\n",
        "    for _, r in top_examples.iterrows():\n",
        "        few_shot_exm.append(f\"\"\"\n",
        "[학습용 광고 메세지]\n",
        "광고 제목:{r['msg_head']}\n",
        "광고 내용:{r['msg_body']}\n",
        "\n",
        "[학습용 정답 결과]\n",
        "{r[f'res_{fs_model}']}\n",
        "\n",
        "[학습용 Chain-of-Thought]\n",
        "{r[f'cot_{fs_model}']}\n",
        "        \"\"\")\n",
        "        row[f\"\"\"fsl_head_{int(r['rank'])}\"\"\"] = r['msg_head']\n",
        "        row[f\"\"\"fsl_body_{int(r['rank'])}\"\"\"] = r['msg_body']\n",
        "\n",
        "    few_shot_exm_str = \"\\n\".join(few_shot_exm)\n",
        "\n",
        "    # print(few_shot_exm_str)\n",
        "\n",
        "    try:\n",
        "        system_message_ax = f\"위 예시들을 참고하여, 다음과 같은 schema에 부합하도록 JSON 개체만을 만들어 주세요.:\\n {schema_ax}\"\n",
        "        prompt = (\n",
        "\"광고 메세지에서 정보들을 추출하고자 한다. 아래에 몇 가지 예시들과 Chain-of-Thought가 있다.\"\n",
        "\"\\n\\n=====학습용 예시=====\\n\"+few_shot_exm_str+\n",
        "\"\\n\\n\"+system_message_ax+\n",
        "\"\\n\\n=====분석 대상 광고 메세지=====\\n\"+msg_text\n",
        "+\"\\n=====후보 상품 이름=====\\n\"+rag_context\n",
        "                )\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "            messages=[\n",
        "                # {\"role\": \"system\", \"content\": system_message_ax},\n",
        "                {\"role\": \"user\", \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        result_raw = response.choices[0].message.content\n",
        "        result = extract_json_objects(result_raw)[0]\n",
        "        row[f'res_ax_f{num_fs}_x'] = result\n",
        "    except Exception as e:\n",
        "        print(f\"Error message when calling ax_f{num_fs}_x: \\n{result_raw}\")\n",
        "        row[f'res_ax_f{num_fs}'] = res_dic_empt#.replace('\\n','')\n",
        "        \n",
        "    return row\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334e96f5-45d0-48b8-9b98-1dca5b6bc91f",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "build_few_shot = True\n",
        "\n",
        "ref_model = \"cld35\"\n",
        "\n",
        "if ref_model == \"cld35\":\n",
        "    llm_modl = llm_cld35\n",
        "elif ref_model == \"cld37\":\n",
        "    llm_modl = llm_cld37\n",
        "else:\n",
        "    llm_modl = llm_chat\n",
        "\n",
        "n_sample_fsl = 500\n",
        "n_fsl_sta = 1\n",
        "n_fsl_end = 1\n",
        "\n",
        "# Create the parser\n",
        "parser = JsonOutputParser()\n",
        "extraction_chain = prompt_cld | llm_modl | parser\n",
        "extraction_chain_cld35 = prompt_cld | llm_cld35 | parser\n",
        "\n",
        "# few_shot_pdf = pd.read_pickle(f\"/home/skinet/myfiles/tos_ace/data/mms_extraction_data_cld_fsl_250404_rd_{n_sample_fsl}.pkl\").iloc[:100]\n",
        "\n",
        "rep_dict = {'[SK텔레콤]':'','[SKT]':''}\n",
        "special_symbols_to_remove = {'‘':'','’':'',':':'',\"'\":'','\"':'','_':'\\n'}\n",
        "\n",
        "def process_prompt(row, few_shot_pdf, ref_model):\n",
        "    res_dic_empt = {\n",
        "        \"purpose\": \"\",\n",
        "        \"target\": \"\",\n",
        "        \"product\": \"\",\n",
        "        \"channel\": \"\"\n",
        "    }\n",
        "    \n",
        "    head = row['head']\n",
        "    body = row['body']\n",
        "    \n",
        "    head = replace_strings(head, special_symbols_to_remove)\n",
        "    body = replace_strings(body, special_symbols_to_remove)\n",
        "            \n",
        "#     for k,v in special_symbols_to_remove.items():\n",
        "#         head = head.replace(k,v)\n",
        "#         body = body.replace(k,v)\n",
        "    \n",
        "    msg_text = f\"\"\"\n",
        "        광고 제목: {head}\n",
        "        광고 내용: {body}\n",
        "    \"\"\"\n",
        "        \n",
        "#     rag_context = get_relevant_context(text, vectorstore, topk=20)['title']\n",
        "    \n",
        "#     row['res_rag'] = rag_context\n",
        "\n",
        "    matches = find_entities_in_text(\n",
        "        msg_text, \n",
        "        entity_list, \n",
        "        min_similarity=70,  # Lower threshold for more matches\n",
        "        ngram_size=3,       # Bigrams work well for Korean\n",
        "        min_entity_length=3 # Only consider entities with 3+ characters\n",
        "    )\n",
        "    \n",
        "    rag_context = \", \".join(set([match['text'] for match in matches])) # matched_entity, text\n",
        "    \n",
        "    if not build_few_shot:\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(\n",
        "            tfidf_desc.transform([msg_text]),\n",
        "            tfidf_matrix_desc\n",
        "        )[0]\n",
        "        \n",
        "        # Assign similarities to dataframe\n",
        "        few_shot_temp = few_shot_pdf.copy()  # Create a copy to avoid modifying original\n",
        "        few_shot_temp['sim'] = similarities\n",
        "        \n",
        "        few_shot_temp_temp = few_shot_temp.query(f\"offer_dt<'{row['offer_dt']}'\").copy()\n",
        "        few_shot_temp_temp['rank'] = few_shot_temp_temp[\"sim\"].rank(method='min', ascending=False)\n",
        "        \n",
        "        for nf in range(n_fsl_sta, n_fsl_end+1):\n",
        "            row = get_llm_result_by_fsl(msg_text, rag_context, few_shot_temp_temp, res_dic_empt, row, ref_model, num_fs=nf)\n",
        "\n",
        "        # for nf in range(n_fsl_sta, n_fsl_end+1):\n",
        "        #     row = get_llm_result_by_fsl_exp(msg_text, rag_context, few_shot_temp_temp, res_dic_empt, row, ref_model, num_fs=nf)\n",
        "\n",
        "\n",
        "        # try:\n",
        "\n",
        "        #     response = client.chat.completions.create(\n",
        "        #         model=\"skt/a.x-3-lg\",  # or your specific model\n",
        "        #         messages=[\n",
        "        #             {\"role\": \"system\", \"content\": f\"당신은 캠페인 메시지에서 정보를 추출하는 전문가입니다. 다음과 같은 schema를 참고하여 JSON 개체를 만들어 주세요:\\n {schema_ax}\"},\n",
        "        #             {\"role\": \"user\", \"content\": msg_text\n",
        "        #              +\"\\n---후보 상품 이름----\\n\"+rag_context\n",
        "        #             }\n",
        "        #         ],\n",
        "        #         temperature=0\n",
        "        #     )\n",
        "            \n",
        "        #     result_raw = response.choices[0].message.content\n",
        "        #     result = extract_json_objects(result_raw)[0]\n",
        "        #     # result = json.dumps(result, indent=4, ensure_ascii=False)\n",
        "        #     row['res_ax_f0'] = result\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error message when calling ax_n: \\n{result_raw}\")\n",
        "        #     row['res_ax_f0'] = res_dic_empt#.replace('\\n','')\n",
        "    \n",
        "    try:\n",
        "        result = extraction_chain.invoke({\"text\": msg_text, \"schema\": str(schema_cld)\n",
        "                                          # , \"rag_context\":rag_context\n",
        "                                         })\n",
        "        # result = json.dumps(result, indent=4, ensure_ascii=False)\n",
        "        row[f'res_{ref_model}'] = result\n",
        "        \n",
        "    except Exception as e:\n",
        "        try:\n",
        "            result = extract_json_objects(e)[0]\n",
        "            row[f'res_{ref_model}'] = result\n",
        "        except Exception as e2:\n",
        "            print(f\"Error message when calling {ref_model}: \\n{e2}\")\n",
        "            row[f'res_{ref_model}'] = res_dic_empt\n",
        "\n",
        "    # try:\n",
        "    #     result = extraction_chain_cld35.invoke({\"text\": msg_text, \"schema\": str(schema_cld)\n",
        "    #                                       # , \"rag_context\":rag_context\n",
        "    #                                      })\n",
        "    #     # result = json.dumps(result, indent=4, ensure_ascii=False)\n",
        "    #     row[f'res_cld35'] = result\n",
        "        \n",
        "    # except Exception as e:\n",
        "    #     try:\n",
        "    #         result = extract_json_objects(e)[0]\n",
        "    #         row[f'res_cld35'] = result\n",
        "    #     except Exception as e2:\n",
        "    #         print(f\"Error message when calling cld35: \\n{e2}\")\n",
        "    #         row[f'res_cld35'] = res_dic_empt\n",
        "        \n",
        "                        \n",
        "    return row\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "import random\n",
        "from functools import partial\n",
        "\n",
        "test_pdf_list = []\n",
        "# mdf = mms_pdf.rename(columns={'msg_nm':'head','mms_phrs':'body'}).to_dict(\"records\")\n",
        "\n",
        "if build_few_shot:\n",
        "    mdf = mms_pdf.rename(columns={'msg_nm':'head','mms_phrs':'body'}).sample(n=n_sample_fsl).to_dict(\"records\")\n",
        "else:\n",
        "    mdf = mms_pdf.query(\"offer_dt>='20240101'\").merge(few_shot_pdf[['index',f'res_{ref_model}']],on='index',how='left').query(f\"res_{ref_model}.isna()\").drop(f\"res_{ref_model}\",axis=1).rename(columns={'msg_nm':'head','mms_phrs':'body'})\n",
        "    mdf = mdf.sample(n=100).to_dict(\"records\")\n",
        "\n",
        "# Create a partial function with few_shot_pdf\n",
        "process_prompt_with_pdf = partial(process_prompt, few_shot_pdf=few_shot_pdf, ref_model=ref_model)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    test_pdf_list += list(executor.map(process_prompt_with_pdf, mdf))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution took {elapsed_time:.4f} seconds\")\n",
        "    \n",
        "# test_pdf = pd.DataFrame(test_pdf_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9472a430-1e30-43c0-bf0d-144fe79240af",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 1000)\n",
        "\n",
        "if build_few_shot:\n",
        "    rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body'}).drop(['msg'], axis=1))\n",
        "    rdf.to_pickle(f\"./data/few_shot_data_chat_250409_rd_{n_sample_fsl}.pkl\")\n",
        "\n",
        "rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body',f'res_ax_f{n_fsl_end}':'res_ax'}).drop(['msg'], axis=1))\n",
        "rdf['msg_body'] = rdf['msg_body'].apply(remove_control_characters)\n",
        "rdf = rdf[(rdf[f'res_ax'].apply(lambda x: x.get('purpose') != '')) & (rdf[f\"res_{ref_model}\"].apply(lambda x: x.get('purpose') != ''))]\n",
        "rdf['res_ax'] = rdf['res_ax'].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "rdf['res_cld'] = rdf['res_cld35'].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "rdf[f\"res_{ref_model}\"] = rdf[f\"res_{ref_model}\"].apply(lambda x:json.dumps(x, indent=4, ensure_ascii=False))\n",
        "# rdf['res_ax'] = rdf['res_ax'].apply(lambda x: json_to_custom_string(x))\n",
        "# rdf[f\"res_{ref_model}\"] = rdf[f\"res_{ref_model}\"].apply(lambda x: json_to_custom_string(x))\n",
        "rdf[['offer_dt','msg_head','msg_body','res_ax','res_cld',f\"res_{ref_model}\"]].sort_values('offer_dt').to_excel(\"./data/mms_extraction_data_250410.xlsx\", index=False, engine='openpyxl')\n",
        "\n",
        "\n",
        "\n",
        "# row = rdf.to_dict(\"records\")[0]\n",
        "    \n",
        "# rdf['res_ax'].apply(print)\n",
        "\n",
        "# rdf[rdf[f'res_ax_f{n_fsl_end}'].apply(lambda x: x.get('purpose') == '')].shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "81db0618-52f0-4508-b1fa-7cc990209ff9",
      "metadata": {
        "tags": []
      },
      "source": [
        "import re\n",
        "\n",
        "tdf = few_shot_pdf.copy()\n",
        "\n",
        "# Function to remove control characters\n",
        "def remove_control_chars(s):\n",
        "    if isinstance(s, str):\n",
        "        return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', s)\n",
        "    return s\n",
        "\n",
        "# Apply to all object columns\n",
        "for col in tdf.select_dtypes(include=['object']).columns:\n",
        "    tdf[col] = tdf[col].apply(remove_control_chars)\n",
        "\n",
        "# tdf.to_excel(\"/home/skinet/myfiles/tos_ace/data/few_shot_data_250408.xlsx\", index=False, encoding='cp949', engine='openpyxl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99908dfe-e5e3-4b17-8b03-6c3a4e2b985c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "rdf = (pd.DataFrame(test_pdf_list).rename(columns={'head':'msg_head','body':'msg_body'}).drop(['msg'], axis=1))\n",
        "\n",
        "for nf in (list(range(n_fsl_sta, n_fsl_end+1))):\n",
        "    rdf = rdf[(rdf[f'res_ax_f{nf}'].apply(lambda x: x.get('purpose') != ''))]\n",
        "\n",
        "    # rdf = rdf[(rdf['res_ax_f'].apply(lambda x: x.get('purpose') != '')) & (rdf['res_ax_n'].apply(lambda x: x.get('purpose') != ''))]\n",
        "sdf = rdf.copy()\n",
        "# # sdf = pd.read_pickle(\"/home/skinet/myfiles/tos_ace/data/mms_extraction_data_vl_250403_01.pkl\")\n",
        "\n",
        "for nf in (list(range(n_fsl_sta, n_fsl_end+1))):\n",
        "    sdf[f'sim_f{nf}'] = sdf.apply(lambda x : calculate_dictionary_similarity(x[f'res_ax_f{nf}'], x[f'res_{ref_model}']), axis=1)\n",
        "    # sdf[f'sim_f{nf}'] = sdf.apply(lambda x : calculate_dictionary_similarity(x[f'res_ax_f{nf}'], x[f'res_cld35']), axis=1)\n",
        "    print(nf, \"\\n\", pd.json_normalize(sdf[f'sim_f{nf}']).mean())\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "cb4a7b40-83d0-4118-8aa0-8003b7860a42",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "1 \n",
        " overall_similarity         0.788906\n",
        "message_info_similarity    0.808221\n",
        "purpose_similarity         0.675500\n",
        "target_similarity          0.583016\n",
        "product_similarity         0.612335\n",
        "channel_similarity         0.731023\n",
        "meta_similarity            0.534437\n",
        "dtype: float64\n",
        "2 \n",
        " overall_similarity         0.821625\n",
        "message_info_similarity    0.828928\n",
        "purpose_similarity         0.692000\n",
        "target_similarity          0.624887\n",
        "product_similarity         0.655527\n",
        "channel_similarity         0.763915\n",
        "meta_similarity            0.542867\n",
        "dtype: float64\n",
        "3 \n",
        " overall_similarity         0.823776\n",
        "message_info_similarity    0.825154\n",
        "purpose_similarity         0.707167\n",
        "target_similarity          0.630087\n",
        "product_similarity         0.653435\n",
        "channel_similarity         0.764581\n",
        "meta_similarity            0.538457\n",
        "dtype: float64\n",
        "4 \n",
        " overall_similarity         0.836997\n",
        "message_info_similarity    0.838324\n",
        "purpose_similarity         0.720500\n",
        "target_similarity          0.631209\n",
        "product_similarity         0.675245\n",
        "channel_similarity         0.767200\n",
        "meta_similarity            0.552510\n",
        "dtype: float64\n",
        "5 \n",
        " overall_similarity         0.835741\n",
        "message_info_similarity    0.828243\n",
        "purpose_similarity         0.715333\n",
        "target_similarity          0.653488\n",
        "product_similarity         0.656675\n",
        "channel_similarity         0.768541\n",
        "meta_similarity            0.556425\n",
        "dtype: float64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd60c94-84f2-4848-80ab-32cd506db132",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def json_to_custom_string(json_obj, indent=0):\n",
        "    result = []\n",
        "    indent_str = \"  \" * indent\n",
        "    \n",
        "    if isinstance(json_obj, dict):\n",
        "        for key, value in json_obj.items():\n",
        "            if isinstance(value, (dict, list)):\n",
        "                result.append(f\"{indent_str}{key}:\")\n",
        "                result.append(json_to_custom_string(value, indent + 1))\n",
        "            else:\n",
        "                result.append(f\"{indent_str}{key}: {value}\")\n",
        "    elif isinstance(json_obj, list):\n",
        "        for item in json_obj:\n",
        "            if isinstance(item, (dict, list)):\n",
        "                result.append(json_to_custom_string(item, indent))\n",
        "            else:\n",
        "                result.append(f\"{indent_str}- {item}\")\n",
        "    \n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "\n",
        "# json_to_custom_string(parsed_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd996df-be9e-4908-99cc-252b5f99d69c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}